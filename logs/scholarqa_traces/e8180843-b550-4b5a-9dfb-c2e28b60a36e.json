{
    "query": "How do translation-based multilingual evaluation datasets compare in effectiveness and bias against datasets natively collected in target languages, particularly regarding their impact on the reliability of cross-lingual evaluations?",
    "user_id": "lib_user",
    "task_id": "e8180843-b550-4b5a-9dfb-c2e28b60a36e",
    "timestamp": "2025-06-23T22:15:07.839991",
    "n_retrieval": 256,
    "n_retrieved": 264,
    "n_candidates": 44,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.530064,
    "decomposed_query": {
        "rewritten_query": "How translation-based multilingual evaluation datasets compare in effectiveness and bias against datasets natively collected in target languages, particularly regarding their impact on the reliability of cross-lingual evaluations.",
        "keyword_query": "translation-based multilingual evaluation datasets effectiveness bias native target languages reliability cross-lingual evaluations",
        "search_filters": {
            "fieldsOfStudy": "Computer Science,Linguistics"
        },
        "cost": 0.01014,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 73,
            "citation_count": 28,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00539/2067889/tacl_a_00539.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2201.13405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46963731",
                    "name": "Olga Majewska"
                },
                {
                    "authorId": "66879943",
                    "name": "E. Razumovskaia"
                },
                {
                    "authorId": "3381663",
                    "name": "E. Ponti"
                },
                {
                    "authorId": "1747849",
                    "name": "Ivan Vulic"
                },
                {
                    "authorId": "145762466",
                    "name": "A. Korhonen"
                }
            ],
            "abstract": "Multilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets\u2014both for modular and end-to-end modeling\u2014suffer from severe limitations. 1) When created from scratch, they are usually small in scale and fail to cover many possible dialogue flows. 2) Translation-based ToD datasets might lack naturalness and cultural specificity in the target language. In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines. These in turn guide the target language annotators in writing dialogues by providing instructions about each turn\u2019s intents and slots. Through this process we annotate a new large-scale dataset for evaluation of multilingual and cross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset (cod) enables natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation in 4 diverse languages: Arabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative analyses of cod versus an equivalent translation-based dataset demonstrate improvements in data quality, unlocked by the outline-based approach. Finally, we benchmark a series of state-of-the-art systems for cross-lingual ToD, setting reference scores for future work and demonstrating that cod prevents over-inflated performance, typically met with prior translation-based ToD datasets.",
            "corpus_id": 246430787,
            "sentences": [
                {
                    "corpus_id": "246430787",
                    "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
                    "text": "For the experiment, we fine-tune mBERT (Devlin et al., 2019) on intent detection. 5 he results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation. \n\nAnalysis of Sentence Encodings. One reason behind the scores observed in based approach. To test this, we obtain sentence encodings of all user turns for one intent from the three datasets via the distilled multilingual USE sentence encoder (Yang et al., 2020;Reimers and Gurevych, 2019). 6 s illustrated in Figure 2, the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets. \n\nFurther Discussion. To meet the urgent, evergrowing demand for large-scale multilingual TOD datasets, data collection methods which efficiently leverage existing resources to generate new data fast without compromising data quality are especially needed. Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable.",
                    "score": 0.49658285694545257,
                    "section_title": "Translation versus Outline-Based",
                    "char_start_offset": 23460,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 83
                        },
                        {
                            "start": 84,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 380
                        },
                        {
                            "start": 381,
                            "end": 640
                        },
                        {
                            "start": 641,
                            "end": 768
                        },
                        {
                            "start": 771,
                            "end": 802
                        },
                        {
                            "start": 803,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 1061
                        },
                        {
                            "start": 1062,
                            "end": 1259
                        },
                        {
                            "start": 1260,
                            "end": 1382
                        },
                        {
                            "start": 1383,
                            "end": 1549
                        },
                        {
                            "start": 1550,
                            "end": 1781
                        },
                        {
                            "start": 1784,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 2038
                        },
                        {
                            "start": 2039,
                            "end": 2227
                        },
                        {
                            "start": 2228,
                            "end": 2320
                        },
                        {
                            "start": 2321,
                            "end": 2362
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 39,
                            "end": 60,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1012,
                            "end": 1031,
                            "matchedPaperCorpusId": "195848217"
                        },
                        {
                            "start": 1031,
                            "end": 1058,
                            "matchedPaperCorpusId": "201646309"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9873046875
                }
            ],
            "relevance_judgement": 0.9873046875,
            "relevance_judgment_input_expanded": "# Title: Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Olga Majewska, E. Razumovskaia, E. Ponti, Ivan Vulic, A. Korhonen\n## Abstract\nMultilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets\u2014both for modular and end-to-end modeling\u2014suffer from severe limitations. 1) When created from scratch, they are usually small in scale and fail to cover many possible dialogue flows. 2) Translation-based ToD datasets might lack naturalness and cultural specificity in the target language. In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines. These in turn guide the target language annotators in writing dialogues by providing instructions about each turn\u2019s intents and slots. Through this process we annotate a new large-scale dataset for evaluation of multilingual and cross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset (cod) enables natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation in 4 diverse languages: Arabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative analyses of cod versus an equivalent translation-based dataset demonstrate improvements in data quality, unlocked by the outline-based approach. Finally, we benchmark a series of state-of-the-art systems for cross-lingual ToD, setting reference scores for future work and demonstrating that cod prevents over-inflated performance, typically met with prior translation-based ToD datasets.\n## Translation versus Outline-Based\nFor the experiment, we fine-tune mBERT (Devlin et al., 2019) on intent detection. 5 he results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation. \n\nAnalysis of Sentence Encodings. One reason behind the scores observed in based approach. To test this, we obtain sentence encodings of all user turns for one intent from the three datasets via the distilled multilingual USE sentence encoder (Yang et al., 2020;Reimers and Gurevych, 2019). 6 s illustrated in Figure 2, the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets. \n\nFurther Discussion. To meet the urgent, evergrowing demand for large-scale multilingual TOD datasets, data collection methods which efficiently leverage existing resources to generate new data fast without compromising data quality are especially needed. Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable.",
            "reference_string": "[246430787 | Majewska et al. | 2022 | Citations: 28]"
        },
        {
            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 34,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283844788",
                    "name": "Shivalika Singh"
                },
                {
                    "authorId": "1910588458",
                    "name": "Angelika Romanou"
                },
                {
                    "authorId": "2080941785",
                    "name": "Cl\u00e9mentine Fourrier"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "119076558",
                    "name": "Jian Gang Ngui"
                },
                {
                    "authorId": "1403849795",
                    "name": "Daniel Vila-Suero"
                },
                {
                    "authorId": "1596821065",
                    "name": "Peerat Limkonchotiwat"
                },
                {
                    "authorId": "1396188646",
                    "name": "Kelly Marchisio"
                },
                {
                    "authorId": "2140097897",
                    "name": "Wei Qi Leong"
                },
                {
                    "authorId": "2239100505",
                    "name": "Yosephine Susanto"
                },
                {
                    "authorId": "2352918897",
                    "name": "Raymond Ng"
                },
                {
                    "authorId": "2283848744",
                    "name": "Shayne Longpre"
                },
                {
                    "authorId": "2309005865",
                    "name": "Wei-Yin Ko"
                },
                {
                    "authorId": "2303318993",
                    "name": "Madeline Smith"
                },
                {
                    "authorId": "2284866282",
                    "name": "Antoine Bosselut"
                },
                {
                    "authorId": "2320522443",
                    "name": "Alice Oh"
                },
                {
                    "authorId": "2334310309",
                    "name": "Andr\u00e9 F. T. Martins"
                },
                {
                    "authorId": "2283849613",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "2290769562",
                    "name": "Daphne Ippolito"
                },
                {
                    "authorId": "2268314228",
                    "name": "Enzo Ferrante"
                },
                {
                    "authorId": "2818759",
                    "name": "Marzieh Fadaee"
                },
                {
                    "authorId": "2445273",
                    "name": "B. Ermi\u015f"
                },
                {
                    "authorId": "2257040307",
                    "name": "Sara Hooker"
                }
            ],
            "abstract": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",
            "corpus_id": 274464561,
            "sentences": [
                {
                    "corpus_id": "274464561",
                    "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
                    "text": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",
                    "score": 0.6277499306688066,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9853515625
                }
            ],
            "relevance_judgement": 0.9853515625,
            "relevance_judgment_input_expanded": "# Title: Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation\n# Venue: arXiv.org\n# Authors: Shivalika Singh, Angelika Romanou, Cl\u00e9mentine Fourrier, David Ifeoluwa Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andr\u00e9 F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, B. Ermi\u015f, Sara Hooker\n## Abstract\nCultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.\n",
            "reference_string": "[274464561 | Singh et al. | 2024 | Citations: 34]"
        },
        {
            "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17163, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346978280",
                    "name": "Mar'ia Andrea Cruz Bland'on"
                },
                {
                    "authorId": "114938178",
                    "name": "Jayasimha Talur"
                },
                {
                    "authorId": "2346980693",
                    "name": "Bruno Charron"
                },
                {
                    "authorId": "2343588002",
                    "name": "Dong Liu"
                },
                {
                    "authorId": "39674628",
                    "name": "Saab Mansour"
                },
                {
                    "authorId": "2346980678",
                    "name": "Marcello Federico"
                }
            ],
            "abstract": "Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience. In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG",
            "corpus_id": 276575630,
            "sentences": [
                {
                    "corpus_id": "276575630",
                    "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
                    "text": "Multilingual metaevaluation is important to reliably measure performance across languages which can vary depending on language characteristics (low vs. high resource, complex morphology, etc) and scripts (Latin vs. non-Latin). Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993;Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks. \n\nTo bridge those gaps, we propose a native metaevaluation multilingual benchmark for RAG systems. Our benchmark is built on top of the popular MIRACL (Zhang et al., 2023) dataset3 that includes native questions across 18 languages and relevance judgements of retrieved passages for multilingual retrieval evaluation. We extend MIRACL by generating answers in five languages with a diverse set of LLMs, and collecting judgements on the faithfulness and relevance of the answers using native expert human annotators. For the latter, we devised a structured annotation process that achieved a high rate of inter-annotator agreement. To evaluate the benchmark and set reference baseline results for others to compare against, we run LLM-as-a-judge experiments with various prompting techniques and state-of-the-art LLMs. \n\nTo summarize, our main contributions are: \n\n\u2022 We built and publicly release the first (to the best of our knowledge) native multilingual meta-evaluation RAG benchmark. \u2022 We developed a rigorous flow chart-based annotation process to achieve high interannotator agreement rate for both faithfulness and relevance judgements. \u2022 We evaluated the quality of the benchmark on three multi-lingual meta-evaluation aspects: prompt selection, model selection, and finegrained analysis. \u2022 We establish reference baselines of multilingual automatic evaluators on our benchmark, showcasing performance improvements when using advanced prompting and LLMs.",
                    "score": 0.5245092851388375,
                    "section_title": "Introduction",
                    "char_start_offset": 1735,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 578
                        },
                        {
                            "start": 579,
                            "end": 697
                        },
                        {
                            "start": 700,
                            "end": 796
                        },
                        {
                            "start": 797,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1515
                        },
                        {
                            "start": 1518,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1841
                        },
                        {
                            "start": 1842,
                            "end": 1994
                        },
                        {
                            "start": 1995,
                            "end": 2160
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 394,
                            "end": 414,
                            "matchedPaperCorpusId": "57174748"
                        },
                        {
                            "start": 558,
                            "end": 577,
                            "matchedPaperCorpusId": "270562911"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97509765625
                }
            ],
            "relevance_judgement": 0.97509765625,
            "relevance_judgment_input_expanded": "# Title: MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Mar'ia Andrea Cruz Bland'on, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico\n## Abstract\nAutomatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience. In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG\n## Introduction\nMultilingual metaevaluation is important to reliably measure performance across languages which can vary depending on language characteristics (low vs. high resource, complex morphology, etc) and scripts (Latin vs. non-Latin). Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993;Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks. \n\nTo bridge those gaps, we propose a native metaevaluation multilingual benchmark for RAG systems. Our benchmark is built on top of the popular MIRACL (Zhang et al., 2023) dataset3 that includes native questions across 18 languages and relevance judgements of retrieved passages for multilingual retrieval evaluation. We extend MIRACL by generating answers in five languages with a diverse set of LLMs, and collecting judgements on the faithfulness and relevance of the answers using native expert human annotators. For the latter, we devised a structured annotation process that achieved a high rate of inter-annotator agreement. To evaluate the benchmark and set reference baseline results for others to compare against, we run LLM-as-a-judge experiments with various prompting techniques and state-of-the-art LLMs. \n\nTo summarize, our main contributions are: \n\n\u2022 We built and publicly release the first (to the best of our knowledge) native multilingual meta-evaluation RAG benchmark. \u2022 We developed a rigorous flow chart-based annotation process to achieve high interannotator agreement rate for both faithfulness and relevance judgements. \u2022 We evaluated the quality of the benchmark on three multi-lingual meta-evaluation aspects: prompt selection, model selection, and finegrained analysis. \u2022 We establish reference baselines of multilingual automatic evaluators on our benchmark, showcasing performance improvements when using advanced prompting and LLMs.",
            "reference_string": "[276575630 | Bland'on et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 46,
            "citation_count": 120,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "2064469779",
                    "name": "Gorka Labaka"
                },
                {
                    "authorId": "1733049",
                    "name": "Eneko Agirre"
                }
            ],
            "abstract": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",
            "corpus_id": 215548041,
            "sentences": [
                {
                    "corpus_id": "215548041",
                    "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                    "text": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018;Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. \n\nClosely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (TRANSLATE-TEST), machine translating the training set into each target language and training the models on their respective languages (TRANSLATE-TRAIN), or using English data to finetune a multilingual model that is then transferred to the rest of languages (ZERO-SHOT). \n\nThe dataset creation and transfer procedures described above result in a mixture of original, 1 human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: TRANSLATE-TEST systems are trained on original data and evaluated on machine translated test sets, ZERO-SHOT systems are trained on original data and evaluated on human translated test sets, and TRANSLATE-TRAIN systems are trained on machine translated data and evaluated on human translated test sets. \n\nDespite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI).",
                    "score": 0.5446236817260166,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 332
                        },
                        {
                            "start": 333,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1362
                        },
                        {
                            "start": 1365,
                            "end": 1545
                        },
                        {
                            "start": 1546,
                            "end": 1970
                        },
                        {
                            "start": 1973,
                            "end": 2101
                        },
                        {
                            "start": 2102,
                            "end": 2413
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 288,
                            "end": 310,
                            "matchedPaperCorpusId": "52271711"
                        },
                        {
                            "start": 310,
                            "end": 331,
                            "matchedPaperCorpusId": "204901567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9716796875
                },
                {
                    "corpus_id": "215548041",
                    "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                    "text": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
                    "score": 0.5967234818288341,
                    "section_title": "Discussion",
                    "char_start_offset": 24144,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 1089
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                }
            ],
            "relevance_judgement": 0.9716796875,
            "relevance_judgment_input_expanded": "# Title: Translation Artifacts in Cross-lingual Transfer Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mikel Artetxe, Gorka Labaka, Eneko Agirre\n## Abstract\nBoth human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.\n## Introduction\nWhile most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018;Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. \n\nClosely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (TRANSLATE-TEST), machine translating the training set into each target language and training the models on their respective languages (TRANSLATE-TRAIN), or using English data to finetune a multilingual model that is then transferred to the rest of languages (ZERO-SHOT). \n\nThe dataset creation and transfer procedures described above result in a mixture of original, 1 human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: TRANSLATE-TEST systems are trained on original data and evaluated on machine translated test sets, ZERO-SHOT systems are trained on original data and evaluated on human translated test sets, and TRANSLATE-TRAIN systems are trained on machine translated data and evaluated on human translated test sets. \n\nDespite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI).\n\n## Discussion\nFuture evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
            "reference_string": "[215548041 | Artetxe et al. | 2020 | Citations: 120]"
        },
        {
            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282542641",
                    "name": "Ashish Agrawal"
                },
                {
                    "authorId": "2187454108",
                    "name": "Barah Fazili"
                },
                {
                    "authorId": "144859542",
                    "name": "P. Jyothi"
                }
            ],
            "abstract": "Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.",
            "corpus_id": 267413041,
            "sentences": [
                {
                    "corpus_id": "267413041",
                    "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                    "text": "This work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
                    "score": 0.5037468411819546,
                    "section_title": "Conclusions",
                    "char_start_offset": 13897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 618
                        },
                        {
                            "start": 621,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1345
                        },
                        {
                            "start": 1348,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1639
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.962890625
                }
            ],
            "relevance_judgement": 0.962890625,
            "relevance_judgment_input_expanded": "# Title: Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Ashish Agrawal, Barah Fazili, P. Jyothi\n## Abstract\nPopular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.\n## Conclusions\nThis work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
            "reference_string": "[267413041 | Agrawal et al. | 2024 | Citations: 3]"
        },
        {
            "title": "NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 43,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.13692",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.13692, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1568852178",
                    "name": "Jannis Vamvas"
                },
                {
                    "authorId": "2082372",
                    "name": "Rico Sennrich"
                }
            ],
            "abstract": "Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.",
            "corpus_id": 248426926,
            "sentences": [
                {
                    "corpus_id": "248426926",
                    "title": "NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures",
                    "text": "Our analysis highlights theoretical and empirical properties of translation-based text similarity measures in a multilingual setting. Direct translation probability is the most straightforward measure (an empirical comparison of inference times is found in Appendix A). However, it treats inputs as target sequences, and we show that accuracy on paraphrase identification can be clearly improved by normalizing with reconstruction probability.\n\nPivot translation probability is advantageous especially when performing cross-lingual comparisons. Finally, translation cross-likelihood has the advantage that it achieves symmetry with a single translation direction, and that the input languages need not be specified. The latter property also has interesting consequences for reference-based evaluation: The metric is expected to ignore whether the generated text matches the language of the reference. This can be seen as a rigorous disentanglement of adequacy from fluency.\n\nIn comparison to baseline measures, translationbased measures are generally slower but show high accuracy on multilingual paraphrase identification, comparatively good reliability on reference-based evaluation of data-to-text generation, and little correlation to alternative measures. Our findings thus show the usefulness of NMT translation probabilities for similarity tasks that require high attention to detail.",
                    "score": 0.4933338448495384,
                    "section_title": "Conclusion",
                    "char_start_offset": 23826,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95947265625
                }
            ],
            "relevance_judgement": 0.95947265625,
            "relevance_judgment_input_expanded": "# Title: NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jannis Vamvas, Rico Sennrich\n## Abstract\nBeing able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library (available at https://github.com/ZurichNLP/nmtscore). Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.\n## Conclusion\nOur analysis highlights theoretical and empirical properties of translation-based text similarity measures in a multilingual setting. Direct translation probability is the most straightforward measure (an empirical comparison of inference times is found in Appendix A). However, it treats inputs as target sequences, and we show that accuracy on paraphrase identification can be clearly improved by normalizing with reconstruction probability.\n\nPivot translation probability is advantageous especially when performing cross-lingual comparisons. Finally, translation cross-likelihood has the advantage that it achieves symmetry with a single translation direction, and that the input languages need not be specified. The latter property also has interesting consequences for reference-based evaluation: The metric is expected to ignore whether the generated text matches the language of the reference. This can be seen as a rigorous disentanglement of adequacy from fluency.\n\nIn comparison to baseline measures, translationbased measures are generally slower but show high accuracy on multilingual paraphrase identification, comparatively good reliability on reference-based evaluation of data-to-text generation, and little correlation to alternative measures. Our findings thus show the usefulness of NMT translation probabilities for similarity tasks that require high attention to detail.",
            "reference_string": "[248426926 | Vamvas et al. | 2022 | Citations: 12]"
        },
        {
            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
            "venue": "NLPPOWER",
            "year": 2022,
            "reference_count": 48,
            "citation_count": 16,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.06356",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2205.06356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "52154863",
                    "name": "Kabir Ahuja"
                },
                {
                    "authorId": "34725175",
                    "name": "Sandipan Dandapat"
                },
                {
                    "authorId": "3010457",
                    "name": "Sunayana Sitaram"
                },
                {
                    "authorId": "143990839",
                    "name": "M. Choudhury"
                }
            ],
            "abstract": "Although recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.",
            "corpus_id": 248780386,
            "sentences": [
                {
                    "corpus_id": "248780386",
                    "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
                    "text": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition. \n\nRecently, there has been some interest in predicting performance of NLP models without actually evaluating them on a test set. Xia et al. (2020) showed that it is possible to build regression models that can accurately predict evaluation scores of NLP models under different experimental settings using various linguistic and dataset specific features. Srinivasan et al. (2021)   (c) Number of multilingual tasks containing test data for each of the 106 languages supported by the MMLMs (mBERT, XLMR). The bars are shaded according to the class taxonomy proposed by Joshi et al. (2020). \n\nFigure 1 results specifically for MMLMs towards predicting their performance on downstream tasks for different languages in zero-shot and few-shot settings, and Ye et al. (2021) propose methods for more reliable performance prediction by estimating confidence intervals as well as predicting fine-grained performance measures. \n\nIn this paper we argue that the performance prediction can be a possible avenue to address the current issues with Multilingual benchmarking by aiding in the estimation of performance of the MMLMs for the languages which lack any evaluation data for a given task. Not only this can help us give a better idea about the performance of a multilingual model on a task across a much larger set of languages and hence aiding in better model selection, but also enables applications in devising data collection strategies to maximize performance (Srinivasan et al., 2022) as well as in selecting the representative set of languages for a benchmark (Xia et al., 2020).",
                    "score": 0.48540115525019834,
                    "section_title": "Introduction",
                    "char_start_offset": 1631,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 428
                        },
                        {
                            "start": 429,
                            "end": 592
                        },
                        {
                            "start": 593,
                            "end": 741
                        },
                        {
                            "start": 744,
                            "end": 870
                        },
                        {
                            "start": 871,
                            "end": 1096
                        },
                        {
                            "start": 1097,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1330
                        },
                        {
                            "start": 1333,
                            "end": 1659
                        },
                        {
                            "start": 1662,
                            "end": 1925
                        },
                        {
                            "start": 1926,
                            "end": 2323
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 871,
                            "end": 888,
                            "matchedPaperCorpusId": "218487089"
                        },
                        {
                            "start": 1310,
                            "end": 1329,
                            "matchedPaperCorpusId": "215828350"
                        },
                        {
                            "start": 1494,
                            "end": 1510,
                            "matchedPaperCorpusId": "231861672"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95751953125
                }
            ],
            "relevance_judgement": 0.95751953125,
            "relevance_judgment_input_expanded": "# Title: Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages\n# Venue: NLPPOWER\n# Authors: Kabir Ahuja, Sandipan Dandapat, Sunayana Sitaram, M. Choudhury\n## Abstract\nAlthough recent Massively Multilingual Language Models (MMLMs) like mBERT and XLMR support around 100 languages, most existing multilingual NLP benchmarks provide evaluation data in only a handful of these languages with little linguistic diversity. We argue that this makes the existing practices in multilingual evaluation unreliable and does not provide a full picture of the performance of MMLMs across the linguistic landscape. We propose that the recent work done in Performance Prediction for NLP tasks can serve as a potential solution in fixing benchmarking in Multilingual NLP by utilizing features related to data and language typology to estimate the performance of an MMLM on different languages. We compare performance prediction with translating test data with a case study on four different multilingual datasets, and observe that these methods can provide reliable estimates of the performance that are often on-par with the translation based approaches, without the need for any additional translation as well as evaluation costs.\n## Introduction\nMachine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition. \n\nRecently, there has been some interest in predicting performance of NLP models without actually evaluating them on a test set. Xia et al. (2020) showed that it is possible to build regression models that can accurately predict evaluation scores of NLP models under different experimental settings using various linguistic and dataset specific features. Srinivasan et al. (2021)   (c) Number of multilingual tasks containing test data for each of the 106 languages supported by the MMLMs (mBERT, XLMR). The bars are shaded according to the class taxonomy proposed by Joshi et al. (2020). \n\nFigure 1 results specifically for MMLMs towards predicting their performance on downstream tasks for different languages in zero-shot and few-shot settings, and Ye et al. (2021) propose methods for more reliable performance prediction by estimating confidence intervals as well as predicting fine-grained performance measures. \n\nIn this paper we argue that the performance prediction can be a possible avenue to address the current issues with Multilingual benchmarking by aiding in the estimation of performance of the MMLMs for the languages which lack any evaluation data for a given task. Not only this can help us give a better idea about the performance of a multilingual model on a task across a much larger set of languages and hence aiding in better model selection, but also enables applications in devising data collection strategies to maximize performance (Srinivasan et al., 2022) as well as in selecting the representative set of languages for a benchmark (Xia et al., 2020).",
            "reference_string": "[248780386 | Ahuja et al. | 2022 | Citations: 16]"
        },
        {
            "title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2020,
            "reference_count": 61,
            "citation_count": 108,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.627.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.06295, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46959445",
                    "name": "Hao Fei"
                },
                {
                    "authorId": "2678094",
                    "name": "Meishan Zhang"
                },
                {
                    "authorId": "145628086",
                    "name": "Donghong Ji"
                }
            ],
            "abstract": "Many efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.",
            "corpus_id": 215754572,
            "sentences": [
                {
                    "corpus_id": "215754572",
                    "title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus",
                    "text": "We first conduct experiments on cross-lingual transfer from the English source to the rest of the other six target languages, respectively, which has been a typical setting for cross-lingual investigations (Wang et al., 2019). The results are summarized in Table 2. We list the F-scores by using only the source corpus (SRC), only the translated target corpus (TGT) and the mixture corpus of source and target (SRC & TGT), comparing the performances of different multilingual word representations as well as different multilingual SRL models. \n\nMultilingual word representations. First, we evaluate the effectiveness of the three different multilingual word representations exploited. We compare their performances under two settings, by using SRC and TGT corpus, respectively. According to the results, we find that the multilingual contextualized word representations (i.e. BERT and ELMo) are better in both two settings, which is consistent with previous studies (Mulcaire et al., 2019;Schuster et al., 2019). Interestingly, the multilingual BERT performs worse than the ELMo, which can be explained by that the ELMo representation is pre-trained based on the corpus which involves in the focused seven languages. This indicates that the official released multilingual BERT can be further improved, since monolingual BERT has been demonstrated to produce better performances than ELMo (Tenney et al., 2019). \n\nTranslated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann and Agic, 2016;Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model. \n\nCombining source and pseudo target.",
                    "score": 0.49206153668581004,
                    "section_title": "Cross-Lingual Transfer from English",
                    "char_start_offset": 17154,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 265
                        },
                        {
                            "start": 266,
                            "end": 542
                        },
                        {
                            "start": 545,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 684
                        },
                        {
                            "start": 685,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 875
                        },
                        {
                            "start": 876,
                            "end": 1012
                        },
                        {
                            "start": 1013,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1410
                        },
                        {
                            "start": 1413,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1557
                        },
                        {
                            "start": 1558,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 1993
                        },
                        {
                            "start": 1994,
                            "end": 2241
                        },
                        {
                            "start": 2244,
                            "end": 2279
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 206,
                            "end": 225,
                            "matchedPaperCorpusId": "202578048"
                        },
                        {
                            "start": 989,
                            "end": 1011,
                            "matchedPaperCorpusId": "67856005"
                        },
                        {
                            "start": 1388,
                            "end": 1409,
                            "matchedPaperCorpusId": "108300988"
                        },
                        {
                            "start": 1947,
                            "end": 1973,
                            "matchedPaperCorpusId": "7805664"
                        },
                        {
                            "start": 1973,
                            "end": 1992,
                            "matchedPaperCorpusId": "202541341"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94775390625
                }
            ],
            "relevance_judgement": 0.94775390625,
            "relevance_judgment_input_expanded": "# Title: Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Hao Fei, Meishan Zhang, Donghong Ji\n## Abstract\nMany efforts of research are devoted to semantic role labeling (SRL) which is crucial for natural language understanding. Supervised approaches have achieved impressing performances when large-scale corpora are available for resource-rich languages such as English. While for the low-resource languages with no annotated SRL dataset, it is still challenging to obtain competitive performances. Cross-lingual SRL is one promising way to address the problem, which has achieved great advances with the help of model transferring and annotation projection. In this paper, we propose a novel alternative based on corpus translation, constructing high-quality training datasets for the target languages from the source gold-standard SRL annotations. Experimental results on Universal Proposition Bank show that the translation-based method is highly effective, and the automatic pseudo datasets can improve the target-language SRL performances significantly.\n## Cross-Lingual Transfer from English\nWe first conduct experiments on cross-lingual transfer from the English source to the rest of the other six target languages, respectively, which has been a typical setting for cross-lingual investigations (Wang et al., 2019). The results are summarized in Table 2. We list the F-scores by using only the source corpus (SRC), only the translated target corpus (TGT) and the mixture corpus of source and target (SRC & TGT), comparing the performances of different multilingual word representations as well as different multilingual SRL models. \n\nMultilingual word representations. First, we evaluate the effectiveness of the three different multilingual word representations exploited. We compare their performances under two settings, by using SRC and TGT corpus, respectively. According to the results, we find that the multilingual contextualized word representations (i.e. BERT and ELMo) are better in both two settings, which is consistent with previous studies (Mulcaire et al., 2019;Schuster et al., 2019). Interestingly, the multilingual BERT performs worse than the ELMo, which can be explained by that the ELMo representation is pre-trained based on the corpus which involves in the focused seven languages. This indicates that the official released multilingual BERT can be further improved, since monolingual BERT has been demonstrated to produce better performances than ELMo (Tenney et al., 2019). \n\nTranslated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann and Agic, 2016;Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model. \n\nCombining source and pseudo target.",
            "reference_string": "[215754572 | Fei et al. | 2020 | Citations: 108]"
        },
        {
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2226458991",
                    "name": "Julen Etxaniz"
                },
                {
                    "authorId": "2481918",
                    "name": "Gorka Azkune"
                },
                {
                    "authorId": "2260104163",
                    "name": "A. Soroa"
                },
                {
                    "authorId": "2251043402",
                    "name": "Oier L\u00f3pez de Lacalle"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                }
            ],
            "abstract": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.",
            "corpus_id": 270380088,
            "sentences": [
                {
                    "corpus_id": "270380088",
                    "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                    "text": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
                    "score": 0.53215044572944,
                    "section_title": "Related Work",
                    "char_start_offset": 20594,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 569
                        },
                        {
                            "start": 572,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1520
                        },
                        {
                            "start": 1523,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1998
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1248,
                            "end": 1268,
                            "matchedPaperCorpusId": "258685666"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94482421875
                }
            ],
            "relevance_judgement": 0.94482421875,
            "relevance_judgment_input_expanded": "# Title: BertaQA: How Much Do Language Models Know About Local Culture?\n# Venue: Neural Information Processing Systems\n# Authors: Julen Etxaniz, Gorka Azkune, A. Soroa, Oier L\u00f3pez de Lacalle, Mikel Artetxe\n## Abstract\nLarge Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.\n## Related Work\nResearch in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
            "reference_string": "[270380088 | Etxaniz et al. | 2024 | Citations: 11]"
        },
        {
            "title": "A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325906821",
                    "name": "Aryan Singhal"
                },
                {
                    "authorId": "2325907453",
                    "name": "Veronica Shao"
                },
                {
                    "authorId": "2325946974",
                    "name": "Gary Sun"
                },
                {
                    "authorId": "2325908566",
                    "name": "Ryan Ding"
                },
                {
                    "authorId": "2325198612",
                    "name": "Jonathan Lu"
                },
                {
                    "authorId": "2325205446",
                    "name": "Kevin Zhu"
                }
            ],
            "abstract": "The rise of digital misinformation has heightened interest in using multilingual Large Language Models (LLMs) for fact-checking. This study systematically evaluates translation bias and the effectiveness of LLMs for cross-lingual claim verification across 15 languages from five language families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT dataset to assess their impact on accuracy and bias, we investigate two distinct translation methods: pre-translation and self-translation. We use mBERT's performance on the English dataset as a baseline to compare language-specific accuracies. Our findings reveal that low-resource languages exhibit significantly lower accuracy in direct inference due to underrepresentation in the training data. Furthermore, larger models demonstrate superior performance in self-translation, improving translation accuracy and reducing bias. These results highlight the need for balanced multilingual training, especially in low-resource languages, to promote equitable access to reliable fact-checking tools and minimize the risk of spreading misinformation in different linguistic contexts.",
            "corpus_id": 273346089,
            "sentences": [
                {
                    "corpus_id": "273346089",
                    "title": "A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification",
                    "text": "The rise of digital misinformation has heightened interest in using multilingual Large Language Models (LLMs) for fact-checking. This study systematically evaluates translation bias and the effectiveness of LLMs for cross-lingual claim verification across 15 languages from five language families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT dataset to assess their impact on accuracy and bias, we investigate two distinct translation methods: pre-translation and self-translation. We use mBERT's performance on the English dataset as a baseline to compare language-specific accuracies. Our findings reveal that low-resource languages exhibit significantly lower accuracy in direct inference due to underrepresentation in the training data. Furthermore, larger models demonstrate superior performance in self-translation, improving translation accuracy and reducing bias. These results highlight the need for balanced multilingual training, especially in low-resource languages, to promote equitable access to reliable fact-checking tools and minimize the risk of spreading misinformation in different linguistic contexts.",
                    "score": 0.48203234401241296,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9423828125
                }
            ],
            "relevance_judgement": 0.9423828125,
            "relevance_judgment_input_expanded": "# Title: A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification\n# Venue: arXiv.org\n# Authors: Aryan Singhal, Veronica Shao, Gary Sun, Ryan Ding, Jonathan Lu, Kevin Zhu\n## Abstract\nThe rise of digital misinformation has heightened interest in using multilingual Large Language Models (LLMs) for fact-checking. This study systematically evaluates translation bias and the effectiveness of LLMs for cross-lingual claim verification across 15 languages from five language families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT dataset to assess their impact on accuracy and bias, we investigate two distinct translation methods: pre-translation and self-translation. We use mBERT's performance on the English dataset as a baseline to compare language-specific accuracies. Our findings reveal that low-resource languages exhibit significantly lower accuracy in direct inference due to underrepresentation in the training data. Furthermore, larger models demonstrate superior performance in self-translation, improving translation accuracy and reducing bias. These results highlight the need for balanced multilingual training, especially in low-resource languages, to promote equitable access to reliable fact-checking tools and minimize the risk of spreading misinformation in different linguistic contexts.\n",
            "reference_string": "[273346089 | Singhal et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08928, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2269539",
                    "name": "Klaudia Thellmann"
                },
                {
                    "authorId": "2325726089",
                    "name": "Bernhard Stadler"
                },
                {
                    "authorId": "2258551638",
                    "name": "Michael Fromm"
                },
                {
                    "authorId": "2258550838",
                    "name": "Jasper Schulze Buschhoff"
                },
                {
                    "authorId": "2325726070",
                    "name": "Alex Jude"
                },
                {
                    "authorId": "2325726142",
                    "name": "Fabio Barth"
                },
                {
                    "authorId": "2258552277",
                    "name": "Johannes Leveling"
                },
                {
                    "authorId": "2347259072",
                    "name": "Nicolas Flores-Herr"
                },
                {
                    "authorId": "2330411614",
                    "name": "Joachim K\u00f6hler"
                },
                {
                    "authorId": "2330412312",
                    "name": "Ren\u00e9 J\u00e4kel"
                },
                {
                    "authorId": "2258668067",
                    "name": "Mehdi Ali"
                }
            ],
            "abstract": "The rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of language-parallel multilingual benchmarks. We introduce a multilingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.",
            "corpus_id": 273323209,
            "sentences": [
                {
                    "corpus_id": "273323209",
                    "title": "Towards Multilingual LLM Evaluation for European Languages",
                    "text": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.",
                    "score": 0.5044232729239992,
                    "section_title": "Related Work",
                    "char_start_offset": 2745,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 601
                        },
                        {
                            "start": 602,
                            "end": 787
                        },
                        {
                            "start": 790,
                            "end": 1008
                        },
                        {
                            "start": 1009,
                            "end": 1074
                        },
                        {
                            "start": 1075,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1286
                        },
                        {
                            "start": 1287,
                            "end": 1460
                        },
                        {
                            "start": 1463,
                            "end": 1559
                        },
                        {
                            "start": 1560,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 1951
                        },
                        {
                            "start": 1954,
                            "end": 2079
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1266,
                            "end": 1285,
                            "matchedPaperCorpusId": "246680398"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9345703125
                }
            ],
            "relevance_judgement": 0.9345703125,
            "relevance_judgment_input_expanded": "# Title: Towards Multilingual LLM Evaluation for European Languages\n# Venue: arXiv.org\n# Authors: Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim K\u00f6hler, Ren\u00e9 J\u00e4kel, Mehdi Ali\n## Abstract\nThe rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of language-parallel multilingual benchmarks. We introduce a multilingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.\n## Related Work\nMultilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.",
            "reference_string": "[273323209 | Thellmann et al. | 2024 | Citations: 10]"
        },
        {
            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 79,
            "citation_count": 62,
            "influential_citation_count": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14857",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14857, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "35871436",
                    "name": "Sneha Kudugunta"
                },
                {
                    "authorId": "2118211280",
                    "name": "Xinyan Velocity Yu"
                },
                {
                    "authorId": "3443287",
                    "name": "Terra Blevins"
                },
                {
                    "authorId": "1821892",
                    "name": "Hila Gonen"
                },
                {
                    "authorId": "1557386977",
                    "name": "Machel Reid"
                },
                {
                    "authorId": "2073587169",
                    "name": "Yulia Tsvetkov"
                },
                {
                    "authorId": "2884561",
                    "name": "Sebastian Ruder"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ],
            "abstract": "Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.",
            "corpus_id": 258865558,
            "sentences": [
                {
                    "corpus_id": "258865558",
                    "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
                    "text": "We create the BUFFET benchmark to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. We adhere to the following design principles with our benchmark. \n\nStandardized few-shot samples. BUFFET provides three different training and validation sets of k-shots (e.g., k = 32) per task for a nonclassification task, or per class for a classification task, for each language. \n\nTask diversity. Existing cross-lingual benchmarks often focus on classification or retrieval (Hu et al., 2020b;Ruder et al., 2021;Liang et al., 2020). BUFFET encompasses a broad range of task types, such as classification, generation, extraction, and structured prediction tasks. By converting all tasks into the same text-to-text format, we eliminate the need for task-specific model modifications or template conversions. 5. \n\nBeyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;Ponti et al. 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;Artetxe et al., 2020b). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
                    "score": 0.5155937372006543,
                    "section_title": "Design Principles",
                    "char_start_offset": 9586,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 235
                        },
                        {
                            "start": 238,
                            "end": 268
                        },
                        {
                            "start": 269,
                            "end": 453
                        },
                        {
                            "start": 456,
                            "end": 471
                        },
                        {
                            "start": 472,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 879
                        },
                        {
                            "start": 880,
                            "end": 882
                        },
                        {
                            "start": 885,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1087
                        },
                        {
                            "start": 1088,
                            "end": 1241
                        },
                        {
                            "start": 1242,
                            "end": 1368
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 549,
                            "end": 567,
                            "matchedPaperCorpusId": "214641214"
                        },
                        {
                            "start": 567,
                            "end": 586,
                            "matchedPaperCorpusId": "233241004"
                        },
                        {
                            "start": 586,
                            "end": 605,
                            "matchedPaperCorpusId": "214794966"
                        },
                        {
                            "start": 1068,
                            "end": 1086,
                            "matchedPaperCorpusId": "218470125"
                        },
                        {
                            "start": 1218,
                            "end": 1240,
                            "matchedPaperCorpusId": "216914383"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9326171875
                }
            ],
            "relevance_judgement": 0.9326171875,
            "relevance_judgment_input_expanded": "# Title: BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, Hannaneh Hajishirzi\n## Abstract\nDespite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. Using BUFFET, we perform thorough evaluations of ten state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. Strong multilingual pre-trained or instruction-tuned models such as BLOOM or ChatGPT often lag behind much smaller mT5-base models given the same number of few-shot samples, particularly in low-resource languages. Our analysis suggests avenues for future research in few-shot cross-lingual transfer.\n## Design Principles\nWe create the BUFFET benchmark to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. We adhere to the following design principles with our benchmark. \n\nStandardized few-shot samples. BUFFET provides three different training and validation sets of k-shots (e.g., k = 32) per task for a nonclassification task, or per class for a classification task, for each language. \n\nTask diversity. Existing cross-lingual benchmarks often focus on classification or retrieval (Hu et al., 2020b;Ruder et al., 2021;Liang et al., 2020). BUFFET encompasses a broad range of task types, such as classification, generation, extraction, and structured prediction tasks. By converting all tasks into the same text-to-text format, we eliminate the need for task-specific model modifications or template conversions. 5. \n\nBeyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;Ponti et al. 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;Artetxe et al., 2020b). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
            "reference_string": "[258865558 | Asai et al. | 2023 | Citations: 62]"
        },
        {
            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 70,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.08964, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218735807",
                    "name": "Wen Yang"
                },
                {
                    "authorId": "2237788942",
                    "name": "Junhong Wu"
                },
                {
                    "authorId": "2188130997",
                    "name": "Chen Wang"
                },
                {
                    "authorId": "2064100826",
                    "name": "Chengqing Zong"
                },
                {
                    "authorId": "2303798059",
                    "name": "Jiajun Zhang"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited\"first-class\"languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose $\\textit{Language Imbalance Driven Rewarding}$, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs. The code is available at https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding",
            "corpus_id": 273323302,
            "sentences": [
                {
                    "corpus_id": "273323302",
                    "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                    "text": "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs. \n\nE GENERALIZING TO EXTREME SCENARIOS E.1 PERFORMANCE ON WEAKER MODEL: LLAMA2 \n\nTable 13 demonstrates that even when starting with a model with weaker multilingual capabilities, such as Llama2-7B-Chat, which exhibits extremely low performance in languages like Russian (ru), German (de), and French (fr) on the X-AlpacaEval, significant improvements can be achieved. By leveraging language imbalance-driven rewarding for self-multilingual optimization across two iterations, the model shows substantial enhancement across all training languages, particularly in those where the original model's performance was initially weaker. To assess the impact of our approach on these languages, we conducted experiments using Llama3-8b-Instruct as the base model. Table 14 shows that even though the model performs weakly in these languages, our approach remains effective in low-resource settings and can iteratively improve the model's performance across all languages.",
                    "score": 0.485626175555021,
                    "section_title": "D.3 HOW TO AVOID TRANSLATIONESE BIAS IN MULTILINGUAL BENCHMARKS EVALUATION",
                    "char_start_offset": 34821,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 359
                        },
                        {
                            "start": 360,
                            "end": 503
                        },
                        {
                            "start": 506,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 856
                        },
                        {
                            "start": 859,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1257
                        },
                        {
                            "start": 1260,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1808
                        },
                        {
                            "start": 1809,
                            "end": 1934
                        },
                        {
                            "start": 1935,
                            "end": 2142
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1101,
                            "end": 1127,
                            "matchedPaperCorpusId": "271244246"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.931640625
                }
            ],
            "relevance_judgement": 0.931640625,
            "relevance_judgment_input_expanded": "# Title: Language Imbalance Driven Rewarding for Multilingual Self-improving\n# Venue: International Conference on Learning Representations\n# Authors: Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang\n## Abstract\nLarge Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited\"first-class\"languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose $\\textit{Language Imbalance Driven Rewarding}$, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs. The code is available at https://github.com/ZNLP/Language-Imbalance-Driven-Rewarding\n## D.3 HOW TO AVOID TRANSLATIONESE BIAS IN MULTILINGUAL BENCHMARKS EVALUATION\nDue to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs. \n\nE GENERALIZING TO EXTREME SCENARIOS E.1 PERFORMANCE ON WEAKER MODEL: LLAMA2 \n\nTable 13 demonstrates that even when starting with a model with weaker multilingual capabilities, such as Llama2-7B-Chat, which exhibits extremely low performance in languages like Russian (ru), German (de), and French (fr) on the X-AlpacaEval, significant improvements can be achieved. By leveraging language imbalance-driven rewarding for self-multilingual optimization across two iterations, the model shows substantial enhancement across all training languages, particularly in those where the original model's performance was initially weaker. To assess the impact of our approach on these languages, we conducted experiments using Llama3-8b-Instruct as the base model. Table 14 shows that even though the model performs weakly in these languages, our approach remains effective in low-resource settings and can iteratively improve the model's performance across all languages.",
            "reference_string": "[273323302 | Yang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 81,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17532, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327246234",
                    "name": "Junhua Liu"
                },
                {
                    "authorId": "2325881142",
                    "name": "Bin Fu"
                }
            ],
            "abstract": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.",
            "corpus_id": 273532366,
            "sentences": [
                {
                    "corpus_id": "273532366",
                    "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                    "text": "To better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks. \n\nIn addition researchers also released multilingual dialogue datasets for task-oriented dialogues, such as GlobalWoZ [52] and X-RiSAWOZ [53], to help developers better develop and evaluate multilingual task-oriented dialogue systems. \n\n2) Methods: Traditional model evaluation methods (Evaluation Methodologies) use supervised training corpora to finetune pre-trained models, with the pipeline being pre-training + fine-tuning. MLLMs have inherent zero-shot and few-shot learning capabilities, and their evaluation methods can be divided into two categories: one uses the original evaluation method for task-related fine-tuning, and the other is based on prompt in-context learning (ICL), with the pipeline changing to pre-training + prompting + prediction. Task-related finetuning requires using training corpora to update model parameters, while ICL does not need to update model parameters, only requiring the design of different prompts for the model to return corresponding results. The evaluation methods for MLLMs are shown in Figure 3. \n\nTask-Specific Fine-tuning: \n\n\u2022 Zero-Shot Cross Lingual Transfer: First perform taskrelated fine-tuning on one language, then evaluate using the test set of another language. \u2022 Few-Shot Cross Lingual Transfer: First fine-tune parameters on English and a small amount of target language, then evaluate on the target language test set. \u2022 Monolingual Fine-tuning: Only fine-tune parameters on the full target language. \u2022 Translate-Train: Use machine-translated target language for parameter fine-tuning.",
                    "score": 0.49427927112238823,
                    "section_title": "B. Evaluation Tasks",
                    "char_start_offset": 30322,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 680
                        },
                        {
                            "start": 683,
                            "end": 915
                        },
                        {
                            "start": 918,
                            "end": 1109
                        },
                        {
                            "start": 1110,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1725
                        },
                        {
                            "start": 1728,
                            "end": 1754
                        },
                        {
                            "start": 1757,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2060
                        },
                        {
                            "start": 2061,
                            "end": 2142
                        },
                        {
                            "start": 2143,
                            "end": 2227
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92822265625
                },
                {
                    "corpus_id": "273532366",
                    "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                    "text": "The datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2. \n\n1) Datasets: Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets Fig. 3: Evaluation methods for MLLMs XTREME, XTREME-R, and XGLUE based on them. The goal of multilingual evaluation datasets is to cover more diverse tasks and languages so that we can better evaluate the generalization performance of MLLMs. At the same time, some researchers are also dedicated to building evaluation datasets for specific languages, such as IndicXTREME for Indian languages. \n\nIn general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation. \n\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks.",
                    "score": 0.5851332341320621,
                    "section_title": "B. Evaluation Tasks",
                    "char_start_offset": 28760,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 383
                        },
                        {
                            "start": 386,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 814
                        },
                        {
                            "start": 815,
                            "end": 976
                        },
                        {
                            "start": 977,
                            "end": 1128
                        },
                        {
                            "start": 1131,
                            "end": 1263
                        },
                        {
                            "start": 1264,
                            "end": 1443
                        },
                        {
                            "start": 1444,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 2242
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9033203125
                }
            ],
            "relevance_judgement": 0.92822265625,
            "relevance_judgment_input_expanded": "# Title: Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact\n# Venue: arXiv.org\n# Authors: Junhua Liu, Bin Fu\n## Abstract\nMultilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.\n## B. Evaluation Tasks\nThe datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2. \n\n1) Datasets: Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets Fig. 3: Evaluation methods for MLLMs XTREME, XTREME-R, and XGLUE based on them. The goal of multilingual evaluation datasets is to cover more diverse tasks and languages so that we can better evaluate the generalization performance of MLLMs. At the same time, some researchers are also dedicated to building evaluation datasets for specific languages, such as IndicXTREME for Indian languages. \n\nIn general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation. \n\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks.\n...\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks. \n\nIn addition researchers also released multilingual dialogue datasets for task-oriented dialogues, such as GlobalWoZ [52] and X-RiSAWOZ [53], to help developers better develop and evaluate multilingual task-oriented dialogue systems. \n\n2) Methods: Traditional model evaluation methods (Evaluation Methodologies) use supervised training corpora to finetune pre-trained models, with the pipeline being pre-training + fine-tuning. MLLMs have inherent zero-shot and few-shot learning capabilities, and their evaluation methods can be divided into two categories: one uses the original evaluation method for task-related fine-tuning, and the other is based on prompt in-context learning (ICL), with the pipeline changing to pre-training + prompting + prediction. Task-related finetuning requires using training corpora to update model parameters, while ICL does not need to update model parameters, only requiring the design of different prompts for the model to return corresponding results. The evaluation methods for MLLMs are shown in Figure 3. \n\nTask-Specific Fine-tuning: \n\n\u2022 Zero-Shot Cross Lingual Transfer: First perform taskrelated fine-tuning on one language, then evaluate using the test set of another language. \u2022 Few-Shot Cross Lingual Transfer: First fine-tune parameters on English and a small amount of target language, then evaluate on the target language test set. \u2022 Monolingual Fine-tuning: Only fine-tune parameters on the full target language. \u2022 Translate-Train: Use machine-translated target language for parameter fine-tuning.",
            "reference_string": "[273532366 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "venue": "Frontiers Comput. Sci.",
            "year": 2024,
            "reference_count": 185,
            "citation_count": 43,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257136845",
                    "name": "Yuemei Xu"
                },
                {
                    "authorId": "2258334185",
                    "name": "Ling Hu"
                },
                {
                    "authorId": "2294513520",
                    "name": "Jiayi Zhao"
                },
                {
                    "authorId": "2294361104",
                    "name": "Zihan Qiu"
                },
                {
                    "authorId": "2294363807",
                    "name": "Yuqi Ye"
                },
                {
                    "authorId": "2294933103",
                    "name": "Hanwen Gu"
                }
            ],
            "abstract": "\n Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.",
            "corpus_id": 268819377,
            "sentences": [
                {
                    "corpus_id": "268819377",
                    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                    "text": "This section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
                    "score": 0.5689987458005816,
                    "section_title": "B. Bias Benchmark",
                    "char_start_offset": 48672,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 552
                        },
                        {
                            "start": 555,
                            "end": 727
                        },
                        {
                            "start": 730,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1241
                        },
                        {
                            "start": 1244,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1549
                        },
                        {
                            "start": 1552,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2150
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 404,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 406,
                            "end": 411,
                            "matchedPaperCorpusId": "173991101"
                        },
                        {
                            "start": 448,
                            "end": 453,
                            "matchedPaperCorpusId": "21670658"
                        },
                        {
                            "start": 455,
                            "end": 460,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 540,
                            "end": 545,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 546,
                            "end": 551,
                            "matchedPaperCorpusId": "252547725"
                        },
                        {
                            "start": 970,
                            "end": 975,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 977,
                            "end": 982,
                            "matchedPaperCorpusId": "219530686"
                        },
                        {
                            "start": 1076,
                            "end": 1081,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 1364,
                            "end": 1369,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 1383,
                            "end": 1388,
                            "matchedPaperCorpusId": "4952494"
                        },
                        {
                            "start": 1455,
                            "end": 1460,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 1649,
                            "end": 1654,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 1666,
                            "end": 1671,
                            "matchedPaperCorpusId": "252547725"
                        },
                        {
                            "start": 1988,
                            "end": 1993,
                            "matchedPaperCorpusId": "135465247"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92724609375
                }
            ],
            "relevance_judgement": 0.92724609375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias\n# Venue: Frontiers Comput. Sci.\n# Authors: Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu\n## Abstract\n\n Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.\n## B. Bias Benchmark\nThis section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
            "reference_string": "[268819377 | Xu et al. | 2024 | Citations: 43]"
        },
        {
            "title": "Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 41,
            "citation_count": 9,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.09588",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.09588, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46299565",
                    "name": "Jaehoon Oh"
                },
                {
                    "authorId": "2051385328",
                    "name": "Jongwoo Ko"
                },
                {
                    "authorId": "70509252",
                    "name": "Se-Young Yun"
                }
            ],
            "abstract": "Translation has played a crucial role in improving the performance on multilingual tasks: (1) to generate the target language data from the source language data for training and (2) to generate the source language data from the target language data for inference. However, prior works have not considered the use of both translations simultaneously. This paper shows that combining them can synergize the results on various multilingual sentence classification tasks. We empirically find that translation artifacts stylized by translators are the main factor of the performance gain. Based on this analysis, we adopt two training methods, SupCon and MixUp, considering translation artifacts. Furthermore, we propose a cross-lingual fine-tuning algorithm called MUSC, which uses SupCon and MixUp jointly and improves the performance. Our code is available at https://github.com/jongwooko/MUSC.",
            "corpus_id": 252968346,
            "sentences": [
                {
                    "corpus_id": "252968346",
                    "title": "Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks",
                    "text": "Large-scale pre-trained multilingual language models (Devlin et al., 2019;Conneau and Lample, 2019;Huang et al., 2019;Conneau et al., 2020;Luo et al., 2021) have shown promising transferability in zeroshot cross-lingual transfer (ZSXLT), where pretrained language models (PLMs) are fine-tuned using a labeled task-specific dataset from a richresource source language (e.g., English or Spanish) and then evaluated on zero-resource target languages. Multilingual PLMs yield a universal representation space across different languages, thereby improving multilingual task performance (Pires et al., 2019;Chen et al., 2019). Recent work has enhanced cross-lingual transferability by reducing the discrepancies between languages based on translation approaches during fine-tuning (Fang et al., 2021;Zheng et al., 2021;Yang et al., 2022). Our paper focuses on when translated datasets are available for cross-lingual transfer (XLT). * Equal contribution Conneau et al. (2018) provided two translationbased XLT baselines: translate-train and translate-test. The former fine-tunes a multilingual PLM (e.g., multilingual BERT) using the original source language and machine-translated target languages simultaneously and then evaluates it on the target languages. Meanwhile, the latter fine-tunes a source language-based PLM (e.g., English BERT) using the original source language and then evaluates it on the machine-translated source language. Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test.",
                    "score": 0.5637125434959647,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 447
                        },
                        {
                            "start": 448,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1879
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 53,
                            "end": 74,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 74,
                            "end": 99,
                            "matchedPaperCorpusId": "58981712"
                        },
                        {
                            "start": 99,
                            "end": 118,
                            "matchedPaperCorpusId": "202541545"
                        },
                        {
                            "start": 118,
                            "end": 139,
                            "matchedPaperCorpusId": "207880568"
                        },
                        {
                            "start": 139,
                            "end": 156,
                            "matchedPaperCorpusId": "235303732"
                        },
                        {
                            "start": 581,
                            "end": 601,
                            "matchedPaperCorpusId": "174798142"
                        },
                        {
                            "start": 601,
                            "end": 619,
                            "matchedPaperCorpusId": "174799553"
                        },
                        {
                            "start": 775,
                            "end": 794,
                            "matchedPaperCorpusId": "221641003"
                        },
                        {
                            "start": 794,
                            "end": 813,
                            "matchedPaperCorpusId": "235436224"
                        },
                        {
                            "start": 813,
                            "end": 831,
                            "matchedPaperCorpusId": "201698093"
                        },
                        {
                            "start": 1650,
                            "end": 1671,
                            "matchedPaperCorpusId": "215548041"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92724609375
                }
            ],
            "relevance_judgement": 0.92724609375,
            "relevance_judgment_input_expanded": "# Title: Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jaehoon Oh, Jongwoo Ko, Se-Young Yun\n## Abstract\nTranslation has played a crucial role in improving the performance on multilingual tasks: (1) to generate the target language data from the source language data for training and (2) to generate the source language data from the target language data for inference. However, prior works have not considered the use of both translations simultaneously. This paper shows that combining them can synergize the results on various multilingual sentence classification tasks. We empirically find that translation artifacts stylized by translators are the main factor of the performance gain. Based on this analysis, we adopt two training methods, SupCon and MixUp, considering translation artifacts. Furthermore, we propose a cross-lingual fine-tuning algorithm called MUSC, which uses SupCon and MixUp jointly and improves the performance. Our code is available at https://github.com/jongwooko/MUSC.\n## Introduction\nLarge-scale pre-trained multilingual language models (Devlin et al., 2019;Conneau and Lample, 2019;Huang et al., 2019;Conneau et al., 2020;Luo et al., 2021) have shown promising transferability in zeroshot cross-lingual transfer (ZSXLT), where pretrained language models (PLMs) are fine-tuned using a labeled task-specific dataset from a richresource source language (e.g., English or Spanish) and then evaluated on zero-resource target languages. Multilingual PLMs yield a universal representation space across different languages, thereby improving multilingual task performance (Pires et al., 2019;Chen et al., 2019). Recent work has enhanced cross-lingual transferability by reducing the discrepancies between languages based on translation approaches during fine-tuning (Fang et al., 2021;Zheng et al., 2021;Yang et al., 2022). Our paper focuses on when translated datasets are available for cross-lingual transfer (XLT). * Equal contribution Conneau et al. (2018) provided two translationbased XLT baselines: translate-train and translate-test. The former fine-tunes a multilingual PLM (e.g., multilingual BERT) using the original source language and machine-translated target languages simultaneously and then evaluates it on the target languages. Meanwhile, the latter fine-tunes a source language-based PLM (e.g., English BERT) using the original source language and then evaluates it on the machine-translated source language. Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test.",
            "reference_string": "[252968346 | Oh et al. | 2022 | Citations: 9]"
        },
        {
            "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 47,
            "citation_count": 34,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.14186",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.14186, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2008183566",
                    "name": "Leonardo Ranaldi"
                },
                {
                    "authorId": "2199247500",
                    "name": "Giulia Pucci"
                },
                {
                    "authorId": "145528474",
                    "name": "A. Freitas"
                }
            ],
            "abstract": "The language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs. In this paper, we propose to empower Instructiontuned LLMs (It-LLMs) in languages other than English by building semantic alignment between them. Hence, we propose CrossAlpaca, an It-LLM with cross-lingual instruction-following and Translation-following demonstrations to improve semantic alignment between languages. We validate our approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA and adapted versions of MMLU and BBH. Our models, tested over six different languages, outperform the It-LLMs tuned on monolingual data. The final results show that instruction tuning on non-English data is not enough and that semantic alignment can be further improved by Translation-following demonstrations.",
            "corpus_id": 261242630,
            "sentences": [
                {
                    "corpus_id": "261242630",
                    "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
                    "text": "To evaluate the performance of the It-LLMs and the impact of the translation-based semantic alignment approach, we used two cross-lingual (XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2020)) and two multi-task (MMLU (Hendrycks et al., 2021) and BBH (Suzgun et al., 2022)) benchmarks. XQUAD and MLQA focus on understanding ques-tions and answers through translation into different languages. MMLU and BBH being multi-task benchmarks include subtasks related to Boolean expressions and QA on basic-level subjects (e.g., chemistry, physics). However, we decided to introduce them to observe whether our approach degrades performance in these tasks. The first two datasets selected are appropriately constructed for multi-language testing, while the second two are available only in English. So we do a preliminary translation step as outlined below. \n\nMultiLingual Question Answering (MLQA) (Lewis et al., 2020) evaluatates cross-lingual question answering performance. The benchmark comprises over 5K extractive QA instances in the SQuAD (Rajpurkar et al., 2016) format in several languages. MLQA is highly parallel, with QA instances aligned across four languages on average. Although comprising different languages, some languages are not represented, such as Italian. To conduct the experiments uniformly, we have translated the examples as also done in the forthcoming MMLU and BBH. \n\nCross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages. \n\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4 .",
                    "score": 0.4892092530076375,
                    "section_title": "Benchmarks",
                    "char_start_offset": 14084,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 395
                        },
                        {
                            "start": 396,
                            "end": 543
                        },
                        {
                            "start": 544,
                            "end": 650
                        },
                        {
                            "start": 651,
                            "end": 792
                        },
                        {
                            "start": 793,
                            "end": 851
                        },
                        {
                            "start": 854,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1273
                        },
                        {
                            "start": 1274,
                            "end": 1389
                        },
                        {
                            "start": 1392,
                            "end": 1652
                        },
                        {
                            "start": 1653,
                            "end": 1720
                        },
                        {
                            "start": 1723,
                            "end": 1953
                        },
                        {
                            "start": 1954,
                            "end": 2049
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 145,
                            "end": 167,
                            "matchedPaperCorpusId": "204901567"
                        },
                        {
                            "start": 174,
                            "end": 194,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 254,
                            "end": 275,
                            "matchedPaperCorpusId": "252917648"
                        },
                        {
                            "start": 893,
                            "end": 913,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 1441,
                            "end": 1463,
                            "matchedPaperCorpusId": "204901567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92578125
                }
            ],
            "relevance_judgement": 0.92578125,
            "relevance_judgment_input_expanded": "# Title: Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Leonardo Ranaldi, Giulia Pucci, A. Freitas\n## Abstract\nThe language ability of Large Language Models (LLMs) is often unbalanced towards English because of the imbalance in the distribution of the pre-training data. This disparity is demanded in further fine-tuning and affecting the cross-lingual abilities of LLMs. In this paper, we propose to empower Instructiontuned LLMs (It-LLMs) in languages other than English by building semantic alignment between them. Hence, we propose CrossAlpaca, an It-LLM with cross-lingual instruction-following and Translation-following demonstrations to improve semantic alignment between languages. We validate our approach on the multilingual Question Answering (QA) benchmarks XQUAD and MLQA and adapted versions of MMLU and BBH. Our models, tested over six different languages, outperform the It-LLMs tuned on monolingual data. The final results show that instruction tuning on non-English data is not enough and that semantic alignment can be further improved by Translation-following demonstrations.\n## Benchmarks\nTo evaluate the performance of the It-LLMs and the impact of the translation-based semantic alignment approach, we used two cross-lingual (XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2020)) and two multi-task (MMLU (Hendrycks et al., 2021) and BBH (Suzgun et al., 2022)) benchmarks. XQUAD and MLQA focus on understanding ques-tions and answers through translation into different languages. MMLU and BBH being multi-task benchmarks include subtasks related to Boolean expressions and QA on basic-level subjects (e.g., chemistry, physics). However, we decided to introduce them to observe whether our approach degrades performance in these tasks. The first two datasets selected are appropriately constructed for multi-language testing, while the second two are available only in English. So we do a preliminary translation step as outlined below. \n\nMultiLingual Question Answering (MLQA) (Lewis et al., 2020) evaluatates cross-lingual question answering performance. The benchmark comprises over 5K extractive QA instances in the SQuAD (Rajpurkar et al., 2016) format in several languages. MLQA is highly parallel, with QA instances aligned across four languages on average. Although comprising different languages, some languages are not represented, such as Italian. To conduct the experiments uniformly, we have translated the examples as also done in the forthcoming MMLU and BBH. \n\nCross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages. \n\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4 .",
            "reference_string": "[261242630 | Ranaldi et al. | 2023 | Citations: 34]"
        },
        {
            "title": "SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining",
            "venue": "WNUT",
            "year": 2021,
            "reference_count": 40,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.wnut-1.27.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.wnut-1.27, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66957859",
                    "name": "L\u00e9o Jacqmin"
                },
                {
                    "authorId": "2483876",
                    "name": "Gabriel Marzinotto"
                },
                {
                    "authorId": "2101678",
                    "name": "Justyna Gromada"
                },
                {
                    "authorId": "1736936",
                    "name": "Ewelina Szczekocka"
                },
                {
                    "authorId": "2611308",
                    "name": "Robert Kolodynski"
                },
                {
                    "authorId": "1722225",
                    "name": "G\u00e9raldine Damnati"
                }
            ],
            "abstract": "Following the increasing performance of neural machine translation systems, the paradigm of using automatically translated data for cross-lingual adaptation is now studied in several applicative domains. The capacity to accurately project annotations remains however an issue for sequence tagging tasks where annotation must be projected with correct spans. Additionally, when the task implies noisy user-generated text, the quality of translation and annotation projection can be affected. In this paper we propose to tackle multilingual sequence tagging with a new span alignment method and apply it to opinion target extraction from customer reviews. We show that provided suitable heuristics, translated data with automatic span-level annotation projection can yield improvements both for cross-lingual adaptation compared to zero-shot transfer, and data augmentation compared to a multilingual baseline.",
            "corpus_id": 241583550,
            "sentences": [
                {
                    "corpus_id": "241583550",
                    "title": "SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining",
                    "text": "Results for data augmentation are shown in Table 7. When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning. \n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages. \n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval. \n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average.",
                    "score": 0.46220783344723304,
                    "section_title": "Data Augmentation",
                    "char_start_offset": 25601,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 51
                        },
                        {
                            "start": 52,
                            "end": 104
                        },
                        {
                            "start": 105,
                            "end": 384
                        },
                        {
                            "start": 387,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 733
                        },
                        {
                            "start": 734,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 985
                        },
                        {
                            "start": 988,
                            "end": 1087
                        },
                        {
                            "start": 1090,
                            "end": 1220
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining\n# Venue: WNUT\n# Authors: L\u00e9o Jacqmin, Gabriel Marzinotto, Justyna Gromada, Ewelina Szczekocka, Robert Kolodynski, G\u00e9raldine Damnati\n## Abstract\nFollowing the increasing performance of neural machine translation systems, the paradigm of using automatically translated data for cross-lingual adaptation is now studied in several applicative domains. The capacity to accurately project annotations remains however an issue for sequence tagging tasks where annotation must be projected with correct spans. Additionally, when the task implies noisy user-generated text, the quality of translation and annotation projection can be affected. In this paper we propose to tackle multilingual sequence tagging with a new span alignment method and apply it to opinion target extraction from customer reviews. We show that provided suitable heuristics, translated data with automatic span-level annotation projection can yield improvements both for cross-lingual adaptation compared to zero-shot transfer, and data augmentation compared to a multilingual baseline.\n## Data Augmentation\nResults for data augmentation are shown in Table 7. When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning. \n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages. \n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval. \n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average.",
            "reference_string": "[241583550 | Jacqmin et al. | 2021 | Citations: 2]"
        },
        {
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327995656",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2258789284",
                    "name": "Weixuan Wang"
                },
                {
                    "authorId": "2349802912",
                    "name": "Sinuo Liu"
                },
                {
                    "authorId": "2331767275",
                    "name": "Huifeng Yin"
                },
                {
                    "authorId": "2356794252",
                    "name": "Xintong Wang"
                },
                {
                    "authorId": "2331686159",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2302342302",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "corpus_id": 277993848,
            "sentences": [
                {
                    "corpus_id": "277993848",
                    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                    "text": "The domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
                    "score": 0.5926090956462529,
                    "section_title": "Domain",
                    "char_start_offset": 9088,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 63,
                            "end": 115
                        },
                        {
                            "start": 118,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 360
                        },
                        {
                            "start": 361,
                            "end": 569
                        },
                        {
                            "start": 570,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1040
                        },
                        {
                            "start": 1041,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1581
                        },
                        {
                            "start": 1584,
                            "end": 1792
                        },
                        {
                            "start": 1793,
                            "end": 1951
                        },
                        {
                            "start": 1952,
                            "end": 2045
                        },
                        {
                            "start": 2046,
                            "end": 2228
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 511,
                            "end": 528,
                            "matchedPaperCorpusId": "252735112"
                        },
                        {
                            "start": 528,
                            "end": 546,
                            "matchedPaperCorpusId": "260334562"
                        },
                        {
                            "start": 690,
                            "end": 713,
                            "matchedPaperCorpusId": "221516475"
                        },
                        {
                            "start": 854,
                            "end": 872,
                            "matchedPaperCorpusId": "263829211"
                        },
                        {
                            "start": 889,
                            "end": 911,
                            "matchedPaperCorpusId": "271245129"
                        },
                        {
                            "start": 1177,
                            "end": 1198,
                            "matchedPaperCorpusId": "273850461"
                        },
                        {
                            "start": 1395,
                            "end": 1415,
                            "matchedPaperCorpusId": "270371898"
                        },
                        {
                            "start": 2088,
                            "end": 2104,
                            "matchedPaperCorpusId": "254043519"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91796875
                }
            ],
            "relevance_judgement": 0.91796875,
            "relevance_judgment_input_expanded": "# Title: The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n# Venue: arXiv.org\n# Authors: Minghao Wu, Weixuan Wang, Sinuo Liu, Huifeng Yin, Xintong Wang, Yu Zhao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang\n## Abstract\nAs large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.\n## Domain\nThe domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
            "reference_string": "[277993848 | Wu et al. | 2025 | Citations: 5]"
        },
        {
            "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
            "venue": "Neural Information Processing Systems",
            "year": 2021,
            "reference_count": 61,
            "citation_count": 73,
            "influential_citation_count": 17,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.11976, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "2118211280",
                    "name": "Xinyan Velocity Yu"
                },
                {
                    "authorId": "11348687",
                    "name": "Jungo Kasai"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ],
            "abstract": "We present Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. Combined with a multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the significance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings.",
            "corpus_id": 236428949,
            "sentences": [
                {
                    "corpus_id": "236428949",
                    "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
                    "text": "Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios. We evaluate F1 and EM scores over the questions with answer annotations from two datasets, following the common evaluation practice in open QA (Lee et al., 2019). \n\nXOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data). XOR-TYDI QA offers both training and evaluation data. \n\nMKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA. MKQA has evaluation data only; XOR-TYDI QA and MKQA have five languages in common.",
                    "score": 0.4664826650761462,
                    "section_title": "Datasets and Knowledge Sources",
                    "char_start_offset": 11704,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 275
                        },
                        {
                            "start": 278,
                            "end": 290
                        },
                        {
                            "start": 291,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 801
                        },
                        {
                            "start": 804,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1090
                        },
                        {
                            "start": 1091,
                            "end": 1173
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 256,
                            "end": 274,
                            "matchedPaperCorpusId": "173990818"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.91357421875,
            "relevance_judgment_input_expanded": "# Title: One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval\n# Venue: Neural Information Processing Systems\n# Authors: Akari Asai, Xinyan Velocity Yu, Jungo Kasai, Hannaneh Hajishirzi\n## Abstract\nWe present Cross-lingual Open-Retrieval Answer Generation (CORA), the first unified many-to-many question answering (QA) model that can answer questions across many languages, even for ones without language-specific annotated data or knowledge sources. We introduce a new dense passage retrieval algorithm that is trained to retrieve documents across languages for a question. Combined with a multilingual autoregressive generation model, CORA answers directly in the target language without any translation or in-language retrieval modules as used in prior work. We propose an iterative training method that automatically extends annotated data available only in high-resource languages to low-resource ones. Our results show that CORA substantially outperforms the previous state of the art on multilingual open QA benchmarks across 26 languages, 9 of which are unseen during training. Our analyses show the significance of cross-lingual retrieval and generation in many languages, particularly under low-resource settings.\n## Datasets and Knowledge Sources\nMultilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios. We evaluate F1 and EM scores over the questions with answer annotations from two datasets, following the common evaluation practice in open QA (Lee et al., 2019). \n\nXOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data). XOR-TYDI QA offers both training and evaluation data. \n\nMKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA. MKQA has evaluation data only; XOR-TYDI QA and MKQA have five languages in common.",
            "reference_string": "[236428949 | Asai et al. | 2021 | Citations: 73]"
        },
        {
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325726142",
                    "name": "Fabio Barth"
                },
                {
                    "authorId": "2302558975",
                    "name": "Georg Rehm"
                }
            ],
            "abstract": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
            "corpus_id": 276421738,
            "sentences": [
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
                    "score": 0.5583017952127597,
                    "section_title": "Challenge 3: Cultural Bias",
                    "char_start_offset": 13020,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 61,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1304
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 149,
                            "matchedPaperCorpusId": "265445838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                }
            ],
            "relevance_judgement": 0.90966796875,
            "relevance_judgment_input_expanded": "# Title: Multilingual European Language Models: Benchmarking Approaches and Challenges\n# Venue: arXiv.org\n# Authors: Fabio Barth, Georg Rehm\n## Abstract\nThe breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.\n## Challenge 3: Cultural Bias\nMost benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
            "reference_string": "[276421738 | Barth et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.12986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324783252",
                    "name": "Skyler Seto"
                },
                {
                    "authorId": "41096186",
                    "name": "Maartje ter Hoeve"
                },
                {
                    "authorId": "2331509164",
                    "name": "He Bai"
                },
                {
                    "authorId": "3344220",
                    "name": "Natalie Schluter"
                },
                {
                    "authorId": "2529182",
                    "name": "David Grangier"
                }
            ],
            "abstract": "Large language models are trained on massive scrapes of the web, as required by current scaling laws. Most progress is made for English, given its abundance of high-quality pretraining data. For most other languages, however, such high quality pretraining data is unavailable. In this work, we study how to boost pretrained model performance in a target language with insufficient pretraining data for training a high performing language model, by enlisting data from an auxiliary language for which high quality data is available. We study this by quantifying the performance gap between training with data in a data-rich auxiliary language compared with training in the target language, exploring the benefits of translation systems, studying the limitations of model scaling when data is limited in the target languages, and proposing new methods for upsampling data from the auxiliary language. Our results show that stronger auxiliary datasets result in performance gains without modification to the model or training objective for close languages, and, in particular, that performance gains due to the development of more information-rich English pretraining datasets can extend to targeted language settings with limited data.",
            "corpus_id": 274149965,
            "sentences": [
                {
                    "corpus_id": "274149965",
                    "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
                    "text": "In this section we list some limitations of our work. \n\nLanguages included. Our primary focus is on English-German language training, as these two Germanic family languages share linguistic similarities (Lewis et al., 2015). German is one of the most well-represented languages in the mC4 dataset, facilitating model comparisons with varying amounts of German and English data. Furthermore, the availability of extensive public resources for German, including translation systems and translated evaluation data, further supports our emphasis on this language pair. We experimented with seven additional datasets including French, Spanish, Italian, Portuguese, Korean, Japanese, and Chinese. However, we note that there are many other languages within mC4 and more broadly which can benefit from having auxiliary English data for pretraining. Due to limited evaluation benchmarks and availability of target language data for comparison, we leave investigation for truly low-resource languages to future work. \n\nEvaluation data. Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models. Model size. Finally, this work studies three model sizes up to 3B models. We note that there are many standard benchmarks that can be evaluated at 1B-3B scale, however many more benchmarks and patterns can appear at larger model sizes. It is important future work to evaluate whether the results extend to larger scales including evaluating potential \"emergent behaviors\" as well as risks at larger scales (Wei et al., 2022).",
                    "score": 0.4580743205049682,
                    "section_title": "Limitations",
                    "char_start_offset": 32229,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 53
                        },
                        {
                            "start": 56,
                            "end": 75
                        },
                        {
                            "start": 76,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 841
                        },
                        {
                            "start": 842,
                            "end": 1007
                        },
                        {
                            "start": 1010,
                            "end": 1026
                        },
                        {
                            "start": 1027,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1405
                        },
                        {
                            "start": 1406,
                            "end": 1523
                        },
                        {
                            "start": 1524,
                            "end": 1731
                        },
                        {
                            "start": 1732,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1805
                        },
                        {
                            "start": 1806,
                            "end": 1967
                        },
                        {
                            "start": 1968,
                            "end": 2157
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9091796875
                }
            ],
            "relevance_judgement": 0.9091796875,
            "relevance_judgment_input_expanded": "# Title: Training Bilingual LMs with Data Constraints in the Targeted Language\n# Venue: arXiv.org\n# Authors: Skyler Seto, Maartje ter Hoeve, He Bai, Natalie Schluter, David Grangier\n## Abstract\nLarge language models are trained on massive scrapes of the web, as required by current scaling laws. Most progress is made for English, given its abundance of high-quality pretraining data. For most other languages, however, such high quality pretraining data is unavailable. In this work, we study how to boost pretrained model performance in a target language with insufficient pretraining data for training a high performing language model, by enlisting data from an auxiliary language for which high quality data is available. We study this by quantifying the performance gap between training with data in a data-rich auxiliary language compared with training in the target language, exploring the benefits of translation systems, studying the limitations of model scaling when data is limited in the target languages, and proposing new methods for upsampling data from the auxiliary language. Our results show that stronger auxiliary datasets result in performance gains without modification to the model or training objective for close languages, and, in particular, that performance gains due to the development of more information-rich English pretraining datasets can extend to targeted language settings with limited data.\n## Limitations\nIn this section we list some limitations of our work. \n\nLanguages included. Our primary focus is on English-German language training, as these two Germanic family languages share linguistic similarities (Lewis et al., 2015). German is one of the most well-represented languages in the mC4 dataset, facilitating model comparisons with varying amounts of German and English data. Furthermore, the availability of extensive public resources for German, including translation systems and translated evaluation data, further supports our emphasis on this language pair. We experimented with seven additional datasets including French, Spanish, Italian, Portuguese, Korean, Japanese, and Chinese. However, we note that there are many other languages within mC4 and more broadly which can benefit from having auxiliary English data for pretraining. Due to limited evaluation benchmarks and availability of target language data for comparison, we leave investigation for truly low-resource languages to future work. \n\nEvaluation data. Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models. Model size. Finally, this work studies three model sizes up to 3B models. We note that there are many standard benchmarks that can be evaluated at 1B-3B scale, however many more benchmarks and patterns can appear at larger model sizes. It is important future work to evaluate whether the results extend to larger scales including evaluating potential \"emergent behaviors\" as well as risks at larger scales (Wei et al., 2022).",
            "reference_string": "[274149965 | Seto et al. | 2024 | Citations: 1]"
        },
        {
            "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
            "venue": "Transactions of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 54,
            "citation_count": 77,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00595/2157340/tacl_a_00595.pdf",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1162/tacl_a_00595?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1162/tacl_a_00595, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118895402",
                    "name": "Xinyu Crystina Zhang"
                },
                {
                    "authorId": "47583894",
                    "name": "Nandan Thakur"
                },
                {
                    "authorId": "2166106776",
                    "name": "Odunayo Ogundepo"
                },
                {
                    "authorId": "2023642",
                    "name": "Ehsan Kamalloo"
                },
                {
                    "authorId": "1419474794",
                    "name": "David Alfonso-Hermelo"
                },
                {
                    "authorId": "2238110973",
                    "name": "Xiaoguang Li"
                },
                {
                    "authorId": "1688015",
                    "name": "Qun Liu"
                },
                {
                    "authorId": "2066076226",
                    "name": "Mehdi Rezagholizadeh"
                },
                {
                    "authorId": "145580839",
                    "name": "Jimmy J. Lin"
                }
            ],
            "abstract": "Abstract MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.",
            "corpus_id": 261557946,
            "sentences": [
                {
                    "corpus_id": "261557946",
                    "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
                    "text": "Since collecting human relevance labels is laborious and costly, other studies have adopted workarounds to build multilingual datasets. For example, Bonifacio et al. ( 2021) automatically translated the MS MARCO dataset (Bajaj et al., 2018) from English into 13 other languages. However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020;Lembersky et al., 2012;Volansky et al., 2015;Avner et al., 2016;Eetemadi and Toutanova, 2014;Rabinovich and Wintner, 2015) and may lead to training data of questionable value. Alternatively, Sun and Duh (2020) built synthetic bilingual retrieval datasets in a resource called CLIRMatrix based on the parallel structure of Wikipedia that covers 139 languages. Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches.",
                    "score": 0.4696263367219294,
                    "section_title": "Comparison to Synthetic Datasets",
                    "char_start_offset": 6780,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 562
                        },
                        {
                            "start": 563,
                            "end": 745
                        },
                        {
                            "start": 746,
                            "end": 919
                        },
                        {
                            "start": 920,
                            "end": 1011
                        },
                        {
                            "start": 1012,
                            "end": 1145
                        },
                        {
                            "start": 1146,
                            "end": 1291
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 367,
                            "end": 387,
                            "matchedPaperCorpusId": "212657414"
                        },
                        {
                            "start": 387,
                            "end": 410,
                            "matchedPaperCorpusId": "11280500"
                        },
                        {
                            "start": 451,
                            "end": 480,
                            "matchedPaperCorpusId": "15699208"
                        },
                        {
                            "start": 480,
                            "end": 509,
                            "matchedPaperCorpusId": "940724"
                        },
                        {
                            "start": 578,
                            "end": 596,
                            "matchedPaperCorpusId": "226262282"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.908203125
                }
            ],
            "relevance_judgement": 0.908203125,
            "relevance_judgment_input_expanded": "# Title: MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages\n# Venue: Transactions of the Association for Computational Linguistics\n# Authors: Xinyu Crystina Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, Jimmy J. Lin\n## Abstract\nAbstract MIRACL is a multilingual dataset for ad hoc retrieval across 18 languages that collectively encompass over three billion native speakers around the world. This resource is designed to support monolingual retrieval tasks, where the queries and the corpora are in the same language. In total, we have gathered over 726k high-quality relevance judgments for 78k queries over Wikipedia in these languages, where all annotations have been performed by native speakers hired by our team. MIRACL covers languages that are both typologically close as well as distant from 10 language families and 13 sub-families, associated with varying amounts of publicly available resources. Extensive automatic heuristic verification and manual assessments were performed during the annotation process to control data quality. In total, MIRACL represents an investment of around five person-years of human annotator effort. Our goal is to spur research on improving retrieval across a continuum of languages, thus enhancing information access capabilities for diverse populations around the world, particularly those that have traditionally been underserved. MIRACL is available at http://miracl.ai/.\n## Comparison to Synthetic Datasets\nSince collecting human relevance labels is laborious and costly, other studies have adopted workarounds to build multilingual datasets. For example, Bonifacio et al. ( 2021) automatically translated the MS MARCO dataset (Bajaj et al., 2018) from English into 13 other languages. However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020;Lembersky et al., 2012;Volansky et al., 2015;Avner et al., 2016;Eetemadi and Toutanova, 2014;Rabinovich and Wintner, 2015) and may lead to training data of questionable value. Alternatively, Sun and Duh (2020) built synthetic bilingual retrieval datasets in a resource called CLIRMatrix based on the parallel structure of Wikipedia that covers 139 languages. Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches.",
            "reference_string": "[261557946 | Zhang et al. | 2023 | Citations: 77]"
        },
        {
            "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
            "venue": "Workshop on Representation Learning for NLP",
            "year": 2021,
            "reference_count": 22,
            "citation_count": 129,
            "influential_citation_count": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.repl4nlp-1.4.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.00572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "3048577",
                    "name": "Jingfei Du"
                },
                {
                    "authorId": "40511414",
                    "name": "Myle Ott"
                },
                {
                    "authorId": "4168565",
                    "name": "Giridhar Anantharaman"
                },
                {
                    "authorId": "2480903",
                    "name": "Alexis Conneau"
                }
            ],
            "abstract": "Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.",
            "corpus_id": 233481097,
            "sentences": [
                {
                    "corpus_id": "233481097",
                    "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
                    "text": "We consider three evaluation benchmarks. For cross-lingual understanding, we use cross-lingual natural language inference and question answering, and use the GLUE benchmark to evaluate the English performance.\n\nCross-lingual Natural Language Inference.\n\nThe XNLI dataset (Conneau et al., 2018) comes with ground-truth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency.\n\nCross-lingual Question Answering. We use the MLQA and XQuad benchmark from  and Artetxe et al. (2019), which extends the English SQuAD benchmark to more languages.\n\nWe report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English.\n\nThe English GLUE Benchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018).",
                    "score": 0.4579765947182617,
                    "section_title": "Evaluation",
                    "char_start_offset": 4890,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1443,
                            "end": 1466,
                            "matchedPaperCorpusId": "3432876"
                        },
                        {
                            "start": 1474,
                            "end": 1495,
                            "matchedPaperCorpusId": "990233"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9072265625
                }
            ],
            "relevance_judgement": 0.9072265625,
            "relevance_judgment_input_expanded": "# Title: Larger-Scale Transformers for Multilingual Masked Language Modeling\n# Venue: Workshop on Representation Learning for NLP\n# Authors: Naman Goyal, Jingfei Du, Myle Ott, Giridhar Anantharaman, Alexis Conneau\n## Abstract\nRecent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed and outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests larger capacity models for language understanding may obtain strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.\n## Evaluation\nWe consider three evaluation benchmarks. For cross-lingual understanding, we use cross-lingual natural language inference and question answering, and use the GLUE benchmark to evaluate the English performance.\n\nCross-lingual Natural Language Inference.\n\nThe XNLI dataset (Conneau et al., 2018) comes with ground-truth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency.\n\nCross-lingual Question Answering. We use the MLQA and XQuad benchmark from  and Artetxe et al. (2019), which extends the English SQuAD benchmark to more languages.\n\nWe report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English.\n\nThe English GLUE Benchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018).",
            "reference_string": "[233481097 | Goyal et al. | 2021 | Citations: 129]"
        },
        {
            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 34,
            "citation_count": 58,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2106.16171, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1388156275",
                    "name": "Iulia Turc"
                },
                {
                    "authorId": "2110237268",
                    "name": "Kenton Lee"
                },
                {
                    "authorId": "144154709",
                    "name": "Jacob Eisenstein"
                },
                {
                    "authorId": "1744179",
                    "name": "Ming-Wei Chang"
                },
                {
                    "authorId": "3259253",
                    "name": "Kristina Toutanova"
                }
            ],
            "abstract": "Despite their success, large pre-trained multilingual models have not completely alleviated the need for labeled data, which is cumbersome to collect for all target languages. Zero-shot cross-lingual transfer is emerging as a practical solution: pre-trained models later fine-tuned on one transfer language exhibit surprising performance when tested on many target languages. English is the dominant source language for transfer, as reinforced by popular zero-shot benchmarks. However, this default choice has not been systematically vetted. In our study, we compare English against other transfer languages for fine-tuning, on two pre-trained multilingual models (mBERT and mT5) and multiple classification and question answering tasks. We find that other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori. Unexpectedly, this can be true even when the training sets were automatically translated from English. This finding can have immediate impact on multilingual zero-shot systems, and should inform future benchmark designs.",
            "corpus_id": 235683093,
            "sentences": [
                {
                    "corpus_id": "235683093",
                    "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
                    "text": "In this section, we list the standard multilingual benchmarks we selected for evaluation. The main desideratum for our datasets is that training data is available in multiple languages. Ideally, all such Model XNLI PAWS-X XQuAD mBERT mT5-Base Table 1: English was out-performed () by other source languages in 5/6 experimental settings. training sets would be produced by humans (or verifiably high-quality). In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer. We will be mindful of this when drawing conclusions from our experiments. \n\nTo ensure that all language-specific subsets have the same size and informational content, we occasionally depart from the established way of using some of these datasets, as elaborated below. \n\nXNLI The Cross-lingual Natural Language Inference corpus (Conneau et al., 2018) consists of premise/hypothesis pairs that are either entailments, contradictions, or neutral. XNLI extends the English MultiNLI dataset (Williams et al., 2018) to 15 languages, including low-resource ones such as Swahili and Urdu; training sets are machine-translated, while the development and test sets are human-translated. \n\nPAWS-X The Cross-lingual Paraphrase Adversaries from Word Scrambling corpus (Yang et al., 2019) is a binary classification task for paraphrase identification. Its 6 training sets were machinetranslated from the English PAWS dataset (Zhang et al., 2019). The development and test sets were human-translated. \n\nXQuAD The Cross-lingual Question Answering Dataset (Artetxe et al., 2020) requires answering questions by identifying answer spans in accompanying paragraphs. XQuAD consists of human translations of the development and test sets of the English SQuAD 1.1 corpus (Rajpurkar et al., 2016) into 10 languages.",
                    "score": 0.48889237960190035,
                    "section_title": "Datasets",
                    "char_start_offset": 10278,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 89
                        },
                        {
                            "start": 90,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 336
                        },
                        {
                            "start": 337,
                            "end": 408
                        },
                        {
                            "start": 409,
                            "end": 571
                        },
                        {
                            "start": 572,
                            "end": 781
                        },
                        {
                            "start": 782,
                            "end": 855
                        },
                        {
                            "start": 858,
                            "end": 1050
                        },
                        {
                            "start": 1053,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1459
                        },
                        {
                            "start": 1462,
                            "end": 1620
                        },
                        {
                            "start": 1621,
                            "end": 1715
                        },
                        {
                            "start": 1716,
                            "end": 1768
                        },
                        {
                            "start": 1771,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2075
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1110,
                            "end": 1132,
                            "matchedPaperCorpusId": "52271711"
                        },
                        {
                            "start": 1269,
                            "end": 1292,
                            "matchedPaperCorpusId": "3432876"
                        },
                        {
                            "start": 1538,
                            "end": 1557,
                            "matchedPaperCorpusId": "201698093"
                        },
                        {
                            "start": 1694,
                            "end": 1714,
                            "matchedPaperCorpusId": "91184042"
                        },
                        {
                            "start": 1822,
                            "end": 1844,
                            "matchedPaperCorpusId": "204901567"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90625
                }
            ],
            "relevance_judgement": 0.90625,
            "relevance_judgment_input_expanded": "# Title: Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer\n# Venue: arXiv.org\n# Authors: Iulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei Chang, Kristina Toutanova\n## Abstract\nDespite their success, large pre-trained multilingual models have not completely alleviated the need for labeled data, which is cumbersome to collect for all target languages. Zero-shot cross-lingual transfer is emerging as a practical solution: pre-trained models later fine-tuned on one transfer language exhibit surprising performance when tested on many target languages. English is the dominant source language for transfer, as reinforced by popular zero-shot benchmarks. However, this default choice has not been systematically vetted. In our study, we compare English against other transfer languages for fine-tuning, on two pre-trained multilingual models (mBERT and mT5) and multiple classification and question answering tasks. We find that other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori. Unexpectedly, this can be true even when the training sets were automatically translated from English. This finding can have immediate impact on multilingual zero-shot systems, and should inform future benchmark designs.\n## Datasets\nIn this section, we list the standard multilingual benchmarks we selected for evaluation. The main desideratum for our datasets is that training data is available in multiple languages. Ideally, all such Model XNLI PAWS-X XQuAD mBERT mT5-Base Table 1: English was out-performed () by other source languages in 5/6 experimental settings. training sets would be produced by humans (or verifiably high-quality). In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer. We will be mindful of this when drawing conclusions from our experiments. \n\nTo ensure that all language-specific subsets have the same size and informational content, we occasionally depart from the established way of using some of these datasets, as elaborated below. \n\nXNLI The Cross-lingual Natural Language Inference corpus (Conneau et al., 2018) consists of premise/hypothesis pairs that are either entailments, contradictions, or neutral. XNLI extends the English MultiNLI dataset (Williams et al., 2018) to 15 languages, including low-resource ones such as Swahili and Urdu; training sets are machine-translated, while the development and test sets are human-translated. \n\nPAWS-X The Cross-lingual Paraphrase Adversaries from Word Scrambling corpus (Yang et al., 2019) is a binary classification task for paraphrase identification. Its 6 training sets were machinetranslated from the English PAWS dataset (Zhang et al., 2019). The development and test sets were human-translated. \n\nXQuAD The Cross-lingual Question Answering Dataset (Artetxe et al., 2020) requires answering questions by identifying answer spans in accompanying paragraphs. XQuAD consists of human translations of the development and test sets of the English SQuAD 1.1 corpus (Rajpurkar et al., 2016) into 10 languages.",
            "reference_string": "[235683093 | Turc et al. | 2021 | Citations: 58]"
        },
        {
            "title": "Challenges and Strategies in Cross-Cultural NLP",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 169,
            "citation_count": 182,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.10020",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.10020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064295987",
                    "name": "Daniel Hershcovich"
                },
                {
                    "authorId": "37922370",
                    "name": "Stella Frank"
                },
                {
                    "authorId": "49568895",
                    "name": "Heather Lent"
                },
                {
                    "authorId": "3295381",
                    "name": "Miryam de Lhoneux"
                },
                {
                    "authorId": "30671790",
                    "name": "Mostafa Abdou"
                },
                {
                    "authorId": "6547490",
                    "name": "Stephanie Brandl"
                },
                {
                    "authorId": "83574123",
                    "name": "Emanuele Bugliarello"
                },
                {
                    "authorId": "2093582149",
                    "name": "Laura Cabello Piqueras"
                },
                {
                    "authorId": "2125376289",
                    "name": "Ilias Chalkidis"
                },
                {
                    "authorId": "1717462692",
                    "name": "Ruixiang Cui"
                },
                {
                    "authorId": "50110151",
                    "name": "Constanza Fierro"
                },
                {
                    "authorId": "82259306",
                    "name": "Katerina Margatina"
                },
                {
                    "authorId": "1660797358",
                    "name": "Phillip Rust"
                },
                {
                    "authorId": "1700187",
                    "name": "Anders S\u00f8gaard"
                }
            ],
            "abstract": "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",
            "corpus_id": 247594499,
            "sentences": [
                {
                    "corpus_id": "247594499",
                    "title": "Challenges and Strategies in Cross-Cultural NLP",
                    "text": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
                    "score": 0.47725431406912344,
                    "section_title": "Data Collection",
                    "char_start_offset": 25564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1107
                        },
                        {
                            "start": 1110,
                            "end": 1311
                        },
                        {
                            "start": 1312,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 2104
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 793,
                            "end": 811,
                            "matchedPaperCorpusId": "238198104"
                        },
                        {
                            "start": 1947,
                            "end": 1965,
                            "matchedPaperCorpusId": "238198104"
                        },
                        {
                            "start": 1965,
                            "end": 1984,
                            "matchedPaperCorpusId": "218470125"
                        },
                        {
                            "start": 1984,
                            "end": 2001,
                            "matchedPaperCorpusId": "237503047"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: Challenges and Strategies in Cross-Cultural NLP\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, Anders S\u00f8gaard\n## Abstract\nVarious efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.\n## Data Collection\nTranslating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
            "reference_string": "[247594499 | Hershcovich et al. | 2022 | Citations: 182]"
        },
        {
            "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 68,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-acl.842.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.18653/v1/2023.findings-acl.842?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.18653/v1/2023.findings-acl.842, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2119502119",
                    "name": "Nihar Ranjan Sahoo"
                },
                {
                    "authorId": "2166204873",
                    "name": "Niteesh Mallela"
                },
                {
                    "authorId": "145532184",
                    "name": "P. Bhattacharyya"
                }
            ],
            "abstract": ",",
            "corpus_id": 259859119,
            "sentences": [
                {
                    "corpus_id": "259859119",
                    "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
                    "text": "For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets. Similar to previous approach, we perform both zero-shot and sequential fine-tuning for translated datasets and report the results in table 5.  4: Comparison of monolingual fine-tuning vs multilingual fine-tuning for all datasets. Four source language models, XLM_ENG, XLM_HI, XLM_IT and XLM_KOR are the fine-tuned XLM-R models on English, Hindi, Italian and Korean datasets respectively. Last four columns correspond to sequential fine-tuning of all datasets using source language models. Best F1-scores are shown in bold.",
                    "score": 0.4627094743671982,
                    "section_title": "MTL based on Translation",
                    "char_start_offset": 18270,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 100
                        },
                        {
                            "start": 101,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 374
                        },
                        {
                            "start": 375,
                            "end": 604
                        },
                        {
                            "start": 605,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 897
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90283203125
                }
            ],
            "relevance_judgement": 0.90283203125,
            "relevance_judgment_input_expanded": "# Title: With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Nihar Ranjan Sahoo, Niteesh Mallela, P. Bhattacharyya\n## Abstract\n,\n## MTL based on Translation\nFor this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets. Similar to previous approach, we perform both zero-shot and sequential fine-tuning for translated datasets and report the results in table 5.  4: Comparison of monolingual fine-tuning vs multilingual fine-tuning for all datasets. Four source language models, XLM_ENG, XLM_HI, XLM_IT and XLM_KOR are the fine-tuned XLM-R models on English, Hindi, Italian and Korean datasets respectively. Last four columns correspond to sequential fine-tuning of all datasets using source language models. Best F1-scores are shown in bold.",
            "reference_string": "[259859119 | Sahoo et al. | 2023 | Citations: 1]"
        },
        {
            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 46,
            "citation_count": 8,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.04582",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.04582, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "36943315",
                    "name": "Leonhard Hennig"
                },
                {
                    "authorId": "143680610",
                    "name": "Philippe E. Thomas"
                },
                {
                    "authorId": "145733288",
                    "name": "Sebastian M\u00f6ller"
                }
            ],
            "abstract": "Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.",
            "corpus_id": 258557740,
            "sentences": [
                {
                    "corpus_id": "258557740",
                    "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
                    "text": "learning (Hu et al., 2020;Nag et al., 2021). Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;Ozaki et al., 2021;Yarmohammadi et al., 2021).\n\nThe MultiTACRED dataset we present in this work covers 12 languages from 9 language families. 1 We select typologically diverse languages which span a large set of linguistic phenomena such as compounding, inflection and pronoun-drop, and for which a monolingual pretrained language model is available. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances. In addition, we experimentally evaluate fine-tuned pretrained monoand multilingual language models (PLM) in common training scenarios, using source language (English), target language, or a mixture of both as training data. We also evaluate an English data fine-tuned model on back-translated test instances to estimate the effect of noise introduced by the MT system on model performance. Our results show that in-language training works well, given a suitable PLM. Cross-lingual zero-shot transfer is acceptable for languages well-represented in the multilingual PLM, and combining English and target language data for training considerably improves performance across the board.\n\nTo summarize, our work aims to answer the following research questions: Can we reaffirm the usefulness of MT and cross-lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can",
                    "score": 0.49807630239258394,
                    "section_title": "Introduction",
                    "char_start_offset": 1679,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 26,
                            "end": 43,
                            "matchedPaperCorpusId": "239024671"
                        },
                        {
                            "start": 278,
                            "end": 297,
                            "matchedPaperCorpusId": "233189585"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                },
                {
                    "corpus_id": "258557740",
                    "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
                    "text": "lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are:\n\n\u2022 We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese.\n\n\u2022 We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages.\n\n\u2022 We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors.",
                    "score": 0.5627699423191689,
                    "section_title": "Introduction",
                    "char_start_offset": 3611,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88427734375
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Leonhard Hennig, Philippe E. Thomas, Sebastian M\u00f6ller\n## Abstract\nRelation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original for many of the target languages, and that multilingual models trained on a combination of English and target language data can outperform their monolingual counterparts. However, we also observe a variety of translation and annotation projection errors, both due to the MT systems and linguistic features of the target languages, such as pronoun-dropping, compounding and inflection, that degrade dataset quality and RE model performance.\n## Introduction\nlearning (Hu et al., 2020;Nag et al., 2021). Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;Ozaki et al., 2021;Yarmohammadi et al., 2021).\n\nThe MultiTACRED dataset we present in this work covers 12 languages from 9 language families. 1 We select typologically diverse languages which span a large set of linguistic phenomena such as compounding, inflection and pronoun-drop, and for which a monolingual pretrained language model is available. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances. In addition, we experimentally evaluate fine-tuned pretrained monoand multilingual language models (PLM) in common training scenarios, using source language (English), target language, or a mixture of both as training data. We also evaluate an English data fine-tuned model on back-translated test instances to estimate the effect of noise introduced by the MT system on model performance. Our results show that in-language training works well, given a suitable PLM. Cross-lingual zero-shot transfer is acceptable for languages well-represented in the multilingual PLM, and combining English and target language data for training considerably improves performance across the board.\n\nTo summarize, our work aims to answer the following research questions: Can we reaffirm the usefulness of MT and cross-lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can\n...\nlingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are:\n\n\u2022 We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese.\n\n\u2022 We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages.\n\n\u2022 We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors.",
            "reference_string": "[258557740 | Hennig et al. | 2023 | Citations: 8]"
        },
        {
            "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 41,
            "citation_count": 91,
            "influential_citation_count": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P19-1227.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/P19-1227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46701066",
                    "name": "Jiahua Liu"
                },
                {
                    "authorId": "2427350",
                    "name": "Yankai Lin"
                },
                {
                    "authorId": "49293587",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "1753344",
                    "name": "Maosong Sun"
                }
            ],
            "abstract": "Open-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.",
            "corpus_id": 196174566,
            "sentences": [
                {
                    "corpus_id": "196174566",
                    "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
                    "text": "In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish,  Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. \n\nThe experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is. Based on the results, we further discuss potential improvement for cross-lingual OpenQA systems. \n\nWe will release the dataset and baseline systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding.",
                    "score": 0.51015903027964,
                    "section_title": "Introduction",
                    "char_start_offset": 2104,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 70
                        },
                        {
                            "start": 71,
                            "end": 517
                        },
                        {
                            "start": 518,
                            "end": 675
                        },
                        {
                            "start": 678,
                            "end": 804
                        },
                        {
                            "start": 805,
                            "end": 981
                        },
                        {
                            "start": 982,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1262
                        },
                        {
                            "start": 1265,
                            "end": 1451
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 494,
                            "end": 515,
                            "matchedPaperCorpusId": "52967399"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9013671875
                }
            ],
            "relevance_judgement": 0.9013671875,
            "relevance_judgment_input_expanded": "# Title: XQA: A Cross-lingual Open-domain Question Answering Dataset\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Jiahua Liu, Yankai Lin, Zhiyuan Liu, Maosong Sun\n## Abstract\nOpen-domain question answering (OpenQA) aims to answer questions through text retrieval and reading comprehension. Recently, lots of neural network-based models have been proposed and achieved promising results in OpenQA. However, the success of these models relies on a massive volume of training data (usually in English), which is not available in many other languages, especially for those low-resource languages. Therefore, it is essential to investigate cross-lingual OpenQA. In this paper, we construct a novel dataset XQA for cross-lingual OpenQA research. It consists of a training set in English as well as development and test sets in eight other languages. Besides, we provide several baseline systems for cross-lingual OpenQA, including two machine translation-based methods and one zero-shot cross-lingual method (multilingual BERT). Experimental results show that the multilingual BERT model achieves the best results in almost all target languages, while the performance of cross-lingual OpenQA is still much lower than that of English. Our analysis indicates that the performance of cross-lingual OpenQA is related to not only how similar the target language and English are, but also how difficult the question set of the target language is. The XQA dataset is publicly available at http://github.com/thunlp/XQA.\n## Introduction\nIn this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish,  Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. \n\nThe experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is. Based on the results, we further discuss potential improvement for cross-lingual OpenQA systems. \n\nWe will release the dataset and baseline systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding.",
            "reference_string": "[196174566 | Liu et al. | 2019 | Citations: 91]"
        },
        {
            "title": "From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?",
            "venue": "EVAL4NLP",
            "year": 2022,
            "reference_count": 51,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2022.eval4nlp-1.3, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2040867021",
                    "name": "Mateusz Krubi\u0144ski"
                },
                {
                    "authorId": "1758528",
                    "name": "Pavel Pecina"
                }
            ],
            "abstract": ",",
            "corpus_id": 253762032,
            "sentences": [
                {
                    "corpus_id": "253762032",
                    "title": "From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?",
                    "text": "One of the strengths of the COMET metric is its multilinguality -the model has seen over 30 language pairs during training. To assess its quality as a summary evaluation tool for non-English data, we evaluated it on the Multi_SummEval dataset (Koto et al., 2021). With only two system outputs annotated (along the Focus and Coverage dimensions), the size of the resource is not sufficient for reporting system-level correlations. Thus, we report the summary-level (segment-level) Pearson correlations. \n\nFor a fair comparison, we wanted to train the COMES model variant using the multilingual data. Due to the lack of sufficient resources, we fall back on using automatic machine translation to translate the English annotated data. This approach has proven successful for e.g. Question Answering (Lewis et al., 2020b;Mackov\u00e1 and Straka, 2020). We limit our analysis to the subset of languages from Multi_SummEval that originates from the MLSUM (Scialom et al., 2020) corpus. We have translated SummEval into German, French, Russian, Turkish and Spanish using the uni-directional models provided by the Helsinki-NLP group (Tiedemann, 2020) and used the data (together with the original SummEval) to train a multilingual COMES model (COMES_MT_ML). \n\nOur findings indicate that in the summary-level evaluation, the original COMET metric is superior to any other variant considered, clearly outperforming the reference-less variant COMET_QE. Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by Braun et al. (2022), which indicate that summary evaluations do not survive translation. On this dataset, even the best performing COMET is still inferior to both ROUGE and BERTScore. Considering, however, the relatively small size of the dataset (270 instances per language, outputs from two systems) we believe that the question about COMET/COMES usefulness for multilingual and summary-level evaluation is still open. For the full results, see Table 8 in Appendix.",
                    "score": 0.46486934474358976,
                    "section_title": "Non-English data",
                    "char_start_offset": 10373,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 123
                        },
                        {
                            "start": 124,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 501
                        },
                        {
                            "start": 504,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 975
                        },
                        {
                            "start": 976,
                            "end": 1246
                        },
                        {
                            "start": 1249,
                            "end": 1438
                        },
                        {
                            "start": 1439,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1775
                        },
                        {
                            "start": 1776,
                            "end": 2012
                        },
                        {
                            "start": 2013,
                            "end": 2059
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 243,
                            "end": 262,
                            "matchedPaperCorpusId": "235313819"
                        },
                        {
                            "start": 797,
                            "end": 818,
                            "matchedPaperCorpusId": "204734128"
                        },
                        {
                            "start": 818,
                            "end": 843,
                            "matchedPaperCorpusId": "220347499"
                        },
                        {
                            "start": 945,
                            "end": 967,
                            "matchedPaperCorpusId": "216868259"
                        },
                        {
                            "start": 1122,
                            "end": 1139,
                            "matchedPaperCorpusId": "222310761"
                        },
                        {
                            "start": 1592,
                            "end": 1611,
                            "matchedPaperCorpusId": "237532546"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89892578125
                }
            ],
            "relevance_judgement": 0.89892578125,
            "relevance_judgment_input_expanded": "# Title: From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?\n# Venue: EVAL4NLP\n# Authors: Mateusz Krubi\u0144ski, Pavel Pecina\n## Abstract\n,\n## Non-English data\nOne of the strengths of the COMET metric is its multilinguality -the model has seen over 30 language pairs during training. To assess its quality as a summary evaluation tool for non-English data, we evaluated it on the Multi_SummEval dataset (Koto et al., 2021). With only two system outputs annotated (along the Focus and Coverage dimensions), the size of the resource is not sufficient for reporting system-level correlations. Thus, we report the summary-level (segment-level) Pearson correlations. \n\nFor a fair comparison, we wanted to train the COMES model variant using the multilingual data. Due to the lack of sufficient resources, we fall back on using automatic machine translation to translate the English annotated data. This approach has proven successful for e.g. Question Answering (Lewis et al., 2020b;Mackov\u00e1 and Straka, 2020). We limit our analysis to the subset of languages from Multi_SummEval that originates from the MLSUM (Scialom et al., 2020) corpus. We have translated SummEval into German, French, Russian, Turkish and Spanish using the uni-directional models provided by the Helsinki-NLP group (Tiedemann, 2020) and used the data (together with the original SummEval) to train a multilingual COMES model (COMES_MT_ML). \n\nOur findings indicate that in the summary-level evaluation, the original COMET metric is superior to any other variant considered, clearly outperforming the reference-less variant COMET_QE. Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by Braun et al. (2022), which indicate that summary evaluations do not survive translation. On this dataset, even the best performing COMET is still inferior to both ROUGE and BERTScore. Considering, however, the relatively small size of the dataset (270 instances per language, outputs from two systems) we believe that the question about COMET/COMES usefulness for multilingual and summary-level evaluation is still open. For the full results, see Table 8 in Appendix.",
            "reference_string": "[253762032 | Krubinski et al. | 2022 | Citations: 3]"
        },
        {
            "title": "Multilingual Multimodal Learning with Machine Translated Text",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 40,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.13134",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.13134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3383271",
                    "name": "Chen Qiu"
                },
                {
                    "authorId": "3095774",
                    "name": "Dan Onea\u0163\u0103"
                },
                {
                    "authorId": "83574123",
                    "name": "Emanuele Bugliarello"
                },
                {
                    "authorId": "37922370",
                    "name": "Stella Frank"
                },
                {
                    "authorId": "50369944",
                    "name": "Desmond Elliott"
                }
            ],
            "abstract": "Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper, we investigate whether machine translating English multimodal data can be an effective proxy for the lack of readily available multilingual data. We call this framework TD-MML: Translated Data for Multilingual Multimodal Learning, and it can be applied to any multimodal dataset and model. We apply it to both pretraining and fine-tuning data with a state-of-the-art model. In order to prevent models from learning from low-quality translated text, we propose two metrics for automatically removing such translations from the resulting datasets. In experiments on five tasks across 20 languages in the IGLUE benchmark, we show that translated data can provide a useful signal for multilingual multimodal learning, both at pretraining and fine-tuning.",
            "corpus_id": 253098629,
            "sentences": [
                {
                    "corpus_id": "253098629",
                    "title": "Multilingual Multimodal Learning with Machine Translated Text",
                    "text": "Our paper investigates the benefits and limitations of machine translated data towards multilingual multimodal learning. In doing so, we solely rely on the M2M-100 model (Fan et al., 2021). This is a large, multi-to-multi translation system, which proved to be easy to use. Our analyses and results are based on the performance of this model. It would be instructive to investigate how the expected performance of translation systems6 affects (i) the proportion of sentences with high 'badness' scores, and (ii) the resulting performance of the multilingual multimodal systems. Moreover, while machine translating a large corpus is a cheaper effort than manually translating the data or scraping it from the web, there is still a one-time effort required to translate the data before using it for training new models. Therefore, we release our multilingual pretraining and fine-tuning datasets. \n\nFrom an experimental angle, although the proposed framework can be applied to any existing architecture, we only evaluate a single model due to computational constraints. \n\nWe would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones. \n\nFinally, a core limitation of the overall translate data framework is that it centers English as the source language. For example, this means only concepts mentioned in English captions can be grounded across languages (Liu et al., 2021), and hence some non-English concepts might never be modelled. However, we show that machine translating data provides a strong starting point that can effortlessly be integrated in a pipeline, upon which language-specific annotations can be added.",
                    "score": 0.46598169581600146,
                    "section_title": "Limitations",
                    "char_start_offset": 27304,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 120
                        },
                        {
                            "start": 121,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 273
                        },
                        {
                            "start": 274,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 577
                        },
                        {
                            "start": 578,
                            "end": 817
                        },
                        {
                            "start": 818,
                            "end": 894
                        },
                        {
                            "start": 897,
                            "end": 1067
                        },
                        {
                            "start": 1070,
                            "end": 1217
                        },
                        {
                            "start": 1218,
                            "end": 1426
                        },
                        {
                            "start": 1429,
                            "end": 1546
                        },
                        {
                            "start": 1547,
                            "end": 1728
                        },
                        {
                            "start": 1729,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 170,
                            "end": 188,
                            "matchedPaperCorpusId": "224814118"
                        },
                        {
                            "start": 1648,
                            "end": 1666,
                            "matchedPaperCorpusId": "238198104"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8955078125
                }
            ],
            "relevance_judgement": 0.8955078125,
            "relevance_judgment_input_expanded": "# Title: Multilingual Multimodal Learning with Machine Translated Text\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Chen Qiu, Dan Onea\u0163\u0103, Emanuele Bugliarello, Stella Frank, Desmond Elliott\n## Abstract\nMost vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper, we investigate whether machine translating English multimodal data can be an effective proxy for the lack of readily available multilingual data. We call this framework TD-MML: Translated Data for Multilingual Multimodal Learning, and it can be applied to any multimodal dataset and model. We apply it to both pretraining and fine-tuning data with a state-of-the-art model. In order to prevent models from learning from low-quality translated text, we propose two metrics for automatically removing such translations from the resulting datasets. In experiments on five tasks across 20 languages in the IGLUE benchmark, we show that translated data can provide a useful signal for multilingual multimodal learning, both at pretraining and fine-tuning.\n## Limitations\nOur paper investigates the benefits and limitations of machine translated data towards multilingual multimodal learning. In doing so, we solely rely on the M2M-100 model (Fan et al., 2021). This is a large, multi-to-multi translation system, which proved to be easy to use. Our analyses and results are based on the performance of this model. It would be instructive to investigate how the expected performance of translation systems6 affects (i) the proportion of sentences with high 'badness' scores, and (ii) the resulting performance of the multilingual multimodal systems. Moreover, while machine translating a large corpus is a cheaper effort than manually translating the data or scraping it from the web, there is still a one-time effort required to translate the data before using it for training new models. Therefore, we release our multilingual pretraining and fine-tuning datasets. \n\nFrom an experimental angle, although the proposed framework can be applied to any existing architecture, we only evaluate a single model due to computational constraints. \n\nWe would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones. \n\nFinally, a core limitation of the overall translate data framework is that it centers English as the source language. For example, this means only concepts mentioned in English captions can be grounded across languages (Liu et al., 2021), and hence some non-English concepts might never be modelled. However, we show that machine translating data provides a strong starting point that can effortlessly be integrated in a pipeline, upon which language-specific annotations can be added.",
            "reference_string": "[253098629 | Qiu et al. | 2022 | Citations: 15]"
        },
        {
            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 76,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.02331, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1484057728",
                    "name": "chaeHun Park"
                },
                {
                    "authorId": "2110019663",
                    "name": "Ko-tik Lee"
                },
                {
                    "authorId": "2151202457",
                    "name": "Hyesu Lim"
                },
                {
                    "authorId": "2293573807",
                    "name": "Jaeseok Kim"
                },
                {
                    "authorId": "2305630071",
                    "name": "Junmo Park"
                },
                {
                    "authorId": "2293447208",
                    "name": "Yu-Jung Heo"
                },
                {
                    "authorId": "2293430063",
                    "name": "Du-Seong Chang"
                },
                {
                    "authorId": "2260653165",
                    "name": "Jaegul Choo"
                }
            ],
            "abstract": "Building a reliable visual question answering~(VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training. To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task. This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test). However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts. We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes. In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts.",
            "corpus_id": 270226445,
            "sentences": [
                {
                    "corpus_id": "270226445",
                    "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
                    "text": "We evaluate models trained on the xGQA dataset with MaXM (Changpinyo et al., 2023), a recently proposed evaluation-only benchmark for multilingual VQA.The MaXM dataset covers seven different languages: English (en), French (fr), Hindi (hi), Hebrew (iw), Romanian (ro), Thai (th), and Chinese (zh  7. The accuracy of the original English evaluation set is increased in both MERGE and TAG compared to solely using translated samples.The overall scores for the translate-test are also improved with data augmentation.These results indicate that augmenting training data with both human and MT texts is helpful for cross-lingual transfer while maintaining its performance on the original English texts.",
                    "score": 0.4779062453118714,
                    "section_title": "Experiments with MaXM dataset",
                    "char_start_offset": 15544,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 151,
                            "end": 431
                        },
                        {
                            "start": 431,
                            "end": 514
                        },
                        {
                            "start": 514,
                            "end": 698
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 57,
                            "end": 82,
                            "matchedPaperCorpusId": "232417173"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89453125
                },
                {
                    "corpus_id": "270226445",
                    "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
                    "text": "Based on our previous results, we confirm that addressing the misalignment of data origins between training and evaluation is effective for the translatetest approach.We now aim to understand how these benefits vary with changes in the MT systems or translation setups.To this end, we conduct experiments by varying (1) the MT system used for translating the training and evaluation sets and (2) the pivot language during the RT translation.\n\nVaried MT systems We use the following four MT systems in our experiments: M2M-100-418M/1.2B(Fan et al., 2021) and NLLB-200-600M/-3.3B(Costa-juss\u00e0 et al., 2022).Each MT system is used to make RT-translated training and translate-test evaluation sets.In detail, we use RT translation with different MT systems to make training sets, and the pivot language is fixed to German (de).All models described in Section 3.1.2are individually trained on these four RT-translated datasets.For the evaluation set, we translate every  target language into English using different MT systems, resulting in four different evaluation sets.\n\nEvaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer.In terms of MT system comparison, models usually show higher accuracy when MT systems used to make training and evaluation sets are in the same model family.In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts.",
                    "score": 0.5089005205518574,
                    "section_title": "Varying NMT and Pivot Languages",
                    "char_start_offset": 13183,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 167,
                            "end": 269
                        },
                        {
                            "start": 269,
                            "end": 441
                        },
                        {
                            "start": 443,
                            "end": 535
                        },
                        {
                            "start": 535,
                            "end": 577
                        },
                        {
                            "start": 577,
                            "end": 604
                        },
                        {
                            "start": 604,
                            "end": 693
                        },
                        {
                            "start": 693,
                            "end": 822
                        },
                        {
                            "start": 822,
                            "end": 859
                        },
                        {
                            "start": 859,
                            "end": 921
                        },
                        {
                            "start": 921,
                            "end": 1066
                        },
                        {
                            "start": 1068,
                            "end": 1224
                        },
                        {
                            "start": 1224,
                            "end": 1429
                        },
                        {
                            "start": 1429,
                            "end": 1586
                        },
                        {
                            "start": 1586,
                            "end": 1710
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 577,
                            "end": 603,
                            "matchedPaperCorpusId": "258615266"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8828125
                }
            ],
            "relevance_judgement": 0.89453125,
            "relevance_judgment_input_expanded": "# Title: Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: chaeHun Park, Ko-tik Lee, Hyesu Lim, Jaeseok Kim, Junmo Park, Yu-Jung Heo, Du-Seong Chang, Jaegul Choo\n## Abstract\nBuilding a reliable visual question answering~(VQA) system across different languages is a challenging problem, primarily due to the lack of abundant samples for training. To address this challenge, recent studies have employed machine translation systems for the cross-lingual VQA task. This involves translating the evaluation samples into a source language (usually English) and using monolingual models (i.e., translate-test). However, our analysis reveals that translated texts contain unique characteristics distinct from human-written ones, referred to as translation artifacts. We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes. In light of this, we present a simple data augmentation strategy that can alleviate the adverse impacts of translation artifacts.\n## Varying NMT and Pivot Languages\nBased on our previous results, we confirm that addressing the misalignment of data origins between training and evaluation is effective for the translatetest approach.We now aim to understand how these benefits vary with changes in the MT systems or translation setups.To this end, we conduct experiments by varying (1) the MT system used for translating the training and evaluation sets and (2) the pivot language during the RT translation.\n\nVaried MT systems We use the following four MT systems in our experiments: M2M-100-418M/1.2B(Fan et al., 2021) and NLLB-200-600M/-3.3B(Costa-juss\u00e0 et al., 2022).Each MT system is used to make RT-translated training and translate-test evaluation sets.In detail, we use RT translation with different MT systems to make training sets, and the pivot language is fixed to German (de).All models described in Section 3.1.2are individually trained on these four RT-translated datasets.For the evaluation set, we translate every  target language into English using different MT systems, resulting in four different evaluation sets.\n\nEvaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer.In terms of MT system comparison, models usually show higher accuracy when MT systems used to make training and evaluation sets are in the same model family.In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts.\n\n## Experiments with MaXM dataset\nWe evaluate models trained on the xGQA dataset with MaXM (Changpinyo et al., 2023), a recently proposed evaluation-only benchmark for multilingual VQA.The MaXM dataset covers seven different languages: English (en), French (fr), Hindi (hi), Hebrew (iw), Romanian (ro), Thai (th), and Chinese (zh  7. The accuracy of the original English evaluation set is increased in both MERGE and TAG compared to solely using translated samples.The overall scores for the translate-test are also improved with data augmentation.These results indicate that augmenting training data with both human and MT texts is helpful for cross-lingual transfer while maintaining its performance on the original English texts.",
            "reference_string": "[270226445 | Park et al. | 2024 | Citations: 3]"
        },
        {
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 47,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2105.06813, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2091578175",
                    "name": "G. Rosa"
                },
                {
                    "authorId": "2003019597",
                    "name": "L. Bonifacio"
                },
                {
                    "authorId": "2092809913",
                    "name": "Leandro Rodrigues de Souza"
                },
                {
                    "authorId": "1809633",
                    "name": "R. Lotufo"
                },
                {
                    "authorId": "143744603",
                    "name": "Rodrigo Nogueira"
                }
            ],
            "abstract": "An effective method for cross-lingual transfer is to fine-tune a bilingual or multilingual model on a supervised dataset in one language and evaluating it on another language in a zero-shot manner. Translating examples at training time or inference time are also viable alternatives. However, there are costs associated with these methods that are rarely addressed in the literature. In this work, we analyze cross-lingual methods in terms of their effectiveness (e.g., accuracy), development and deployment costs, as well as their latencies at inference time. Our experiments on three tasks indicate that the best cross-lingual method is highly task-dependent. Finally, by combining zero-shot and translation methods, we achieve the state-of-the-art in two of the three datasets used in this work. Based on these results, we question the need for manually labeled training data in a target language. Code and translated datasets are available at https://github.com/unicamp-dl/cross-lingual-analysis",
            "corpus_id": 234679313,
            "sentences": [
                {
                    "corpus_id": "234679313",
                    "title": "A cost-benefit analysis of cross-lingual transfer methods",
                    "text": "An effective method for cross-lingual transfer is to fine-tune a bilingual or multilingual model on a supervised dataset in one language and evaluating it on another language in a zero-shot manner. Translating examples at training time or inference time are also viable alternatives. However, there are costs associated with these methods that are rarely addressed in the literature. In this work, we analyze cross-lingual methods in terms of their effectiveness (e.g., accuracy), development and deployment costs, as well as their latencies at inference time. Our experiments on three tasks indicate that the best cross-lingual method is highly task-dependent. Finally, by combining zero-shot and translation methods, we achieve the state-of-the-art in two of the three datasets used in this work. Based on these results, we question the need for manually labeled training data in a target language. Code and translated datasets are available at https://github.com/unicamp-dl/cross-lingual-analysis",
                    "score": 0.5323296197134935,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89404296875
                }
            ],
            "relevance_judgement": 0.89404296875,
            "relevance_judgment_input_expanded": "# Title: A cost-benefit analysis of cross-lingual transfer methods\n# Venue: arXiv.org\n# Authors: G. Rosa, L. Bonifacio, Leandro Rodrigues de Souza, R. Lotufo, Rodrigo Nogueira\n## Abstract\nAn effective method for cross-lingual transfer is to fine-tune a bilingual or multilingual model on a supervised dataset in one language and evaluating it on another language in a zero-shot manner. Translating examples at training time or inference time are also viable alternatives. However, there are costs associated with these methods that are rarely addressed in the literature. In this work, we analyze cross-lingual methods in terms of their effectiveness (e.g., accuracy), development and deployment costs, as well as their latencies at inference time. Our experiments on three tasks indicate that the best cross-lingual method is highly task-dependent. Finally, by combining zero-shot and translation methods, we achieve the state-of-the-art in two of the three datasets used in this work. Based on these results, we question the need for manually labeled training data in a target language. Code and translated datasets are available at https://github.com/unicamp-dl/cross-lingual-analysis\n",
            "reference_string": "[234679313 | Rosa et al. | 2021 | Citations: 12]"
        },
        {
            "title": "SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment",
            "venue": "International Workshop on Semantic Evaluation",
            "year": 2020,
            "reference_count": 68,
            "citation_count": 8,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2020.semeval-1.2.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2020.semeval-1.2, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2472657",
                    "name": "Goran Glavas"
                },
                {
                    "authorId": "1747849",
                    "name": "Ivan Vulic"
                },
                {
                    "authorId": "145762466",
                    "name": "A. Korhonen"
                },
                {
                    "authorId": "2029669151",
                    "name": "Simone Paolo Ponzetto"
                }
            ],
            "abstract": "Lexical entailment (LE) is a fundamental asymmetric lexico-semantic relation, supporting the hierarchies in lexical resources (e.g., WordNet, ConceptNet) and applications like natural language inference and taxonomy induction. Multilingual and cross-lingual NLP applications warrant models for LE detection that go beyond language boundaries. As part of SemEval 2020, we carried out a shared task (Task 2) on multilingual and cross-lingual LE. The shared task spans three dimensions: (1) monolingual vs. cross-lingual LE, (2) binary vs. graded LE, and (3) a set of 6 diverse languages (and 15 corresponding language pairs). We offered two different evaluation tracks: (a) Dist: for unsupervised, fully distributional models that capture LE solely on the basis of unannotated corpora, and (b) Any: for externally informed models, allowed to leverage any resources, including lexico-semantic networks (e.g., WordNet or BabelNet). In the Any track, we recieved runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction.",
            "corpus_id": 227231710,
            "sentences": [
                {
                    "corpus_id": "227231710",
                    "title": "SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment",
                    "text": "We first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vuli\u0107 et al., 2019b). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets. \n\nWe have followed the standard word pair translation procedure (Leviant and Reichart, 2015;Camacho-Collados et al., 2017)  of the target language. We observed the translation agreement in the range of 80%-90% across the five target languages. Translation disagreements were resolved by a third annotator who selected the better of the two differing translations. We allowed for multi-word translations only if there was no appropriate single word translation, e.g., typewriter (EN) \u2192 pisa\u0107i stroj (HR). \n\nConcept Scoring and Cross-Lingual Datasets. The resulting 2,616 concept pairs in all five target languages were annotated using a procedure analogous to that for EN HyperLex: the rating interval was [0, 6], and each word pair was rated by 4 (for DE, IT, HR) or 5 (for TR, SQ) native speakers. We then constructed the cross-lingual datasets automatically, leveraging word pair translations and scores in five target languages.",
                    "score": 0.5087390774158912,
                    "section_title": "Data",
                    "char_start_offset": 5687,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 158
                        },
                        {
                            "start": 159,
                            "end": 291
                        },
                        {
                            "start": 292,
                            "end": 410
                        },
                        {
                            "start": 411,
                            "end": 617
                        },
                        {
                            "start": 618,
                            "end": 739
                        },
                        {
                            "start": 740,
                            "end": 915
                        },
                        {
                            "start": 916,
                            "end": 1011
                        },
                        {
                            "start": 1014,
                            "end": 1159
                        },
                        {
                            "start": 1160,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1515
                        },
                        {
                            "start": 1518,
                            "end": 1561
                        },
                        {
                            "start": 1562,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 1943
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 136,
                            "end": 157,
                            "matchedPaperCorpusId": "196183388"
                        },
                        {
                            "start": 884,
                            "end": 914,
                            "matchedPaperCorpusId": "7665329"
                        },
                        {
                            "start": 1104,
                            "end": 1134,
                            "matchedPaperCorpusId": "7665329"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment\n# Venue: International Workshop on Semantic Evaluation\n# Authors: Goran Glavas, Ivan Vulic, A. Korhonen, Simone Paolo Ponzetto\n## Abstract\nLexical entailment (LE) is a fundamental asymmetric lexico-semantic relation, supporting the hierarchies in lexical resources (e.g., WordNet, ConceptNet) and applications like natural language inference and taxonomy induction. Multilingual and cross-lingual NLP applications warrant models for LE detection that go beyond language boundaries. As part of SemEval 2020, we carried out a shared task (Task 2) on multilingual and cross-lingual LE. The shared task spans three dimensions: (1) monolingual vs. cross-lingual LE, (2) binary vs. graded LE, and (3) a set of 6 diverse languages (and 15 corresponding language pairs). We offered two different evaluation tracks: (a) Dist: for unsupervised, fully distributional models that capture LE solely on the basis of unannotated corpora, and (b) Any: for externally informed models, allowed to leverage any resources, including lexico-semantic networks (e.g., WordNet or BabelNet). In the Any track, we recieved runs that push state-of-the-art across all languages and language pairs, for both binary LE detection and graded LE prediction.\n## Data\nWe first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vuli\u0107 et al., 2019b). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets. \n\nWe have followed the standard word pair translation procedure (Leviant and Reichart, 2015;Camacho-Collados et al., 2017)  of the target language. We observed the translation agreement in the range of 80%-90% across the five target languages. Translation disagreements were resolved by a third annotator who selected the better of the two differing translations. We allowed for multi-word translations only if there was no appropriate single word translation, e.g., typewriter (EN) \u2192 pisa\u0107i stroj (HR). \n\nConcept Scoring and Cross-Lingual Datasets. The resulting 2,616 concept pairs in all five target languages were annotated using a procedure analogous to that for EN HyperLex: the rating interval was [0, 6], and each word pair was rated by 4 (for DE, IT, HR) or 5 (for TR, SQ) native speakers. We then constructed the cross-lingual datasets automatically, leveraging word pair translations and scores in five target languages.",
            "reference_string": "[227231710 | Glavas et al. | 2020 | Citations: 8]"
        },
        {
            "title": "Modelling Latent Translations for Cross-Lingual Transfer",
            "venue": "arXiv.org",
            "year": 2021,
            "reference_count": 59,
            "citation_count": 18,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2107.11353, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3381663",
                    "name": "E. Ponti"
                },
                {
                    "authorId": "3422710",
                    "name": "Julia Kreutzer"
                },
                {
                    "authorId": "1747849",
                    "name": "Ivan Vulic"
                },
                {
                    "authorId": "145732771",
                    "name": "Siva Reddy"
                }
            ],
            "abstract": "While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation. To remedy this, we propose a new technique that integrates both steps of the traditional pipeline (translation and classification) into a single model, by treating the intermediate translations as a latent random variable. As a result, 1) the neural machine translation system can be fine-tuned with a variant of Minimum Risk Training where the reward is the accuracy of the downstream task classifier. Moreover, 2) multiple samples can be drawn to approximate the expected loss across all possible translations during inference. We evaluate our novel latent translation-based model on a series of multilingual NLU tasks, including commonsense reasoning, paraphrase identification, and natural language inference. We report gains for both zero-shot and few-shot learning setups, up to 2.7 accuracy points on average, which are even more prominent for low-resource languages (e.g., Haitian Creole). Finally, we carry out in-depth analyses comparing different underlying NMT models and assessing the impact of alternative translations on the downstream performance.",
            "corpus_id": 236318351,
            "sentences": [
                {
                    "corpus_id": "236318351",
                    "title": "Modelling Latent Translations for Cross-Lingual Transfer",
                    "text": "Cross-lingual knowledge transfer supports the development of natural language technology for many of the world's languages (Ruder et al., 2019;Ponti et al., 2019, inter alia). The approach currently predominant for cross-lingual transfer relies on massively multilingual pre-trained encoders (Conneau et al., 2020;Xue et al., 2020;Liu et al., 2020) that are fine-tuned on a source language and perform zero-shot (Wu and Dredze, 2019;Ponti et al., 2021) or few-shot (Lauscher et al., 2020;Zhao et al., 2020) prediction in a target language. \n\nAn alternative approach for cross-lingual transfer, translate test, is based on translating the evaluation set into the source language and leveraging a monolingual classifier instead (Banea et al., 2008;Durrett et al., 2012;Conneau et al., 2018). This approach is currently under-investigated and usually relegated to the role of a baseline due to its lower flexibility, e.g., it is not suitable for sequence labelling tasks. Yet, it achieves the state-of-the-art results in most benchmarks for multilingual Natural Language Understanding and Question Answering tasks (Hu et al., 2020;Ponti et al., 2020;Ruder et al., 2021, inter alia). Moreover, the availability of off-the-shelf translation models for multiple languages (Wu et al., 2016;Tiedemann and Thottingal, 2020;Liu et al., 2020) provides coverage for transfer to a large number of target languages. Indeed, very recent preliminary results suggest that the translation-based transfer might even outperform monolingual pre-trained models in languages different from English (Isbister et al., 2021). \n\nTranslation-based transfer, however, currently suffers from two main limitations. First, the errors in translation accumulate along the pipeline. In fact, sentences that are possibly not faithful to the original in the target language and/or not grammatical in the source language are fed to the classifier, which degrades its performance.",
                    "score": 0.4914709974101118,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 539
                        },
                        {
                            "start": 542,
                            "end": 789
                        },
                        {
                            "start": 790,
                            "end": 968
                        },
                        {
                            "start": 969,
                            "end": 1179
                        },
                        {
                            "start": 1180,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1599
                        },
                        {
                            "start": 1602,
                            "end": 1683
                        },
                        {
                            "start": 1684,
                            "end": 1747
                        },
                        {
                            "start": 1748,
                            "end": 1941
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 123,
                            "end": 143,
                            "matchedPaperCorpusId": "26127787"
                        },
                        {
                            "start": 292,
                            "end": 314,
                            "matchedPaperCorpusId": "207880568"
                        },
                        {
                            "start": 331,
                            "end": 348,
                            "matchedPaperCorpusId": "215433131"
                        },
                        {
                            "start": 412,
                            "end": 433,
                            "matchedPaperCorpusId": "126167342"
                        },
                        {
                            "start": 433,
                            "end": 452,
                            "matchedPaperCorpusId": "210966228"
                        },
                        {
                            "start": 465,
                            "end": 488,
                            "matchedPaperCorpusId": "226262344"
                        },
                        {
                            "start": 726,
                            "end": 746,
                            "matchedPaperCorpusId": "2432354"
                        },
                        {
                            "start": 746,
                            "end": 767,
                            "matchedPaperCorpusId": "2895378"
                        },
                        {
                            "start": 767,
                            "end": 788,
                            "matchedPaperCorpusId": "52271711"
                        },
                        {
                            "start": 1111,
                            "end": 1128,
                            "matchedPaperCorpusId": "214641214"
                        },
                        {
                            "start": 1128,
                            "end": 1147,
                            "matchedPaperCorpusId": "218470125"
                        },
                        {
                            "start": 1283,
                            "end": 1314,
                            "matchedPaperCorpusId": "221097277"
                        },
                        {
                            "start": 1314,
                            "end": 1331,
                            "matchedPaperCorpusId": "215433131"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: Modelling Latent Translations for Cross-Lingual Transfer\n# Venue: arXiv.org\n# Authors: E. Ponti, Julia Kreutzer, Ivan Vulic, Siva Reddy\n## Abstract\nWhile achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation. To remedy this, we propose a new technique that integrates both steps of the traditional pipeline (translation and classification) into a single model, by treating the intermediate translations as a latent random variable. As a result, 1) the neural machine translation system can be fine-tuned with a variant of Minimum Risk Training where the reward is the accuracy of the downstream task classifier. Moreover, 2) multiple samples can be drawn to approximate the expected loss across all possible translations during inference. We evaluate our novel latent translation-based model on a series of multilingual NLU tasks, including commonsense reasoning, paraphrase identification, and natural language inference. We report gains for both zero-shot and few-shot learning setups, up to 2.7 accuracy points on average, which are even more prominent for low-resource languages (e.g., Haitian Creole). Finally, we carry out in-depth analyses comparing different underlying NMT models and assessing the impact of alternative translations on the downstream performance.\n## Introduction\nCross-lingual knowledge transfer supports the development of natural language technology for many of the world's languages (Ruder et al., 2019;Ponti et al., 2019, inter alia). The approach currently predominant for cross-lingual transfer relies on massively multilingual pre-trained encoders (Conneau et al., 2020;Xue et al., 2020;Liu et al., 2020) that are fine-tuned on a source language and perform zero-shot (Wu and Dredze, 2019;Ponti et al., 2021) or few-shot (Lauscher et al., 2020;Zhao et al., 2020) prediction in a target language. \n\nAn alternative approach for cross-lingual transfer, translate test, is based on translating the evaluation set into the source language and leveraging a monolingual classifier instead (Banea et al., 2008;Durrett et al., 2012;Conneau et al., 2018). This approach is currently under-investigated and usually relegated to the role of a baseline due to its lower flexibility, e.g., it is not suitable for sequence labelling tasks. Yet, it achieves the state-of-the-art results in most benchmarks for multilingual Natural Language Understanding and Question Answering tasks (Hu et al., 2020;Ponti et al., 2020;Ruder et al., 2021, inter alia). Moreover, the availability of off-the-shelf translation models for multiple languages (Wu et al., 2016;Tiedemann and Thottingal, 2020;Liu et al., 2020) provides coverage for transfer to a large number of target languages. Indeed, very recent preliminary results suggest that the translation-based transfer might even outperform monolingual pre-trained models in languages different from English (Isbister et al., 2021). \n\nTranslation-based transfer, however, currently suffers from two main limitations. First, the errors in translation accumulate along the pipeline. In fact, sentences that are possibly not faithful to the original in the target language and/or not grammatical in the source language are fed to the classifier, which degrades its performance.",
            "reference_string": "[236318351 | Ponti et al. | 2021 | Citations: 18]"
        },
        {
            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 42,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.09249",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.09249, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2562211",
                    "name": "Xiaoyu Shen"
                },
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "36126076",
                    "name": "B. Byrne"
                },
                {
                    "authorId": "1786911",
                    "name": "A. Gispert"
                }
            ],
            "abstract": "Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers\u2019 questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate.We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets.",
            "corpus_id": 258715321,
            "sentences": [
                {
                    "corpus_id": "258715321",
                    "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
                    "text": "Based on the collected dataset, we report baseline results on two subtasks: (a) candidate ranking, which selects the best English candidate that contains the information to answer the non-English question; (b) answer generation, which generates a natural-sounding non-English answer to present to the user based on the selected English candidate. We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability. The translation-based approach is the most effective for candidate ranking while the multilingual-finetuning works the best for answer generation. Nonetheless, on both tasks, there is a substantial gap between the English-based and cross-lingual performances. In the following, we first elaborate on the problem formulation for the cross-lingual PQA task ( \u00a72), then explain the xPQA data collection process ( \u00a73), and present experiment results ( \u00a75.2) and conclusions ( \u00a76).",
                    "score": 0.45716914378098533,
                    "section_title": "Answer:",
                    "char_start_offset": 2499,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 473
                        },
                        {
                            "start": 474,
                            "end": 661
                        },
                        {
                            "start": 662,
                            "end": 808
                        },
                        {
                            "start": 809,
                            "end": 921
                        },
                        {
                            "start": 922,
                            "end": 1138
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89208984375
                }
            ],
            "relevance_judgement": 0.89208984375,
            "relevance_judgment_input_expanded": "# Title: xPQA: Cross-Lingual Product Question Answering in 12 Languages\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Xiaoyu Shen, Akari Asai, B. Byrne, A. Gispert\n## Abstract\nProduct Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers\u2019 questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate.We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets.\n## Answer:\nBased on the collected dataset, we report baseline results on two subtasks: (a) candidate ranking, which selects the best English candidate that contains the information to answer the non-English question; (b) answer generation, which generates a natural-sounding non-English answer to present to the user based on the selected English candidate. We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability. The translation-based approach is the most effective for candidate ranking while the multilingual-finetuning works the best for answer generation. Nonetheless, on both tasks, there is a substantial gap between the English-based and cross-lingual performances. In the following, we first elaborate on the problem formulation for the cross-lingual PQA task ( \u00a72), then explain the xPQA data collection process ( \u00a73), and present experiment results ( \u00a75.2) and conclusions ( \u00a76).",
            "reference_string": "[258715321 | Shen et al. | 2023 | Citations: 8]"
        },
        {
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "venue": "",
            "year": 2021,
            "reference_count": 52,
            "citation_count": 113,
            "influential_citation_count": 17,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2108.13897, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003019597",
                    "name": "L. Bonifacio"
                },
                {
                    "authorId": "1472908948",
                    "name": "Israel Campiotti"
                },
                {
                    "authorId": "1809633",
                    "name": "R. Lotufo"
                },
                {
                    "authorId": "143744603",
                    "name": "Rodrigo Nogueira"
                }
            ],
            "abstract": "The MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by finetuning monolingual and multilingual reranking models, as well as a multilingual dense retrieval model on this dataset. We also evaluated models finetuned using the mMARCO dataset in a zero-shot scenario on Mr. TyDi dataset, demonstrating that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Our experiments also show that a distilled multilingual reranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and finetuned models are available at https://github.com/unicamp-dl/mMARCO.",
            "corpus_id": 237364084,
            "sentences": [
                {
                    "corpus_id": "237364084",
                    "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
                    "text": "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages. \n\nMore recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO. This dataset is a suitable candidate as it shows good transfer learning capabilities, as well as being a popular choice for evaluating deep learning models (Craswell et al., 2020(Craswell et al., , 2021)). By translating the dataset we are able to leverages extensive manually annotated data. Lastly, as far as we know, no previous work has translated a large IR dataset such as the MS MARCO dataset to multiple languages. \n\nOur contributions are translating the MS MARCO (both training and evaluation sets) to 13 different languages such as Portuguese, Spanish, German, and Chinese. We finetuned mono and multilingual deep learning-based models both considering dense retrieval and reranking approaches using our translated dataset. Moreover, we evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Additionally, we made available the mMARCO dataset with 13 languages and all models finetuned on our translated dataset.",
                    "score": 0.5763572833350403,
                    "section_title": "Introduction",
                    "char_start_offset": 1757,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 181,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 807
                        },
                        {
                            "start": 808,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1303
                        },
                        {
                            "start": 1304,
                            "end": 1390
                        },
                        {
                            "start": 1391,
                            "end": 1520
                        },
                        {
                            "start": 1523,
                            "end": 1681
                        },
                        {
                            "start": 1682,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2075
                        },
                        {
                            "start": 2076,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89111328125
                }
            ],
            "relevance_judgement": 0.89111328125,
            "relevance_judgment_input_expanded": "# Title: mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset\n# Venue: \n# Authors: L. Bonifacio, Israel Campiotti, R. Lotufo, Rodrigo Nogueira\n## Abstract\nThe MS MARCO ranking dataset has been widely used for training deep learning models for IR tasks, achieving considerable effectiveness on diverse zero-shot scenarios. However, this type of resource is scarce in languages other than English. In this work, we present mMARCO, a multilingual version of the MS MARCO passage ranking dataset comprising 13 languages that was created using machine translation. We evaluated mMARCO by finetuning monolingual and multilingual reranking models, as well as a multilingual dense retrieval model on this dataset. We also evaluated models finetuned using the mMARCO dataset in a zero-shot scenario on Mr. TyDi dataset, demonstrating that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Our experiments also show that a distilled multilingual reranker is competitive with non-distilled models while having 5.4 times fewer parameters. Lastly, we show a positive correlation between translation quality and retrieval effectiveness, providing evidence that improvements in translation methods might lead to improvements in multilingual information retrieval. The translated datasets and finetuned models are available at https://github.com/unicamp-dl/mMARCO.\n## Introduction\nAs most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages. \n\nMore recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO. This dataset is a suitable candidate as it shows good transfer learning capabilities, as well as being a popular choice for evaluating deep learning models (Craswell et al., 2020(Craswell et al., , 2021)). By translating the dataset we are able to leverages extensive manually annotated data. Lastly, as far as we know, no previous work has translated a large IR dataset such as the MS MARCO dataset to multiple languages. \n\nOur contributions are translating the MS MARCO (both training and evaluation sets) to 13 different languages such as Portuguese, Spanish, German, and Chinese. We finetuned mono and multilingual deep learning-based models both considering dense retrieval and reranking approaches using our translated dataset. Moreover, we evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Additionally, we made available the mMARCO dataset with 13 languages and all models finetuned on our translated dataset.",
            "reference_string": "[237364084 | Bonifacio et al. | 2021 | Citations: 113]"
        },
        {
            "title": "Revisiting Machine Translation for Cross-lingual Classification",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 44,
            "citation_count": 38,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.14240",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14240, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "28554843",
                    "name": "Vedanuj Goswami"
                },
                {
                    "authorId": "2116473",
                    "name": "Shruti Bhosale"
                },
                {
                    "authorId": "144270981",
                    "name": "Angela Fan"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                }
            ],
            "abstract": "Machine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translate-test), or translating the training set into the target languages and finetuning a multilingual model (translate-train). However, most research in the area focuses on the multilingual models rather than the MT component. We show that, by using a stronger MT system and mitigating the mismatch between training on original text and running inference on machine translated text, translate-test can do substantially better than previously assumed. The optimal approach, however, is highly task dependent, as we identify various sources of cross-lingual transfer gap that affect different tasks and approaches differently. Our work calls into question the dominance of multilingual models for cross-lingual classification, and prompts to pay more attention to MT-based baselines.",
            "corpus_id": 258841859,
            "sentences": [
                {
                    "corpus_id": "258841859",
                    "title": "Revisiting Machine Translation for Cross-lingual Classification",
                    "text": "We next dissect the specific sources of cross-lingual transfer gap that each approach is sensitive to: Translate-test. All the degradation comes from MT, as no multilingual model is used. We distinguish between (i) the information lost in the translation process (either caused by translation errors or superficial patterns removed by MT), and (ii) the distribution shift between the original data seen during training and the machine translated data seen during evaluation (e.g., stylistic differences like translationese that existing models might struggle generalizing to, even if no information is lost). \n\nZero-shot. All the degradation comes from the multilingual model, as no MT is used. We distinguish between (i) source-language representation quality 6 relative to a monolingual model (English-only models being typically stronger than their multilingual counterparts, but only usable with translate-test), (ii) target-language representation quality relative to the source language (the representations of the target languagetypically less-resourced-being worse than those of the source language), and (iii) representation misalignment between the source and the target language (even when a model has a certain capability in both languages, there can be a performance gap when generalizing from the source to the target language if the languages are not well-aligned). \n\nTranslate-train. The degradation comes from both MT and the multilingual model. However, while both source and target language representation quality have an impact, 7 this approach is not sensitive to representation misalignment, as the model is trained and tested in the same language. Regarding MT, there is no translation and therefore no information lost at test time, so we can consider potential translation errors at training time to be further inducing a distribution shift. \n\nFinally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those 6 We consider that a pretrained model A has learned higher quality representations than model B if fine-tuning it in our target task results in better downstream performance.",
                    "score": 0.592352434779523,
                    "section_title": "Sources of cross-lingual transfer gap",
                    "char_start_offset": 18036,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 118
                        },
                        {
                            "start": 119,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 608
                        },
                        {
                            "start": 611,
                            "end": 621
                        },
                        {
                            "start": 622,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 1380
                        },
                        {
                            "start": 1383,
                            "end": 1399
                        },
                        {
                            "start": 1400,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1670
                        },
                        {
                            "start": 1671,
                            "end": 1866
                        },
                        {
                            "start": 1869,
                            "end": 2081
                        },
                        {
                            "start": 2082,
                            "end": 2342
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.884765625
                },
                {
                    "corpus_id": "258841859",
                    "title": "Revisiting Machine Translation for Cross-lingual Classification",
                    "text": "As we have just seen, the optimal cross-lingual learning approach is highly task dependent. In this section, we try to characterize the specific factors that explain this different behavior. To that end, we build on the concept of cross-lingual transfer gap, which is defined as the difference in performance between the source language that we have training data in (typically English) and the target language that we are evaluating on (Hu et al., 2020). While prior work has used this as an absolute metric to compare the cross-lingual generalization capabilities of different multilingual models, we argue that such a transfer gap can be attributed to different sources depending on the approach used, which we try to quantify empirically. \n\nIn what follows, we identify the specific sources of transfer gap that each approach is sensitive to ( \u00a74.1), propose a methodology to estimate their impact using a monolingual dataset ( \u00a74.2), and present the estimates that we obtain for various tasks and target languages ( \u00a74.3).",
                    "score": 0.47120021733817086,
                    "section_title": "Analyzing the variance across tasks",
                    "char_start_offset": 16967,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 91
                        },
                        {
                            "start": 92,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 742
                        },
                        {
                            "start": 745,
                            "end": 1027
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 437,
                            "end": 454,
                            "matchedPaperCorpusId": "214641214"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87451171875
                }
            ],
            "relevance_judgement": 0.884765625,
            "relevance_judgment_input_expanded": "# Title: Revisiting Machine Translation for Cross-lingual Classification\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mikel Artetxe, Vedanuj Goswami, Shruti Bhosale, Angela Fan, Luke Zettlemoyer\n## Abstract\nMachine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translate-test), or translating the training set into the target languages and finetuning a multilingual model (translate-train). However, most research in the area focuses on the multilingual models rather than the MT component. We show that, by using a stronger MT system and mitigating the mismatch between training on original text and running inference on machine translated text, translate-test can do substantially better than previously assumed. The optimal approach, however, is highly task dependent, as we identify various sources of cross-lingual transfer gap that affect different tasks and approaches differently. Our work calls into question the dominance of multilingual models for cross-lingual classification, and prompts to pay more attention to MT-based baselines.\n## Analyzing the variance across tasks\nAs we have just seen, the optimal cross-lingual learning approach is highly task dependent. In this section, we try to characterize the specific factors that explain this different behavior. To that end, we build on the concept of cross-lingual transfer gap, which is defined as the difference in performance between the source language that we have training data in (typically English) and the target language that we are evaluating on (Hu et al., 2020). While prior work has used this as an absolute metric to compare the cross-lingual generalization capabilities of different multilingual models, we argue that such a transfer gap can be attributed to different sources depending on the approach used, which we try to quantify empirically. \n\nIn what follows, we identify the specific sources of transfer gap that each approach is sensitive to ( \u00a74.1), propose a methodology to estimate their impact using a monolingual dataset ( \u00a74.2), and present the estimates that we obtain for various tasks and target languages ( \u00a74.3).\n\n## Sources of cross-lingual transfer gap\nWe next dissect the specific sources of cross-lingual transfer gap that each approach is sensitive to: Translate-test. All the degradation comes from MT, as no multilingual model is used. We distinguish between (i) the information lost in the translation process (either caused by translation errors or superficial patterns removed by MT), and (ii) the distribution shift between the original data seen during training and the machine translated data seen during evaluation (e.g., stylistic differences like translationese that existing models might struggle generalizing to, even if no information is lost). \n\nZero-shot. All the degradation comes from the multilingual model, as no MT is used. We distinguish between (i) source-language representation quality 6 relative to a monolingual model (English-only models being typically stronger than their multilingual counterparts, but only usable with translate-test), (ii) target-language representation quality relative to the source language (the representations of the target languagetypically less-resourced-being worse than those of the source language), and (iii) representation misalignment between the source and the target language (even when a model has a certain capability in both languages, there can be a performance gap when generalizing from the source to the target language if the languages are not well-aligned). \n\nTranslate-train. The degradation comes from both MT and the multilingual model. However, while both source and target language representation quality have an impact, 7 this approach is not sensitive to representation misalignment, as the model is trained and tested in the same language. Regarding MT, there is no translation and therefore no information lost at test time, so we can consider potential translation errors at training time to be further inducing a distribution shift. \n\nFinally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those 6 We consider that a pretrained model A has learned higher quality representations than model B if fine-tuning it in our target task results in better downstream performance.",
            "reference_string": "[258841859 | Artetxe et al. | 2023 | Citations: 38]"
        },
        {
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 60,
            "citation_count": 509,
            "influential_citation_count": 125,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.653.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.07475, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "145222654",
                    "name": "Patrick Lewis"
                },
                {
                    "authorId": "9185192",
                    "name": "Barlas O\u011fuz"
                },
                {
                    "authorId": "1905713",
                    "name": "Ruty Rinott"
                },
                {
                    "authorId": "48662861",
                    "name": "Sebastian Riedel"
                },
                {
                    "authorId": "144518416",
                    "name": "Holger Schwenk"
                }
            ],
            "abstract": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
            "corpus_id": 204734128,
            "sentences": [
                {
                    "corpus_id": "204734128",
                    "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                    "text": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
                    "score": 0.47244516548111337,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8828125
                },
                {
                    "corpus_id": "204734128",
                    "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
                    "text": "First, we state our desired properties for a crosslingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section 3). We then describe our annotation protocol, which seeks to fulfil these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \n\nDiverse Languages A primary goal of crosslingual research is to develop systems that work well in many languages. The dataset should enable quantitative performance comparison across languages with different linguistic resources, language families and scripts. \n\nExtractive QA Cross-lingual understanding benchmarks are typically based on classification (Conneau et al., 2018). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extractive QA datasets in a number of languages (see Section 3), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \n\nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desirable to select a textual domain that matches existing extractive QA training resources, in order to isolate the change in performance due to language transfer. \n\nTo satisfy these desiderata, we identified the method described below and illustrated in Figure 1. Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset.",
                    "score": 0.4949651730633069,
                    "section_title": "The MLQA corpus",
                    "char_start_offset": 4847,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 80
                        },
                        {
                            "start": 81,
                            "end": 203
                        },
                        {
                            "start": 204,
                            "end": 285
                        },
                        {
                            "start": 288,
                            "end": 377
                        },
                        {
                            "start": 378,
                            "end": 467
                        },
                        {
                            "start": 468,
                            "end": 590
                        },
                        {
                            "start": 591,
                            "end": 682
                        },
                        {
                            "start": 685,
                            "end": 809
                        },
                        {
                            "start": 810,
                            "end": 936
                        },
                        {
                            "start": 937,
                            "end": 1073
                        },
                        {
                            "start": 1076,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1336
                        },
                        {
                            "start": 1339,
                            "end": 1453
                        },
                        {
                            "start": 1454,
                            "end": 1550
                        },
                        {
                            "start": 1551,
                            "end": 1768
                        },
                        {
                            "start": 1771,
                            "end": 1849
                        },
                        {
                            "start": 1850,
                            "end": 2026
                        },
                        {
                            "start": 2029,
                            "end": 2127
                        },
                        {
                            "start": 2128,
                            "end": 2273
                        },
                        {
                            "start": 2274,
                            "end": 2437
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8701171875
                }
            ],
            "relevance_judgement": 0.8828125,
            "relevance_judgment_input_expanded": "# Title: MLQA: Evaluating Cross-lingual Extractive Question Answering\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Patrick Lewis, Barlas O\u011fuz, Ruty Rinott, Sebastian Riedel, Holger Schwenk\n## Abstract\nQuestion answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.\n## The MLQA corpus\nFirst, we state our desired properties for a crosslingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section 3). We then describe our annotation protocol, which seeks to fulfil these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \n\nDiverse Languages A primary goal of crosslingual research is to develop systems that work well in many languages. The dataset should enable quantitative performance comparison across languages with different linguistic resources, language families and scripts. \n\nExtractive QA Cross-lingual understanding benchmarks are typically based on classification (Conneau et al., 2018). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extractive QA datasets in a number of languages (see Section 3), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \n\nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desirable to select a textual domain that matches existing extractive QA training resources, in order to isolate the change in performance due to language transfer. \n\nTo satisfy these desiderata, we identified the method described below and illustrated in Figure 1. Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset.",
            "reference_string": "[204734128 | Lewis et al. | 2019 | Citations: 509]"
        },
        {
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2019,
            "reference_count": 42,
            "citation_count": 6589,
            "influential_citation_count": 1462,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.acl-main.747.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1911.02116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2480903",
                    "name": "Alexis Conneau"
                },
                {
                    "authorId": "40267343",
                    "name": "Kartikay Khandelwal"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "113810201",
                    "name": "Vishrav Chaudhary"
                },
                {
                    "authorId": "2293203",
                    "name": "Guillaume Wenzek"
                },
                {
                    "authorId": "144204682",
                    "name": "Francisco Guzm\u00e1n"
                },
                {
                    "authorId": "3024698",
                    "name": "Edouard Grave"
                },
                {
                    "authorId": "40511414",
                    "name": "Myle Ott"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "1759422",
                    "name": "Veselin Stoyanov"
                }
            ],
            "abstract": "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
            "corpus_id": 207880568,
            "sentences": [
                {
                    "corpus_id": "207880568",
                    "title": "Unsupervised Cross-lingual Representation Learning at Scale",
                    "text": "We consider four evaluation benchmarks. For crosslingual understanding, we use cross-lingual natural language inference, named entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models. \n\nCross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other lan-guages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project. \n\nNamed Entity Recognition. For NER, we consider the CoNLL-2002(Sang, 2002) and CoNLL-2003 (Tjong Kim Sang andDe Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018). \n\nCross-lingual Question Answering. We use the MLQA benchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. GLUE Benchmark.",
                    "score": 0.49690971746855694,
                    "section_title": "Evaluation",
                    "char_start_offset": 10139,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 39
                        },
                        {
                            "start": 40,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 290
                        },
                        {
                            "start": 293,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 457
                        },
                        {
                            "start": 458,
                            "end": 596
                        },
                        {
                            "start": 597,
                            "end": 678
                        },
                        {
                            "start": 679,
                            "end": 1140
                        },
                        {
                            "start": 1141,
                            "end": 1217
                        },
                        {
                            "start": 1220,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1600
                        },
                        {
                            "start": 1601,
                            "end": 1700
                        },
                        {
                            "start": 1703,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1889
                        },
                        {
                            "start": 1890,
                            "end": 1991
                        },
                        {
                            "start": 1992,
                            "end": 2007
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1328,
                            "end": 1345,
                            "matchedPaperCorpusId": "2470716"
                        },
                        {
                            "start": 1655,
                            "end": 1675,
                            "matchedPaperCorpusId": "6042994"
                        },
                        {
                            "start": 1680,
                            "end": 1699,
                            "matchedPaperCorpusId": "52010710"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8798828125
                }
            ],
            "relevance_judgement": 0.8798828125,
            "relevance_judgment_input_expanded": "# Title: Unsupervised Cross-lingual Representation Learning at Scale\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov\n## Abstract\nThis paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.\n## Evaluation\nWe consider four evaluation benchmarks. For crosslingual understanding, we use cross-lingual natural language inference, named entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models. \n\nCross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other lan-guages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project. \n\nNamed Entity Recognition. For NER, we consider the CoNLL-2002(Sang, 2002) and CoNLL-2003 (Tjong Kim Sang andDe Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018). \n\nCross-lingual Question Answering. We use the MLQA benchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. GLUE Benchmark.",
            "reference_string": "[207880568 | Conneau et al. | 2019 | Citations: 6589]"
        },
        {
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "venue": "The Web Conference",
            "year": 2023,
            "reference_count": 94,
            "citation_count": 76,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.13132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2087723977",
                    "name": "Yiqiao Jin"
                },
                {
                    "authorId": "32556330",
                    "name": "Mohit Chandra"
                },
                {
                    "authorId": "145816931",
                    "name": "Gaurav Verma"
                },
                {
                    "authorId": "49995036",
                    "name": "Yibo Hu"
                },
                {
                    "authorId": "2256118804",
                    "name": "Munmun De Choudhury"
                },
                {
                    "authorId": "2254102372",
                    "name": "Srijan Kumar"
                }
            ],
            "abstract": "Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems. This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XLingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all.",
            "corpus_id": 264405758,
            "sentences": [
                {
                    "corpus_id": "264405758",
                    "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
                    "text": "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], \n\nand ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text? \n\nFrom Table A2, it can be noted that our evaluation revealed ChatGPT to outperform other approaches in translations from English to both Chinese and Spanish, while Google Translate exhibits superior performance in English-to-Hindi translation. Thus, for optimal results in each non-English language, we harnessed the best-performing model to achieve the highest translation quality.",
                    "score": 0.4853129009429891,
                    "section_title": "A DETAILS OF DATASET CONSTRUCTION",
                    "char_start_offset": 36719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 107
                        },
                        {
                            "start": 108,
                            "end": 320
                        },
                        {
                            "start": 323,
                            "end": 339
                        },
                        {
                            "start": 340,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 891
                        },
                        {
                            "start": 894,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 992
                        },
                        {
                            "start": 993,
                            "end": 1083
                        },
                        {
                            "start": 1086,
                            "end": 1328
                        },
                        {
                            "start": 1329,
                            "end": 1467
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 315,
                            "end": 319,
                            "matchedPaperCorpusId": "4623739"
                        },
                        {
                            "start": 614,
                            "end": 618,
                            "matchedPaperCorpusId": "248887528"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8759765625
                }
            ],
            "relevance_judgement": 0.8759765625,
            "relevance_judgment_input_expanded": "# Title: Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries\n# Venue: The Web Conference\n# Authors: Yiqiao Jin, Mohit Chandra, Gaurav Verma, Yibo Hu, Munmun De Choudhury, Srijan Kumar\n## Abstract\nLarge language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems. This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XLingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all.\n## A DETAILS OF DATASET CONSTRUCTION\nObserving the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], \n\nand ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text? \n\nFrom Table A2, it can be noted that our evaluation revealed ChatGPT to outperform other approaches in translations from English to both Chinese and Spanish, while Google Translate exhibits superior performance in English-to-Hindi translation. Thus, for optimal results in each non-English language, we harnessed the best-performing model to achieve the highest translation quality.",
            "reference_string": "[264405758 | Jin et al. | 2023 | Citations: 76]"
        },
        {
            "title": "Practical Transformer-based Multilingual Text Classification",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2021,
            "reference_count": 39,
            "citation_count": 28,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-industry.16.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://aclanthology.org/2021.naacl-industry.16, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2108725400",
                    "name": "Cindy Wang"
                },
                {
                    "authorId": "2339397",
                    "name": "Michele Banko"
                }
            ],
            "abstract": "Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.",
            "corpus_id": 235097287,
            "sentences": [
                {
                    "corpus_id": "235097287",
                    "title": "Practical Transformer-based Multilingual Text Classification",
                    "text": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation3 as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples. In the multi-all setting, we translate data from every language into every other language, yielding N 3 Data \n\nWe choose sentiment analysis and hate speech detection as evaluation tasks due to their relevance to industry applications and the availability of multilingual datasets. An overview of the datasets is shown in Table 2.",
                    "score": 0.48120253734229745,
                    "section_title": "Data Augmentation",
                    "char_start_offset": 6411,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 472
                        },
                        {
                            "start": 473,
                            "end": 581
                        },
                        {
                            "start": 584,
                            "end": 753
                        },
                        {
                            "start": 754,
                            "end": 802
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86669921875
                }
            ],
            "relevance_judgement": 0.86669921875,
            "relevance_judgment_input_expanded": "# Title: Practical Transformer-based Multilingual Text Classification\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Cindy Wang, Michele Banko\n## Abstract\nTransformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data.\n## Data Augmentation\nIn real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation3 as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples. In the multi-all setting, we translate data from every language into every other language, yielding N 3 Data \n\nWe choose sentiment analysis and hate speech detection as evaluation tasks due to their relevance to industry applications and the availability of multilingual datasets. An overview of the datasets is shown in Table 2.",
            "reference_string": "[235097287 | Wang et al. | 2021 | Citations: 28]"
        },
        {
            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
            "venue": "Journal of Artificial Intelligence Research",
            "year": 2021,
            "reference_count": 317,
            "citation_count": 34,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.jair.org/index.php/jair/article/download/13083/26828",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2104.08570, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66879943",
                    "name": "E. Razumovskaia"
                },
                {
                    "authorId": "1666177566",
                    "name": "Goran Glavavs"
                },
                {
                    "authorId": "46963731",
                    "name": "Olga Majewska"
                },
                {
                    "authorId": "3381663",
                    "name": "E. Ponti"
                },
                {
                    "authorId": "145762466",
                    "name": "A. Korhonen"
                },
                {
                    "authorId": "1747849",
                    "name": "Ivan Vulic"
                }
            ],
            "abstract": "In task-oriented dialogue (ToD), a user holds a conversation with an artificial agent\u00a0 with the aim of completing a concrete task. Although this technology represents one of\u00a0 the central objectives of AI and has been the focus of ever more intense research and\u00a0 development efforts, it is currently limited to a few narrow domains (e.g., food ordering,\u00a0 ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an\u00a0 extensive overview of existing methods and resources in multilingual ToD as an entry point\u00a0 to this exciting and emerging field. We find that the most critical factor preventing the\u00a0 creation of truly multilingual ToD systems is the lack of datasets in most languages for\u00a0 both training and evaluation. In fact, acquiring annotations or human feedback for each\u00a0 component of modular systems or for data-hungry end-to-end systems is expensive and\u00a0 tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or\u00a0 few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English),\u00a0 either by means of (i) machine translation or (ii) multilingual representations. These\u00a0 approaches are currently viable only for typologically similar languages and languages with\u00a0 parallel / monolingual corpora available. On the other hand, their effectiveness beyond these\u00a0 boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks\u00a0 (especially for natural language generation and end-to-end evaluation). To overcome this\u00a0 limitation, we draw parallels between components of the ToD pipeline and other NLP tasks,\u00a0 which can inspire solutions for learning in low-resource scenarios. Finally, we list additional\u00a0 challenges that multilinguality poses for related areas (such as speech, fluency in generated\u00a0 text, and human-centred evaluation), and indicate future directions that hold promise to\u00a0 further expand language coverage and dialogue capabilities of current ToD systems.\u00a0",
            "corpus_id": 235313293,
            "sentences": [
                {
                    "corpus_id": "235313293",
                    "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
                    "text": "language spills its lexical and structural properties over the target language (Lembersky et al., 2012;Volansky et al., 2015). These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level. We refer the reader to the paper for some concrete examples.\n\nSecondly, the information and the topics touched upon in a conversation (e.g., in the domain of airline travels, names of destinations or flight companies) may vary across cultures and locales. However, translation-based approaches reflect the perspective of the Englishspeaking culture, its 'presupposed' factual knowledge, and the worldview of its community of speakers (Clark et al., 2020). For these reasons, ToD benchmarks should be ideally based on original utterances grounded in the appropriate locale and culture.\n\nRecognising this need, there have been some very recent developments in the direction of creating localised and culturally adapted ToD datasets. Namely, Ding et al. (2022) undertake an automatic approach to localisation in which the English slots are substituted by their local counterparts, obtained via Web-crawling suitable values. In contrast, Majewska et al. (2022) ask the native speakers to",
                    "score": 0.5709914880019716,
                    "section_title": "Outlook for Multilingual ToD Datasets",
                    "char_start_offset": 35207,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 79,
                            "end": 103,
                            "matchedPaperCorpusId": "11280500"
                        },
                        {
                            "start": 1666,
                            "end": 1686,
                            "matchedPaperCorpusId": "212657414"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8642578125
                }
            ],
            "relevance_judgement": 0.8642578125,
            "relevance_judgment_input_expanded": "# Title: Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems\n# Venue: Journal of Artificial Intelligence Research\n# Authors: E. Razumovskaia, Goran Glavavs, Olga Majewska, E. Ponti, A. Korhonen, Ivan Vulic\n## Abstract\nIn task-oriented dialogue (ToD), a user holds a conversation with an artificial agent\u00a0 with the aim of completing a concrete task. Although this technology represents one of\u00a0 the central objectives of AI and has been the focus of ever more intense research and\u00a0 development efforts, it is currently limited to a few narrow domains (e.g., food ordering,\u00a0 ticket booking) and a handful of languages (e.g., English, Chinese). This work provides an\u00a0 extensive overview of existing methods and resources in multilingual ToD as an entry point\u00a0 to this exciting and emerging field. We find that the most critical factor preventing the\u00a0 creation of truly multilingual ToD systems is the lack of datasets in most languages for\u00a0 both training and evaluation. In fact, acquiring annotations or human feedback for each\u00a0 component of modular systems or for data-hungry end-to-end systems is expensive and\u00a0 tedious. Hence, state-of-the-art approaches to multilingual ToD mostly rely on (zero- or\u00a0 few-shot) cross-lingual transfer from resource-rich languages (almost exclusively English),\u00a0 either by means of (i) machine translation or (ii) multilingual representations. These\u00a0 approaches are currently viable only for typologically similar languages and languages with\u00a0 parallel / monolingual corpora available. On the other hand, their effectiveness beyond these\u00a0 boundaries is doubtful or hard to assess due to the lack of linguistically diverse benchmarks\u00a0 (especially for natural language generation and end-to-end evaluation). To overcome this\u00a0 limitation, we draw parallels between components of the ToD pipeline and other NLP tasks,\u00a0 which can inspire solutions for learning in low-resource scenarios. Finally, we list additional\u00a0 challenges that multilinguality poses for related areas (such as speech, fluency in generated\u00a0 text, and human-centred evaluation), and indicate future directions that hold promise to\u00a0 further expand language coverage and dialogue capabilities of current ToD systems.\u00a0\n## Outlook for Multilingual ToD Datasets\nlanguage spills its lexical and structural properties over the target language (Lembersky et al., 2012;Volansky et al., 2015). These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level. We refer the reader to the paper for some concrete examples.\n\nSecondly, the information and the topics touched upon in a conversation (e.g., in the domain of airline travels, names of destinations or flight companies) may vary across cultures and locales. However, translation-based approaches reflect the perspective of the Englishspeaking culture, its 'presupposed' factual knowledge, and the worldview of its community of speakers (Clark et al., 2020). For these reasons, ToD benchmarks should be ideally based on original utterances grounded in the appropriate locale and culture.\n\nRecognising this need, there have been some very recent developments in the direction of creating localised and culturally adapted ToD datasets. Namely, Ding et al. (2022) undertake an automatic approach to localisation in which the English slots are substituted by their local counterparts, obtained via Web-crawling suitable values. In contrast, Majewska et al. (2022) ask the native speakers to",
            "reference_string": "[235313293 | Razumovskaia et al. | 2021 | Citations: 34]"
        },
        {
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07243, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305680161",
                    "name": "Vera Neplenbroek"
                },
                {
                    "authorId": "2299270429",
                    "name": "Arianna Bisazza"
                },
                {
                    "authorId": "2147411696",
                    "name": "R. Fern'andez"
                }
            ],
            "abstract": "Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias. Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings. The dataset and code are available at https://github.com/Veranep/MBBQ.",
            "corpus_id": 270379819,
            "sentences": [
                {
                    "corpus_id": "270379819",
                    "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
                    "text": "We present the Multilingual Bias Benchmark for Question-answering (MBBQ), consisting of a hand-checked translation of the English BBQ dataset into Dutch, Spanish, and Turkish,   and a parallel control set to measure task performance independent from bias.MBBQ covers stereotypes from 6 bias categories that are commonly held across all 4 languages, allowing for an investigation of cross-lingual stereotypes, with differences that are due to inconsistencies in model behavior across languages rather than cultural shifts.In this paper, we evaluated 7 LLMs on the MBBQ dataset.Our results show that 1) the ability of generative LLMs to answer multiple choice questions significantly differs across languages, 2) for the less accurate models, the extent to which they exhibit stereotypical behavior significantly differs across languages, and 3) the biases of a generative LLM differ across bias categories.Based on our findings, we recommend evaluating model bias across different bias categories, rather than reporting on the bias of a model as a whole, and separating measurements of model bias from their performance, especially cross-lingually.We hope that our work will spark further research in the direction of multilingual debiasing, to ensure that these models do not exhibit biased behavior regardless of the language used to prompt them.",
                    "score": 0.45965084434356784,
                    "section_title": "Conclusion",
                    "char_start_offset": 27691,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 255,
                            "end": 521
                        },
                        {
                            "start": 521,
                            "end": 576
                        },
                        {
                            "start": 576,
                            "end": 905
                        },
                        {
                            "start": 905,
                            "end": 1147
                        },
                        {
                            "start": 1147,
                            "end": 1347
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86328125
                }
            ],
            "relevance_judgement": 0.86328125,
            "relevance_judgment_input_expanded": "# Title: MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs\n# Venue: arXiv.org\n# Authors: Vera Neplenbroek, Arianna Bisazza, R. Fern'andez\n## Abstract\nGenerative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias. Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings. The dataset and code are available at https://github.com/Veranep/MBBQ.\n## Conclusion\nWe present the Multilingual Bias Benchmark for Question-answering (MBBQ), consisting of a hand-checked translation of the English BBQ dataset into Dutch, Spanish, and Turkish,   and a parallel control set to measure task performance independent from bias.MBBQ covers stereotypes from 6 bias categories that are commonly held across all 4 languages, allowing for an investigation of cross-lingual stereotypes, with differences that are due to inconsistencies in model behavior across languages rather than cultural shifts.In this paper, we evaluated 7 LLMs on the MBBQ dataset.Our results show that 1) the ability of generative LLMs to answer multiple choice questions significantly differs across languages, 2) for the less accurate models, the extent to which they exhibit stereotypical behavior significantly differs across languages, and 3) the biases of a generative LLM differ across bias categories.Based on our findings, we recommend evaluating model bias across different bias categories, rather than reporting on the bias of a model as a whole, and separating measurements of model bias from their performance, especially cross-lingually.We hope that our work will spark further research in the direction of multilingual debiasing, to ensure that these models do not exhibit biased behavior regardless of the language used to prompt them.",
            "reference_string": "[270379819 | Neplenbroek et al. | 2024 | Citations: 8]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "254043519",
            "title": "Beyond Counting Datasets: A Survey of Multilingual Dataset Construction and Necessary Resources",
            "text": "Translating English data into the target languages are used in 33 datasets, but most datasets collected data in its original language. Yet, many recent and highly cited datasets for cross-lingual transfer evaluation (Artetxe et al., 2020b;Conneau et al., 2018) are created with translation-based approaches, which we discuss in detail below. Table 7 summarizes statistics on the motivation aspect, with a breakdown for the number of datasets for each motivation with and without translation. The most frequent driver for dataset creation was to cover multiple languages for a single task (62 datasets, covering 217 languages), often for downstream tasks with high economic demands, such as QA or summarization (Blasi et al., 2022). There were 16 languages (e.g., Chinese, Arabic) that had their own benchmark suites labeled as the monolingual general model category, which seemed to align with the availability of language-proficient NLP researchers. We discuss the relationship between dataset availability and the number of languageproficient researchers in Section 5.",
            "score": 0.6288680588743837,
            "section_title": "Input and Label Derivation via Translation.",
            "char_start_offset": 16100,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83447265625
        },
        {
            "corpus_id": "274464561",
            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
            "text": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",
            "score": 0.6277499306688066,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9853515625
        },
        {
            "corpus_id": "215548041",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "text": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
            "score": 0.5967234818288341,
            "section_title": "Discussion",
            "char_start_offset": 24144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "277993848",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "text": "The domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
            "score": 0.5926090956462529,
            "section_title": "Domain",
            "char_start_offset": 9088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 63,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 528,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 528,
                    "end": 546,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 690,
                    "end": 713,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 854,
                    "end": 872,
                    "matchedPaperCorpusId": "263829211"
                },
                {
                    "start": 889,
                    "end": 911,
                    "matchedPaperCorpusId": "271245129"
                },
                {
                    "start": 1177,
                    "end": 1198,
                    "matchedPaperCorpusId": "273850461"
                },
                {
                    "start": 1395,
                    "end": 1415,
                    "matchedPaperCorpusId": "270371898"
                },
                {
                    "start": 2088,
                    "end": 2104,
                    "matchedPaperCorpusId": "254043519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91796875
        },
        {
            "corpus_id": "258841859",
            "title": "Revisiting Machine Translation for Cross-lingual Classification",
            "text": "We next dissect the specific sources of cross-lingual transfer gap that each approach is sensitive to: Translate-test. All the degradation comes from MT, as no multilingual model is used. We distinguish between (i) the information lost in the translation process (either caused by translation errors or superficial patterns removed by MT), and (ii) the distribution shift between the original data seen during training and the machine translated data seen during evaluation (e.g., stylistic differences like translationese that existing models might struggle generalizing to, even if no information is lost). \n\nZero-shot. All the degradation comes from the multilingual model, as no MT is used. We distinguish between (i) source-language representation quality 6 relative to a monolingual model (English-only models being typically stronger than their multilingual counterparts, but only usable with translate-test), (ii) target-language representation quality relative to the source language (the representations of the target languagetypically less-resourced-being worse than those of the source language), and (iii) representation misalignment between the source and the target language (even when a model has a certain capability in both languages, there can be a performance gap when generalizing from the source to the target language if the languages are not well-aligned). \n\nTranslate-train. The degradation comes from both MT and the multilingual model. However, while both source and target language representation quality have an impact, 7 this approach is not sensitive to representation misalignment, as the model is trained and tested in the same language. Regarding MT, there is no translation and therefore no information lost at test time, so we can consider potential translation errors at training time to be further inducing a distribution shift. \n\nFinally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those 6 We consider that a pretrained model A has learned higher quality representations than model B if fine-tuning it in our target task results in better downstream performance.",
            "score": 0.592352434779523,
            "section_title": "Sources of cross-lingual transfer gap",
            "char_start_offset": 18036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1670
                },
                {
                    "start": 1671,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "273532366",
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "text": "The datasets introduced in Section III-A mainly focus on unsupervised data. This section will introduce benchmarks for evaluating MLLMs on multilingual supervised datasets. The tasks in the benchmark mainly include text classification tasks (single sentence or sentence pair), QA tasks (Text-span Prediction), sequence labeling tasks, and text generation tasks, as shown in Figure 2. \n\n1) Datasets: Constructing robust and comprehensive evaluation tasks can help us better understand the effectiveness of large models. This type of evaluation is a very active research area in English, such as the GLUE and the more difficult SuperGLUE evaluation dataset benchmark, as well as the recently established multilingual evaluation datasets Fig. 3: Evaluation methods for MLLMs XTREME, XTREME-R, and XGLUE based on them. The goal of multilingual evaluation datasets is to cover more diverse tasks and languages so that we can better evaluate the generalization performance of MLLMs. At the same time, some researchers are also dedicated to building evaluation datasets for specific languages, such as IndicXTREME for Indian languages. \n\nIn general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation. \n\nTo better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks.",
            "score": 0.5851332341320621,
            "section_title": "B. Evaluation Tasks",
            "char_start_offset": 28760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 2242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "232307434",
            "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets",
            "text": "This risk of undesired downstream effects calls for future studies with a careful treatment of intertwined effects such as data size and domain, languagespecific phenomena, evaluation data and metric biases. To give the reader a brief glimpse of the impact of data quality for the example of translation, we compare the C% metric from our audit with the translation quality (sentencepiece-BLEU, spBLEU) of the multilingual translation model M2M124 for 124 languages (Goyal et al., 2021). It was trained on WikiMatrix and CCAligned, and similar data collected with the same tools, which we expect to show similar biases. Translation quality is evaluated on the trusted, humantranslated FloReS benchmark (Goyal et al., 2021). For the 21 languages present in both the audit and the FloReS benchmark, we found a positive correlation (Spearman) between the data quality scores and spBLEU of \u03c1 = 0.44 (p = 0.041). This is not as large as the correlation with data size (\u03c1 = 0.66, p = 0.00078), but it nonetheless helps to explain translation quality-the correlation between the product of C% and data size (in other words, the expected total number of good sentences in the dataset), is the highest yet, with a value of \u03c1 = 0.73 (p = 0.00013).12 Representation washing Since there are datasets which contain many low-resource languages, the community may feel a sense of progress and growing equity, despite the actual quality of the resources for these languages. Similarly, if low-quality datasets are used as benchmarks they may exaggerate model performance, making low-resource NLP appear more solved than it is-or conversely, if models perform poorly when trained with such data, it may be  Table 6: Examples of \"parallel\" data where the translation has a different meaning than the source, but the form looks the same. (We added translations of the non-English side.) Such data may encourage hallucinations of fake \"facts\". wrongly assumed that the task of learning models for these languages is harder than it actually is or infeasible given current resources.",
            "score": 0.5788318181951742,
            "section_title": "Dataset Mis-labeling",
            "char_start_offset": 28749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2061
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "As most languages have none or a limited number of labeled datasets, using automatic translation is an attainable way to derive the same resources for a wider range of languages. \n\nMore recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO. This dataset is a suitable candidate as it shows good transfer learning capabilities, as well as being a popular choice for evaluating deep learning models (Craswell et al., 2020(Craswell et al., , 2021)). By translating the dataset we are able to leverages extensive manually annotated data. Lastly, as far as we know, no previous work has translated a large IR dataset such as the MS MARCO dataset to multiple languages. \n\nOur contributions are translating the MS MARCO (both training and evaluation sets) to 13 different languages such as Portuguese, Spanish, German, and Chinese. We finetuned mono and multilingual deep learning-based models both considering dense retrieval and reranking approaches using our translated dataset. Moreover, we evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone. Additionally, we made available the mMARCO dataset with 13 languages and all models finetuned on our translated dataset.",
            "score": 0.5763572833350403,
            "section_title": "Introduction",
            "char_start_offset": 1757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89111328125
        },
        {
            "corpus_id": "235313293",
            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
            "text": "language spills its lexical and structural properties over the target language (Lembersky et al., 2012;Volansky et al., 2015). These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level. We refer the reader to the paper for some concrete examples.\n\nSecondly, the information and the topics touched upon in a conversation (e.g., in the domain of airline travels, names of destinations or flight companies) may vary across cultures and locales. However, translation-based approaches reflect the perspective of the Englishspeaking culture, its 'presupposed' factual knowledge, and the worldview of its community of speakers (Clark et al., 2020). For these reasons, ToD benchmarks should be ideally based on original utterances grounded in the appropriate locale and culture.\n\nRecognising this need, there have been some very recent developments in the direction of creating localised and culturally adapted ToD datasets. Namely, Ding et al. (2022) undertake an automatic approach to localisation in which the English slots are substituted by their local counterparts, obtained via Web-crawling suitable values. In contrast, Majewska et al. (2022) ask the native speakers to",
            "score": 0.5709914880019716,
            "section_title": "Outlook for Multilingual ToD Datasets",
            "char_start_offset": 35207,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 103,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 1666,
                    "end": 1686,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8642578125
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "This section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
            "score": 0.5689987458005816,
            "section_title": "B. Bias Benchmark",
            "char_start_offset": 48672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 404,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 406,
                    "end": 411,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 448,
                    "end": 453,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 455,
                    "end": 460,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 540,
                    "end": 545,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 546,
                    "end": 551,
                    "matchedPaperCorpusId": "252547725"
                },
                {
                    "start": 970,
                    "end": 975,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 977,
                    "end": 982,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1076,
                    "end": 1081,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1364,
                    "end": 1369,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1383,
                    "end": 1388,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1455,
                    "end": 1460,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1649,
                    "end": 1654,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1666,
                    "end": 1671,
                    "matchedPaperCorpusId": "252547725"
                },
                {
                    "start": 1988,
                    "end": 1993,
                    "matchedPaperCorpusId": "135465247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "269758132",
            "title": "Analyzing Language Bias Between French and English in Conventional Multilingual Sentiment Analysis Models",
            "text": "The expansion of multilingual datasets in Natural Language Processing (NLP) brings both opportunities and challenges.This study underlines the necessity of balanced datasets for equitable language performance.We discovered that linguistic features and dataset composition significantly influence model outcomes, exposing gaps that need targeted interventions.\n\nNotably, performance disparities between French and English demonstrate prevalent language biases in NLP models, emphasizing the need to refine training and evaluation methodologies to accommodate linguistic diversity better.As more comprehensive datasets become available, it is crucial to conduct comparative tests between monolingual and multilingual models.Such testing could reveal the strengths and limitations of each approach, guiding more effective strategies for addressing language-specific nuances that multilingual models often overlook.\n\nEnsuring fairness and equity in NLP is paramount as the field evolves.We must guarantee equitable access to resources for low-resource languages to achieve fair treatment across linguistic boundaries.This study contributes to ongoing efforts to equalize linguistic representation in NLP, advocating for technology that equally benefits all users.By focusing on monolingual models and rigorous testing against their multilingual counterparts, we can enhance the precision and fairness of NLP applications.",
            "score": 0.5683582558922129,
            "section_title": "Conclusion",
            "char_start_offset": 27292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 359
                },
                {
                    "start": 361,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 722
                },
                {
                    "start": 722,
                    "end": 911
                },
                {
                    "start": 913,
                    "end": 983
                },
                {
                    "start": 983,
                    "end": 1113
                },
                {
                    "start": 1113,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80810546875
        },
        {
            "corpus_id": "267027810",
            "title": "Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges",
            "text": "There is a pressing need to collect and annotate comprehensive and balanced datasets, especially in low-resource languages [13,29,67,81]. While zero-shot crosslingual transfer has its limitations, even a small amount of target data can substantially enhance model fine-tuning [158]. To avoid dataset bias, such datasets can span multiple languages, dialects, cultural contexts, and diverse types of offensive language [123]. More diverse strategies for data collection, such as sampling from various platforms and user demographics, can yield richer and more annotation-worthy data [158]. Additionally, the creation of domain-specific evaluation datasets, such as XHATE-999 [70] and Multilingual HateCheck (MHC) [159], provides a multifaceted and precise perspective for assessing cross-lingual detection.",
            "score": 0.5678674039417552,
            "section_title": "Dataset Creation.",
            "char_start_offset": 70027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 805
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "250391030"
                },
                {
                    "start": 127,
                    "end": 130,
                    "matchedPaperCorpusId": "244464114"
                },
                {
                    "start": 130,
                    "end": 133,
                    "matchedPaperCorpusId": "237439648"
                },
                {
                    "start": 133,
                    "end": 136,
                    "matchedPaperCorpusId": "254153537"
                },
                {
                    "start": 276,
                    "end": 281,
                    "matchedPaperCorpusId": "253018845"
                },
                {
                    "start": 418,
                    "end": 423,
                    "matchedPaperCorpusId": "201669180"
                },
                {
                    "start": 582,
                    "end": 587,
                    "matchedPaperCorpusId": "253018845"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "227231076"
                },
                {
                    "start": 712,
                    "end": 717,
                    "matchedPaperCorpusId": "249889128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6513671875
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "In this section, we verify how monolingual and multilingual models perform when varying pretraining, finetuning, and evaluation languages. As the number of pairwise language combinations is large, we only use English and Portuguese in this ablation study. The monolingual models we use are an English (Raffel et al., 2019) and a Portuguese (Carmo et al., 2020)  Table 5 shows the results. Rows (1) to (3) use models mostly pretrained on a single language (T5 and miniLM), and rows (4) to (7) report results for multilingual pretrained models (MT5 and MMINILM). When finetuned on monolingual datasets, we observe insignificant differences between monolingual and multilingual models in both English and Portuguese evaluation sets. We observe that for the monolingual configuration, MINILM outperforms T5 in English (1). We argue that the model was able to leverage training data for both languages and thus reduced most inaccuracies introduced by noisy translations. \n\nWhen considering the results from English dataset, it is important to mention that both multilingual rerankers were finetuned on a smaller amount of English examples than the models in row (1). This shows that the translation did not harm the resulting datasets significantly. Albeit the lexical mismatch problem discussed in Section 4, both rerankers were able to learn the task from the translated data. \n\nRows (4) and (5) in Table 5 exhibit the results for MT5 finetuned on monolingual datasets. Whilst the results were lower for the English dataset when compared to monolingual T5 (1), the results on the Portuguese version were marginally higher than the ones observed in row (2). This cross-lingual evaluation shows how a multilingual model finetuned on one language and evaluated on another still can reach good results. Although this observation is not true for the Portuguese-English cross-lingual evaluation (row 5), as the result was below the monolingual one (row 1), the difference is small.",
            "score": 0.5651343460326509,
            "section_title": "Ablation Study",
            "char_start_offset": 21080,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1373
                },
                {
                    "start": 1376,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "252968346",
            "title": "Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks",
            "text": "Large-scale pre-trained multilingual language models (Devlin et al., 2019;Conneau and Lample, 2019;Huang et al., 2019;Conneau et al., 2020;Luo et al., 2021) have shown promising transferability in zeroshot cross-lingual transfer (ZSXLT), where pretrained language models (PLMs) are fine-tuned using a labeled task-specific dataset from a richresource source language (e.g., English or Spanish) and then evaluated on zero-resource target languages. Multilingual PLMs yield a universal representation space across different languages, thereby improving multilingual task performance (Pires et al., 2019;Chen et al., 2019). Recent work has enhanced cross-lingual transferability by reducing the discrepancies between languages based on translation approaches during fine-tuning (Fang et al., 2021;Zheng et al., 2021;Yang et al., 2022). Our paper focuses on when translated datasets are available for cross-lingual transfer (XLT). * Equal contribution Conneau et al. (2018) provided two translationbased XLT baselines: translate-train and translate-test. The former fine-tunes a multilingual PLM (e.g., multilingual BERT) using the original source language and machine-translated target languages simultaneously and then evaluates it on the target languages. Meanwhile, the latter fine-tunes a source language-based PLM (e.g., English BERT) using the original source language and then evaluates it on the machine-translated source language. Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test.",
            "score": 0.5637125434959647,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 74,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 74,
                    "end": 99,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "202541545"
                },
                {
                    "start": 118,
                    "end": 139,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 139,
                    "end": 156,
                    "matchedPaperCorpusId": "235303732"
                },
                {
                    "start": 581,
                    "end": 601,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 601,
                    "end": 619,
                    "matchedPaperCorpusId": "174799553"
                },
                {
                    "start": 775,
                    "end": 794,
                    "matchedPaperCorpusId": "221641003"
                },
                {
                    "start": 794,
                    "end": 813,
                    "matchedPaperCorpusId": "235436224"
                },
                {
                    "start": 813,
                    "end": 831,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 1650,
                    "end": 1671,
                    "matchedPaperCorpusId": "215548041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92724609375
        },
        {
            "corpus_id": "259095896",
            "title": "M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
            "text": "To boost the language diversity and support the evaluation across different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2, VIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data into 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each task in our first version. More multilingual samples will be supported in the future. We adopt the distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual translation models. As there are no native speakers for different languages, we adopt an automatic filtering mechanism to ensure the translation quality, where languages with translation BLEU scores from English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages are kept (see Appendix for detailed language names).",
            "score": 0.5630338410210626,
            "section_title": "Stage IV: Key Datasets Translation",
            "char_start_offset": 12869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 891
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "258557740",
            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
            "text": "lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can provide insights for understanding language-specific challenges in RE, and further research in cross-lingual representation and transfer learning. The contributions of this paper are:\n\n\u2022 We introduce MultiTACRED, a translation of the widely used, large-scale TACRED dataset into 12 typologically diverse target languages: Arabic, German, Spanish, French, Finnish, Hindi, Hungarian, Japanese, Polish, Russian, Turkish, and Chinese.\n\n\u2022 We present an evaluation of monolingual, cross-lingual, and multilingual models to evaluate target language performance for all 12 languages.\n\n\u2022 We present insights into the quality of machine translation for RE, analyzing alignment as well as language-specific errors.",
            "score": 0.5627699423191689,
            "section_title": "Introduction",
            "char_start_offset": 3611,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "276421759",
            "title": "Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation",
            "text": "These findings emphasize the need for future research aimed at refining model architectures and dataset structures to enhance cross-lingual robustness. Possible directions include: Balanced Multilingual Datasets: The discrepancies observed across languages suggest that dataset imbalances in degradation conditions and subjective ratings contribute to inconsistent performance. A concerted effort to develop high-quality, linguistically diverse datasets with standardized degradation pipelines is crucial. Calibration Mechanisms: Scaling functions and fine-tuning strategies tailored to specific linguistic traits may help mitigate biases introduced by dataset variability. Particularly, improving discontinuity detection across languages appears critical given its low PCC results across both models. Reliance on Subjective Labels: The observed skew in the English discontinuity dimension suggests that objective scoring methods may introduce unintended biases, leading to inconsistencies across languages. To ensure more reliable model evaluation, future work should prioritize subjective MOS labels over objective scores, thereby reducing the risk of error propagation from automated scoring models.",
            "score": 0.558340444372411,
            "section_title": "Future Directions",
            "char_start_offset": 14227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
            "score": 0.5583017952127597,
            "section_title": "Challenge 3: Cultural Bias",
            "char_start_offset": 13020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1304
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 149,
                    "matchedPaperCorpusId": "265445838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "265019003",
            "title": "Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval",
            "text": "We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.",
            "score": 0.5512127153975315,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "276558229",
            "title": "Chitrarth: Bridging Vision and Language for a Billion People",
            "text": "Figure 8 explores the impact of multilingual training data on the English academic benchmarks. We compare our model's performance when trained with only English, bilingual, and multilingual data across both stages. Consistent with the findings of [Scao et al., 2022], expanding the range of languages in the training data improves multilingual capabilities but leads to decreased performance on academic English datasets. This underscores a key challenge in balancing cross-lingual performance.",
            "score": 0.5501286254765643,
            "section_title": "Results",
            "char_start_offset": 19282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51318359375
        },
        {
            "corpus_id": "269033414",
            "title": "XNLIeu: a dataset for cross-lingual NLI in Basque",
            "text": "Most famous NLI datasets have also been reported to include biases and artifacts (Gururangan et al., 2018;Poliak et al., 2018;Tsuchiya, 2018;McCoy et al., 2019) that should be considered when analyzing the results, as they seem to have critical effects on the performance of systems.Artetxe et al. (2020) analyzes the effect that translated datasets have in cross-lingual settings, due to the so-called translationese (Volansky et al., 2013), and concludes that mismatches between the origin of training and evaluation datasets cause an important impact on the robustness of evaluation.\n\nEvaluation of LLMs.Nowadays, the focus of the research on evaluation has shifted due to the outstanding growth of LLMs.These models can achieve comparable results to fine-tuned pre-trained models with zero-shot and few-shot approaches for evaluation.Consequently, the focus has shifted towards assessing the models' overall capabilities rather than their performance on specific tasks (Guo et al., 2023) was not previously available for this language.\n\n3 The XNLIeu dataset\n\nXNLIeu has been created by machine-translating the English XNLI development and test sets to Basque2 followed by a manual post-edition step3 .Some examples of XNLIeu are shown in Table 2.We also release the machine-translated version prior to post-edition, dubbed XNLIeu MT , which we use to analyze the effect of post-edition (see Section 5.1).Additionally, we created an original Basque test set from scratch, henceforth referred to as native, and compared the results with XNLIeu and XNLIeu MT (see Section 5.2).Inspired by Bowman et al. (2015) and Artetxe et al. (2020), we performed the following steps to build the native dataset:\n\n\u2022 As a starting point, we extracted 5, 000 sentences from recent news in Basque, ensuring that they were not previously seen by the models used in the experiments.For this, we scraped Basque News sites and selected sentences from documents whose creation time was posterior to the release date of the pretraining corpora.",
            "score": 0.5497903731329434,
            "section_title": "NLI biases & artifacts.",
            "char_start_offset": 6065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 586
                },
                {
                    "start": 588,
                    "end": 607
                },
                {
                    "start": 607,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 1039
                },
                {
                    "start": 1041,
                    "end": 1061
                },
                {
                    "start": 1063,
                    "end": 1205
                },
                {
                    "start": 1205,
                    "end": 1250
                },
                {
                    "start": 1250,
                    "end": 1408
                },
                {
                    "start": 1408,
                    "end": 1578
                },
                {
                    "start": 1578,
                    "end": 1699
                },
                {
                    "start": 1701,
                    "end": 1864
                },
                {
                    "start": 1864,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 81,
                    "end": 106,
                    "matchedPaperCorpusId": "4537113"
                },
                {
                    "start": 106,
                    "end": 126,
                    "matchedPaperCorpusId": "21382535"
                },
                {
                    "start": 126,
                    "end": 141,
                    "matchedPaperCorpusId": "5074049"
                },
                {
                    "start": 141,
                    "end": 160,
                    "matchedPaperCorpusId": "59599752"
                },
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 1590,
                    "end": 1610,
                    "matchedPaperCorpusId": "14604520"
                },
                {
                    "start": 1615,
                    "end": 1636,
                    "matchedPaperCorpusId": "215548041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "248119025",
            "title": "A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference",
            "text": "Experimental setup We use the XNLI dataset (Conneau et al., 2018) to construct translation pairs where source and target are English and other languages, respectively. We use awesome-align (Dou and Neubig, 2021) to align attribution scores for the corresponding words in translation pairs. 4 4 We use the model provided by the authors, which was multilingually fine-tuned without consistency optimization, due to its good zero-shot performance. We examine the effect of word alignments in Appendix D. score and the loss. InputXGradient with L 2 aggregation is the best performing method in both cases.\n\nFigure 2: Comparison of cross-lingual faithfulness along output and aggregation dimensions. L 2 mostly outperforms mean (\u00b5) aggregation and calculations with respect to the loss are the same as or slightly better than ones with respect to the top prediction score.\n\nWe fine-tune mBERT and XLM-R base for English on the MNLI dataset (Williams et al., 2018a) with 3 different seeds for each. For cross-lingual faithfulness evaluation, we only use the languages that are common in the top-5 languages for both types of cross-lingual models(when performing zero-shot prediction on non-English languages). This gives Bulgarian, German, Spanish and French (C = 5). The cross-lingual performance of our models on all XNLI languages appears in Appendix A. Table 1 shows cross-lingual faithfulness results for each attribution method, when computing attributions with regard to top prediction or loss, and when aggregating input scores with L 2 or mean aggregation. The results exhibit a large variation, indicating that our cross-lingual faithfulness evaluation is able to expose differences between attribution methods. InputXGradient with L 2 aggregation is the most faithful attribution method for both types of attribution calculation. We also observe that gradient-based attribution methods (first 8 rows in Table 1) usually generate more faithful explanations than perturbation-based ones (last two rows), in line with prior work (Atanasova et al., 2020). Figure 2 shows the effect of aggregation methods and output mechanisms on cross-lingual faithfulness. In all cases, L 2 aggregation out",
            "score": 0.5472945925850874,
            "section_title": "Faithfulness Experiments",
            "char_start_offset": 14296,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "In this work, we translate and make available mMARCO, a multilingual IR dataset in 13 dif-ferent languages. This resource can be used for training and evaluating models. Additionally, we train and evaluate several monolingual and multilingual Transformer-based models on these datasets and provide benchmarks for further study in multilingual IR. As a way to encourage future creation of more datasets in new languages, we made available our translation code. In addition, all models and datasets are available at HuggingFace. Our findings indicate that multilingual models finetuned on multilingual datasets achieve competitive results when compared to monolingual approaches (both in pretraining and finetuning approaches). Furthermore, we show that the translation quality has a significant impact on retrieval results. This outcome showed to be even greater when considering non-Latin based languages, such as Russian and Chinese, where the commercial translation achieved higher results in contrast to translations from open sourced models. Our results also showed that translating datasets is a feasible mechanism to overcome the labeled data scarcity. \n\nFurthermore, we demonstrated how a lighter distilled model, MINILM, is competitive when finetuned in the same way as a much larger model. As future work, we would like to evaluate our finetuned models on a dataset in a language never seen during the language model pretraining or finetuning.",
            "score": 0.5462637900461722,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 25324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1452
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85205078125
        },
        {
            "corpus_id": "215548041",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "text": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018;Artetxe et al., 2020). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models. The resulting multilingual datasets are generally used for evaluation only, relying on existing English datasets for training. \n\nClosely related to that, cross-lingual transfer learning aims to leverage large datasets available in one language-typically English-to build multilingual models that can generalize to other languages. Previous work has explored 3 main approaches to that end: machine translating the test set into English and using a monolingual English model (TRANSLATE-TEST), machine translating the training set into each target language and training the models on their respective languages (TRANSLATE-TRAIN), or using English data to finetune a multilingual model that is then transferred to the rest of languages (ZERO-SHOT). \n\nThe dataset creation and transfer procedures described above result in a mixture of original, 1 human translated and machine translated data when dealing with cross-lingual models. In fact, the type of text a system is trained on does not typically match the type of text it is exposed to at test time: TRANSLATE-TEST systems are trained on original data and evaluated on machine translated test sets, ZERO-SHOT systems are trained on original data and evaluated on human translated test sets, and TRANSLATE-TRAIN systems are trained on machine translated data and evaluated on human translated test sets. \n\nDespite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI).",
            "score": 0.5446236817260166,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1970
                },
                {
                    "start": 1973,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2413
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 310,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 310,
                    "end": 331,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9716796875
        },
        {
            "corpus_id": "3080112",
            "title": "A Unified Multilingual Semantic Representation of Concepts",
            "text": "Monolingual. We show in Table 2 the performance of different systems in terms of Spearman and Pearson correlations on the English, German, and French RG-65 datasets. On the German and French datasets, our system outperforms the comparison systems according to both evaluation measures. It achieves considerable Spearman and Pearson correlation leads of 0.1 and 0.2, respectively, on the French dataset in comparison to the best system. Also on the English RG-65 dataset, our system attains competitive performance according to both Spearman and Pearson correla- tions. We note that most state-of-the-art systems on the dataset (e.g., ADW) are restricted to the English language only. \n\nCross-lingual. Pearson correlation results on the three cross-lingual RG-65 datasets are presented in Table 3. Similarly to the monolingual experiments, our system proves highly reliable in the cross-lingual setting, improving the performance of the comparison systems on all three language pairs. Moreover, MUFFIN pivot attains the best results among the pivot systems on all datasets, confirming the reliability of our system in the monolingual setting. We note that since the cross-lingual datasets were built by translating the word pairs in the original English RG-65 dataset, the pivot-based comparison systems proved to be highly competitive, outperforming the CL-MSR-2.0 system by a considerable margin.",
            "score": 0.5416577376955081,
            "section_title": "Results",
            "char_start_offset": 20052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "7681779",
            "title": "A Framework for the Construction of Monolingual and Cross-lingual Word Similarity Datasets",
            "text": "Along with the monolingual evaluation, we also performed an evaluation on four of the automatically created cross-lingual datasets. The evaluated language pairs were Spanish-English, Spanish-French, Spanish-German, and English-Farsi. In each case a proficient speaker of both languages was selected to carry out the evaluation. The Pearson correlations of the human judges with the automatically generated scores were 0.89 for Spanish-English, 0.94 for Spanish-French, 0.91 for Spanish-German, and 0.92 for English-Farsi, showing the reliability of our cross-lingual dataset creation process and reinforcing the quality of the newly created monolingual datasets.",
            "score": 0.541620383562673,
            "section_title": "Cross-lingual Datasets",
            "char_start_offset": 12157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 662
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75634765625
        },
        {
            "corpus_id": "252531677",
            "title": "An Empirical Study on Cross-X Transfer for Legal Judgment Prediction",
            "text": "Across all cross-lingual settings (groups B-C -Table 1), the use of Adapters improves substantially the overall performance. The multilingual adapterbased XLM-R in group B1 (Table 1) has compa-rable performance to the NativeBERTs models of group A2, where the training dataset has been artificially augmented with machine translations. In a similar setting (group B2 -Table 1), the multilingual adapter-based XLM-R in group B2 has the best overall results, combining the benefits of both cross-lingual transfer and data augmentation. \n\nWith respect to cross-lingual performance parity, the adapter-based XLM-R model has also the highest performance parity (least diff. in the last column of Table 1), while augmenting the dataset with NMT translations leads to both the worst-case (language) performance and best performance for the least represented language (Italian). \n\nIn conclusion, cross-lingual transfer with an augmented dataset comprised of the original and machine-translated versions of all documents, has the best overall performance with a vibrant improvement (3% compared to our strong baselinessecond part of Group A1 in Table 1) in Italian, the least represented language.",
            "score": 0.541426557616469,
            "section_title": "Fine-tuning with Adapters",
            "char_start_offset": 11702,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 533
                },
                {
                    "start": 536,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.826171875
        },
        {
            "corpus_id": "253098917",
            "title": "Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings",
            "text": "In this sense, our experiments show that, contrary to what previous research sug-gested (Fei et al., 2020;Li et al., 2021), the zeroshot model-based approach obtains the best results when high-capacity multilingual models including the target language and domain are available. Second, when the performance of the multilingual language model is not optimal for the specific target language or domain (for example when working on a text genre and domain for which available language models have not been trained), or when the required hardware to work with high-capacity language models is not easily accessible, then datatransfer based on translate and project constitutes a competitive option. Third, we observe that machine translation data often generates training and test data which is, due to important differences in language use, markedly different to the signal received when using gold standard data in the target language. These discrepancies seem to explain the larger error rate of the translate and project method with respect to the zero-shot technique. Finally, we create manually projected datasets for four languages and automatically projected datasets for seven languages. We use them to train and evaluate cross-lingual sequence labelling models. Additionally, they are also used to extrinsically evaluate machine translation and word alignment systems. These new datasets, together with the code to generate them are publicly available to facilitate the reproducibility of results and its use in future research.1 2 Related work 2.1 Data-based cross-lingual transfer Data-based cross-lingual transfer methods aim to automatically generate labelled data for a target language. Some of these methods exploit parallel data. Ehrmann et al. (2011) automatically annotate the English version of a multi-parallel corpus and projects the annotations into all the other languages using statistical alignments of phrases. Wang and Manning (2014) project model expectations rather than labels, which facilities transfer of model uncertainty across languages. Ni et al. (2017) use a heuristic scheme that effectively selects goodquality projection-labeled data from noisy data. They also project word embeddings from a target language into a source language, so that the source-language sequence labelling system can be applied to the target language without re-training.",
            "score": 0.5385318662044805,
            "section_title": "Introduction",
            "char_start_offset": 4011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 106,
                    "matchedPaperCorpusId": "215754572"
                },
                {
                    "start": 1743,
                    "end": 1764,
                    "matchedPaperCorpusId": "18220955"
                },
                {
                    "start": 1934,
                    "end": 1957,
                    "matchedPaperCorpusId": "6723413"
                },
                {
                    "start": 2070,
                    "end": 2086,
                    "matchedPaperCorpusId": "13410520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7763671875
        },
        {
            "corpus_id": "276724936",
            "title": "WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense Retrieval",
            "text": "To validate the presented bitext mining method, the resulting dataset is compared to existing parallel corpora in terms of average translation quality. Therefore, a random sample of 20,000 bitext pairs is selected from each corpus and evaluated based on the GEM-BA metric, assigning scores on a 0 to 100 scale. The final average scores per bitext dataset are listed in Table 4. Among the evaluated bitext datasets, WMT 2019 stands out as the largest, containing 124 million bitext pairs across nine language pairs. The corpus extracted from WebFAQ is two orders of magnitude smaller, but yet surpasses both BUCC 2018 and Tatoeba in terms of dataset size. Regarding translation quality, our dataset achieves the highest average GEMBA score and the lowest standard deviation. This suggests that the cosine similarity thresholding for cross-lingual sentence embeddings, described in the methodology, effectively produces a large-scale bitext corpus that remains comparable to existing resources in terms of translation accuracy. The six-point gap in scores compared to WMT 2019 and BUCC 2018 supports this claim. We believe that this holds true despite potential biases being introduced by using the same approach for LLM-based translation assessment for both dataset generation and evaluation. Additionally, our findings are corroborated by the comparison with mMARCO, an automatically translated retrieval dataset, which was created witha focus on translation quality, yet exhibits significantly 14 https://huggingface.co/datasets/PaDaS-Lab/webfaq-bitexts lower GEMBA scores than our dataset. Overall, these findings confirm the effectiveness of our approach combining cross-lingual sentence embeddings with automated quality assessment in mining cross-lingual QA pairs.",
            "score": 0.5378854750399037,
            "section_title": "Quality Evaluation",
            "char_start_offset": 30381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1769
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "234679313",
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "text": "An effective method for cross-lingual transfer is to fine-tune a bilingual or multilingual model on a supervised dataset in one language and evaluating it on another language in a zero-shot manner. Translating examples at training time or inference time are also viable alternatives. However, there are costs associated with these methods that are rarely addressed in the literature. In this work, we analyze cross-lingual methods in terms of their effectiveness (e.g., accuracy), development and deployment costs, as well as their latencies at inference time. Our experiments on three tasks indicate that the best cross-lingual method is highly task-dependent. Finally, by combining zero-shot and translation methods, we achieve the state-of-the-art in two of the three datasets used in this work. Based on these results, we question the need for manually labeled training data in a target language. Code and translated datasets are available at https://github.com/unicamp-dl/cross-lingual-analysis",
            "score": 0.5323296197134935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89404296875
        },
        {
            "corpus_id": "270380088",
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "text": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
            "score": 0.53215044572944,
            "section_title": "Related Work",
            "char_start_offset": 20594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 1248,
                    "end": 1268,
                    "matchedPaperCorpusId": "258685666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94482421875
        },
        {
            "corpus_id": "233241129",
            "title": "Adaptive Sparse Transformer for Multilingual Translation",
            "text": "Datasets. Models are evaluated on two widely used multilingual translation datasets at different scales. More details of these datasets are included in Appendix. \n\n\u2022 Public-24. This medium-scale dataset contains parallel corpora between English and 24 languages, collected from public sources such as WMT shared tasks (Liu et al., 2020). It provides O2M and M2O translations. \n\n\u2022 OPUS-100. This is a large-scale multilingual translation dataset covering 100 languages (Zhang et al., 2020). It serves for M2M translations. \n\nBaselines. We include the following strong baselines which are commonly used in multilingual translation. \n\n\u2022 Multilingual Transformer (Vaswani et al., 2017). A single Transformer model for multilingual translation shares all parameters among languages. \n\n\u2022 Multi-decoder Transformer (Sen et al., 2019;Kong et al., 2021). Similar to the multilingual Transformer, it has an encoder-decoder architecture, but replaces the decoder with multiple decoders. It is used for one-to-many translation, and target languages are clustered into families based on their proximity (Lewis, 2009). One decoder is shared by target languages from the same family, and each family has an exclusive decoder. \n\n\u2022 Multi-encoder Transformer. Similar to multidecoder Transformer, it replaces a single encoder with multiple encoders. It is used for many-to-one translation, and each encoder corresponds to one family of source languages. \n\n\u2022 Adapter based Transformer (Bapna and Firat, 2019) Transformer. Each language pair is routed to its corresponding adapter layers which are finetuned on the same data with other Transformer parameters frozen. \n\nEvaluation metrics. As we consider both translation quality and model efficiency, two metrics are used for model evaluation. BLEU measures the translation quality by comparing the predicted and reference translations. As for the efficiency, we report the decoding speed, i.e., the number of decoded tokens per second (tok/s) when one GPU is used with a batch size of 4096 tokens during inference.",
            "score": 0.5317037072320414,
            "section_title": "Experimental Setup",
            "char_start_offset": 18710,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 161
                },
                {
                    "start": 164,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 777
                },
                {
                    "start": 780,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 488,
                    "matchedPaperCorpusId": "272727920"
                },
                {
                    "start": 659,
                    "end": 681,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 808,
                    "end": 826,
                    "matchedPaperCorpusId": "196170603"
                },
                {
                    "start": 1466,
                    "end": 1489,
                    "matchedPaperCorpusId": "202660912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83056640625
        },
        {
            "corpus_id": "270619905",
            "title": "Selected Languages are All You Need for Cross-lingual Truthfulness Transfer",
            "text": "To build a multilingual benchmark, we first select a few target languages from a diverse linguistic pool. Our evaluation system incorporates nine different languages: English (En), French (Fr), German (De), Spanish (Es), Chinese (Zh), Japanese (Ja), Russian (Ru), Thai (Th) and Arabic (Ar). For the questions and answers in original dataset, we translate them into the target languages through prompting GPT-4 2 . The prompt template we used is shown in Appendix A.1. \n\nConsidering the instability of LLM-based translation, we also recruited native speakers to validate the translations. Each translation is reviewed by three native speakers, who vote on its correctness. If at least two of the three agree, the translation is considered accurate. The average accuracy of the translation is 96.4% including both the questions and corresponding answers. We further manually checked and rectified these translation errors.",
            "score": 0.5303461965377729,
            "section_title": "Multilingualism Formulation",
            "char_start_offset": 6310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 920
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8232421875
        },
        {
            "corpus_id": "26127787",
            "title": "A Survey of Cross-lingual Word Embedding Models",
            "text": "The first two widely used tasks are intrinsic evaluation tasks: They evaluate cross-lingual embeddings in a controlled in vitro setting that is geared towards revealing certain characteristics of the representations. The major downside with these tasks is that good performance on them does not generalize necessarily to good performance on downstream tasks (Tsvetkov, Sitaram, Faruqui, Lample, Littell, Mortensen, Black, Levin, & Dyer, 2016;Schnabel, Labutov, Mimno, & Joachims, 2015).\n\nWord similarity This task evaluates how well the notion of word similarity according to humans is emulated in the vector space. Multi-lingual word similarity datasets are multilingual extensions of datasets that have been used for evaluating English word representations. Many of these originate from psychology research and consist of word pairs -ranging from synonyms (e.g., car -automobile) to unrelated terms (e.g., noon -string) -that have been annotated with a relatedness score by human subjects. The most commonly used ones of these human judgement datasets are: a) the RG dataset (Rubenstein & Goodenough, 1965); b) the MC dataset (Miller & Charles, 1991); c) the WordSim-353 dataset (Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002), a superset of MC; and d) the SimLex-999 dataset (Hill, Reichart, & Korhonen, 2015). Extending them to the multilingual setting then mainly involves translating the word pairs into different languages: WordSim-353 has been translated to Spanish, Romanian, and Arabic (Hassan & Mihalcea, 2009) and to German, Italian, and Russian (Leviant & Reichart, 2015); RG was translated to German (Gurevych, 2005), French, (Joubarne &Inkpen, 2011), Spanish andFarsi (Camacho-Collados, Pilehvar, &Navigli, 2015); and SimLex-999 was translated to German, Italian and Russian (Leviant & Reichart, 2015)",
            "score": 0.5294855349751624,
            "section_title": "Tasks",
            "char_start_offset": 70954,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 442,
                    "matchedPaperCorpusId": "6157443"
                },
                {
                    "start": 442,
                    "end": 485,
                    "matchedPaperCorpusId": "6197592"
                },
                {
                    "start": 1077,
                    "end": 1108,
                    "matchedPaperCorpusId": "18309234"
                },
                {
                    "start": 1128,
                    "end": 1152,
                    "matchedPaperCorpusId": "145580646"
                },
                {
                    "start": 1181,
                    "end": 1255,
                    "matchedPaperCorpusId": "12956853"
                },
                {
                    "start": 1305,
                    "end": 1339,
                    "matchedPaperCorpusId": "3226120"
                },
                {
                    "start": 1523,
                    "end": 1548,
                    "matchedPaperCorpusId": "1856431"
                },
                {
                    "start": 1641,
                    "end": 1657,
                    "matchedPaperCorpusId": "16584933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "270521335",
            "title": "Datasets for Multilingual Answer Sentence Selection",
            "text": "We conducted several experiments to estimate the benefits provided by our multilingual datasets, assessing their impact on different aspects of model performance.\n\nCross-Lingual: Models trained on the mASNQ dataset consistently outperformed those trained on ASNQ, demonstrating higher MAP and P@1 scores across all languages.This confirms the effectiveness of mASNQ in enhancing cross-lingual model performance (Appendix C).\n\nRanks Correlation: Models trained on mASNQ and mWikiQA showed strong positive correlations in their ranking outputs compared to those trained on ASNQ and WikiQA.This indicates consistent translation quality and robust model performance (Appendix D).\n\nPassage Ranking: Models trained on mMARCO outperformed those trained on MSMARCO, emphasizing the significant advantages provided by adapting models trained on our multilingual datasets for various tasks (Appendix E).",
            "score": 0.5293614824304309,
            "section_title": "Ablation Studies",
            "char_start_offset": 14409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 164,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 424
                },
                {
                    "start": 426,
                    "end": 587
                },
                {
                    "start": 587,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 893
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81982421875
        },
        {
            "corpus_id": "261822560",
            "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
            "text": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages. In this paper, we create SIB-200\u2014a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU). For many of the languages covered in SIB-200, this is the first publicly available evaluation dataset for NLU. The dataset is based on Flores-200 machine translation corpus. We annotated the English portion of the dataset and extended the sentence-level annotation to the remaining 204 languages covered in the corpus. Despite the simplicity of this task, our evaluation in full-supervised setting, cross-lingual transfer setting and prompting of large language model setting show that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages. We found that languages unseen during the pre-training of multilingual language models, languages from under-represented families (like Nilotic and Altantic-Congo), and languages from the regions of Africa, Americas, Oceania and South East Asia, often have the lowest performance on our topic classification dataset. We hope our dataset %will encourages a more inclusive evaluation of multilingual language models on a more diverse set of languages.",
            "score": 0.52900714588032,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "257206042",
            "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
            "text": "Due to all these challenges and the ubiquity of language technologies that are used by large populations of non-English speaking users, addressing fairness and bias, taking into account diverse linguistic, socio-linguistic, and cultural factors, is of utmost importance. Interdisciplinary and multicultural teams are crucial to identifying, measuring, and mitigating harms caused by bias in multilingual models. Better evaluation benchmarks covering diverse linguistic phenomena and cultures will lead to better fairness evaluation. \n\nRegarding data collection, as discussed in Section 3.1, it would be prudent to avoid directly translating datasets for training or evaluation in applications where fairness is critical. As we have shown in this survey, it is not enough to collect datasets in multiple languages for measuring and mitigating bias, although even these are lacking for most languages worldwide. Zero-shot techniques that ignore the cultural nuances of a language should be used with care in fairness-critical applications, as linguistically similar languages may have different cultural values and vice versa. Finally, multilingual models and systems need to incorporate shared value systems that take into account diverse cultures, although some cultural differences may still go unacknowledged.",
            "score": 0.5283533899767427,
            "section_title": "Moving Towards Inclusive Systems in All Languages",
            "char_start_offset": 34171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72314453125
        },
        {
            "corpus_id": "276724936",
            "title": "WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense Retrieval",
            "text": "The state-of-the-art approach for creating crosslingual datasets is -beyond translation -automated sentence alignment via similarity search over text embeddings. Notable methods include LASER (Language-Agnostic SEntence Representations) [2] and its successor LaBSE (Language-Agnostic BERT Sentence Embeddings) [11]. LASER emplys a sequence-to-sequence architecture for encoding, while LaBSE utilizes a Transformer-based model architecture. These advancements in multilingual sentence embeddings contribute to reduced error rates in cross-lingual similarity search and thus enable efficient bitext mining. Beyond bitext mining, recent research has explored the application of Large Language Models (LLMs) for automated translation evaluation. For example, Kocmi et al. introduce GEMBA [18], a GPT-based metric for translation evaluation, and demonstrate that LLMs can assess translation quality on par with human evaluators. These findings suggest that LLMs can play a critical role in both generating and validating cross-lingual datasets. \n\nNotable datasets in the field of bitext mining include WMT 2019 [12], a massive dataset of 124M bitext pairs spanning nine language combinations, introduced as part of the reocurring translation task at the Conference on Machine Translation (WMT). Another resource for bitext pairs is Tatoeba 5 , a community-driven collection of sentences and their translations provided in a multitude of languages, parts of which are easily accessible through HuggingFace 6 . Additionally, the BUCC 2018 7 dataset, originating from the 11th Workshop on Building and Using Comparable Corpora (BUCC2018), contains 35k bitext pairs in four language combinations.",
            "score": 0.525717436210873,
            "section_title": "RELATED WORK",
            "char_start_offset": 9093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1039
                },
                {
                    "start": 1042,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1687
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "56895585"
                },
                {
                    "start": 310,
                    "end": 314,
                    "matchedPaperCorpusId": "220347683"
                },
                {
                    "start": 784,
                    "end": 788,
                    "matchedPaperCorpusId": "257232490"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.689453125
        },
        {
            "corpus_id": "257206042",
            "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
            "text": "Several datasets have been put forward for the purpose of multilingual evaluation, and Table 1 describes these datasets along with details regarding their utility. These include the languages they cover, whether or not they evaluate bias over pretrained representations or a downstream task, and the downstream tasks and dimensions they cater toward. Zhao et al. (2020) was among the first papers to quantify biases in multilingual spaces and does so using both extrinsic and intrinsic evaluation techniques. Their findings indicate that some factors that influence bias in multilingual embeddings include the language's linguistic properties, the target language used for the alignment of the embeddings, and transfer learning on these embeddings induces bias. Additionally, there is the possibility that non-Germanic languages do not align well with Germanic ones, and further work would be required to derive conclusions as to how this affects fairness measurements. Huang et al. (2020) released the first multilingual Twitter corpus for hate speech detection, annotated with the author's demographic attributes (age, country, gender, race/ethnicity), which allows for fairness evaluation across hate speech classifiers. Through experiments, they prove that variations in language, which are highly correlated with demographic attributes (Preo\u0163iuc-Pietro and Ungar, 2018;Osiapem, 2007), can result in biased classifiers. However, there are some promising results from Liang et al. (2020), which proposes a novel debiasing method using Dufter and Sch\u00fctze (2019). While the multilingual model is originally debiased over English, results show its effectiveness for zero-shot debiasing over Chinese. C\u00e2mara et al. (2022) measures both unisectional and intersectional social biases over gender, race, Table 1: Datasets for fairness evaluation beyond English. I = Intrinsic, E = Extrinsic and ethnicity in multilingual language models. This is particularly relevant, as in a practical setting, treating identities as composites of various demographic attributes is a necessity. Kaneko et al. (2022) measures gender bias in masked language models and proposes a method to use parallel corpora to evaluate bias in languages shown to have high correlations with human bias annotations.",
            "score": 0.5248552575329877,
            "section_title": "An Outline of Fairness Evaluation in the Context of Multilinguality",
            "char_start_offset": 20046,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2280
                }
            ],
            "ref_mentions": [
                {
                    "start": 351,
                    "end": 369,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 970,
                    "end": 989,
                    "matchedPaperCorpusId": "211259366"
                },
                {
                    "start": 1341,
                    "end": 1374,
                    "matchedPaperCorpusId": "51799739"
                },
                {
                    "start": 1374,
                    "end": 1388,
                    "matchedPaperCorpusId": "145543366"
                },
                {
                    "start": 1471,
                    "end": 1490,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 1538,
                    "end": 1563,
                    "matchedPaperCorpusId": "120368714"
                },
                {
                    "start": 1700,
                    "end": 1720,
                    "matchedPaperCorpusId": "248006238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81640625
        },
        {
            "corpus_id": "276575630",
            "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
            "text": "Multilingual metaevaluation is important to reliably measure performance across languages which can vary depending on language characteristics (low vs. high resource, complex morphology, etc) and scripts (Latin vs. non-Latin). Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993;Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks. \n\nTo bridge those gaps, we propose a native metaevaluation multilingual benchmark for RAG systems. Our benchmark is built on top of the popular MIRACL (Zhang et al., 2023) dataset3 that includes native questions across 18 languages and relevance judgements of retrieved passages for multilingual retrieval evaluation. We extend MIRACL by generating answers in five languages with a diverse set of LLMs, and collecting judgements on the faithfulness and relevance of the answers using native expert human annotators. For the latter, we devised a structured annotation process that achieved a high rate of inter-annotator agreement. To evaluate the benchmark and set reference baseline results for others to compare against, we run LLM-as-a-judge experiments with various prompting techniques and state-of-the-art LLMs. \n\nTo summarize, our main contributions are: \n\n\u2022 We built and publicly release the first (to the best of our knowledge) native multilingual meta-evaluation RAG benchmark. \u2022 We developed a rigorous flow chart-based annotation process to achieve high interannotator agreement rate for both faithfulness and relevance judgements. \u2022 We evaluated the quality of the benchmark on three multi-lingual meta-evaluation aspects: prompt selection, model selection, and finegrained analysis. \u2022 We establish reference baselines of multilingual automatic evaluators on our benchmark, showcasing performance improvements when using advanced prompting and LLMs.",
            "score": 0.5245092851388375,
            "section_title": "Introduction",
            "char_start_offset": 1735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 414,
                    "matchedPaperCorpusId": "57174748"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "270562911"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97509765625
        },
        {
            "corpus_id": "222124957",
            "title": "Which *BERT? A Survey Organizing Contextualized Encoders",
            "text": "All of the areas discussed in this paper are applicable to multilingual encoders. However, progress in training, architecture, datasets, and evaluations are occurring concurrently, making it difficult to draw conclusions. We need more comparisons between competitive multilingual and monolingual systems or datasets. To this end, Wu and Dredze (2020) find that monolingual BERTs in low-resource languages are outperformed by multilingual BERT. Additionally, as zero-shot (or few-shot) cross-lingual transfer has inherently high variance (Keung et al., 2020), the variance of models should also be reported. \n\nWe anticipate cross-lingual performance being a new dimension to consider when evaluating text representations. For example, it will be exciting to discover how a small, highly-performant mono-lingual encoder contrasts against a multilingual variant; e.g., what is the minimum number of parameters needed to support a new language? Or, how does model size relate to the phylogenetic diversity of languages supported?",
            "score": 0.522614566792309,
            "section_title": "Evaluating Multilinguality",
            "char_start_offset": 31427,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1025
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396240234375
        },
        {
            "corpus_id": "273532366",
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "text": "Robust MLLMs can perform better on cross-lingual downstream tasks [48], [49]. The multilingual pre-training datasets introduced above are mostly automatically mined from the internet, and their data quality cannot be well guaranteed. Therefore, it is necessary to evaluate these corpora mined from the internet using different dimensions. \n\nIn the ACL 2023 Tutorial on MLLMs, Microsoft's researchers proposed to evaluate the quality of multilingual pretraining datasets from four aspects: multilingual distribution, data quality, source, and governance. In terms of multilingual distribution, researchers used the CommonCrawl dataset as an example, pointing out that although it includes more than 100 languages, 57 languages account for less than 0.001%, so when using it to construct multilingual datasets, one should consider whether to generate data consistent with downstream task languages. \n\nIn terms of data quality, it is suggested that researchers constructing datasets should not only focus on low-resource corpora but also pay attention to samples that are misclassified into other languages. They may be classified into some highresource language categories due to similarity with other languages and poor quality. At the same time, attention should be paid to machine-generated corpora and corpora containing pornographic or inappropriate information due to limited recognition tools. Researchers suggest evaluating the quality of multilingual pre-training datasets from dimensions such as data quantity, quality, number of domains, sustainability, and shareability.",
            "score": 0.52086364536677,
            "section_title": "A. Pre-training Resources",
            "char_start_offset": 27156,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1580
                }
            ],
            "ref_mentions": [
                {
                    "start": 66,
                    "end": 70,
                    "matchedPaperCorpusId": "218629929"
                },
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "253255430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.394775390625
        },
        {
            "corpus_id": "7665329",
            "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity",
            "text": "Subtask 1, i.e., multilingual semantic similarity, has five datasets for the five languages of the task, i.e., English, Farsi, German, Italian, and Spanish. These datasets were manually created with the help of trained annotators (as opposed to Mechanical Turk) that were native or fluent speakers of the target language. Based on these five datasets, 10 cross-lingual datasets were automatically generated (described in Section 2.2) for subtask 2, i.e., cross-lingual semantic similarity. \n\nIn this section we focus on the creation of the evaluation test sets. We additionally created a set of small trial datasets by following a similar process. These datasets were used by some participants during system development.",
            "score": 0.520161961796662,
            "section_title": "Task Data",
            "char_start_offset": 8053,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 489
                },
                {
                    "start": 492,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 720
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "269033414",
            "title": "XNLIeu: a dataset for cross-lingual NLI in Basque",
            "text": "Previous work has emphasized the importance of the origin of the train and test data in cross-lingual settings, i.e., whether they are original or created through translation.In particular, Artetxe et al. (2020) show that a mismatch in the origin between training and test data may have a serious impact on the results, particularly when comparing different cross-lingual strategies.Moreover, NLI datasets are known to be biased and contain artifacts that lead models to rely on superficial clues (Gururangan et al., 2018;Poliak et al., 2018;Tsuchiya, 2018;McCoy et al., 2019).To analyze the impact of these factors in XNLIeu, we have created a Native test set completely from scratch with original premises extracted from sources with content in Basque and arXiv:2404.06996v1[cs.CL] 10 Apr 2024 hypotheses provided by Basque speakers, which were specifically told to avoid such biases.\n\nUsing these datasets, we have conducted a series of experiments using mono-and multilingual language models for Basque, both discriminative and generative, and have tested different training variants for cross-lingual NLI in Basque.The experiments set a new baseline for NLI in Basque, and have served us to analyze the effect of professional post-edition compared to the automatic machine-translation system.We have also identified the most effective cross-lingual strategy for NLI in Basque, considering both translated and native sets.\n\nThis paper makes the following contributions:\n\n\u2022 We develop and release a new dataset for cross-lingual NLI in Basque, which is created by translating the English XNLI, through machine-translation and post-edition.We also release a machine-translated only version of the dataset, as well as a small native dataset for comparison purposes.\n\n\u2022 We conduct a series of cross-lingual Basque NLI experiments using several language models and following different cross-lingual strategies, and establish new baselines to facilitate research on Basque NLU.\n\n\u2022 We provide a detailed analysis of the results of our experiments to assess the impact of using different models, strategies and data sources.",
            "score": 0.5189732061722938,
            "section_title": "Introduction",
            "char_start_offset": 1846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 886
                },
                {
                    "start": 888,
                    "end": 1120
                },
                {
                    "start": 1120,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1426
                },
                {
                    "start": 1428,
                    "end": 1473
                },
                {
                    "start": 1475,
                    "end": 1642
                },
                {
                    "start": 1642,
                    "end": 1766
                },
                {
                    "start": 1768,
                    "end": 1975
                },
                {
                    "start": 1977,
                    "end": 2120
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 211,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 497,
                    "end": 522,
                    "matchedPaperCorpusId": "4537113"
                },
                {
                    "start": 522,
                    "end": 542,
                    "matchedPaperCorpusId": "21382535"
                },
                {
                    "start": 542,
                    "end": 557,
                    "matchedPaperCorpusId": "5074049"
                },
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "59599752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.751953125
        },
        {
            "corpus_id": "273662078",
            "title": "SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval",
            "text": "Multilingual and Cross-lingual QA have emerged as critical areas of NLP research, driven by the advance of LLMs and the creation of diverse multilingual QA datasets. Benchmarks such as TYDI QA (Clark et al., 2020), MLQA (Lewis et al., 2019), XQuAD (Artetxe et al., 2019), andMKQA (Longpre et al., 2021) have been instrumental in enabling evaluation across a wide range of languages, representing different language families and typologies. Most of these multilingual datasets, however, are still limited to evaluation on benchmarking datasets and not yet tested in real-world settings. \n\nVarious works have focused on generating synthetic multilingual QA data for training downstream QA models (Riabi et al., 2020;Shakeri et al., 2020). Notable contributions include PAXQA (Li and Callison-Burch, 2023), generating crosslingual QA datasets by leveraging parallel corpora and without requiring annotated non-English QA data. More recently, Engl\u00e4nder et al. (2024) introduced M2QA, a multi-domain multilingual QA benchmark of Indian languages. \n\nFor QA, zero-shot transfer learning has been studied extensively; models are typically fine-tuned on English data and then applied directly to other languages, leveraging cross-lingual capabilities of models like mBERT (Devlin, 2018) and XLM-R (Conneau, 2019). CORA (Asai et al., 2020) introduces a unified model for cross-lingual retrieval and multilingual QA, using NMT for data augmentation, either by translating English QA pairs into target languages or by translating non-English queries to English at inference time. Duan et al. (2021) assemble triples from different languages and use a knowledge injection strategy, via link prediction, to enrich a model's multilinguality. \n\nLatest significant contributions include QAMELEON (Agrawal et al., 2023), which uses prompt-tuning to generate synthetic QA data with as few as 5 examples per language.",
            "score": 0.5189000131067217,
            "section_title": "Multilingual QA",
            "char_start_offset": 6642,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1897
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 213,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 694,
                    "end": 714,
                    "matchedPaperCorpusId": "225067135"
                },
                {
                    "start": 1568,
                    "end": 1586,
                    "matchedPaperCorpusId": "245935213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60302734375
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "No 43.46 Yes 42.24  (2021). First row is the reported XGLM results, and the second is our reproduction of their results to validate our multilingual evaluation setup. Last two rows show that our multilingual model matches the XGLM results. \n\nEnglish-only evaluation. We first evaluate our multilingual model on the same set of English benchmarks we have used previously, in Table 6. \n\nMultilinguality significantly lowers accuracy on the English benchmark, which is in line with the results from Lin et al. ( 2021). \n\nMultilingual evaluation. Zero-shot multilingual evaluation is more challenging to setup because it requires writing new prompts for each new language. Therefore, instead of manually writing prompts for each language, we follow the strategy proposed by Lin et al. ( 2021), using English prompts for non-English examples-this can be viewed as cross-lingual zero-shot generalization. They validated this strategy by demonstrating its ability to achieve zero-shot performance on par with (and sometimes even better than) humanwritten language-specific prompts. This strategy also demonstrates cross-lingual abilities. \n\nWe evaluate on XNLI (Conneau et al., 2018), a multilingual NLI dataset that covers 8 of the languages we use for training. Our evaluation is different from the zero-shot evaluation of the XTREME benchmark (Hu et al., 2020). XTREME first finetunes the model on the English training data of each downstream task, then evaluates it on the non-English dataset, attempting cross-lingual generalization. Our evaluation avoids any finetuning, and instead relies entirely on zero-shot generalization. Results. Language-specific scaling laws. To explore how scale influences multilinguality, we train a wider range of models (i.e. 0.3-6B parameters) on a larger corpus of more than 300B tokens of text drawn from a variety of languages (Lauren\u00e7on et al., 2022).",
            "score": 0.5171643323894551,
            "section_title": "Embedding Norm Average EAI Results",
            "char_start_offset": 15670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1886
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6064453125
        },
        {
            "corpus_id": "257913598",
            "title": "LAHM : Large Annotated Dataset for Multilingual & Multi-Domain Hate Speech Identification",
            "text": "We adopted the methodology to utilize open source machine translation models for translation of English data to multilingual data and vice-versa. To select the best machine translation model for each of our languages, we evaluated a number of models on a small manually annotated dataset. The translations were carried out for each of the these languages: Hindi, Arabic, French, German and Spanish. Two types of translations were carried out using the translation models: \n\n1. Translation of English dataset collected from various open source hate speech datasets. \n\nAll the English language samples in this dataset were translated to other 5 languages. 2. Translation of multilingual data collected from Twitter. This data was collected for the above 5 languages, and each of this language data was translated into corresponding English language data. \n\nConsideration for choosing and evaluating the translation models was based on whether they were open source, free/easy to use, and the translation quality. Translation models used were: \n\n1. Google sheets translation 2. m2m-100-1.2B 10 . IndicTrans 11The evaluation metrics used were bleu, rougeL and semantic similarity. For semantic similarity, all-mpnet-base-v2 model from sentence-transformers library was used to calculate the cosine score between input and translated sentence embeddings. Figure 4.3.2 shows comparisons of bleu, rougeL and semantic similarity scores for the 3 models on different languages. After comparing the performances and taking other considerations into account, indicTrans model was selected for all translations of Indic languages (Hindi), and m2m-100-1.2B was selected for the other languages.",
            "score": 0.5156306982566726,
            "section_title": "Machine Translation Experiments",
            "char_start_offset": 13221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 564
                },
                {
                    "start": 567,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1681
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61572265625
        },
        {
            "corpus_id": "258865558",
            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
            "text": "We create the BUFFET benchmark to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. We adhere to the following design principles with our benchmark. \n\nStandardized few-shot samples. BUFFET provides three different training and validation sets of k-shots (e.g., k = 32) per task for a nonclassification task, or per class for a classification task, for each language. \n\nTask diversity. Existing cross-lingual benchmarks often focus on classification or retrieval (Hu et al., 2020b;Ruder et al., 2021;Liang et al., 2020). BUFFET encompasses a broad range of task types, such as classification, generation, extraction, and structured prediction tasks. By converting all tasks into the same text-to-text format, we eliminate the need for task-specific model modifications or template conversions. 5. \n\nBeyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;Ponti et al. 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;Artetxe et al., 2020b). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
            "score": 0.5155937372006543,
            "section_title": "Design Principles",
            "char_start_offset": 9586,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 235
                },
                {
                    "start": 238,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1368
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 567,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 567,
                    "end": 586,
                    "matchedPaperCorpusId": "233241004"
                },
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 1068,
                    "end": 1086,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1218,
                    "end": 1240,
                    "matchedPaperCorpusId": "216914383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9326171875
        },
        {
            "corpus_id": "273404117",
            "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
            "text": "In this paper, we introduced the Cross Lingual Auto Evaluation (CIA) Suite, a comprehensive framework for multilingual evaluation using LLMs. Our analysis demonstrated that fine-tuning LLMs on INTEL significantly improves evaluation accuracy, particularly in low-resource languages. Results from the RECON test set indicate that our fine-tuned models outperform even large proprietary models. Additionally, our evaluation against human assessments revealed a strong alignment between our models and human judgments, highlighting the effectiveness of cross-lingual fine-tuning in enhancing evaluation metrics across languages. Through extensive ablation studies, we explored zero-shot evaluation with our HERCULE model, established the importance of reference answers, examined various modeling choices, and assessed the effectiveness of weight merging techniques. By making our code, datasets, and models publicly available, we aim to encourage further research in developing and evaluating robust multilingual models.",
            "score": 0.5150057538226076,
            "section_title": "Conclusion",
            "char_start_offset": 27483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1018
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7841796875
        },
        {
            "corpus_id": "7665329",
            "title": "SemEval-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity",
            "text": "Over the past few years multilingual embeddings that represent lexical items from multiple languages in a unified semantic space have garnered considerable research attention (Zou et al., 2013;de Melo, 2015;Vuli\u0107 and Moens, 2016;Ammar et al., 2016;Upadhyay et al., 2016), while at the same time cross-lingual applications have also been increasingly studied (Xiao and Guo, 2014;Franco-Salvador et al., 2016). However, there have been very few reliable datasets for evaluating cross-lingual systems. Similarly to the case of multilingual datasets, these cross-lingual datasets have been constructed on the basis of conventional English word similarity datasets: MC-30 and WordSim-353 (Hassan and Mihalcea, 2009), and RG-65 (Camacho-Collados et al., 2015). As a result, they inherit the issues affecting their parent datasets mentioned in the previous subsection: while MC-30 and RG-65 are composed of only 30 and 65 pairs, WordSim-353 conflates similarity and relatedness in different languages. Moreover, the datasets of Hassan and Mihalcea (2009) were not re-scored after having been translated to the other languages, thus ignoring possible semantic shifts across languages and producing unreliable scores for many translated word pairs. For this subtask we provided ten high quality cross-lingual datasets, constructed according to the procedure of Camacho-Collados et al. (2015), in a semi-automatic manner exploiting the monolingual datasets of subtask 1. These datasets constitute a reliable evaluation framework across five languages.",
            "score": 0.5148475163737305,
            "section_title": "Subtask 2: Cross-lingual Semantic Similarity",
            "char_start_offset": 6498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1541
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 193,
                    "matchedPaperCorpusId": "931054"
                },
                {
                    "start": 193,
                    "end": 207,
                    "matchedPaperCorpusId": "29661510"
                },
                {
                    "start": 207,
                    "end": 229,
                    "matchedPaperCorpusId": "16452496"
                },
                {
                    "start": 248,
                    "end": 270,
                    "matchedPaperCorpusId": "5357629"
                },
                {
                    "start": 358,
                    "end": 378,
                    "matchedPaperCorpusId": "501172"
                },
                {
                    "start": 378,
                    "end": 407,
                    "matchedPaperCorpusId": "30389163"
                },
                {
                    "start": 683,
                    "end": 710,
                    "matchedPaperCorpusId": "1856431"
                },
                {
                    "start": 722,
                    "end": 753,
                    "matchedPaperCorpusId": "7681779"
                },
                {
                    "start": 1021,
                    "end": 1047,
                    "matchedPaperCorpusId": "1856431"
                },
                {
                    "start": 1352,
                    "end": 1382,
                    "matchedPaperCorpusId": "7681779"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.853515625
        },
        {
            "corpus_id": "234679313",
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "text": "In many languages, a common problem when using machine learning models for natural language processing (NLP) tasks is the low availability of high-quality datasets for finetuning (Huang et al., 2019), as data annotation is costly both in terms of money and time spent (Dandapat et al., 2009;Sabou et al., 2012). In contrast, multilingual pretrained models show surprisingly good cross-lingual zero-shot performance, i.e., models fine-tuned only on a dataset of a high-resource language, such as English, perform well on another language on the same task (Wu and Dredze, 2019;Conneau et al., 2020;Xue et al., 2021). Due to improvements in machine translation in the last few years (Wu et al., 2016;Lepikhin et al., 2020), automatically translating datasets from a high-resource to a low-resource language has also become an effective cross-lingual transfer strategy. \n\nCurrent literature, however, mostly focuses on the development and understanding of transfer learning methods that potentially lead to a better model with respect to some target task metrics, ignoring development costs, such as training data translation, and recurring costs, such as inference cost per example. This work analyzes the feasibility and cost-effectiveness of cross-lingual methods to answer the following question: given the availability of large supervised datasets in English and models pretrained on various languages, what is the most cost-effective way to use these resources for tasks in other languages? The answer to this question allows us to effectively develop and deploy natural language processing systems for tasks where there is not sufficient labeled data to fine-tune the models. To answer it, we analyze the following transfer learning techniques: 1) fine-tuning a model on a source language and evaluating it on the target language without translation, i.e., in a zeroshot manner; 2) automatic translation of the training dataset to the target language; 3) automatic translation of the test set to the source language at inference time and evaluation of a model fine-tuned on English. \n\nOur main contribution is to evaluate crosslingual transfer methods while also considering their financial and computational costs.",
            "score": 0.5141350619672449,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 179,
                    "end": 199,
                    "matchedPaperCorpusId": "202539539"
                },
                {
                    "start": 268,
                    "end": 291,
                    "matchedPaperCorpusId": "7380903"
                },
                {
                    "start": 291,
                    "end": 310,
                    "matchedPaperCorpusId": "9295759"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6142578125
        },
        {
            "corpus_id": "231749968",
            "title": "The GEM Benchmark: Natural Language Generation, its Evaluation and Metrics",
            "text": "Instead, a living benchmark that can adjust its datasets and specific evaluation metrics can be much more powerful and long-lived. This can, for example, be seen in Dynabench, 1 (Potts et al., 2020) which has a static evaluation, but interactively adds more test data through a human-in-the-loop approach. \n\nIncreasing multilingualism of NLG research. Another potentially harmful choice by benchmark creators is the choice of the languages of the included datasets. It is often assumed that work on English transfers to other languages (Bender, 2011). However, this assumption does not consider differences between the languages that lead to higher modeling complexity, for example, a richer morphology or a flexible word-order. Still, the majority of work in NLP and almost all benchmarks exclusively focus on English (e.g., Wang et al., 2019b;Liu et al., 2020a;McCann et al., 2018). Even if multiple languages are considered, the availability of data in a language often does not represent the number of speakers of a language. This means that work on languages with little available data can potentially impact many more people than work on highly resourced languages (Joshi et al., 2020). \n\nAs a result, many recent benchmarking and dataset creation efforts in NLU develop and focus on tasks that are inherently multilingual or which explore cross-lingual transfer. For example, XTREME (Hu et al., 2020) introduces a benchmark covering 40 languages across multiple NLU and retrieval tasks, XCOPA (Ponti et al., 2020) is a commonsense reasoning dataset for eleven languages, and MLQA (Lewis et al., 2020b) is a dataset for extractive question answering across seven languages. We can observe a similar recent trend in natural language generation, where ML-Sum (Scialom et al., 2020) and WikiLingua (Ladhak et al., 2020) were created as multilingual summarization datasets. There also have been first steps toward including NLG tasks in multilingual NLU benchmarks. For example, XGLUE includes Question and News Title Generation (Liang et al., 2020).",
            "score": 0.5139216440979187,
            "section_title": "Benchmarks in NLG",
            "char_start_offset": 7495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 826,
                    "end": 845,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1171,
                    "end": 1191,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1390,
                    "end": 1407,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1500,
                    "end": 1520,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1587,
                    "end": 1607,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1763,
                    "end": 1785,
                    "matchedPaperCorpusId": "216868259"
                },
                {
                    "start": 1801,
                    "end": 1822,
                    "matchedPaperCorpusId": "222177239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396484375
        },
        {
            "corpus_id": "264146917",
            "title": "Investigating Bias in Multilingual Language Models: Cross-Lingual Transfer of Debiasing Techniques",
            "text": "We investigate whether this performance disparity extends to gender, religious, and racial biases. Related studies demonstrate the effectiveness of cross-lingual debiasing for individual techniques and selected bias scopes (Liang et al., 2020b;Lauscher et al., 2021). We show how to reduce bias in mBERT across different languages by conducting a benchmark of state-of-the-art (SOTA) debiasing techniques and providing guidance on its implementation. To facilitate further research and reproducibility, we make the code and additional data available to the research community2 . \n\nOur contributions can be summarized as follows: 1) We provide a benchmark of different SOTA debiasing techniques across multiple languages in a multilingual large language model. 2) We find that SentenceDebias is the most effective for crosslingual debiasing, reducing the bias in mBERT by 13%. 3) We provide implementation guidelines for debiasing multilingual models and highlight the differences in the cross-lingual transferability of different debiasing techniques. We find that most projection-based techniques applied to one language yield similar predictions across evaluation languages. We also recommend performing the techniques with an additional pretraining step on the lowest resource language within the multilingual model for optimal results.",
            "score": 0.5136120354506178,
            "section_title": "Introduction",
            "char_start_offset": 2085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1339
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 244,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 244,
                    "end": 266,
                    "matchedPaperCorpusId": "237440429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "248119025",
            "title": "A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference",
            "text": "tribution score for each word in the reference language as the sum of the attribution scores of the corresponding words in the target language:\n\nBy aligning scores, we obtain equivalent attribution scores in the target language for each word in the source language. For the example in Figure 1, we have \u03c9\n\nFinally, we define the cross-lingual faithfulness (\u03c1) of a dataset as the average Spearman correlation between attribution scores for English and aligned attribution scores for all other languages:\n\nThe main advantage of this approach is in avoiding the OOD problem: Translation pairs form naturally occurring perturbations that are part of the model's training distribution, unlike the synthetic inputs formed by erasure-based lingual model performs best on it and since the word aligner we use was originally fine-tuned and evaluated on en-XX language pairs. methods. We also reduce language-specific bias by using translations of the same sentence in different languages. Furthermore, our approach provides a grayscale notion of faithfulness, as advocated by Jacovi and Goldberg (2020).",
            "score": 0.5135550515365603,
            "section_title": "Crosslingual Faithfulness Evaluation",
            "char_start_offset": 11795,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "234679313",
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "text": "The experiments were conducted in three typologically different languages and they concluded that the lexical overlap between languages contributes little to the cross-lingual success, while the depth of the network plays an important role. Conneau and Lample (2019) proposed two methods to pretrain cross-lingual language models, one unsupervised and the other supervised that achieved state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. They also have shown that cross-lingual language models can provide significant improvements on the perplexity of low-resource languages. Conneau et al. (2020) presented a Transformer-based multilingual model named XLM-R, pretrained on one hundred languages and a strong com-petitor to monolingual models on several zeroshot benchmarks. Shi et al. (2020) showed that multilingual models perform well on crosslingual document ranking tasks. They also investigated translating the training data and the translation of documents at inference time, and concluded that both approaches achieve competitive results. Cabezudo et al. (2020) analyzed multiple approaches of using mBERT for natural language inference task in Portuguese. They investigated the consequences of adding external data to improve training in two different forms: multilingual data and an automatically translated corpus. They achieved the state-of-the-art on ASSIN corpus using a multilingual pretrained BERT model and showed that using external data did not improve the model's performance or the improvements are not significant. Rodrigues et al. (2020) showed that automatically translating examples to English and using a model fine-tuned on an English dataset can outperform multilingual models fine-tuned on the target language. Isbister et al. (2021) demonstrated that a combination of English language models and modern machine translation outperforms native language models in most Scandinavian languages on sentiment analysis. They argued that it is more effective to translate data from lowresource languages into English than to pretrain a new language model on a low-resource language. Our paper expands this study to the tasks of question answering, natural language inference and passage text ranking.",
            "score": 0.5132866660975435,
            "section_title": "Introduction",
            "char_start_offset": 4012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1997
                },
                {
                    "start": 1998,
                    "end": 2159
                },
                {
                    "start": 2160,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 831,
                    "end": 848,
                    "matchedPaperCorpusId": "226284008"
                },
                {
                    "start": 1103,
                    "end": 1125,
                    "matchedPaperCorpusId": "211265082"
                },
                {
                    "start": 1593,
                    "end": 1616,
                    "matchedPaperCorpusId": "214676745"
                },
                {
                    "start": 1796,
                    "end": 1818,
                    "matchedPaperCorpusId": "233324253"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "244920860",
            "title": "Dataset Geography: Mapping Language Data to Language Users",
            "text": "One important aspect of our study is the evaluation of cross-lingual consistency while performing multilingual NER or El tasks. In (Bianchi et al., 2021), the authors focus on the consistency evaluation of language-invariant properties. In an ideal scenario, the properties should not be changed via the language transformation models but commercially available models are not prone to avoid domain dependency. Effective measurement of dataset quality is another aspect of fast-growing significance. Training large language models require huge amount of data and as a result, the inference generated by these pretrained language model as well as the finetuned models often show inherent data bias. In a recent work (Swayamdipta et al., 2020), the authors present how data-quality aware design-decision can improve the overall model performance. They formulated categorization of data-regions based on characteristics such as out-of-distribution feature, class-probability fluctuation and annotation-level discrepancy.\n\nUsually, multilingual datasets are collected from diverse places. So it is important to assess whether the utility of these datasets are representative enough to reflect upon the native speakers. We find the MasakhaNER (Adelani et al., 2021) is one such dataset that was collected from local sources and the data characteristics can be mapped to local users as a result. In addition, language models often requires to be truly language-agnostic depending on the tasks, but one recent work shows that, the current state-of-the-art language applications are far from achieving this goal (Joshi et al., 2020). The authors present quantitative assessment of available applications and language-resource trajectories which turns out not uniformly distributed over the usefulness of targeted users and speakers from all parts of the world.",
            "score": 0.512399339389602,
            "section_title": "A Related Work",
            "char_start_offset": 31005,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 715,
                    "end": 741,
                    "matchedPaperCorpusId": "221856637"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49755859375
        },
        {
            "corpus_id": "273229002",
            "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
            "text": "Multilingual evaluation methods and the development of benchmarks not only facilitate the assessment of diverse language representations in LLMs but also help in monitoring cross-lingual generalization, to assess the effect of quantization across multiple languages (Marchisio et al., 2024), the development of language-specific models (Tejaswi et al., 2024), and the optimization of safety preferences (Li et al., 2024e), among others. In Table 4, we list benchmarks with the largest language coverage. This list includes benchmarks referenced by MEGA (Ahuja et al., 2023), MEGA-VERSE (Ahuja et al., 2024), xP3 (Muennighoff et al., 2023), the Aya collection (Singh et al., 2024), the lm-evaluation-harness framework (Gao et al., 2023;Biderman et al., 2024), and inter alia. These datasets comprise a mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation Lai et al. (2023b). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages. Despite the efforts reflected in Table 4, the community is still lacking highly multilingual benchmarks for tasks such as natural language understanding or text generation.",
            "score": 0.5111272393895685,
            "section_title": "A.2 MULTILINGUAL EVALUATION BENCHMARKS",
            "char_start_offset": 36079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1351
                }
            ],
            "ref_mentions": [
                {
                    "start": 553,
                    "end": 573,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 586,
                    "end": 605,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 967,
                    "end": 985,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 1075,
                    "end": 1095,
                    "matchedPaperCorpusId": "235658519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "266999171",
            "title": "MiTTenS: A Dataset for Evaluating Gender Mistranslation",
            "text": "MiTTenS can be used in evaluation for external audits of a deployed system, during model development, or monitoring during training. Here, we  demonstrate using the dataset for automated evaluation of 2en translation with a range of systems (details for reproducing are in Appendix A). For an 2xx human evaluation protocol see Anil et al. (2023). We leave demonstration of LLM-based evaluation (Zheng et al., 2023) for future work. \n\nEvaluation results are shown in Figure 2, and we highlight specific areas of improvement for each system with disaggregated analysis by language and evaluation set in Table 3. Disaggregated analysis with precise evaluation data enables targeted improvements, and scales as additional evaluation sets are added over time. Even though systems show relatively high overall accuracy, in Figure 2 all systems perform worse on passages that require translation to \"she\" as compared to \"he\", which may be related to patterns of representation in training datasets (Chowdhery et al., 2022). Performance in Table 3 is often worst on Encoded in nouns or Late binding evaluation sets. Surprisingly, we see areas of weakness even in high resource languages such as Spanish, and different areas of weakness in the same model families. There is no clear pattern to which languages are most challenging across systems, demonstrating the importance of empirical evaluations, and that MiTTenS can be used to pinpoint areas for targeted improvement.",
            "score": 0.5110778309109143,
            "section_title": "Evaluation",
            "char_start_offset": 9493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1465
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6845703125
        },
        {
            "corpus_id": "235683093",
            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
            "text": "While popular multilingual zero-shot benchmarks such as XTREME (Hu et al., 2020) and XGLUE (Liang et al., 2020) provide development and test sets in tens of diverse low-resource languages, their transfer sets are limited to English. Even for tasks such as TyDi QA (Clark et al., 2020) that originally had training data in multiple languages, the authors of the benchmark removed any non-English data from the transfer set. The effect is that most studies mentioned above defaulted their analysis to English as the only transfer language. More recently however, the XTREME-R benchmark (Ruder et al., 2021) introduced two truly cross-lingual retrieval tasks, where both training and evaluation data use a mixture of languages (Roy et al., 2020;Botha et al., 2020). \n\nPrevious studies on the effectiveness of source languages focus on a single target language, or a small set of related ones. Lauscher et al. (2020) observe strong correlations between transfer performance and multiple measures of linguistic proximity between the transfer and target language, including syntax and morphology. Lin et al. (2019) automatically identify the most effective transfer languages via a ranker that leverages various distances (geographic, genetic, syntactic, phonological, etc) between a single target language and multiple transfer candidates. In contrast, we seek to find languages that transfer to many targets, potentially not even known a priori. \n\nThe pursuit of a language that can help others is also reminiscent of pivot-based machine translation (source \u2192 pivot + pivot \u2192 target), where a high-resource pivot bridges the gap between pairs of languages with insufficient parallel training data (Cheng et al., 2017;Kim et al., 2019;Dabre et al., 2021). English was shown to not always be the best pivot for machine translation (Paul et al., 2013;Dabre et al., 2015), which prompted us to investigate whether that is also the case for zeroshot cross-lingual transfer.",
            "score": 0.5109942936801964,
            "section_title": "Transfer Language Selection",
            "char_start_offset": 6446,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 111,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 724,
                    "end": 742,
                    "matchedPaperCorpusId": "215745460"
                },
                {
                    "start": 742,
                    "end": 761,
                    "matchedPaperCorpusId": "226254361"
                },
                {
                    "start": 890,
                    "end": 912,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1091,
                    "end": 1108,
                    "matchedPaperCorpusId": "170078772"
                },
                {
                    "start": 1693,
                    "end": 1713,
                    "matchedPaperCorpusId": "12887697"
                },
                {
                    "start": 1713,
                    "end": 1730,
                    "matchedPaperCorpusId": "202712706"
                },
                {
                    "start": 1825,
                    "end": 1844,
                    "matchedPaperCorpusId": "17222548"
                },
                {
                    "start": 1844,
                    "end": 1863,
                    "matchedPaperCorpusId": "6167315"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "275758182",
            "title": "Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text Detection",
            "text": "Our work suggests a fundamental flaw in evaluations that rely on synthetic data. Many evaluation protocols leverage machine-generated text, from machine translation for cross-lingual assessment [Wang and Hershcovich, 2023] to language models judging other models' outputs [Zhu et al., 2023] and other synthetic data training and evaluations [Gu\u00e9pin et al., 2023]. Our results suggest such evaluations may be systematically biased-the signals they measure may be confounded by the synthetic nature of their data rather than the properties they aim to assess. This raises three critical questions: (i) how does using language models to evaluate other models impact benchmark reliability, given their shared biases in processing synthetic text? (ii) are synthetic data-based evaluations measuring intended properties or merely detecting machine generation artifacts? (iii) why does synthetic text behavior transfer so consistently across different model scales and architectures? \n\nRecent work [Mireshghallah et al., 2023] suggests these patterns stem from fundamental similarities in how language models encode and process text.",
            "score": 0.5104090607680852,
            "section_title": "Introduction",
            "char_start_offset": 3865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "252547725",
            "title": "Are Pretrained Multilingual Models Equally Fair across Languages?",
            "text": "Pretrained multilingual language models can help bridge the digital language divide, enabling high-quality NLP models for lower-resourced languages. Studies of multilingual models have so far focused on performance, consistency, and cross-lingual generalisation. However, with their wide-spread application in the wild and downstream societal impact, it is important to put multilingual models under the same scrutiny as monolingual models. This work investigates the group fairness of multilingual models, asking whether these models are equally fair across languages. To this end, we create a new four-way multilingual dataset of parallel cloze test examples (MozArt), equipped with demographic information (balanced with regard to gender and native tongue) about the test participants. We evaluate three multilingual models on MozArt \u2013mBERT, XLM-R, and mT5\u2013 and show that across the four target languages, the three models exhibit different levels of group disparity, e.g., exhibiting near-equal risk for Spanish, but high levels of disparity for German.",
            "score": 0.5102241825782159,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67724609375
        },
        {
            "corpus_id": "196174566",
            "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
            "text": "In this paper, we introduce a cross-lingual OpenQA dataset called XQA. It consists of a training set in English, and development and test sets in English, French, German, Portuguese, Polish,  Moreover, we build several baseline systems that use the information of multilingual data from publicly available corpora for cross-lingual OpenQA, including two translation-based methods that translate training data and test data respectively and one zero-shot cross-lingual method (multilingual BERT (Devlin et al., 2019)). We evaluate the performance of the proposed baselines in terms of text retrieval and reading comprehension for different target languages on the XQA dataset. \n\nThe experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is. Based on the results, we further discuss potential improvement for cross-lingual OpenQA systems. \n\nWe will release the dataset and baseline systems online with the hope that this could contribute to the research of cross-lingual OpenQA and overall cross-lingual language understanding.",
            "score": 0.51015903027964,
            "section_title": "Introduction",
            "char_start_offset": 2104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1451
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 515,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "52098405",
            "title": "Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach",
            "text": "In this section, we describe the evaluation tasks, the datasets used, and the experimental details of the proposed approach. Evaluation tasks. We evaluate our approach on several tasks: \u2022 To evaluate the quality of the bilingual mappings generated, we evaluate our algorithms primarily for the bilingual lexicon induction (BLI) task, i.e., word translation task and compare Precision@1 with previously reported state-of-the-art results on benchmark datasets (Dinu and Baroni, 2015;Artetxe et al., 2016;Conneau et al., 2018). \n\n\u2022 We also evaluate on the cross-lingual word similarity task using the SemEval 2017 dataset. \n\n\u2022 To ensure that quality of embeddings on monolingual tasks does not degrade, we evaluate the quality of our embeddings on the monolingual word analogy task (Artetxe et al., 2016). \n\n\u2022 To illustrate the utility of representing embeddings of multiple language in a single latent space, we evaluate our multilingual embeddings on the one-hop translation task, i.e., a direct dictionary between the source and target languages is not available, but the source and target languages share a bilingual dictionary with a pivot language. Datasets. For bilingual and multilingual experiments, we report results on the following widely used, publicly available datasets: \n\n\u2022 VecMap: This dataset was originally made available by Dinu and Baroni (2015) with subsequent extensions by other researchers (Artetxe et al., 2017(Artetxe et al., , 2018a)). It contains bilingual dictionaries from English (en) to four languages: Italian (it), German (de), Finnish (fi) and Spanish (es). The detailed experimental settings for this BLI task can be found in Artetxe et al. (2018b). \n\n\u2022 MUSE: This dataset was originally made available by Conneau et al. (2018). It contains bilingual dictionaries from English to many languages such as Spanish (es), French (fr), German (de), Russian (ru), Chinese (zh), and vice versa.",
            "score": 0.5096638469373835,
            "section_title": "Experimental Settings",
            "char_start_offset": 18372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 619
                },
                {
                    "start": 622,
                    "end": 802
                },
                {
                    "start": 805,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1920
                }
            ],
            "ref_mentions": [
                {
                    "start": 458,
                    "end": 481,
                    "matchedPaperCorpusId": "17910711"
                },
                {
                    "start": 481,
                    "end": 502,
                    "matchedPaperCorpusId": "1040556"
                },
                {
                    "start": 502,
                    "end": 523,
                    "matchedPaperCorpusId": "3470398"
                },
                {
                    "start": 779,
                    "end": 801,
                    "matchedPaperCorpusId": "1040556"
                },
                {
                    "start": 1341,
                    "end": 1363,
                    "matchedPaperCorpusId": "17910711"
                },
                {
                    "start": 1412,
                    "end": 1433,
                    "matchedPaperCorpusId": "13335042"
                },
                {
                    "start": 1433,
                    "end": 1459,
                    "matchedPaperCorpusId": "4334731"
                },
                {
                    "start": 1660,
                    "end": 1682,
                    "matchedPaperCorpusId": "21728524"
                },
                {
                    "start": 1740,
                    "end": 1761,
                    "matchedPaperCorpusId": "3470398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.720703125
        },
        {
            "corpus_id": "270226445",
            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
            "text": "Based on our previous results, we confirm that addressing the misalignment of data origins between training and evaluation is effective for the translatetest approach.We now aim to understand how these benefits vary with changes in the MT systems or translation setups.To this end, we conduct experiments by varying (1) the MT system used for translating the training and evaluation sets and (2) the pivot language during the RT translation.\n\nVaried MT systems We use the following four MT systems in our experiments: M2M-100-418M/1.2B(Fan et al., 2021) and NLLB-200-600M/-3.3B(Costa-juss\u00e0 et al., 2022).Each MT system is used to make RT-translated training and translate-test evaluation sets.In detail, we use RT translation with different MT systems to make training sets, and the pivot language is fixed to German (de).All models described in Section 3.1.2are individually trained on these four RT-translated datasets.For the evaluation set, we translate every  target language into English using different MT systems, resulting in four different evaluation sets.\n\nEvaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer.In terms of MT system comparison, models usually show higher accuracy when MT systems used to make training and evaluation sets are in the same model family.In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts.",
            "score": 0.5089005205518574,
            "section_title": "Varying NMT and Pivot Languages",
            "char_start_offset": 13183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 441
                },
                {
                    "start": 443,
                    "end": 535
                },
                {
                    "start": 535,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 859
                },
                {
                    "start": 859,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 1066
                },
                {
                    "start": 1068,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1429
                },
                {
                    "start": 1429,
                    "end": 1586
                },
                {
                    "start": 1586,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 577,
                    "end": 603,
                    "matchedPaperCorpusId": "258615266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "227231710",
            "title": "SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment",
            "text": "We first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vuli\u0107 et al., 2019b). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets. \n\nWe have followed the standard word pair translation procedure (Leviant and Reichart, 2015;Camacho-Collados et al., 2017)  of the target language. We observed the translation agreement in the range of 80%-90% across the five target languages. Translation disagreements were resolved by a third annotator who selected the better of the two differing translations. We allowed for multi-word translations only if there was no appropriate single word translation, e.g., typewriter (EN) \u2192 pisa\u0107i stroj (HR). \n\nConcept Scoring and Cross-Lingual Datasets. The resulting 2,616 concept pairs in all five target languages were annotated using a procedure analogous to that for EN HyperLex: the rating interval was [0, 6], and each word pair was rated by 4 (for DE, IT, HR) or 5 (for TR, SQ) native speakers. We then constructed the cross-lingual datasets automatically, leveraging word pair translations and scores in five target languages.",
            "score": 0.5087390774158912,
            "section_title": "Data",
            "char_start_offset": 5687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1943
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 157,
                    "matchedPaperCorpusId": "196183388"
                },
                {
                    "start": 884,
                    "end": 914,
                    "matchedPaperCorpusId": "7665329"
                },
                {
                    "start": 1104,
                    "end": 1134,
                    "matchedPaperCorpusId": "7665329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "270521667",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "text": "The datasets and models are publicly available.\n\nIn Kassner et al. (2021)'s work, the authors investigate the ability of multilingual BERT (mBERT) to serve as a multilingual knowledge base by translating English datasets (TREx and GoogleRE) into 53 languages.They propose a typed querying approach that outperforms the original fill-in-theblank method and show that mBERT exhibits varying performance across languages, with strong results for 21 languages and weak performance for 32 others.The study reveals language-specific biases in mBERT's predictions and demonstrates that pooling predictions across languages can improve performance, even surpassing monolingual English BERT.The paper highlights the importance of extending research on language models as knowledge bases to multiple languages for diversity and accessibility.Srinivasan et al. (2021) propose a method to predict the performance of multilingual models like mBERT and XLM-Roberta on languages and tasks for which evaluation datasets are unavailable.The authors train a regression model using XGBoost with features based on pretraining data size, typological features, and target-pivot language similarity to predict performance scores.They evaluate the predictor's ability to generalize to unseen languages and find that while the model performs well in predicting scores for languages seen during training, the performance drops when applied to new languages.The results suggest that improvements are needed for the predictor to replace the creation of test sets in new languages.However, the model is able to predict relative performance trends across languages and tasks effectively.XTREME-R (Ruder et al., 2021) is an improved multilingual benchmark covering 50 diverse languages and 10 challenging tasks, including retrieval from a multilingual pool.Experiments and analyses show that recent progress has been uneven across tasks, with gains concentrated on retrieval.Performance remains poor on low-resource languages and non-Latin scripts, with translate-train approaches generally performing best.XTREME-R introduces an extensible diagnostic and evaluation suite, including a massively multilingual test suite (MULTICHECKLIST) and fine-grained evaluation via EXPLAINABOARD.The benchmark aims to facilitate nuanced evaluation and comparison of multilingual models through an interactive public leader-board with detailed metadata.",
            "score": 0.50871190271943,
            "section_title": "Multi-lingual performance evaluation",
            "char_start_offset": 55444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 49,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 832
                },
                {
                    "start": 832,
                    "end": 1020
                },
                {
                    "start": 1020,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1944
                },
                {
                    "start": 1944,
                    "end": 2076
                },
                {
                    "start": 2076,
                    "end": 2252
                },
                {
                    "start": 2252,
                    "end": 2408
                }
            ],
            "ref_mentions": [
                {
                    "start": 1666,
                    "end": 1686,
                    "matchedPaperCorpusId": "233241004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "235313293",
            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
            "text": "Although cross-lingual transfer in ToD is conceptually sound and practically feasible, there is only anecdotal evidence for its wide-range effectiveness (Schuster et al., 2019a;, primarily due to the scarcity of multilingual evaluation benchmarks. For the same reason, it also remains unclear how different transfer approaches (e.g., machine translation and multilingual encoders) compare against each other, and which one should be preferred in relation to particular sets of ToD-related tasks, languages, and domains.",
            "score": 0.5050773873805865,
            "section_title": "Why Is Developing Multilingual Dialogue Systems Difficult?",
            "char_start_offset": 16604,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 177,
                    "matchedPaperCorpusId": "53110354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3818359375
        },
        {
            "corpus_id": "257219401",
            "title": "Cross-Lingual Question Answering over Knowledge Base as Reading Comprehension",
            "text": "We evaluate our method on two datasets, QALD-M, a small evaluation dataset in 11 languages, and WebQSP-zh, a new dataset with a larger size and more realistic expressions.\n\nQALD-M QALD-M is a series of evaluation campaigns on question answering over linked data. We use the version provided by Zhou et al. (2021) and filter the out-of-scope ones. It consists of testing questions for 11 non-English languages (fa, de, ro, it, ru, fr, nl, es, hi, pt, pt_BR) over DBPedia. The numbers of used questions for each language range from 66 to 363. We use QALD-M mainly for zero-shot evaluation. See Appendix A.1 for more details.\n\nWebQSP-zh Considering that the size of QALD-M is small and its multilingual questions are mostly literal translations without language-dependent paraphrasing, we collect a new xKBQA dataset WebQSP-zh, with 3,098 questions for training and 1,639 questions for test.\n\nTo collect WebQSP-zh, we employ two Chinese native speakers proficient in English to manually translate all the questions in WebQSP (Yih et al., 2016), a widely-used English KBQA dataset, together with another annotator responsible for checking translation quality. To provide a more realistic benchmark for cross-lingual evaluation, the annotators are instructed to pay much attention to commonsense knowledge and natural vocabulary choices during translation. For example, in the upper example of Table 1, the phrase be in in the WebQSP question has multiple translations in Chinese. Based on the commonsense knowledge that Anna Kendrick is an actress, it is translated as \u51fa\u6f14/star in instead of its literal meaning \u5728\u505a/be doing. In the lower example of Table 1, the annotator chooses the Chinese word \u524d\u82cf\u8054/former Soviet Union for translation instead of \u82cf\u8054/Soviet Union because the former is more often used by native Chinese speakers. See Appendix A.2 for more statistics, annotation details, and examples.",
            "score": 0.5046609340304448,
            "section_title": "Datasets",
            "char_start_offset": 14513,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "273323209",
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "text": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.",
            "score": 0.5044232729239992,
            "section_title": "Related Work",
            "char_start_offset": 2745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 1266,
                    "end": 1285,
                    "matchedPaperCorpusId": "246680398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "274192674",
            "title": "Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora",
            "text": "We evaluate the proposed method on several parallel datasets from the OPUS Machine Translation project dataset collection2 (Tiedemann, 2012) that includes aligned translations between Spanish and English. We focus on parallel datasets to facilitate the comparison of the gender polarity method for English with our LLM-based gender representation bias quantification method for Spanish. The datasets were chosen based on their extended use in machine translation and NLP model training in general. In particular, we evaluated our approach on the Europarl, CCAligned, Global Voices, and WMT-News datasets. \n\nEuroparl: The Europarl dataset (Koehn, 2005) is a multilingual corpus extracted from the proceedings of the European Parliament. It is widely used for research in the NLP community, particularly for tasks such as machine translation, language modeling, and cross-lingual studies. It includes transcripts in 21 European languages, including Spanish and English. The texts cover parliamentary debates and speeches, offering a diverse range of topics and formal language styles. We use the bilingual Spanish-English dataset in version v7, covering the period from April 1996 to November 2011, which consists of 1,965,734 sentences for each language. \n\nCCAligned: This dataset (El-Kishky et al., 2020) is a large-scale multilingual corpus of billions of sentences derived from web-crawled data. Specifically, the corpus used in our experiments was created from 68 Common Crawl snapshots up until March 2020 (v1). It is designed to support research in tasks involving machine translation and crosslingual studies. The bilingual Spanish-English corpus contains 15,250,346 sentences for each language. \n\nGlobal Voices: The Global Voices dataset (Nguyen and Daum\u00e9 III, 2019) is a multilingual corpus collected from the Global Voices website, which features news articles and stories written by a global network of authors and translated by volunteers into multiple languages. This dataset, consisting of 359,002 sentence pairs in Spanish and English, is broadly used for research in machine translation, multilingual text analysis, and other NLP tasks. We used the version v2018q4.",
            "score": 0.5040059183686625,
            "section_title": "Datasets",
            "char_start_offset": 21977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1701
                },
                {
                    "start": 1704,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 140,
                    "matchedPaperCorpusId": "15453873"
                },
                {
                    "start": 638,
                    "end": 651,
                    "matchedPaperCorpusId": "38407095"
                },
                {
                    "start": 1745,
                    "end": 1772,
                    "matchedPaperCorpusId": "203610372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "267413041",
            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
            "text": "This work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
            "score": 0.5037468411819546,
            "section_title": "Conclusions",
            "char_start_offset": 13897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.962890625
        },
        {
            "corpus_id": "258686285",
            "title": "Multilingual Previously Fact-Checked Claim Retrieval",
            "text": "Complexity of crosslingual evaluation. Phenomena such as same language bias or false positive rate make the evaluation of multilingual and crosslingual datasets inherently complex. If we were to abstract the whole evaluation into a sin-gle number, as is often done in practice, we would have completely missed these pitfalls. Without an in-depth evaluation, we might have been misled while applying our methods in practice, e.g., while developing helpful tools for fact-checkers. Our evaluation procedures were previously impossible to develop in the absence of linguistically diverse PFCR datasets. \n\nMachine translation beats multilingual TEMs. These two technologies represent the two main multilingual and crosslingual learning paradigms -label transfer and parameter transfer (Pikuliak et al., 2021). Machine translation is a clear winner in our case. English TEMs significantly outperform multilingual approaches for both monolingual and crosslingual retrieval. 3, it seems that the performance for COVID-19 is significantly worse than for the rest of the dataset. However, this might not necessarily mean that the methods are having issues with the domain shift. The sheer amount of fact-checks written about COVID-19 makes it hard for the methods to pick the desired fact-check in the presence of thousands of other very similar ones. This is evident considering that BM25 also has worse results, even though it should be less prone to domain shift based on its design.",
            "score": 0.502454396651341,
            "section_title": "Discussion",
            "char_start_offset": 24646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 39,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1477
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "218487087",
            "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
            "text": "We first evaluate the bias in the MLBs monolingual dataset by predicting the occupations of the bios in each language. 6 From Table 6 we observe that: 1) Bias commonly exists across all languages (|Diff| > 0) when using different aligned embeddings, meaning that the model works differently for male and female groups. we find that aligning the embeddings to ENDEB or a gender-rich language reduces the bias in the downstream task. This is aligned with our previous observation in Section 3. \n\nBias in Transfer Learning Multilingual word embeddings are widely used in cross-lingual transfer learning (Ruder et al., 2019). In this section, we conduct experiments to understand how the bias in multilingual word embeddings impacts the bias in transfer learning. To do this, we train our model in one language (i.e., source language) and transfer it to another language based on the aligned embeddings obtained in Section 3.2. For the transfer learning, we train the model on the training corpus of the source language and randomly choose 20% of the dataset from the target language and use them to fine-tune the model. 7 Here, we do not aim at achieving state-of-the-art transfer learning performance but pay more attention to the bias analysis. Table 7 shows that the bias is present when we do the transfer learning regardless of the direction of transfer learning.",
            "score": 0.501857605092985,
            "section_title": "Bias in Monolingual BiosBias",
            "char_start_offset": 21859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1365
                }
            ],
            "ref_mentions": [
                {
                    "start": 600,
                    "end": 620,
                    "matchedPaperCorpusId": "26127787"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51904296875
        },
        {
            "corpus_id": "271909221",
            "title": "Synergistic Approach for Simultaneous Optimization of Monolingual, Cross-lingual, and Multilingual Information Retrieval",
            "text": "Datasets. We evaluate the retrieval effectiveness of different models on three distinct datasets: XQuAD-R (Roy et al., 2020) and MLQA-R (Roy et al., 2020). 2 XQuAD-R and MLQA-R are question-answering datasets with parallel questions and passages in 11 languages and 7 languages, respectively. Thus, these two datasets can be used to evaluate monolingual, cross-lingual, and multilingual retrieval effectiveness. Appendix A.2 provides comprehensive details about the evaluation datasets. Furthermore, we report the detailed monolingual retrieval effectiveness on MIRACL dev (Zhang et al., 2022) in Table 12 and 13 in Appendix A.3.3. \n\nMetrics and Settings. We report the mean average precision (mAP) for XQuAD-R and MLQA-R since the metric considers the retrieval quality when multiple relevant passages for a given query exist. 3  We conduct retrieval using the queries with X Q language against the corpus with X C language and report the macro-averaged mAP over all the crosslingual (denoting Cr.) combinations language pairs (X Q \u0338 = X C ), and the other monolingual (denoting Mo.) combinations (X Q = X C ). For example, in XQuAD-R (MLQA-R), we have 11 and 7 parallel languages; thus, there are 110 (42) and 11 (7) cross-lingual and monolingual retrieval settings, respectively. For multilingual (denoting Mul.) retrieval, we conduct retrieval using the queries with X Q language against all the parallel corpus in dif- 2 The evaluation of the models is conducted on datasets that are completely separate and distinct from the ones used for training. More specifically, the models have not encountered any data samples, whether from the training or testing splits, of the evaluation datasets during their training process. This ensures an unbiased assessment of the ability of the models to generalize and perform effectively on unseen data. 3 The results for the Recall metric are in Appendix A.3.1. \n\nlanguage bias (\u2193) ferent languages. We report the detailed results for specific languages in Appendix A.3.",
            "score": 0.5008596893277275,
            "section_title": "Evaluation",
            "char_start_offset": 9268,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 9
                },
                {
                    "start": 10,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 124,
                    "matchedPaperCorpusId": "215745460"
                },
                {
                    "start": 136,
                    "end": 154,
                    "matchedPaperCorpusId": "215745460"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "268987378",
            "title": "Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language Translation",
            "text": "A.1 Supplementary Material on Datasets No Language Left Behind (NLLB) The NLLB (Costa-juss\u00e0 et al., 2022) corpus consists of translation training datasets for low-resource languages and is automatically created through the process of bitext mining.We employed a selection process based on the LASER score, where we chose the top 100,000 and 25,000 translation pairs from the selected language pair for dataset size variation.However, NLLB lacks coverage and human quality control due to the noisy nature of the entire procedure and is only suitable for training purposes.Government corpus (Gvt) The government document corpus (Fernando et al., 2020) is a multiway parallel corpus for Sinhala, Tamil, and English.It comprises a range of official Sri Lankan government documents, including annual and committee reports, content sourced from government websites, procurement-related documents, and legislative acts.Samanantar corpus (Sam) The Samanantar corpus (Ramesh et al., 2023) is the largest publicly available Parallel Corpora Collection for 11 Indic Languages.The data is derived from two sources: existing databases and new data automatically collected through web crawling and sentence alignment techniques.FLORES The FLORES dataset (Goyal et al., 2022) is a multiway multilingual translation evaluation dataset.FLORES-101 is comprised of translations from 842 unique web articles, comprising a total of 3001 sentences.Because all translations are fully aligned, the resulting dataset allows for a more accurate assessment of model quality on the long tail of LRLs, including the evaluation of many-to-many multilingual translation systems.The professional rigor and reliability of the results are strengthened by using an out-of-domain evaluation of this type, resulting in a more robust and objective evaluation of the model's translation performance across many domains.FLORES-200 expands the language coverage to twice that of FLORES-101.We used FLORES-200 (Costa-juss\u00e0 et al., 2022) for Sinhala since it is not in FLORES-101, and dev-test split for both FLORES-101 and FLORES-200.",
            "score": 0.5006664050577506,
            "section_title": "A Appendix",
            "char_start_offset": 15481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 248,
                    "end": 425
                },
                {
                    "start": 425,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 712
                },
                {
                    "start": 712,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1880
                },
                {
                    "start": 1880,
                    "end": 1949
                },
                {
                    "start": 1949,
                    "end": 2092
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5849609375
        },
        {
            "corpus_id": "267412633",
            "title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models",
            "text": "Recent advances in training multilingual language models on large datasets seem to have shown promising results in knowledge transfer across languages and achieve high performance on downstream tasks. However, we question to what extent the current evaluation benchmarks and setups accurately measure zero-shot cross-lingual knowledge transfer. In this work, we challenge the assumption that high zero-shot performance on target tasks reflects high cross-lingual ability by introducing more challenging setups involving instances with multiple languages. Through extensive experiments and analysis, we show that the observed high performance of multilingual models can be largely attributed to factors not requiring the transfer of actual linguistic knowledge, such as task- and surface-level knowledge. More specifically, we observe what has been transferred across languages is mostly data artifacts and biases, especially for low-resource languages. Our findings highlight the overlooked drawbacks of existing cross-lingual test data and evaluation setups, calling for a more nuanced understanding of the cross-lingual capabilities of multilingual models.",
            "score": 0.4995574852824528,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86181640625
        },
        {
            "corpus_id": "258557740",
            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
            "text": "learning (Hu et al., 2020;Nag et al., 2021). Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;Ozaki et al., 2021;Yarmohammadi et al., 2021).\n\nThe MultiTACRED dataset we present in this work covers 12 languages from 9 language families. 1 We select typologically diverse languages which span a large set of linguistic phenomena such as compounding, inflection and pronoun-drop, and for which a monolingual pretrained language model is available. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances. In addition, we experimentally evaluate fine-tuned pretrained monoand multilingual language models (PLM) in common training scenarios, using source language (English), target language, or a mixture of both as training data. We also evaluate an English data fine-tuned model on back-translated test instances to estimate the effect of noise introduced by the MT system on model performance. Our results show that in-language training works well, given a suitable PLM. Cross-lingual zero-shot transfer is acceptable for languages well-represented in the multilingual PLM, and combining English and target language data for training considerably improves performance across the board.\n\nTo summarize, our work aims to answer the following research questions: Can we reaffirm the usefulness of MT and cross-lingual annotation projection, in our study for creating large-scale, high quality multilingual datasets for RE? How do pretrained mono-and multilingual encoders compare to each other, in within-language as well as crosslingual evaluation scenarios? Answers to these questions can",
            "score": 0.49807630239258394,
            "section_title": "Introduction",
            "char_start_offset": 1679,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 43,
                    "matchedPaperCorpusId": "239024671"
                },
                {
                    "start": 278,
                    "end": 297,
                    "matchedPaperCorpusId": "233189585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "277509912",
            "title": "State-of-the-Art Translation of Text-to-Gloss using mBART : A case study of Bangla",
            "text": "The performance of the models across various datasets and evaluation metrics is summarized in Table III. The evaluation metrics include sacreBLEU, BLEU-1, BLEU-2, BLEU-3, BLEU-4, and COMET, which provide a comprehensive assessment of translation quality. The mbart-large-50 model, fine-tuned on specific datasets, consistently outperformed other configurations, demonstrating the advantages of pre-trained multilingual transformers for gloss translation tasks. \n\nOn the bangla-gloss dataset, the mbart-large-50 model achieved the highest sacreBLEU score of 79.53 and excelled across all BLEU metrics, with a BLEU-4 score of 44.71. The COMET score of 0.965 further validates the high quality of these translations, showcasing the effectiveness of domainspecific fine-tuning. For the multilingual-gloss dataset, the same model achieved a sacreBLEU score of 21.36 and a BLEU-4 score of 42.62, highlighting its capability to generalize across multiple languages, albeit with a slight reduction in performance compared to the Bangla-specific dataset. \n\nThe sequence-to-sequence model with an attention mechanism, trained on the rule-based-multilingual-gloss dataset, produced the lowest sacreBLEU score (6.63) and BLEU metrics. This outcome indicates the limitations of a simpler architecture in handling complex gloss translation tasks, particularly in comparison to the mbart-large-50 model. Note that, we skip RNN and GRU performance due to their very low performance in our experiment. While the BLEU and sacreBLEU scores are competitive, the COMET score of 0.624 suggests challenges in capturing nuanced semantic equivalence in this dataset. These results emphasize the effectiveness of fine-tuning large pre-trained models on domain-specific datasets for gloss translation.",
            "score": 0.4976968897063882,
            "section_title": "IV. RESULT",
            "char_start_offset": 17835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 460
                },
                {
                    "start": 463,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1774
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "(1) Translation from existing datasets in a major language, e.g., English (Lin et al., 2021;Ponti et al., 2020;Conneau et al., 2018;Artetxe et al., 2020;Yang et al., 2019); (2) Compilation of similar tasks across multiple languages (Zhang et al., 2023c;Hu et al., 2023;Adelani et al., 2022;Roy et al., 2020;Malmasi and Dras, 2015); (3) Creation from multilingual resources following the same dataset creation process (Keung et al., 2020;Huang et al., 2020;Buchholz and Marsi, 2006;Clark et al., 2020;Schwenk and Li, 2018;Kabra et al., 2023).However, (1) translated datasets often do not account for language-specific culture, knowledge, common sense, or linguistic phenomena, leading to a bias towards the background of the source language (Hu et al., 2021;Lin et al., 2021;Acharya et al., 2020;Clark et al., 2020;Park et al., 2021;Kurihara et al., 2022).\n\n(2) Simply compiling datasets curated for each individual language could allow the evaluation of language-specific knowledge and common sense.However, it is difficult to align tasks across languages since most tasks differ in their creation methods data sources or philosophies.Thus, it just leads to evaluating the transfer capability among comparable tasks, and not evaluating the true transfer capabilities across languages.Therefore, (3) only the datasets created from multilingual resources can enable the evaluation of language transfer capability, considering the differences in language-specific knowledge and common sense.Nevertheless, the manual creation of such datasets is limited by the availability of annotators and financial costs.",
            "score": 0.49735312019290046,
            "section_title": "Introduction",
            "char_start_offset": 5533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 855
                },
                {
                    "start": 857,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1488
                },
                {
                    "start": 1488,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 111,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 290,
                    "end": 307,
                    "matchedPaperCorpusId": "215745460"
                },
                {
                    "start": 307,
                    "end": 330,
                    "matchedPaperCorpusId": "6325317"
                },
                {
                    "start": 417,
                    "end": 437,
                    "matchedPaperCorpusId": "222141483"
                },
                {
                    "start": 456,
                    "end": 481,
                    "matchedPaperCorpusId": "13075323"
                },
                {
                    "start": 500,
                    "end": 521,
                    "matchedPaperCorpusId": "21732778"
                },
                {
                    "start": 757,
                    "end": 774,
                    "matchedPaperCorpusId": "235421949"
                },
                {
                    "start": 814,
                    "end": 832,
                    "matchedPaperCorpusId": "234790338"
                },
                {
                    "start": 832,
                    "end": 854,
                    "matchedPaperCorpusId": "249687727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "276249513",
            "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
            "text": "Table 2 displays the correlation between multilingual CLIPScore values and human ratings across the different languages. Our finetuned version achieves significantly better correlations with human judgments, both in reference-based and reference-free settings, across all evaluated languages and correlation metrics. The finetuned CLIPScore model strongly correlates with human preferences in high-resource languages (i.e., English, French, German, Spanish, and Chinese), and it also exhibits equally strong performance in medium-and low-resource languages. Importantly, our finetuned CLIPScore model outperformed the pre-finetuned version even when the latter was using references, achieving a higher average correlation across all metrics without using any references. This observation is particularly useful as it encourages the application of the model to new instances, without requiring additional human input. \n\nAppendix H provides additional results regarding different model sizes and loss variants for model finetuning. The findings in the appendix support the idea that smaller CLIP models can obtain higher gains in correlation with human judgements when using our finetuning strategy, compared to the original models. \n\nDelving deeper into the impact of MT quality, we note that in an ideal scenario, i.e. assuming perfect machine translation results and balanced CLIP performance across languages, the correlations between CLIPScore values across the different languages would equal one, signifying a perfect alignment. To explore deviations from this ideal behaviour, we use heatmaps to visually represent the interrelationships between CLIPScore values across the different languages. \n\nFigure 1 presents Pearson correlation scores between languages, for the best multilingual CLIP-Score model (presented in squared cells), and for our finetuned version (presented in circular cells). The image contains three heatmaps: \n\n\u2022 The first, on the left, shows CLIPScore correlations for the full VICR test dataset, comparing the finetuned model (upper triangle with circular cells) and the pre-finetuned model (lower triangle with squared cells). \u2022 The second and third heatmaps display correlations based on translation quality. They focus on translations with COMETKiwi scores in the bottom 25% (lower triangle) and top 25% percentile (upper triangle).",
            "score": 0.4972288490767549,
            "section_title": "Correlation on Multilingual Data",
            "char_start_offset": 16897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1935
                },
                {
                    "start": 1938,
                    "end": 2156
                },
                {
                    "start": 2157,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7744140625
        },
        {
            "corpus_id": "274023326",
            "title": "P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs",
            "text": "To summarize, our contributions are as follows: \n\n\u2022 We develop a multilingual multi-task benchmark P-MMEVAL that includes both fundamental and capability-specialized tasks, which ensures consistent language coverage across various datasets and provides parallel samples across different languages. This benchmark facilitates a thorough assessment of multilingual capabilities and enables unprecedented fairness and consistency in evaluating crosslingual transfer capabilities. \n\n\u2022 Our experiments offer a comprehensive analysis of the multilingual capabilities of various LLMs, showcasing performance across different prompts, models, languages, and tasks. \n\nOur analyses underscore a significant benchmark sensitivity in evaluating multilingual capabilities, indicating that the \"nativeness\" of the benchmark dramatically affects the observed multilingual evaluation results. \n\n\u2022 We introduce the cross-lingual accuracy consistency ratio (CACR) to analyze the effectiveness of knowledge transfer from English to other languages across various target languages and task scenarios. Our analysis indicates that, among the tested tasks, code knowl-edge is the easiest to transfer, while logical reasoning proves the most difficult. Regarding specific languages, transfer is facilitated by linguistic similarity.",
            "score": 0.497061381681695,
            "section_title": "Introduction",
            "char_start_offset": 3574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 50,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.849609375
        },
        {
            "corpus_id": "207880568",
            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
            "text": "We consider four evaluation benchmarks. For crosslingual understanding, we use cross-lingual natural language inference, named entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models. \n\nCross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other lan-guages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project. \n\nNamed Entity Recognition. For NER, we consider the CoNLL-2002(Sang, 2002) and CoNLL-2003 (Tjong Kim Sang andDe Meulder, 2003) datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from Lample et al. (2016) and Akbik et al. (2018). \n\nCross-lingual Question Answering. We use the MLQA benchmark from Lewis et al. (2019), which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English. GLUE Benchmark.",
            "score": 0.49690971746855694,
            "section_title": "Evaluation",
            "char_start_offset": 10139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2007
                }
            ],
            "ref_mentions": [
                {
                    "start": 1328,
                    "end": 1345,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 1655,
                    "end": 1675,
                    "matchedPaperCorpusId": "6042994"
                },
                {
                    "start": 1680,
                    "end": 1699,
                    "matchedPaperCorpusId": "52010710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "Fig. 1. An illustration of the relationship between corpora, misalignment, and bias. The misalignment and bias produced by MLLM arise in part from the bias and imbalanced language proportions of the training corpora. \n\n[8] and under-representation on low-resource languages leads to MLLMs' poor performance on these languages. Inspired by the impressive performance of monolingual representation models like Word2vec [9] and GloVe [10], recent research has made great progress in multilingual representation. In section IV, we review previous research on multilingual word embeddings and classify them into three categories: static multilingual representation, contextual multilingual representation and combined multilingual representation. We also analyze the impact of various factors on multilingual alignment performance, including initial alignment solution, linearity of mapping function, language typological distance, and pretraining data and settings of MLLMs. \n\nThirdly, MLLMs are prone to produce harmful outcomes and social bias [11] in part due to bias is naturally present in cross-cultural datasets and the design of MLLMs' modeling processes [12]. Previous studies have explored bias in various NLP tasks and demographic groups. However, these studies are specific to English-based models [13], [14] and cannot be generalized to other languages. What are the types of bias in existing MLLMs? What are the main de-biasing techniques available for MLLMs? Does the removal of these biases affect the performance of the large language models? What are the existing bias evaluation datasets for MLLMs? These are very worthwhile research questions. This survey tries to answer these questions and offers valuable insights for bias on MLLMs. \n\nSummarizing the above discussion, the contributions of this survey are as follows: \n\n\u2022 We present an overview of MLLMs and analyze the the language imbalance challenge within MLLMs, their capacity to support low-resource languages and their potential for cross-lingual transfer learning. \u2022 We provide an overview of the datasets and corpora utilized by existing MLLMs to offer a comprehensive insight into the language distribution within these training datasets.",
            "score": 0.49678532669469555,
            "section_title": "Misalignment",
            "char_start_offset": 3986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 8,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 216
                },
                {
                    "start": 219,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 420,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "1957433"
                },
                {
                    "start": 1042,
                    "end": 1046,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 1306,
                    "end": 1310,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 1312,
                    "end": 1316,
                    "matchedPaperCorpusId": "215828184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "246430787",
            "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
            "text": "For the experiment, we fine-tune mBERT (Devlin et al., 2019) on intent detection. 5 he results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation. \n\nAnalysis of Sentence Encodings. One reason behind the scores observed in based approach. To test this, we obtain sentence encodings of all user turns for one intent from the three datasets via the distilled multilingual USE sentence encoder (Yang et al., 2020;Reimers and Gurevych, 2019). 6 s illustrated in Figure 2, the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets. \n\nFurther Discussion. To meet the urgent, evergrowing demand for large-scale multilingual TOD datasets, data collection methods which efficiently leverage existing resources to generate new data fast without compromising data quality are especially needed. Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable.",
            "score": 0.49658285694545257,
            "section_title": "Translation versus Outline-Based",
            "char_start_offset": 23460,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2320
                },
                {
                    "start": 2321,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 39,
                    "end": 60,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1012,
                    "end": 1031,
                    "matchedPaperCorpusId": "195848217"
                },
                {
                    "start": 1031,
                    "end": 1058,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9873046875
        },
        {
            "corpus_id": "258840943",
            "title": "LLM-powered Data Augmentation for Enhanced Crosslingual Performance",
            "text": "The zero-shot cross-lingual approach is commonly used when a multilingual dataset is insufficient. In this subsection, we investigate whether a synthetically generated multilingual dataset outperforms training solely in English. We choose the XCOPA dataset and explore two settings: synthetic multilingual data by asking LLMs to generate responses in the target languages directly and translating the English-generated data to target languages with Google Translate API. We exclude Dolly-v2 and StableVicuna due to their limited effectiveness in generating non-English text. Although GPT-4 exhibits the most promising performance, it is significantly costlier compared to ChatGPT. Therefore, we also consider using ChatGPT as a contrasting Table 5: Accuracy on XCOPA. ORI corresponds to the original data, GEN EN and GEN XX represents data generated in English and target languages. T rans denotes translations of the English-generated data. We show languages that are available in all settings. Improvement and decline in performance are represented with green and red shadows. \n\nexperiment under resource-constrained conditions. \n\nTable 5 shows the results for the languages that are available for all settings, excluding TR and TH (unavailable for LLM-generation, refer to \u00a74), and QU (not supported by the Google Translate API). We can see the impact of the generated data varies across different fine-tuned models and languages, aligning with the findings of Kumar et al. (2022). Training on GPT-4 synthesized data displays consistent improvement across all scenarios and languages, except the zero-shot crosslingual result on HT with XLMR-Large. \n\nMore fluctuating results can be observed with ChatGPT-generated data. A comparison between GENEN + ORI and GENXX + ORI indicates that utilising data generated in target languages generally leads to improved performance with GPT-4 generated data, as well as in base models with ChatGPT-generated data. However, for XLMR-Large, employing ChatGPT-generated data in target languages mostly yields negative outcomes. In languages such as TA and VI, training on generated data in the target languages results in more performance degradation compared to zero-shot cross-lingual transfer.",
            "score": 0.496122157253554,
            "section_title": "Multilingual Data Generation",
            "char_start_offset": 13480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 1464,
                    "end": 1483,
                    "matchedPaperCorpusId": "250144691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475830078125
        },
        {
            "corpus_id": "274422399",
            "title": "LLM Teacher-Student Framework for Text Classification With No Manually Annotated Data: A Case Study in IPTC News Topic Classification",
            "text": "Moreover, the zero-shot cross-lingual performance is often comparable to, and in most cases even exceeds, the models' performance in monolingual and multilingual settings -where the models are evaluated on the language which is included in their (either monolingual or multilingual) fine-tuning dataset. These findings, pertaining to Research Question 3 (RQ3), demonstrate that student models exhibit high performance even when applied to languages that are not part of the fine-tuning training dataset. \n\nSecond, we compare the performance of student models on a target language when they are fine-tuned on 5,000 instances of (1) monolingual data in the target language, or (2) multilingual data that are balanced across the target language and three additional languages. Through these experiments, we address Research Question 4 (RQ4), which focuses on investigating whether the development of language-specific models offers any advantages over the development of a multilingual model trained on an equivalent amount of data, consisting of instances in the target language and other languages. If the experiments would show that monolingual results are significantly higher, this might require to host potentially many language-specific models for cases where top performance is important enough. The results presented in Table 4, where the monolingual performance is highlighted in gray, however, reveal that the multilingual model achieves performance that is comparable to, or on certain language-specific test sets surpasses that of the model fine-tuned only on the target language. This finding highlights the benefits of training student models on multiple languages, through which the robustness of the model is quite likely to improve. More importantly, this result allows the final inference procedure to be very simple, with a single multilingual model that can be applied to any text written in one of the 100 supported languages on which the XLM-RoBERTa model was pretrained [54].",
            "score": 0.496055696519915,
            "section_title": "D. MONOLINGUAL, MULTILINGUAL AND CROSS-LINGUAL PERFORMANCE",
            "char_start_offset": 33318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 1991,
                    "end": 1995,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.446044921875
        },
        {
            "corpus_id": "270870062",
            "title": "Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models",
            "text": "This section aims to explore the proposed third question: Is there a correlation between the crosslingual consistency performance of LLMs and their multilingual capabilities?\n\nWe investigate the potential correlation between cross-lingual consistency and multilingual capabilities of LLMs through multilingual translation tasks.Using the Flores-200 development test (devtest) dataset (Goyal et al., 2021;Team, 2022), we selected 12 test languages, creating a comprehensive test set with 132 translation directions.Based on this test set, we evaluated the translation capabilities of two LLMs: Bloomz-7b and Baichuan2-7b.To mitigate the impact of tokenization on translation metrics for certain languages (such as Chinese, Japanese, and Korean), we used the CHRF metric (Popovic, 2017) to quantify the performance of the models in each translation direction.bution of xSC scores between different languages for two models, while the right side displays the zero-shot translation performance scores between different languages.The results indicate a consistent distribution trend between the performance of LLMs in multilingual translation tasks and their xSC scores.Specifically, these models demonstrate higher translation accuracy and cross-lingual semantic consistency in tasks involving Germanic languages (such as English, German, and Dutch) and Indo-Romance languages (such as French, Spanish, Italian, and Portuguese).In contrast, the performance and cross-linguistic consistency are relatively weaker in translation tasks that do not involve these two language families.Darker points indicate the model's average performance across all translation tasks involving that particular language, while lighter points correspond to the model's average xAC score for that language.The results show a clear positive correlation between the multilingual translation capabilities of LLMs and their average xAC scores.This correlation is consistent not only across different models, indicating that the higher the average xAC score, the stronger the overall multilingual translation performance, but also within the same model across different languages, showing that the higher the average xAC score for a particular language, the stronger the model's average performance in all translation tasks centered on that language.",
            "score": 0.49536603407291835,
            "section_title": "Relation Between Cross-Lingual Consistency and Translation Capabilities",
            "char_start_offset": 19437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 176,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 620
                },
                {
                    "start": 620,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1165
                },
                {
                    "start": 1165,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1577
                },
                {
                    "start": 1577,
                    "end": 1780
                },
                {
                    "start": 1780,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2319
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 784,
                    "matchedPaperCorpusId": "12942757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439453125
        },
        {
            "corpus_id": "252873187",
            "title": "CROP: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation",
            "text": "Table 3 compares the performance of our method with previous relevant methods initialed by different cross-lingual pretrained language models including mBERT and XLM-R. Given our translation model, the multilingual translated annotated corpora (Translate-Train) from the data of source languages can be used to improve the model performance compared to the XLM-R. Particularly, our proposed method gains significant improvement compared to other languages by a large margin (nearly +6 F1 points), due to the effectiveness of cross-lingual entity projection. \n\nAll experimental results demonstrate that our proposed framework strengthens transferability from the source language to nearly 39 target languages. \n\nAblation Study To verify the effectiveness of our method, we separately study the effects of the model-based transfer by cross-lingual pretrained model and the data-based transfer by cross-lingual entity projection. Our method has two advantages: \n\n(1) the model is trained on the original multilingual corpora with pseudo labels, which avoids the extra translation error. (2) our method uses the multilingual model trained on 41 languages to improve the entity projection of low-resource languages. In Table 4, Transformer \u2462 without any transfer methods gets the worst performance (only 15.1 F1 scores). \n\nOur method \u2461 without any pretrained model outperforms Transformer \u2462 by +43.0 F1 points, which has the similar transferability to the cross-lingual pretrained language models. Combining the merits of the cross-lingual pretrained model and selftraining for multiple languages, we obtain the best performance on the XTREME-40 benchmark. \n\nDistribution of Multilingual Corpora An important difference between our method and the previous baselines is that we provide an effective way to leverage the unlabeled corpora of target languages. The raw data is first translated to the source language data and annotated by the NER model trained on the original dataset. Then, the translated source sentences are back-translated to target languages, where the entity labels are projected to the target raw words. Our cross-lingual entity projection avoids the extra translation errors instead of direct utilization for translated labeled corpora.",
            "score": 0.4950019874022647,
            "section_title": "XTREME-40",
            "char_start_offset": 16187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 557
                },
                {
                    "start": 560,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1315
                },
                {
                    "start": 1318,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2252
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83251953125
        },
        {
            "corpus_id": "204734128",
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "text": "First, we state our desired properties for a crosslingual QA evaluation dataset. We note that whilst some existing datasets exhibit these properties, none exhibit them all in combination (see section 3). We then describe our annotation protocol, which seeks to fulfil these desiderata. \n\nParallel The dataset should consist of instances that are parallel across many languages. First, this makes comparison of QA performance as a function of transfer language fairer. Second, additional evaluation setups become possible, as questions in one language can be applied to documents in another. Finally, annotation cost is also reduced as more instances can be shared between languages. \n\nNatural Documents Building a parallel QA dataset in many languages requires access to parallel documents in those languages. Manually translating documents at sufficient scale entails huge translator workloads, and could result in unnatural documents. Exploiting existing naturally-parallel documents is advantageous, providing high-quality documents without requiring manual translation. \n\nDiverse Languages A primary goal of crosslingual research is to develop systems that work well in many languages. The dataset should enable quantitative performance comparison across languages with different linguistic resources, language families and scripts. \n\nExtractive QA Cross-lingual understanding benchmarks are typically based on classification (Conneau et al., 2018). Extracting spans in different languages represents a different language understanding challenge. Whilst there are extractive QA datasets in a number of languages (see Section 3), most were created at different times by different authors with different annotation setups, making cross-language analysis challenging. \n\nTextual Domain We require a naturally highly language-parallel textual domain. Also, it is desirable to select a textual domain that matches existing extractive QA training resources, in order to isolate the change in performance due to language transfer. \n\nTo satisfy these desiderata, we identified the method described below and illustrated in Figure 1. Wikipedia represents a convenient textual domain, as its size and multi-linguality enables collection of data in many diverse languages at scale. It has been used to build many existing QA training resources, allowing us to leverage these to train QA models, without needing to build our own training dataset.",
            "score": 0.4949651730633069,
            "section_title": "The MLQA corpus",
            "char_start_offset": 4847,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2026
                },
                {
                    "start": 2029,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2437
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "204734128",
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "text": "We provide baselines using state-of-the-art crosslingual techniques. We develop machine translation baselines which map answer spans based on the attention matrices from a translation model, and use multilingual BERT (Devlin et al., 2019) and XLM (Lample and Conneau, 2019) as zero-shot approaches. We use English for our training language and adopt SQuAD as a training dataset. We find that zero-shot XLM transfers best, but all models lag well behind training-language performance. \n\nIn summary, we make the following contributions: i) We develop a novel annotation pipeline to construct large multilingual, highly-parallel extractive QA datasets ii) We release MLQA, a 7language evaluation dataset for cross-lingual QA iii) We define two cross-lingual QA tasks, including a novel generalised cross-lingual QA task iv) We provide baselines using state-of-the-art techniques, and demonstrate significant room for improvement.",
            "score": 0.4947004479707954,
            "section_title": "Introduction",
            "char_start_offset": 3901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 926
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 238,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "273532366",
            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
            "text": "To better measure the performance of MLLMs on crosslingual zero-shot and few-shot tasks, multiple research institutions collaborated to build the xP3 [27] dataset, covering 46 languages, including English and machine-translated prompts; Microsoft's team built the MEGA [50] evaluation dataset, including 16 evaluation tasks, covering more than 70 languages; the University of Washington, Google, and Allen AI jointly released their multilingual evaluation dataset BUFFET [51], including 15 evaluation tasks, covering 54 languages, and additionally providing fixed few-shot sets and instructions to better measure the effectiveness of MLLMs on few-shot crosslingual transfer tasks. \n\nIn addition researchers also released multilingual dialogue datasets for task-oriented dialogues, such as GlobalWoZ [52] and X-RiSAWOZ [53], to help developers better develop and evaluate multilingual task-oriented dialogue systems. \n\n2) Methods: Traditional model evaluation methods (Evaluation Methodologies) use supervised training corpora to finetune pre-trained models, with the pipeline being pre-training + fine-tuning. MLLMs have inherent zero-shot and few-shot learning capabilities, and their evaluation methods can be divided into two categories: one uses the original evaluation method for task-related fine-tuning, and the other is based on prompt in-context learning (ICL), with the pipeline changing to pre-training + prompting + prediction. Task-related finetuning requires using training corpora to update model parameters, while ICL does not need to update model parameters, only requiring the design of different prompts for the model to return corresponding results. The evaluation methods for MLLMs are shown in Figure 3. \n\nTask-Specific Fine-tuning: \n\n\u2022 Zero-Shot Cross Lingual Transfer: First perform taskrelated fine-tuning on one language, then evaluate using the test set of another language. \u2022 Few-Shot Cross Lingual Transfer: First fine-tune parameters on English and a small amount of target language, then evaluate on the target language test set. \u2022 Monolingual Fine-tuning: Only fine-tune parameters on the full target language. \u2022 Translate-Train: Use machine-translated target language for parameter fine-tuning.",
            "score": 0.49427927112238823,
            "section_title": "B. Evaluation Tasks",
            "char_start_offset": 30322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2060
                },
                {
                    "start": 2061,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2227
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92822265625
        },
        {
            "corpus_id": "261682140",
            "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
            "text": "Conventional benchmarks typically emphasize a single metric evaluation per dataset. As illustrated in Figure 2, it becomes apparent that they do not provide enough coverage in multilingual FM evaluation. Therefore, in addition to standard metrics, we introduce two new evaluation dimensions called instruction sensitivity and cross-lingual consistency to measure a model's stability across instructions and languages. Regarding standard evaluation metrics, we use accuracy scores for multiple-choice questions. In the case of translation assessments, we report the BLEU-4 score (Papineni et al., 2002), while for summarization tasks, we deploy the average of ROUGE-1/2/L scores (Lin, 2004). \n\nCross-Lingual Consistency Besides the standard Accuracy metric for evaluating multi-choice questions, we compute the cross-lingual Consistency score as a measurement of whether the answers are consistent for the same question in 7 different languages without considering the answer's correctness. Specifically, for a question set Q = {q 1 , q 2 , ..., q N }, each question q i is represented in 7 languages q i = {q i eng , q i zho , q i ind , q i spa , q i vie , q i msa , q i f il }, and a i lang is model's answer to q i lang , the Consistency score is computed as \n\n.,ls}\u2208C(s,q i ) M {l 1 ,l 2 ,..,ls} C s where s \u2208 [2, 7]. It measures the answer's consistency of any combination of s languages. The model gets rewarded if it generates consistent answers across the sampled languages. The consistency requirement is enhanced to more languages with increased s. Given that both Accuracy and Consistency alone do not provide a comprehensive assessment of models' performance on crosslingual datasets, we introduce the AC3 score as a holistic measure, which is calculated as the harmonic mean of both scores: \n\nwhere AC3 is within range [0, 1]. We deploy AC3 with s = 3 as the default value for Cross-MMLU and Cross-LogiQA datasets. Figure 8 illustrates the impact on variable s.",
            "score": 0.49402216554782746,
            "section_title": "Evaluation Protocols",
            "char_start_offset": 17012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1802
                },
                {
                    "start": 1805,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 578,
                    "end": 601,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 678,
                    "end": 689,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7255859375
        },
        {
            "corpus_id": "248426926",
            "title": "NMTScore: A Multilingual Analysis of Translation-based Text Similarity Measures",
            "text": "Our analysis highlights theoretical and empirical properties of translation-based text similarity measures in a multilingual setting. Direct translation probability is the most straightforward measure (an empirical comparison of inference times is found in Appendix A). However, it treats inputs as target sequences, and we show that accuracy on paraphrase identification can be clearly improved by normalizing with reconstruction probability.\n\nPivot translation probability is advantageous especially when performing cross-lingual comparisons. Finally, translation cross-likelihood has the advantage that it achieves symmetry with a single translation direction, and that the input languages need not be specified. The latter property also has interesting consequences for reference-based evaluation: The metric is expected to ignore whether the generated text matches the language of the reference. This can be seen as a rigorous disentanglement of adequacy from fluency.\n\nIn comparison to baseline measures, translationbased measures are generally slower but show high accuracy on multilingual paraphrase identification, comparatively good reliability on reference-based evaluation of data-to-text generation, and little correlation to alternative measures. Our findings thus show the usefulness of NMT translation probabilities for similarity tasks that require high attention to detail.",
            "score": 0.4933338448495384,
            "section_title": "Conclusion",
            "char_start_offset": 23826,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95947265625
        },
        {
            "corpus_id": "277787023",
            "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting",
            "text": "Bias in language models has garnered widespread attention in English, but this challenge is especially pronounced for low-resource languages, which often lack sufficient training data. This study focused on two main aspects: Evaluating bias in a multilingual setting and demonstrating the cross-lingual transferability of debiasing methods, offering new perspectives for ensuring fairness in multilingual models. \n\nIn this study we proposed a new evaluation metric to enable fair comparisons between different models, constructed a multilingual bias evaluation dataset, consisting of languages with different resource status, and benchmarked major multilingual language models. We have found that for LLMs transfer learning of fairness is much better for non-English language compared to previous models, and suggested that there exists a nuance of social bias when dealing with different linguistic and cultural backgrounds. Our study demonstrates that high-resource language debias methods can be effectively transferred to low-resource languages for bias mitigation. SenDeb emerged as the most effective technique, suggesting that bias subspaces may share cross-linguistic properties, enabling crosslanguage debiasing. This finding opens new possibilities for developing universal debiasing methods across diverse languages. \n\nBased on this work, more could be explored to further advance fairness in multilingual NLP, to improve inclusivity and ethical integrity of NLP worldwide: \n\n\u2022 Expand and Diversify Bias Evaluation Datasets, covering more languages and cultural contexts to enhance the comprehensiveness of bias evaluation. \n\n\u2022 Conduct more detailed studies on different bias types and develop more language-aware debias methods. Also explore debiasing techniques more specifically tailored to causal LMs. \n\n\u2022 Enhance model performance in low-resource languages with larger-scale training and finetuning approaches. \n\n\u2022 Explain why bias subspace are shared mutually in various languages on a interpretability level and design better approach to align them. \n\nDevelop a universal debiasing method that works equally well on a pan-linguistic scale.",
            "score": 0.49329332006350185,
            "section_title": "Conclusion and Future Directions",
            "char_start_offset": 26396,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 412
                },
                {
                    "start": 415,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1926
                },
                {
                    "start": 1929,
                    "end": 2067
                },
                {
                    "start": 2070,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "239009796",
            "title": "GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems",
            "text": "multilingual ToD dataset in three new targetlanguages via machine translation and crawled ontologies in the target-language countries.\n\nOur method only requires minor human efforts to post-edit a few hundred machine-translated dialogue templates in the target languages for evaluation. Besides, as cross-lingual transfer via pre-2 See comparisons of these use cases in Appendix A trained multilingual models (Devlin et al., 2019;Conneau et al., 2020;Liu et al., 2020;Xue et al., 2021) has proven effective in many cross-lingual tasks, we further investigate another question: How do these multilingual models trained on the English ToD dataset transfer knowledge to our globalized dataset? To answer this question, we prepare a few baselines by evaluating popular ToD systems on our created test datasets in a zero-shot cross-lingual transfer setting as well as a few-shot setting.\n\nOur contributions include the following:\n\n\u2022 To the best of our knowledge, we provide the first step towards analyzing three unexplored use cases for multilingual ToD systems.\n\n\u2022 We propose a cost-effective method that creates a new multilingual ToD dataset from an existing English dataset. Our dataset consists of highquality test sets which are first translated by machines and then post-edited by professional translators in three target languages (Chinese, Spanish and Indonesian). We also leverage machine translation to extend the language coverage of test data to another 17 target languages.\n\n\u2022 Our experiments show that current multilingual systems and translate-train methods fail in zeroshot cross-lingual transfer on the dialogue state tracking task. To tackle this problem, we propose several data augmentation methods to train strong baseline models in both zero-shot and fewshot cross-lingual transfer settings.\n\nIn order to globalize an existing English ToD dataset for the three aforementioned use cases, we propose an approach consisting of four steps as shown in Figure 2: (1) we first extract dialogue templates from the English ToD dataset by replacing English-specific entities with a set of generalpurpose placeholders ( \u00a72.1); (2) we then translate the templates to a target language for both training and test data, with one key distinction that we only post-edit the test data by professional",
            "score": 0.4925062117961287,
            "section_title": "Introduction",
            "char_start_offset": 3900,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72021484375
        },
        {
            "corpus_id": "215754572",
            "title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus",
            "text": "We first conduct experiments on cross-lingual transfer from the English source to the rest of the other six target languages, respectively, which has been a typical setting for cross-lingual investigations (Wang et al., 2019). The results are summarized in Table 2. We list the F-scores by using only the source corpus (SRC), only the translated target corpus (TGT) and the mixture corpus of source and target (SRC & TGT), comparing the performances of different multilingual word representations as well as different multilingual SRL models. \n\nMultilingual word representations. First, we evaluate the effectiveness of the three different multilingual word representations exploited. We compare their performances under two settings, by using SRC and TGT corpus, respectively. According to the results, we find that the multilingual contextualized word representations (i.e. BERT and ELMo) are better in both two settings, which is consistent with previous studies (Mulcaire et al., 2019;Schuster et al., 2019). Interestingly, the multilingual BERT performs worse than the ELMo, which can be explained by that the ELMo representation is pre-trained based on the corpus which involves in the focused seven languages. This indicates that the official released multilingual BERT can be further improved, since monolingual BERT has been demonstrated to produce better performances than ELMo (Tenney et al., 2019). \n\nTranslated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann and Agic, 2016;Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model. \n\nCombining source and pseudo target.",
            "score": 0.49206153668581004,
            "section_title": "Cross-Lingual Transfer from English",
            "char_start_offset": 17154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1410
                },
                {
                    "start": 1413,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2241
                },
                {
                    "start": 2244,
                    "end": 2279
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 225,
                    "matchedPaperCorpusId": "202578048"
                },
                {
                    "start": 989,
                    "end": 1011,
                    "matchedPaperCorpusId": "67856005"
                },
                {
                    "start": 1388,
                    "end": 1409,
                    "matchedPaperCorpusId": "108300988"
                },
                {
                    "start": 1947,
                    "end": 1973,
                    "matchedPaperCorpusId": "7805664"
                },
                {
                    "start": 1973,
                    "end": 1992,
                    "matchedPaperCorpusId": "202541341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94775390625
        },
        {
            "corpus_id": "277787023",
            "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting",
            "text": "This study aims to systematically evaluate model bias in a multilingual setting. We adopt the masked language model prediction probability method proposed by Nadeem et al. (2020) to measure the bias of language models toward specific social attributes. To compare biases across different models and languages, we propose a new evaluation metric that standardizes model evaluation indicators, enabling more precise cross-model bias comparison. \n\nBased on the CrowS-Pairs dataset (Nangia et al., 2020), we created a new multilingual bias evaluation dataset with English, Chinese, Russian, Thai, and Indonesian; each with four representative bias types: gender, race-color, nationality and religion. These languages were selected based on their global use and the distribution of online resources, with Thai and Indonesian considered low-resource languages. In our experiments, we evaluated a series of widely used multilingual language models. We noticed significant differences in the type of model bias across languages. Our findings emphasize the importance of incorporating diverse cultural and linguistic backgrounds into research to ensure fairness on a global scale. \n\nLittle research has been done on exploring application in debiasing methods across languages. In this work, we further fine-tuned multilingual models on English Wikipedia datasets with CDA, DO, computed the bias subspace with SenDeb, and measured bias shifts across English and four other languages. \n\nWe summarize our contribution as follows. \n\n\u2022 we proposed NBS ( \u00a73.1 & \u00a7A) as a method to evaluate model bias in a multilingual setting by measuring the normalized probability of a masked-word prediction in a biased context. \n\n\u2022 we curated a new dataset ( \u00a73.2) for bias evaluation with five languages, representatively selected based on the language resource conditions, with Thai and Indonesian considered low-resource languages. \n\n\u2022 our results show that the impact of different bias types (e.g., gender, religion) varies across languages and cultures in the six multilingual models we tested ( \u00a73.3.2). \n\n\u2022 we demonstrated that debiasing strategies can be effectively transferred to other languages through cross-lingual knowledge sharing ( \u00a74.3). \n\n2 Related Works",
            "score": 0.4917685982160499,
            "section_title": "Introduction",
            "char_start_offset": 2126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1517
                },
                {
                    "start": 1520,
                    "end": 1700
                },
                {
                    "start": 1703,
                    "end": 1907
                },
                {
                    "start": 1910,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2227
                },
                {
                    "start": 2230,
                    "end": 2245
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "236318351",
            "title": "Modelling Latent Translations for Cross-Lingual Transfer",
            "text": "Cross-lingual knowledge transfer supports the development of natural language technology for many of the world's languages (Ruder et al., 2019;Ponti et al., 2019, inter alia). The approach currently predominant for cross-lingual transfer relies on massively multilingual pre-trained encoders (Conneau et al., 2020;Xue et al., 2020;Liu et al., 2020) that are fine-tuned on a source language and perform zero-shot (Wu and Dredze, 2019;Ponti et al., 2021) or few-shot (Lauscher et al., 2020;Zhao et al., 2020) prediction in a target language. \n\nAn alternative approach for cross-lingual transfer, translate test, is based on translating the evaluation set into the source language and leveraging a monolingual classifier instead (Banea et al., 2008;Durrett et al., 2012;Conneau et al., 2018). This approach is currently under-investigated and usually relegated to the role of a baseline due to its lower flexibility, e.g., it is not suitable for sequence labelling tasks. Yet, it achieves the state-of-the-art results in most benchmarks for multilingual Natural Language Understanding and Question Answering tasks (Hu et al., 2020;Ponti et al., 2020;Ruder et al., 2021, inter alia). Moreover, the availability of off-the-shelf translation models for multiple languages (Wu et al., 2016;Tiedemann and Thottingal, 2020;Liu et al., 2020) provides coverage for transfer to a large number of target languages. Indeed, very recent preliminary results suggest that the translation-based transfer might even outperform monolingual pre-trained models in languages different from English (Isbister et al., 2021). \n\nTranslation-based transfer, however, currently suffers from two main limitations. First, the errors in translation accumulate along the pipeline. In fact, sentences that are possibly not faithful to the original in the target language and/or not grammatical in the source language are fed to the classifier, which degrades its performance.",
            "score": 0.4914709974101118,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 143,
                    "matchedPaperCorpusId": "26127787"
                },
                {
                    "start": 292,
                    "end": 314,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 331,
                    "end": 348,
                    "matchedPaperCorpusId": "215433131"
                },
                {
                    "start": 412,
                    "end": 433,
                    "matchedPaperCorpusId": "126167342"
                },
                {
                    "start": 433,
                    "end": 452,
                    "matchedPaperCorpusId": "210966228"
                },
                {
                    "start": 465,
                    "end": 488,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 726,
                    "end": 746,
                    "matchedPaperCorpusId": "2432354"
                },
                {
                    "start": 746,
                    "end": 767,
                    "matchedPaperCorpusId": "2895378"
                },
                {
                    "start": 767,
                    "end": 788,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1111,
                    "end": 1128,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1128,
                    "end": 1147,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1283,
                    "end": 1314,
                    "matchedPaperCorpusId": "221097277"
                },
                {
                    "start": 1314,
                    "end": 1331,
                    "matchedPaperCorpusId": "215433131"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "253098917",
            "title": "Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings",
            "text": "This means that, during fine-tuning on the translated data, the model is not receiving any signal to learn that \"Economy\" may be part of an organization entity. This may explain why the zero-shot method performs better for cases such as \"Econom\u00eda\", \"Hacienda\", \"Plan\" and \"Junta\", listed in Table 7. \n\nSummarizing, we see that machine translation data often generates a signal which is, due to inherent differences in language use, different to the signal received when using gold-labelled data in the target language. This disagreement seems to be the most common reason for the larger number of false positive and negatives of the data-based cross-lingual transfer method with respect to the zero-shot technique. A detailed error analysis demonstrates that databased cross-lingual transfer is hindered by machine translations which, although linguistically sound, do not align with the cultural behaviour of the target language use. Moreover, the results also show that the different word alignments methods (for annotation projection) are of high quality, obtaining comparable results with respect to manually generated alignments. \n\nIn any case, our results establish that there is still room for improving the cross-lingual performance of zero-resource sequence labelling. zero-shot model transfer approach work for Indo-European languages, while their performance for other language families remains unknown. Finally, the error analysis was performed for the EN-ES language pair only. \n\nIn any case, we believe that our main claim still holds. Even though MT quality has substantially improved over the last few years, our results indicate the current optimal solution to perform crosslingual transfer is by using large multilingual language models such as XLM-RoBERTa-large. Thus, our error analysis suggests that this might be due to important differences in language use. More specifically, MT often generates a textual signal which is different to what the models are exposed to when using gold standard data, which affects both the fine-tuning and evaluation processes. This is confirmed by our error analysis which shows that mistranslations are not the main source of errors in the data-transfer method. use the train set as both, train and development data.",
            "score": 0.49122979278251333,
            "section_title": "Where do the models fail?",
            "char_start_offset": 28585,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 299
                },
                {
                    "start": 302,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1490
                },
                {
                    "start": 1493,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.685546875
        },
        {
            "corpus_id": "277787023",
            "title": "Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting",
            "text": "Our bias evaluation dataset was constructed through translation, primarily based on an American cultural and linguistic context. This could introduce unfair comparison given that the datasets do not necessarily reflect the sociocultural realities of other languages, potentially affecting the accuracy of our cross-lingual bias evaluations. The bias evaluation and debias methods in this study were designed for masked language models. While we approximate their application to causal language models (autoregressive transformers), there may be inherent differences in how bias manifests and propagates in these models. While our study suggests that bias subspaces share cross-linguistic properties, the extent to which this holds across all languages remains an open question. The effectiveness of debiasing methods may vary for languages with fundamentally different linguistic structures or those underrepresented in pretraining corpora. Further research is needed to validate and refine our findings across a wider linguistic and typological spectrum. \n\naugmentation for mitigating gender stereotypes in languages with rich morphology. arXiv preprint arXiv:1906.04571.",
            "score": 0.49120452888616084,
            "section_title": "Limitations",
            "char_start_offset": 28569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1055
                },
                {
                    "start": 1058,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1172
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "253420640",
            "title": "Cross-lingual Transfer Learning for Check-worthy Claim Identification over Twitter",
            "text": "Architectures based on mBERT demonstrated effectiveness in cross-lingual transfer learning in several text classification tasks [45]. Up to our knowledge, this is the first study of this kind and scale for the problem of check-worthiness detection. We mainly address the following research questions: \n\n[RQ1] Given labeled data in a source language, how effective is zero-shot cross-lingual check-worthiness prediction on a different target language? [RQ2] Does translation between source and target languages improve the performance? [RQ3] How much improvement can we achieve by adding few labeled examples in the target language to labeled examples in source language (i.e., few-shot transfer learning)? [RQ4] Will adversarial training with unlabeled examples in target language improve over the zero-shot cross-lingual setup? [RQ5] Can we improve the performance if we transfer from multiple source languages to a single target language? [RQ6] How effective is cross-lingual transfer compared to the state of the art models? \n\nOur contribution in this work is four-fold: \n\n\u2022 We extensively explore and benchmark diverse methods to train cross-lingual check-worthiness prediction models including zero-shot, few-shot, and translation-based approaches. \n\nExisting studies for the task have not provided such a largescale comparative study with different variants. \u2022 We demonstrate that for some language pairs, cross-lingual transfer learning (e.g., from Arabic to Turkish) is at least comparable or even significantly better than monolingual models exclusively trained on target language. While for other languages (Bulgarian and Spanish), cross-lingual transfer is not effective regardless of the setup or source language. \u2022 Our results show that for some target languages, cross-lingual transfer models are as good as state-of-the art models developed for check-worthiness estimation on the same dataset. \u2022 This study is the first to experiment with adversarial training for cross-lingual check-worthiness prediction. \n\nThe remainder of this paper is presented as follows. We summarize existing studies in Section 2. Section 3 presents the approach and design of experiments we follow in this study.",
            "score": 0.49084597022135734,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 300
                },
                {
                    "start": 303,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1253
                },
                {
                    "start": 1256,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2021
                },
                {
                    "start": 2024,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "126167342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "259859100",
            "title": "C-XNLI: Croatian Extension of XNLI Dataset",
            "text": "Comprehensive multilingual evaluations have been encouraged by emerging cross-lingual benchmarks and constrained by existing parallel datasets. To partially mitigate this limitation, we extended the Cross-lingual Natural Language Inference (XNLI) corpus with Croatian. The development and test sets were translated by a professional translator, and we show that Croatian is consistent with other XNLI dubs. The train set is translated using Face-book\u2019s 1.2B parameter m2m_100 model. We thoroughly analyze the Croatian train set and compare its quality with the existing machine-translated German set. The comparison is based on 2000 manually scored sentences per language using a variant of the Direct Assessment (DA) score commonly used at the Conference on Machine Translation (WMT). Our findings reveal that a less-resourced language like Croatian is still lacking in translation quality of longer sentences compared to German. However, both sets have a substantial amount of poor quality translations, which should be considered in translation-based training or evaluation setups.",
            "score": 0.4906736899336139,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8408203125
        },
        {
            "corpus_id": "276408514",
            "title": "Sparse Autoencoder Features for Classifications and Transferability",
            "text": "This section evaluates the cross-lingual robustness of SAE features. We investigate whether features extracted from multilingual datasets are consistent with those found in monolingual contexts and explore the correlation between SAE feature transferability and cross-lingual prediction performance. We conduct three primary experiments: (1) comparing native and cross-lingual transfer, (2) evaluating different feature selection methods, and (3) assessing the impact of training data sampling. \n\nDataset: We use the multilingual toxicity detection dataset (Dementieva et al., 2024), which contains text in five languages labeled with a binary toxicity label: English (EN), Chinese (ZH), French (FR), Spanish (ES), and Russian (RU).",
            "score": 0.4905454286552304,
            "section_title": "Multilingual Classification and Transferability",
            "char_start_offset": 14074,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7578125
        },
        {
            "corpus_id": "260886825",
            "title": "Comparison between parameter-efficient techniques and full fine-tuning: A case study on multilingual news article classification",
            "text": "Additionally, due to the specific data collection methodology used for MultiEURLEX, this dataset is not likely to contain irrelevant text. \n\nMore recent research by Xenouleas et al. [13] has questioned whether the findings of Chalkidis et al. would generalise to other datasets, since MultiEURLEX consists mostly of parallel texts (the same content in multiple languages). When the dataset is modified to include only non-parallel documents, they found that translation-based methods outperform multilingual models. We note that, this is likely dependent on domain and on whether the relevant properties are significantly affected by translation: legal documents are more likely to be properly represented in the target language than, for example, the language-specific linguistic properties signalling certain persuasion techniques. \n\nTo our knowledge, no comparison of LoRA to FFT method in a similar multilingual scenario exists. \n\nIt should also be noted that three of the systems that participated in the original SemEval-2023 Task 3 [14] evaluation exercise used adapters. Teams HHU [22] and NAP [23] entered only sub-task 3, in which they used adapters, whereas SheffieldVeraAI [15] applied adapters to sub-tasks 1 and 2. Initial performance analysis in these sub-tasks showed the effect of adapters to be inconsistent across the different sub-tasks. Namely, adapters achieved higher average performance for monolingual models in sub-task 1, while hindering performance of monolingual models in sub-task 2 but achieving better results there for multilingual models. \n\nTo gain better understanding of the effectiveness of adapter methods across a range of complex classification tasks, this paper performs a more detailed validation. In order to ensure comparable results across the three SemEval 2023 Shared task 3 sub-tasks, we keep the same models and settings across all three sub-tasks and the original data split provided by the organisers for training. Moreover, this paper presents a new comparable investigation of the properties of LoRA on the same multilingual multilabel classification problems.",
            "score": 0.49018275408594425,
            "section_title": "Application of adapters and LoRA",
            "char_start_offset": 9417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 833
                },
                {
                    "start": 836,
                    "end": 932
                },
                {
                    "start": 935,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "249461682"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "259376573"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "259376746"
                },
                {
                    "start": 1102,
                    "end": 1106,
                    "matchedPaperCorpusId": "258352322"
                },
                {
                    "start": 1185,
                    "end": 1189,
                    "matchedPaperCorpusId": "257557214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.779296875
        },
        {
            "corpus_id": "270226041",
            "title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback",
            "text": "To investigate the importance of cross-lingual properties in aligning LLMs with human preferences, we conduct an ablation study on the same five benchmarks as shown in Table 3.In particular, we employ the DPO algorithm (Rafailov et al., 2023) to finetune our model, xLLMs-100, on two distinct datasets.The first dataset is our constructed cross-lingual human feedback dataset, where instructions and outputs are in different languages.The second dataset is a traditional monolingual human feedback dataset (Lai et al., 2023a), where both instructions and outputs are in the same language.Table 4 show the results categorized by lowand high-resource languages.We observe that (1) Aligning xLLMs-100 using our cross-lingual hu-  man feedback dataset yields superior results compared with using monolingual human feedback.This improvement is evident for datasets with generation tasks such as XL-SUM and FLORES, showing that our novel cross-lingual human feedback dataset effectively simulates the multilingual generation task, reducing the possibilities of generating incorrect outputs.\n\n(2) Finetuning xLLMs-100 on our cross-lingual human feedback dataset is more effective for low-resource languages than high-resource ones.This is due to the fact that highresource languages already exhibit strong understanding and generation capabilities in the vanilla LLMs (as shown in Table 3), which mitigates the impact of further finetuning xLLMs-100 on the cross-lingual preference data.\n\n(3) It is worth noting that despite aligning xLLMs-100 with crosslingual human preferences, its performance in lowresource languages is still not as good as that in high-resource languages.Although our constructed cross-lingual feedback dataset enhances multilingual performance, the inclusion of additional languages (our dataset currently includes 30 languages) might be necessary to support all lowresource languages in the evaluation benchmarks.",
            "score": 0.48985896859112654,
            "section_title": "Different Human Feedback Datasets",
            "char_start_offset": 26877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 588
                },
                {
                    "start": 588,
                    "end": 659
                },
                {
                    "start": 659,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 1084
                },
                {
                    "start": 1086,
                    "end": 1224
                },
                {
                    "start": 1224,
                    "end": 1480
                },
                {
                    "start": 1482,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1931
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 525,
                    "matchedPaperCorpusId": "260334562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "264405758",
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "text": "Regarding consistency (Section 4), GPT-3.5 tends to generate more consistent responses on English compared to non-English languages. We observe a maximum performance decrease of 9.1% in Spanish, 28.3% in Chinese, and 50.5% in Hindi when compared to English. All language pairs, except English-Spanish, exhibit statistically significant differences in performance, demonstrating the existence of language disparity. Regarding verifiability (Section 5), English and Spanish demonstrate comparable performances, whereas the performances for Chinese and Hindi are notably lower. In the most extreme case, Chinese and Hindi exhibit decreases of 14.6% and 23.4% on Macro F1, respectively. \n\nOur research carries significant real-world implications on multiple fronts. The evaluation framework proposed in our work possesses practical utility for policymakers, practitioners, and healthcare professionals for evaluating large language models and comparing their relative performance. Through our examination of LLMs' capabilities in major languages, we aspire to acquire a comprehensive understanding of their global effectiveness, which stands to influence a vast and linguistically diverse global population, impacting both linguistic accessibility and information reliability. Furthermore, our framework exhibits versatility and adaptability beyond healthcare, extending its applicability to other domains. \n\nOur contributions are summarized as follows: \u2022 Novel Framework. We propose XLingEval, a comprehensive evaluation framework for LLMs in the healthcare domain that focuses on three fundamental criteria: correctness, verifiability, and consistency. Our framework features the gaps in equity in LLM development across multiple languages, and demonstrates generalizability in this evaluation across different LLMs. \u2022 Novel Medical Benchmark. We propose XLingHealth, a Cross-Lingual Healthcare benchmark for clinical health inquiry that features the top four most spoken languages in the world. \u2022 Extensive Multilingual Evaluation. We performed comprehensive evaluation on the four most spoken languages, and found significant language disparity across these languages. \n\nWe have released all of our code, data, and tools on GitHub1 .",
            "score": 0.4892454891608732,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2168
                },
                {
                    "start": 2171,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.486083984375
        },
        {
            "corpus_id": "261242630",
            "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
            "text": "To evaluate the performance of the It-LLMs and the impact of the translation-based semantic alignment approach, we used two cross-lingual (XQUAD (Artetxe et al., 2019), MLQA (Lewis et al., 2020)) and two multi-task (MMLU (Hendrycks et al., 2021) and BBH (Suzgun et al., 2022)) benchmarks. XQUAD and MLQA focus on understanding ques-tions and answers through translation into different languages. MMLU and BBH being multi-task benchmarks include subtasks related to Boolean expressions and QA on basic-level subjects (e.g., chemistry, physics). However, we decided to introduce them to observe whether our approach degrades performance in these tasks. The first two datasets selected are appropriately constructed for multi-language testing, while the second two are available only in English. So we do a preliminary translation step as outlined below. \n\nMultiLingual Question Answering (MLQA) (Lewis et al., 2020) evaluatates cross-lingual question answering performance. The benchmark comprises over 5K extractive QA instances in the SQuAD (Rajpurkar et al., 2016) format in several languages. MLQA is highly parallel, with QA instances aligned across four languages on average. Although comprising different languages, some languages are not represented, such as Italian. To conduct the experiments uniformly, we have translated the examples as also done in the forthcoming MMLU and BBH. \n\nCross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages. \n\nMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4 .",
            "score": 0.4892092530076375,
            "section_title": "Benchmarks",
            "char_start_offset": 14084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 851
                },
                {
                    "start": 854,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1389
                },
                {
                    "start": 1392,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 167,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 174,
                    "end": 194,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 893,
                    "end": 913,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1441,
                    "end": 1463,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92578125
        },
        {
            "corpus_id": "235683093",
            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
            "text": "In this section, we list the standard multilingual benchmarks we selected for evaluation. The main desideratum for our datasets is that training data is available in multiple languages. Ideally, all such Model XNLI PAWS-X XQuAD mBERT mT5-Base Table 1: English was out-performed () by other source languages in 5/6 experimental settings. training sets would be produced by humans (or verifiably high-quality). In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer. We will be mindful of this when drawing conclusions from our experiments. \n\nTo ensure that all language-specific subsets have the same size and informational content, we occasionally depart from the established way of using some of these datasets, as elaborated below. \n\nXNLI The Cross-lingual Natural Language Inference corpus (Conneau et al., 2018) consists of premise/hypothesis pairs that are either entailments, contradictions, or neutral. XNLI extends the English MultiNLI dataset (Williams et al., 2018) to 15 languages, including low-resource ones such as Swahili and Urdu; training sets are machine-translated, while the development and test sets are human-translated. \n\nPAWS-X The Cross-lingual Paraphrase Adversaries from Word Scrambling corpus (Yang et al., 2019) is a binary classification task for paraphrase identification. Its 6 training sets were machinetranslated from the English PAWS dataset (Zhang et al., 2019). The development and test sets were human-translated. \n\nXQuAD The Cross-lingual Question Answering Dataset (Artetxe et al., 2020) requires answering questions by identifying answer spans in accompanying paragraphs. XQuAD consists of human translations of the development and test sets of the English SQuAD 1.1 corpus (Rajpurkar et al., 2016) into 10 languages.",
            "score": 0.48889237960190035,
            "section_title": "Datasets",
            "char_start_offset": 10278,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2075
                }
            ],
            "ref_mentions": [
                {
                    "start": 1110,
                    "end": 1132,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1269,
                    "end": 1292,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1538,
                    "end": 1557,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 1694,
                    "end": 1714,
                    "matchedPaperCorpusId": "91184042"
                },
                {
                    "start": 1822,
                    "end": 1844,
                    "matchedPaperCorpusId": "204901567"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90625
        },
        {
            "corpus_id": "218487791",
            "title": "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation",
            "text": "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity. In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders. Reference-free evaluation holds the promise of web-scale comparison of MT systems. We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER. We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish \u201ctranslationese\u201d, i.e., low-quality literal translations. We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling. In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.",
            "score": 0.4884944438700937,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63623046875
        },
        {
            "corpus_id": "267412633",
            "title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models",
            "text": "Employing this setup, we show that, unlike previous assumptions, MMTs are not able to effectively connect the underlying semantics between languages in a zero-shot manner. It is worth mentioning that the evaluation of MMTs using multiple languages in the input offers both theoretical advantages and reflects real-world scenarios in NLP systems. \n\nWe extend our study to investigate if the failures come from the lack of across-language fine-tuning data. We find that even by fine-tuning MMTs on across-language data that involves two languages in an instance, they still can not successfully trans-fer knowledge between languages in a zero-shot setting. \n\nLooking for the reasons behind the ineffectiveness of MMTs on the across setup, we examine the impact of individual samples on the cross-lingual performance and identify a specific subset where MMTs struggle. Our findings demonstrate that models achieve exaggerated high performance by strongly relying on spurious features and data artifacts. We notice that cross-lingual transfer primarily involves learned biases and shallow knowledge rather than linguistic knowledge. Notably, this phenomenon disproportionately affects low-resource languages, exacerbating the challenges faced by MMTs in achieving true cross-lingual competence. \n\nAs part of our methodology, we design control tasks (Hewitt and Liang, 2019) in which, during fine-tuning, the words within the instances have been randomly shuffled, and then, the model is evaluated on the original, i.e., not shuffled, test data. Surprisingly, our experiments show that although these new tasks do not provide the model with meaningful linguistic knowledge related to the target task, there is only a slight drop in their cross-lingual performance in both single and twolanguage evaluation settings. These results demonstrate that MMTs' understanding tends to be more reliant on surface-level patterns rather than linguistic comprehension. \n\nOur experiments show from several angles that current MMTs' cross-lingual evaluation setups do not give us a clear and faithful picture of their cross-lingual ability. Our findings question the extent of high crosslinguality in language models and prompt us to pay more attention to the interpretation of knowledge transfer and cross-lingual ability in multilingual models solely based on their performance on downstream tasks.",
            "score": 0.4883801355384313,
            "section_title": "Introduction",
            "char_start_offset": 1994,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 345
                },
                {
                    "start": 348,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1950
                },
                {
                    "start": 1953,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2380
                }
            ],
            "ref_mentions": [
                {
                    "start": 1345,
                    "end": 1369,
                    "matchedPaperCorpusId": "202538609"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "261682406",
            "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
            "text": "The availability of large multilingual corpora has accelerated the progress of multilingual natural language processing (NLP) models [64,16,42,8,46]. However, most publicly available generaldomain multilingual corpora contain 100-200 languages [64,46,2], with some datasets containing more languages in specific domains such as religious content [4], children's books [40] or dialects [3]. \n\nA common approach to creating such datasets is to mine language specific data from general web crawls such as CommonCrawl [52,38,63] to create datasets. We simply take this approach and scale it. We train a document-level LangID model on 498 languages to obtain CommonCrawl annotations at a document level and obtain a 5-trillion token, document-level monolingual dataset. \n\nHowever, such web-scale corpora are known to be noisy and contain undesirable content [48,43,18], with their multilingual partitions often having their own specific issues such as unusable text, misaligned and mislabeled/ambiguously labeled data [35]. To mitigate this, we manually audit our data. Based on our findings, we discard 79 of the languages from our preliminary dataset, rename or combine several languages and apply additional preprocessing steps. Finally, to validate the efficacy of our dataset, we train multilingual machine translation models of various sizes up to 10.7B parameters, as well as an 8B decoder-only model, and then evaluate these models on highly multilingual translation evaluation sets. \n\nIn Section 2, we describe the creation and composition of MADLAD-400, and discuss the results of the audit. Then, in Section A.5, we describe the parallel data we collect using publicly available sources to train the multilingual machine translation models described in Section 4.1. In Section 4, we describe the training process of the multilingual machine translation models and 8B decoder-only model, and then evaluate these models on highly multilingual translation datasets. Finally, we discuss the limitations of this work and directions for future work.",
            "score": 0.48816851811987655,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 137,
                    "matchedPaperCorpusId": "225040574"
                },
                {
                    "start": 244,
                    "end": 248,
                    "matchedPaperCorpusId": "225040574"
                },
                {
                    "start": 368,
                    "end": 372,
                    "matchedPaperCorpusId": "253116724"
                },
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "257378329"
                },
                {
                    "start": 853,
                    "end": 857,
                    "matchedPaperCorpusId": "228084012"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "232307434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58203125
        },
        {
            "corpus_id": "270870062",
            "title": "Evaluating Knowledge-based Cross-lingual Inconsistency in Large Language Models",
            "text": "Our research attempts to address the following three key questions: In what aspects does cross-lingual inconsistency manifest within LLMs?By analyzing the performance of existing models, we supplement the xSC with the Cross-lingual Accuracy Consistency metric (xAC) and the Cross-lingual Timeliness Consistency metric (xTC).By harmonically averaging these three metrics, we provide a comprehensive assessment of cross-lingual inconsistency in LLMs.Our findings indicate that these inconsistencies manifest not only in semantic understanding but also in accuracy and timeliness, underscoring the multifaceted nature of this issue.\n\nIs there a relationship between the cross-lingual consistency of LLMs and their multilingual capabilities?Our experiments validate a positive correlation between the models' cross-lingual consistency and their multilingual translation abilities, grounded in multilingual translation tasks.This suggests that improvements in multilingual translation capabilities can enhance cross-lingual consistency, offering a potential pathway for mitigating the inconsistencies observed.\n\nThis study is dedicated to exploring how Large Language Models (LLMs) perform in terms of cross-lingual consistency.We have selected factual knowledge-based question-and-answer tasks as our evaluative instrument and have experimented with five distinct LLMs across a dozen languages.\n\nIt is important to highlight that while such questionand-answer tasks can benefit from enhanced performance through Retrieval-augmented Generation (RAG), the true test for LLMs lies in scenarios that require reliance on their internal knowledge bases to address indirect queries.Our research, therefore, zeroes in on these types of tasks intending to evaluate and foster the consistency and precision with which LLMs handle cross-lingual information.However, the MAKQA dataset currently only supports 12 languages, most of which are resourcerich.Given the limited performance of LLMs in low-resource languages, we think that the current collection of languages is sufficient to preliminarily demonstrate the model's cross-lingual consistency among common languages.In the future, we plan to expand the dataset to include more language support, especially for those languages that are less resourced, to more comprehensively evaluate the cross-lingual capabilities of LLMs.",
            "score": 0.48790190083693297,
            "section_title": "Conclusion",
            "char_start_offset": 22407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 448
                },
                {
                    "start": 448,
                    "end": 629
                },
                {
                    "start": 631,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 920
                },
                {
                    "start": 920,
                    "end": 1105
                },
                {
                    "start": 1107,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1390
                },
                {
                    "start": 1392,
                    "end": 1671
                },
                {
                    "start": 1671,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 1938
                },
                {
                    "start": 1938,
                    "end": 2157
                },
                {
                    "start": 2157,
                    "end": 2364
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "253098773",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "text": "Datasets We consider 4 multilingual classification datasets to study calibration of MMLMs which include: i) The Cross-Lingual NLI Corpus (XNLI) (Conneau et al., 2018), ii) Multilingual Dataset for Causal Commonsense Reasoning (XCOPA) (Ponti et al., 2020), iii) Multilingual Amazon Reviews Corpus (MARC) (Keung et al., 2020) and, iv) Cross-lingual Adversarial Dataset for Paraphrase Identification (PAWS-X) (Yang et al., 2019). Statistics of these datasets can be found in gual BERT (mBERT) (Devlin et al., 2019), and XLM-RoBERTa (XLMR) (Conneau et al., 2020). mBERT is only available in the base variant with 12 layers and for XLMR we use the large variant with 24 layers. We use English training data to fine-tune the two MMLMs on all the tasks and evaluate the accuracies and ECEs on the test data for different languages. For the few-shot case we use the validation data in target languages to do continued fine-tuning (FSL) and temperature scaling (Self-TS). Refer to Section A.3 in the Appendix for more details.",
            "score": 0.4869307012037012,
            "section_title": "Experimental Setup",
            "char_start_offset": 7089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1017
                }
            ],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 254,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 406,
                    "end": 425,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 490,
                    "end": 511,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 536,
                    "end": 558,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44091796875
        },
        {
            "corpus_id": "272647601",
            "title": "Cross-lingual Transfer and Multilingual Learning for Detecting Harmful Behaviour in African Under-Resourced Language Dialogue",
            "text": "In this section, we discuss the previous work related to our research. (Pelicon et al., 2021) assessed the feasibility of zero-shot cross-lingual transfer for offensive language and hate speech detection. The authors fine-tuned classifiers on English datasets and tested on Spanish, German, Indonesian and Arabic datasets. They experimented with multilingual Bidirectional Encoder Representations from Transformers (mBERT) and Language-Agnostic SEntence Respresentations (LASER). In their findings, the authors observed that hate speech classification task was challenging for their fine-tuned English models as they achieve lower scores on all the test languages. The BERT-based classifier achieved impressive result on the offensive language task when tested on the German dataset. Eronen et al. (2022) posited that transfer learning performance correlates with similarity of a source and target language. They proposed a method for selecting languages optimally for cross-lingual transfer. The authors showed that there is a correlation between linguistic similarity and classifier performance. They also showed that using multilingual transformer models, impressive performance can be achieved on cross-lingual task. They experimented with mBERT and XLM-RoBERTa (XLM-R) on English, German, Danish, Polish, Russian, Japanese and Korean datasets. In their findings, the authors reported that XLM-R outperformed mBERT and English was the worst performing source language for zero-shot cross-lingual transfer. \n\nLeveraging machine translated datasets to carry out tasks in under-resource languages is not uncommon in NLP, especially for cross-lingual tasks (Hassan et al., 2022). Lai et al. (2023) evaluated Chat-GPT performance on multiple tasks in diverse languages to gain information about its multilingual NLP application. The datasets for each languages were obtained from CommonCrawl3 corpus and translated to the target languages. Adewumi et al. (2023) translated a portion of the English multi-domain MultiWOZ dataset, to create task-oriented dialogue datasets for six African languages. \n\nIn this work, as shown in Figure 1, we leverage cross-lingual transfer learning, using pretrained Transformer models, to detect harmful dialogues.",
            "score": 0.4868577134458118,
            "section_title": "Related Work",
            "char_start_offset": 4329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2096
                },
                {
                    "start": 2099,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 784,
                    "end": 804,
                    "matchedPaperCorpusId": "249263180"
                },
                {
                    "start": 1657,
                    "end": 1678,
                    "matchedPaperCorpusId": "235417255"
                },
                {
                    "start": 1680,
                    "end": 1697,
                    "matchedPaperCorpusId": "258079179"
                },
                {
                    "start": 1939,
                    "end": 1960,
                    "matchedPaperCorpusId": "260387895"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6103515625
        },
        {
            "corpus_id": "261822560",
            "title": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
            "text": "In order to demonstrate the effectiveness of our data set for multilingual evaluation, we benchmark the performance across various models and group the results by categorizations (Table 3). As XLM-R consistently outperforms Glot-500 across almost all language families, we use XLM-R as the baseline model in the cross-lingual transfer experiments.9 \n\nComparing English versus other languages, finetuning XLM-R on English achieved an accuracy of 92.1%, indicating that the task itself is not difficult if given a properly pre-trained MLM and \u223c 700 training samples. However, when fine-tuning the same model in other languages, the performance drops vastly to an average accuracy of 75.9%. Similarly, in the cross-lingual transfer and zero-shot prompt scenarios, the performance further drops.",
            "score": 0.48672320887304377,
            "section_title": "Baseline results",
            "char_start_offset": 15960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 791
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7685546875
        },
        {
            "corpus_id": "273323302",
            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
            "text": "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs. \n\nE GENERALIZING TO EXTREME SCENARIOS E.1 PERFORMANCE ON WEAKER MODEL: LLAMA2 \n\nTable 13 demonstrates that even when starting with a model with weaker multilingual capabilities, such as Llama2-7B-Chat, which exhibits extremely low performance in languages like Russian (ru), German (de), and French (fr) on the X-AlpacaEval, significant improvements can be achieved. By leveraging language imbalance-driven rewarding for self-multilingual optimization across two iterations, the model shows substantial enhancement across all training languages, particularly in those where the original model's performance was initially weaker. To assess the impact of our approach on these languages, we conducted experiments using Llama3-8b-Instruct as the base model. Table 14 shows that even though the model performs weakly in these languages, our approach remains effective in low-resource settings and can iteratively improve the model's performance across all languages.",
            "score": 0.485626175555021,
            "section_title": "D.3 HOW TO AVOID TRANSLATIONESE BIAS IN MULTILINGUAL BENCHMARKS EVALUATION",
            "char_start_offset": 34821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1127,
                    "matchedPaperCorpusId": "271244246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.931640625
        },
        {
            "corpus_id": "248780386",
            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
            "text": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition. \n\nRecently, there has been some interest in predicting performance of NLP models without actually evaluating them on a test set. Xia et al. (2020) showed that it is possible to build regression models that can accurately predict evaluation scores of NLP models under different experimental settings using various linguistic and dataset specific features. Srinivasan et al. (2021)   (c) Number of multilingual tasks containing test data for each of the 106 languages supported by the MMLMs (mBERT, XLMR). The bars are shaded according to the class taxonomy proposed by Joshi et al. (2020). \n\nFigure 1 results specifically for MMLMs towards predicting their performance on downstream tasks for different languages in zero-shot and few-shot settings, and Ye et al. (2021) propose methods for more reliable performance prediction by estimating confidence intervals as well as predicting fine-grained performance measures. \n\nIn this paper we argue that the performance prediction can be a possible avenue to address the current issues with Multilingual benchmarking by aiding in the estimation of performance of the MMLMs for the languages which lack any evaluation data for a given task. Not only this can help us give a better idea about the performance of a multilingual model on a task across a much larger set of languages and hence aiding in better model selection, but also enables applications in devising data collection strategies to maximize performance (Srinivasan et al., 2022) as well as in selecting the representative set of languages for a benchmark (Xia et al., 2020).",
            "score": 0.48540115525019834,
            "section_title": "Introduction",
            "char_start_offset": 1631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2323
                }
            ],
            "ref_mentions": [
                {
                    "start": 871,
                    "end": 888,
                    "matchedPaperCorpusId": "218487089"
                },
                {
                    "start": 1310,
                    "end": 1329,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1494,
                    "end": 1510,
                    "matchedPaperCorpusId": "231861672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95751953125
        },
        {
            "corpus_id": "264405758",
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "text": "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], \n\nand ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text? \n\nFrom Table A2, it can be noted that our evaluation revealed ChatGPT to outperform other approaches in translations from English to both Chinese and Spanish, while Google Translate exhibits superior performance in English-to-Hindi translation. Thus, for optimal results in each non-English language, we harnessed the best-performing model to achieve the highest translation quality.",
            "score": 0.4853129009429891,
            "section_title": "A DETAILS OF DATASET CONSTRUCTION",
            "char_start_offset": 36719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "4623739"
                },
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "248887528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8759765625
        },
        {
            "corpus_id": "258841429",
            "title": "mmT5: Modular Multilingual Pre-Training Solves Source Language Hallucinations",
            "text": "We conduct experiments across datasets in zero-shot cross-lingual transfer and multilingual training scenarios. For zero-shot cross-lingual transfer, we train the model on a subset of languages (e.g. only English) and evaluate the model on held-out data of the same task in other languages. In multilingual training, we finetune the model on multiple languages of the same task, and evaluate the model on the same set of languages. As the language-specific modular components are replaced at inference time, we do not update the parameters of the modular components. We do the same for our shared model variants, in order for the number of trainable parameters to be equal for comparable scenarios. 4 For each dataset, we select the best model checkpoint based on performance on the validation set. Evaluation Tasks For zero-shot cross-lingual transfer, we evaluate on the XQuAD (Artetxe et al., 2020) and TyDi QA GoldP (Clark et al., 2020) question answering datasets; on the XNLI (Conneau et al., 2018) natural language inference dataset; on XL-Sum (Hasan et al., 2021) for summarization;5 and MASSIVE (FitzGerald et al., 2023) for semantic parsing. 6 We mainly fine-tune the model on English training data and evaluate on the target languages (Hu et al., 2020). For XL-Sum, we additionally evaluate in a multi-source zero-shot transfer setting where we train jointly on data in Arabic, English, Japanese and Chinese (XL-Sum ar,en,ja,zh ). \n\nFor multilingual training, we evaluate on semantic parsing (MASSIVE) and summarization (XL-Sum) datasets. For each dataset, we fine-tune and evaluate the model on all languages jointly. \n\nBaselines Our main comparison method is mT5 S , a shared model that is pre-trained with the same hyper-parameters, setup, and number of parameters as our modular model. We also compare to the published results of the mT5 encoderdecoder model (Xue et al., 2021).",
            "score": 0.4844484208674836,
            "section_title": "Experimental setting",
            "char_start_offset": 10019,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1629
                },
                {
                    "start": 1632,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 1893
                }
            ],
            "ref_mentions": [
                {
                    "start": 879,
                    "end": 901,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 982,
                    "end": 1004,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1051,
                    "end": 1071,
                    "matchedPaperCorpusId": "235658519"
                },
                {
                    "start": 1104,
                    "end": 1129,
                    "matchedPaperCorpusId": "248239933"
                },
                {
                    "start": 1246,
                    "end": 1263,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.428955078125
        },
        {
            "corpus_id": "261682140",
            "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
            "text": "We show the evaluation results on five datasets for cross-lingual consistency and cultural reasoning in Figure 5 and our key findings are as follows. \n\nFirstly, GPT-4 demonstrates outstanding performance on most datasets, surpassing others by a substantial margin across cultures and languages, demonstrating its superior capability in handling multilingual tasks. \n\nSecond, becoming an expert in cultural knowledge necessitates extensive pre-training with a di- verse and extensive collection of multilingual textual data such as books, articles, websites, historical documents, and cultural artifacts. Baichuan-2 model has shown remarkable performance in understanding Chinese culture (CN-Eval), even outperforming GPT4. In contrast, LLaMA models are primarily focused on English, with approximately 90% of English pre-training data. This specialization makes them less proficient in handling multilingual and multicultural scenarios. Detailed results regarding cross-lingual consistency are presented in Figure 6. The report includes comprehensive evaluation metrics: AC3, Accuracy, Consistency, and Accuracy for each language. The consistency score clearly demonstrates that BLOOMZ stands out for its better performance in aligning knowledge across languages, solidifying its position as a leading open-source multilingual foundational model. Even being the worst in overall accuracy, BLOOMZ surpasses ChatGPT in cross-lingual consistency, achieving a score of 54% compared to 47%. However, they are still showing unsatisfactory consistency scores, highlighting the inconsistency in the sharing of common knowledge across various languages (Our Finding 3). While GPT-4 achieves a 75% consistency score, it drops to 64% when s = 6 as shown in Figure 8, which suggests ample opportunity to further enhance cross-lingual knowledge alignment, aiming for optimal multilingual models. \n\nLast, when assessing models' accuracy in indi-vidual languages, it is evident that their problemsolving capability in English usually surpasses that in other selected languages. This illustrates that the proficiency of models varies unevenly across different languages (Our Finding 4). Compared to high-resource languages, the performance of low-resource languages is inferior. For example, English, Chinese and Spanish rank within the top 5 out of 46 languages present in BLOOMZ corpus.",
            "score": 0.4843069895366599,
            "section_title": "Evaluation Results and Discussion",
            "char_start_offset": 21384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1882
                },
                {
                    "start": 1885,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2170
                },
                {
                    "start": 2171,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2372
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "270719963",
            "title": "Evaluating Neural Networks\u2019 Ability to Generalize against Adversarial Attacks in Cross-Lingual Settings",
            "text": "In this section, we investigate the performance of two state-of-the-art multilingual models, XLM-Roberta and mBART, on the challenging cross-lingual natural language inference (XNLI) task [14].We evaluate these models using the XNLI dataset, which is a large and diverse collection of natural language inference examples in multiple languages.This dataset, built on the MultiNLI corpus, includes 14 additional languages and serves as a valuable resource for assessing the performance of cross-lingual natural language inference (NLI) models.By using XNLI, we aim to determine how well these models handle cross-lingual sentence representations and achieve state-of-the-art results in this domain.The ability to handle data across multiple languages is a crucial requirement for modern natural language processing (NLP) systems, enabling them to cater to diverse global audiences and facilitate cross-lingual communication.\n\nTable 1 shows a subset of English data from the XNLI dataset; we have 3 columns: hypothesis, premise, and class label (target) 0, 1, and 2. The distribution of these class labels is 33.33 percent per class in our data.To assess the models' robustness in handling translated data, we employed two popular translation services, Google Translate, to generate translated versions of the test data.By evaluating the models on both the original and translated test sets, we aimed to gain insights into their ability to maintain consistent performance when dealing with potential translation errors and linguistic divergences.\n\nWe used Google Translate because of its advanced techniques that significantly improve translation accuracy, particularly for low-resource languages.These improvements include enhanced model architecture, better noise handling in datasets, and effective multilingual transfer learning through M4 modeling, as well as leveraging monolingual data.These strategies ensure that Google Translate can deliver high-quality translations across a wide range of languages, making it a reliable tool for generating realistic multilingual test data.",
            "score": 0.4842608202926022,
            "section_title": "Methods",
            "char_start_offset": 12498,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 922
                },
                {
                    "start": 924,
                    "end": 1142
                },
                {
                    "start": 1142,
                    "end": 1317
                },
                {
                    "start": 1317,
                    "end": 1543
                },
                {
                    "start": 1545,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1890
                },
                {
                    "start": 1890,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "52271711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78125
        },
        {
            "corpus_id": "268691698",
            "title": "Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language",
            "text": "Because of the promising results of cross-lingual learning, we also assessed the performance of using a combined dataset of both English and Ger-  (Plum et al., 2022) and German dataset, evaluated on German gold set.\n\nman.The aim of this approach would be to boost performance, with the model learning from both datasets, while still only targeting one language for evaluation.Overall, we do see a significant increase in both macro and weighted F1 scores.\n\nEspecially precision in quite low-performing relations, such as deathplace and sibling, we see an increase.In terms of models, it is clear that XLM-R benefits more from this data, as opposed to only minor improvements with mBERT.These findings show that multilingual learning is beneficial with the datasets created with GDS, highlighted by Table 6.\n\nComparing the results of Multilingual BERT trained on the English and German multilingual dataset, we find it outperforms the same model trained in the cross-lingual setting, shown in Table 5.This is most noticeable in terms of recall, while in terms of precision the difference is not as much.Compared to the results of the monolingual training, shown in Table 4, the difference in recall is less, however the difference in precision is higher.Overall, the idea that the multilingual learning set-ting would combine the good results from the two separate datasets appears to hold true.",
            "score": 0.483990492542818,
            "section_title": "Multilingual Learning",
            "char_start_offset": 20205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 218,
                    "end": 222
                },
                {
                    "start": 222,
                    "end": 377
                },
                {
                    "start": 377,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 565
                },
                {
                    "start": 565,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 807
                },
                {
                    "start": 809,
                    "end": 1001
                },
                {
                    "start": 1001,
                    "end": 1103
                },
                {
                    "start": 1103,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1395
                }
            ],
            "ref_mentions": [
                {
                    "start": 147,
                    "end": 166,
                    "matchedPaperCorpusId": "248496225"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44287109375
        },
        {
            "corpus_id": "220962184",
            "title": "NLPDove at SemEval-2020 Task 12: Improving Offensive Language Detection with Cross-lingual Transfer",
            "text": "Multilingual pre-trained language models allow seamless cross-lingual transfer; they map texts in different languages into the same representation space without introducing extra costs. Cross-lingual transfer can be especially helpful when the training data is limited for the target language, i.e., low-resource settings. Transferring examples from high-resource languages to low-resource languages often results in great improvements (Chen et al., 2018;Xu and Yang, 2017).\n\nAlthough it is clear that cross-lingual transfer can be of help, the question of which data to additionally use remains an open problem. Should we use the entire dataset, or is there a subset better than the entire dataset? We hypothesize that not all samples in the same transfer language will be equally helpful to the target language task, leading to the conclusion that a carefully selected subset will give higher performance gain than using the whole dataset.\n\nTo address the how part of data selection, we propose a new metric, translation embedding distance (TED), to quantify the transferability of a data sample or a language. We first define a sample's transferability as how useful the transfer language sample is when transferred to the target language task. Here, we hypothesize that transferability as a concept correlates with translatability; if a sentence is easily translated to the target language, it is more likely to be a useful training sample. To measure TED, we follow the three steps: first, we (machine) translate the training samples in transfer languages into the target language. 2 Then, we extract sentence embeddings for both the original sentence and its translation using mBERT. More specifically, we applied mean pooling over the second-to-last (penultimate) layer of mBERT and used it as the sentence embedding. Finally, we measure the L2 distance between the two representations and refer to it as the TED of the instance. The transferability of a language, which we define as language-level TED, is simply the average of instance-level TED of samples in the transfer language.\n\nWe can then rank transfer samples according to their TED scores and choose top-k samples with the lowest distance. Likewise, to find the most transferable languages, we can select the top-k languages based on their language-",
            "score": 0.48333081795076266,
            "section_title": "Cross-lingual Data Selection",
            "char_start_offset": 6320,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "9387600"
                },
                {
                    "start": 455,
                    "end": 473,
                    "matchedPaperCorpusId": "3013625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6865234375
        },
        {
            "corpus_id": "276741970",
            "title": "Enhancing Non-English Capabilities of English-Centric Large Language Models through Deep Supervision Fine-Tuning",
            "text": "\u2022 SFT (Ouyang et al. 2022), which is instruction-tuned with English instruction datasets. \u2022 TFT (Zhu et al. 2023b), which is instruction-tuned using the original English instruction datasets translated into the target languages. \u2022 SDRRL (Zhang et al. 2024), which is a method based on Self-Distillation. Besides using English instructiontuning data and its multilingual code-switching extensions, it also incorporates partially translated data and completion data for fine-tuning. \u2022 DFT-logits, our method that applies logits-based supervision to guide the model's intermediate layers. \n\n\u2022 DFT-feature, our method that uses feature alignment to maintain consistent internal representations between English and tgt language. \n\nEvaluation Dataset understanding dataset to evaluate models' performance in cross-lingual inference tasks. \n\n\u2022 XCOPA (Cross-lingual Choice of Plausible Alternatives): XCOPA (Ponti et al. 2020) is a benchmark designed to evaluate the ability of models to apply commonsense reasoning, requiring both world knowledge and the ability to generalize it to new languages. \n\n\u2022 XStoryCloze (Cross-lingual Story Cloze Test) XSto-ryCloze (Lin et al. 2022) is a cross-lingual dataset for evaluating models' ability to understand stories and generate plausible endings. \n\n\u2022 MMLU (Massive Multitask Language Understanding) MMLU (Hendrycks et al. 2020) is a large-scale multitask language understanding dataset covering multiple domains (such as history, geography, science, law, etc.) and various languages. \n\nFor all evaluation datasets, we conducted tests using a zero-shot setting. We used the F1 score for XQuAD, MLQA, and MKQA, and the MC1 metric for TruthfulQA. For other NLU datasets, accuracy was used as the evaluation metric.",
            "score": 0.4830840686379332,
            "section_title": "Comparison of Methods",
            "char_start_offset": 13387,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1747
                }
            ],
            "ref_mentions": [
                {
                    "start": 6,
                    "end": 26,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1153,
                    "end": 1169,
                    "matchedPaperCorpusId": "245334784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "248834196",
            "title": "Consistent Human Evaluation of Machine Translation across Language Pairs",
            "text": "In a second study, we examined the introduction of a calibration set to create meaningful scores that can be compared across language pairs. This enables for instance the decision if a machine translation system for a language pair is good enough to be put into production. \n\nEvaluators judge 1012 sentences pairs for a single language pair in both language directions. In this study, we only use the XSTS score. Translations are judged against the source sentence. Machine translations a generated with a state-of-the-art multilingual machine translation system. Evaluators also judge the human reference translation. \n\nThe crucial addition to the sentence pairs to be judged is a calibration set of sentence pairs that is common across all languages. It consists of 1000 pairs of a machine translation into English and a corresponding English reference translation. These sentence pairs are carefully selected to span a wide quality range, based on human-scored translations from previous evaluations where multiple evaluators agreed on the score. These scores were used as the consensus quality score. \n\nA fair objection to using such a calibration set is that we are asking evaluators to perform two different tasks -comparing machine translation against a source sentence (English and non-English), and comparing machine translation against a reference (English and English)but posit that they will use the same standards when making quality assessments. \n\nBecause the calibration set is fixed, its quality is fixed, and the average score each evaluator assigns to the sentence pairs in the set should be the same. Hence, we can use the actual score assigned by each evaluator and the official fixed score as the basis to make adjustments to each evaluator's score. For instance, if an evaluator gives the calibration too high score, then we detect that they are too lenient and their scores need to be corrected downward. \n\nNote that there is also a second fixed point that could be used for score adjustment: the average score each evaluator gives to the reference translation. These professionally translated and vetted translations should receive high scores, and we could adjust each evaluator's scores so that they average adjusted score for reference translations is a fixed value. The underlying assumption here is that reference translations are of identical quality across all language pairs.",
            "score": 0.48248437367939206,
            "section_title": "Study on Calibration",
            "char_start_offset": 15183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54736328125
        },
        {
            "corpus_id": "252873067",
            "title": "DATScore: Evaluating Translation with Data Augmented Translations",
            "text": "We compare the performance of our metric against BLEU and three other reference-based unsupervised metrics: BERTScore5 , MoverScore6 and BARTScore7 (detailed in Subsection 2.1 and Section 3), using their official implementations. Experimental results are reported in Table 1 and 2. Following their original settings, we use different underlying language models for each baseline metric. For BERTScore and MoverScore, RoBERTa-Large (RL; Liu et al., 2019) and Bert-Base (BB) are used respectively when we evaluate to-English translations, and mBERT (Devlin et al., 2019) for from-English translations. In the case of BARTScore, we use a BART-Large (BL) checkpoint (finetuned on CNNDM (See et al., 2017) and (Hu et al., 2019) datasets) for evaluating to-English translations, and an mBART-50 model (Escolano et al., 2021) for from-English translations. \n\nOverall, results show that, on average, across all language pairs, DATScore significantly outperforms all 4 baseline metrics under their original model settings (rows 1a-1d and 2a-2d). Specifically, with respect to the best performing baseline BERTScore (row 1b and 2b), our metric provides a performance boost of 0.7 for to-English case and of 9.8 for from-English case on WMT17 dataset in Table 1, and achieves a gain of 0.7 and of 7.3 respectively on WMT18 dataset in Table 2. These averaging results demonstrate the superiority and applicability of DATScore in evaluating general machine translations of many languages. Moreover, it is interesting to note that our improvement is much more significant in from-English case, which makes DATScore particularly well-suited to evaluate hypothesis translations in non-English languages, often with low resource. We hypothesize that this is due to the inconsistency of underlying language models. The baselines adopt a monolingual model for evaluating English, but a multilingual one for non-English languages. However, DATScore uses a single multilingual M2M-100 model for both cases.",
            "score": 0.48232925274718574,
            "section_title": "Main results",
            "char_start_offset": 15800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 568,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 682,
                    "end": 700,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "207984304"
                },
                {
                    "start": 795,
                    "end": 818,
                    "matchedPaperCorpusId": "215754272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71630859375
        },
        {
            "corpus_id": "216868259",
            "title": "MLSUM: The Multilingual Summarization Corpus",
            "text": "The research community has produced several multilingual datasets for tasks other than summarization. We report two recent efforts below, noting that both i) rely on human translations, and ii) only provide evaluation data. \n\nThe Cross-Lingual NLI Corpus The SNLI corpus (Bowman et al., 2015) is a large scale dataset for natural language inference (NLI). It is composed of a collection of 570k human-written English sentence pairs, associated with their label, entailment, contradiction, or neutral. The Multi-Genre Natural Language Inference (MultiNLI) corpus is an extension of SNLI, comparable in size, but including a more diverse range of text. Conneau et al. (2018) introduced the Cross-Lingual NLI Corpus (XNLI) to evaluate transfer learning from English to other languages: based on MultiNLI, a collection of 5,000 test and 2,500 dev pairs were translated by humans in 15 languages. \n\nMLQA Given a paragraph and a question, the Question Answering (QA) task consists in providing the correct answer. Large scale datasets such as (Rajpurkar et al., 2016;Choi et al., 2018;Trischler et al., 2016) have driven fast progress.2 However, these datasets are only in English. To assess how well models perform on other languages, Lewis et al. (2019) recently proposed MLQA, an evaluation dataset for cross-lingual extractive QA composed of 5K QA instances in 7 languages. \n\nXTREME The Cross-lingual TRansfer Evaluation of Multilingual Encoders benchmark covers 40 languages over 9 tasks. The summarization task is not included in the benchmark. \n\nXGLUE In order to train and evaluate their performance across a diverse set of cross-lingual tasks, Liang et al. (2020) recently released XGLUE, covering both Natural Language Understanding and Generation scenarios. While no summarization task is included, it comprises a News Title Generation task: the data is crawled from a commercial news website and provided in form of article-title pairs for 5 languages (German, English, French, Spanish and Russian).",
            "score": 0.48232791406521475,
            "section_title": "Existing Multilingual Datasets",
            "char_start_offset": 5072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 223
                },
                {
                    "start": 226,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1371
                },
                {
                    "start": 1374,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 2005
                }
            ],
            "ref_mentions": [
                {
                    "start": 1061,
                    "end": 1079,
                    "matchedPaperCorpusId": "202718969"
                },
                {
                    "start": 1079,
                    "end": 1102,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1647,
                    "end": 1666,
                    "matchedPaperCorpusId": "214794966"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8466796875
        },
        {
            "corpus_id": "273346089",
            "title": "A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification",
            "text": "The rise of digital misinformation has heightened interest in using multilingual Large Language Models (LLMs) for fact-checking. This study systematically evaluates translation bias and the effectiveness of LLMs for cross-lingual claim verification across 15 languages from five language families: Romance, Slavic, Turkic, Indo-Aryan, and Kartvelian. Using the XFACT dataset to assess their impact on accuracy and bias, we investigate two distinct translation methods: pre-translation and self-translation. We use mBERT's performance on the English dataset as a baseline to compare language-specific accuracies. Our findings reveal that low-resource languages exhibit significantly lower accuracy in direct inference due to underrepresentation in the training data. Furthermore, larger models demonstrate superior performance in self-translation, improving translation accuracy and reducing bias. These results highlight the need for balanced multilingual training, especially in low-resource languages, to promote equitable access to reliable fact-checking tools and minimize the risk of spreading misinformation in different linguistic contexts.",
            "score": 0.48203234401241296,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9423828125
        },
        {
            "corpus_id": "245855870",
            "title": "Facebook AI\u2019s WMT21 News Translation Task Submission",
            "text": "We participate in the WMT2021 shared task on news translation and submit a multilingual translation system. In recent years, multilingual translation has gained significant interest as an alternative to developing separate, specialized systems for different language directions (Firat et al., 2016;Tan et al., 2019;Aharoni et al., 2019;Zhang et al., 2020;Tang et al., 2020;Arivazhagan et al., 2019). Multilingual systems have great potential for simplicity and consolidation, making them attractive options for the development and maintenance of commercial translation technologies. From a research standpoint, studies of transfer learning between related languages and developing methods that incorporate low-resource languages are strong motivators for grouping languages together in one system (Dabre et al., 2019;Fan et al., 2021). \n\nDespite such motivations, existing multilingual translation systems have been unable to show that the translation quality of multilingual systems surpasses that of bilingual. Several works compare to bilingual baselines, but these baselines do not incorporate standard techniques used across the field -such as backtranslation or dense model scaling. Further, multilingual translation systems are often developed on non-standard training datasets and use different evaluation datasets. These factors make it difficult to assess the performance of multilingual translation, particularly when compared to the most competitive bilingual models. \n\nIn this work, our aim is to demonstrate against the winning WMT2020 models and our bilingual WMT2021 systems that multilingual translation models have stronger performance than bilingual ones. We focus on 14 language directions: English to and from Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese. We create an unconstrained system that utilizes both WMT distributed and publicly available training data, apply large-scale backtranslation, and explore dense and mixture-of-expert architectures. We compare the impact of various techniques on bilingual and multilingual systems, demonstrating where multilingual systems have an advantage. Our final multilingual submission improves the translation quality on average +2.0 compared to the WMT2020 winning models, and ranks first in 7 directions based on automatic evaluation on the WMT2021 leaderboard.",
            "score": 0.4819223893799549,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1479
                },
                {
                    "start": 1482,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1992
                },
                {
                    "start": 1993,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2348
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 298,
                    "matchedPaperCorpusId": "6359641"
                },
                {
                    "start": 298,
                    "end": 315,
                    "matchedPaperCorpusId": "201666341"
                },
                {
                    "start": 315,
                    "end": 336,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 336,
                    "end": 355,
                    "matchedPaperCorpusId": "216144650"
                },
                {
                    "start": 797,
                    "end": 817,
                    "matchedPaperCorpusId": "208163390"
                },
                {
                    "start": 817,
                    "end": 834,
                    "matchedPaperCorpusId": "224814118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.409912109375
        },
        {
            "corpus_id": "196183388",
            "title": "Multilingual and Cross-Lingual Graded Lexical Entailment",
            "text": "terpretability (Pad\u00f3 et al., 2009), and cross-lingual lexical substitution (Mihalcea et al., 2010). \n\nIn this work, we introduce the first set of benchmarks and methods that target cross-lingual and multilingual graded lexical entailment. We make several important contributions related to GR-LE in multilingual settings. First, we extend the research on GR-LE beyond English (Vuli\u0107 et al., 2017;Rei et al., 2018) and provide new human-annotated GR-LE datasets in three other languages: German, Italian, and Croatian. Second, following an established methodology for constructing evaluation datasets for cross-lingual lexico-semantic relations (Camacho-Collados et al., 2015, 2017), we automatically derive a collection of six cross-lingual GR-LE datasets: CL-HYPERLEX. We analyse in detail the cross-lingual datasets (e.g., by comparing the scores to human-elicited ratings), demonstrating their robustness and reliability. \n\nIn order to provide a competitive baseline on new monolingual and cross-lingual datasets, we next introduce a cross-lingual specialisation/retrofitting method termed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel): starting from any two monolingual distributional spaces, CLEAR induces a bilingual cross-lingual space that reflects the asymmetric nature of the LE relation. Such a crosslingual LE-specialised space is illustrated in Figure 1. CLEAR is an extension of the monolingual LEAR specialisation method (Vuli\u0107 and Mrk\u0161i\u0107, 2018). The key idea of CLEAR is to leverage external lexical knowledge (i.e., information on word relations from WordNet, BabelNet, or ConceptNet) to rescale vector norms which reflect the concept hierarchy, while simultaneously pushing (i.e., \"attracting\") desirable word pairs closer (by vector direction) to reflect their semantic similarity in the cross-lingual LE-specialised space.",
            "score": 0.4815368142431279,
            "section_title": "Introduction",
            "char_start_offset": 2564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 102,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 924
                },
                {
                    "start": 927,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1851
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 34,
                    "matchedPaperCorpusId": "12539736"
                },
                {
                    "start": 75,
                    "end": 98,
                    "matchedPaperCorpusId": "7275077"
                },
                {
                    "start": 396,
                    "end": 413,
                    "matchedPaperCorpusId": "43957586"
                },
                {
                    "start": 644,
                    "end": 674,
                    "matchedPaperCorpusId": "7681779"
                },
                {
                    "start": 674,
                    "end": 681,
                    "matchedPaperCorpusId": "8456453"
                },
                {
                    "start": 1445,
                    "end": 1469,
                    "matchedPaperCorpusId": "4966757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "235097287",
            "title": "Practical Transformer-based Multilingual Text Classification",
            "text": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation3 as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019). In single target language settings, we translate training data from other languages into the target language, yielding N times the number of training examples. In the multi-all setting, we translate data from every language into every other language, yielding N 3 Data \n\nWe choose sentiment analysis and hate speech detection as evaluation tasks due to their relevance to industry applications and the availability of multilingual datasets. An overview of the datasets is shown in Table 2.",
            "score": 0.48120253734229745,
            "section_title": "Data Augmentation",
            "char_start_offset": 6411,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 802
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86669921875
        },
        {
            "corpus_id": "235683093",
            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
            "text": "Despite their success, large pre-trained multilingual models have not completely alleviated the need for labeled data, which is cumbersome to collect for all target languages. Zero-shot cross-lingual transfer is emerging as a practical solution: pre-trained models later fine-tuned on one transfer language exhibit surprising performance when tested on many target languages. English is the dominant source language for transfer, as reinforced by popular zero-shot benchmarks. However, this default choice has not been systematically vetted. In our study, we compare English against other transfer languages for fine-tuning, on two pre-trained multilingual models (mBERT and mT5) and multiple classification and question answering tasks. We find that other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori. Unexpectedly, this can be true even when the training sets were automatically translated from English. This finding can have immediate impact on multilingual zero-shot systems, and should inform future benchmark designs.",
            "score": 0.48104943468742656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "264491136",
            "title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages",
            "text": "\u2022 For most of the languages in our dataset WEATHub, we had access to at least two annotators for cross-verifying the accuracy of the human translations to determine if the translated words fit into the context of that particular WEAT category. However, for some languages, we only have one annotator per language. We plan to make our dataset available via open-source, opening it up to future crowdsourcing possibilities where we would look to get at least two annotators for each language where possible. \n\n\u2022 While we have tried to cover as many languages from the global South as possible, we acknowledge that 24 languages are indeed a tiny proportion of the 7000 languages in the world, some of which do not even have text representations. Bias detection and mitigation in speech is one direction of research we plan to work on in the future. Another critical step is to facilitate crowdsourcing of WEATHub to cover low resource languages (\"The Left-Behinds\" and \"The Scraping-Bys\") from the taxonomy introduced in (Joshi et al., 2020) so that we can have fairer systems not just for high resource languages but for all languages. \n\n\u2022 Among many other studies, Kurita et al. (2019) has previously shown how WEAT can be an unreliable metric for contextualized embeddings from transformer models. Silva et al. (2021) furthers this by showing how \"debiasing\" based on WEAT alone does not truly represent a bias-free model as other metrics still find bias. Badilla et al. (2020) provides a framework to simultaneously compare and rank embeddings based on different metrics. However, these studies reinforce that we need better metrics to study intrinsic biases in transformer models. We believe the target and attribute pairs we provide as part of WEATHub in multiple languages is an important step towards a better multilingual metric for evaluating intrinsic biases in language models. A Effect sizes for original WEAT categories from DistilmBERT and FastText   C Effect sizes for original WEAT categories from XLM-RoBERTa",
            "score": 0.48088512616461826,
            "section_title": "Limitations",
            "char_start_offset": 31900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 1164,
                    "end": 1184,
                    "matchedPaperCorpusId": "225060926"
                },
                {
                    "start": 1298,
                    "end": 1317,
                    "matchedPaperCorpusId": "235097394"
                },
                {
                    "start": 1456,
                    "end": 1477,
                    "matchedPaperCorpusId": "220483093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "260736143",
            "title": "Extrapolating Large Language Models to Non-English by Aligning Languages",
            "text": "We use stanford alpaca6 as the code base. More training details are provided in Appendix A. Evaluation Dataset To evaluate LLM's performance on non-English languages, we use two benchmark cross-lingual datasets, XQUAD (Artetxe et al., 2020) and MLQA (Lewis et al., 2020), which requires the model to reason over the given context and answer the given question. In addition, we create a new multilingual evaluation set MI-EVAL (introduced in Appendix B) to assess the capability of LLM in following multilingual instructions. These multilingual multi-way test sets also allow us to compare language ability across languages. To evaluate LLM's translation ability, we follow Zhu et al. (2023) and use multilingual translation dataset FLORES-101 (Goyal et al., 2022). Details of the prompts used for all these tasks are provided in Appendix C. \n\nEvaluation Metrics On XQUAD, MLQA and MI-EVAL, we follow Liu et al. (2023) and Wang et al. (2023a) to use ChatGPT for generation quality evaluation. On XQUAD, MLQA, we also report exact-matching results in Appendix D. For translation tasks, we use COMET (Rei et al., 2020), BLEURT (Sellam et al., 2020) and sentence-piece BLEU (Papineni et al., 2002) as metrics7 . \n\nMore evaluation details can be referred to Appendix D.",
            "score": 0.4806517259166486,
            "section_title": "EXPERIMENT SETTING",
            "char_start_offset": 12657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1264
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 240,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 250,
                    "end": 270,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1097,
                    "end": 1115,
                    "matchedPaperCorpusId": "221819581"
                },
                {
                    "start": 1124,
                    "end": 1145,
                    "matchedPaperCorpusId": "215548699"
                },
                {
                    "start": 1170,
                    "end": 1193,
                    "matchedPaperCorpusId": "11080756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.763671875
        },
        {
            "corpus_id": "259108222",
            "title": "T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text Classification",
            "text": "We have carried out various experiments on three popular cross-lingual text classification datasets, namely XNLI (Conneau et al., 2018), a crosslingual natural language inference dataset; ML-Doc (Schwenk and Li, 2018), a corpus for multilingual news article classification; and Mul-tiEURLEX (Chalkidis et al., 2021), a multilingual legal, multi-label document classification dataset. For MultiEURLEX, we have conducted the experiments with only 6 of its 23 languages due to their much larger size, yet trying to broadly represent the available language families (Germanic, Romance, Slavic and Hellenic). Table 1 shows the languages and the number of classes in each dataset. As common in cross-lingual transfer learning, English has been used as the highresource language, while the other languages have been used for evaluation. We have also carried out few-shot, cross-lingual fine-tuning using 10 and 100 samples held out from the validation set of each language3 .",
            "score": 0.48059631744435166,
            "section_title": "Experimental set-up 4.1 Datasets",
            "char_start_offset": 14473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 968
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 135,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 195,
                    "end": 217,
                    "matchedPaperCorpusId": "21732778"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "The cross-lingual transfer performance of multilingual LMs is often evaluated from English to other language directions due to linguistic resource reasons.The X-CSQA dataset (Lin et al., 2021), which consists solely of machine-translated questions from CSQA's development and test splits, captures only the one-way cross-lingual transfer performance of LMs that were trained in English to evaluate their performance in other languages.In contrast, mCSQA supports the evaluation of crosslingual language transfer performance in any directions among multilingual LMs that were trained in each of the eight languages.\n\nFigure 6 shows the results of the multilingual LM, XLM-R LARGE , which was fine-tuned in each of the eight languages separately and then evaluated across all eight languages on mCSQA, using the same settings as in Table 10.The results from Figure 6 show that, regardless of the language in which they were trained, cross-lingual transfer abilities are observed in most cases for any languages given the relative lower drop of performance when compared with the monolingual performance.Moreover, in the Easy sets, the drop is within 10% for most language pairs, while in the Hard sets, it ex- ceeds 20%.This indicates that questions that are relatively easy to judge (Easy sets) facilitate the language transfer capability, but questions requiring deep background knowledge (Hard sets) necessitate language-specific training and the development of LMs.",
            "score": 0.480347116558364,
            "section_title": "Evaluation of Multilingual LMs' Cross-Lingual Transfer Capabilities",
            "char_start_offset": 18134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 614
                },
                {
                    "start": 616,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1218
                },
                {
                    "start": 1218,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 174,
                    "end": 192,
                    "matchedPaperCorpusId": "235421949"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "271050875",
            "title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
            "text": "These datasets provide sentences that reflect stereotypes.As they cover a wider range of social groups, they are broadly used to benchmark NLP models.While some shortcoming of e.g.CrowS-Pairs and Stere-oSet could be mitigated, as suggested by Blodgett et al. (2021), the work of Liu (2024) demonstrates the value of the stereotype pairs to assess differences between disadvantaged and advantaged groups.\n\nMultilingual bias.Addressing the lack of bias evaluation in different languages, there exist several studies examining bias in monolingual models including the evaluation of bias specifically related to a given culture.For instance, Malik et al. (2022) and Vashishtha et al. (2023) focus on the evaluation of bias in Indian culture and Indic languages.Zmigrod et al. (2019) and Zhou et al. (2019) focus on the mitigation of stereotypes in gender-inflected languages.Besides a monolingual evaluation, Zhou et al. (2019) also evaluate bias in bilingual embeddings.\n\nMultilinguality as bias mitigation.Similar to our work, Levy et al. (2023) compares biases and the impact of multilingual training across multiple languages by assessing bias in a downstream sentiment analysis task using templates adapted from Czarnowska et al. (2021).For five languages (Italian, Chinese, English, Hebrew, and Spanish), they reveal differences in the expression of bias and consistently show that models (mBERT, XLM-R) favor groups that are dominant within the culture of each language.Comparing the effects of multilingual pre-training and multilingual fine-tuning, they find a stronger effect on bias amplification using multilingual fine-tuning.Notably, Ahn and Oh (2021) evaluate bias in monolingual models for six languages -English, German, Spanish, Korean, Turkish, and Chineseand propose the use of multilingual models as a bias mitigation technique.Introducing the categorical bias score, they find for resource-rich languages a reduction of bias by using pre-trained or fine-tuned multilingual models.",
            "score": 0.47955374895692543,
            "section_title": "Related Work",
            "char_start_offset": 3957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 58,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 403
                },
                {
                    "start": 405,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 967
                },
                {
                    "start": 969,
                    "end": 1004
                },
                {
                    "start": 1004,
                    "end": 1238
                },
                {
                    "start": 1238,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1635
                },
                {
                    "start": 1635,
                    "end": 1845
                },
                {
                    "start": 1845,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 265,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 279,
                    "end": 289,
                    "matchedPaperCorpusId": "267311680"
                },
                {
                    "start": 638,
                    "end": 657,
                    "matchedPaperCorpusId": "239009591"
                },
                {
                    "start": 662,
                    "end": 686,
                    "matchedPaperCorpusId": "259341896"
                },
                {
                    "start": 757,
                    "end": 778,
                    "matchedPaperCorpusId": "184486914"
                },
                {
                    "start": 783,
                    "end": 801,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 905,
                    "end": 923,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 1025,
                    "end": 1043,
                    "matchedPaperCorpusId": "258823009"
                },
                {
                    "start": 1213,
                    "end": 1237,
                    "matchedPaperCorpusId": "235658325"
                },
                {
                    "start": 1644,
                    "end": 1661,
                    "matchedPaperCorpusId": "237491723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459716796875
        },
        {
            "corpus_id": "236171330",
            "title": "Evaluation of contextual embeddings on less-resourced languages",
            "text": "In particular, dictionaries obtained by triangulation via English are of poor quality, and non-isomorphic translation might be more affected by imprecise anchor points. \n\nIn general, even the best cross-lingual ELMo models lag behind the reference model without cross-lingual transfer. The differences in Macro F 1 score are small for some languages (e.g., 5.5% for English-Swedish), but they are significantly larger for most languages. The average gap between the best cross-lingual model in each language and the monolingual reference is 14.7% for ELMo models. \n\nIn Table 15, we present the results of cross-lingual transfer for contextual BERT models. We compared massively multilingual BERT models (mBERT and XLM-R) with trilingual BERT models (TRI). \n\nThe results show a clear advantage of trilingual models compared to massively multilingual models. The trilingual models dominate in 11 out of 12 transfers, except the transfer from English to Estonian, where XLM-R is better for 0.1%. The results also show that the transfer from a similar language is more successful than transfer from English. The average difference between the most successful transfer from English and the most successful transfer from a similar language averaged over target languages is considerable, i.e. 4.6%. \n\nComparing cross-lingual transfer of ELMo (in Table 14) with variants of multilingual BERT (in Table 15), the transfer with BERT is considerably more successful. This indicates that ELMo, while useful for explicit extraction of embedding vectors, is less competitive with BERT in the prediction model transfer, especially if we consider that ELMo requires additional effort for preparation of contextual mapping datasets, while BERT does not need it. \n\nFinally, the comparison between the best cross-lingual models (in the bottom part of Table 15) and the best monolingual models (reference scores taken from Table 5) shows that with cross-lingual transfer we lose on average 5.2%. This is a very encouraging result, showing that modern cross-lingual technologies have made significant progress and can bridge the technological gap for less-resourced languages.",
            "score": 0.47895198965722297,
            "section_title": "Named entity recognition",
            "char_start_offset": 61175,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 171,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2155
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "269187651",
            "title": "Language Ranker: A Metric for Quantifying LLM Performance Across High and Low-Resource Languages",
            "text": "In the last section, we introduced and evaluated the Language Ranker, demonstrating its ability to quantify the multilingual capabilities of LLMs by comparing their internal representations against an English baseline. This provided a robust measure of how LLMs perform across different languages, especially highlighting the disparities between highresource and low-resource languages. \n\nBuilding on these insights, in this section, we delve deeper into the proposed metric to explore its credibility and reliability further. Specifically, we aim to answer the follow-ing questions in the following three subsections: RQ5: Is choosing English as the benchmark a wise choice? RQ6: What does the subspace of each language look like? RQ7: Is choosing cosine similarity a wise choice? \n\nWhy Using English as Baseline? (RQ5) \n\nIn the above sections, we choose English as a baseline. This is based on the a priori assumption that low-resource languages generally perform worse than high-resource languages. But if we choose other high-resource languages as baselines, will we get the same performance? In other words, how can we ensure that our metric is not affected by the English language itself? To answer this question, we divided our probing datasets into three types: High Resource-High Resource (H-H), High Resource-Low Resource (H-L), and Low Resource-Low Resource (L-L). To fulfill our requirement, we utilize Tatoeba-Challenge (Tiedemann 2020) as our dataset instead of opus-100 because the latter is an English-centric dataset which means there is no Low Resource-Low Resource language pair. Tatoeba-Challenge is a challenge set for machine translation that contains 32G translation units in 2,539 bitexts. The whole data set covers 487 languages linked to each other in 4,024 language pairs.",
            "score": 0.4786098405247573,
            "section_title": "Further Analysis of Proposed Metric",
            "char_start_offset": 14646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1799
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "276287513",
            "title": "Examining Multilingual Embedding Models Cross-Lingually Through LLM-Generated Adversarial Examples",
            "text": "When considering which model to use to perform semantic search within a specific domain and language pair, standard benchmarks such as MTEB (Muennighoff et al., 2023) do not cover every domain within all language pairs. We posit that one should aim to directly evaluate semantic search for the target language pair within the domain of interest but the lack of dataset for such evaluation cases makes it difficult to take fundamental decisions for the application uses, such as the choice of an embedding model or whether to use English as a pivot language to perform the task. \n\nWe propose a new simple and efficient dataset generation and evaluation scenario that simulates real-world challenges of cross-lingual semantic retrieval in large text collections. Our approach involves the use of LLMs to create adversarial examples from parallel sentences that challenge the cross-lingual embedding capabilities of multilingual models. Concretely speaking, four distractor sentences are generated for each parallel target sentence. We require these sentences to be very similar to the original target sentence in terms of syntactic structure and surface word forms, but to be semantically dissimilar. Our proposed task, Cross-Lingual Semantic Discrimination (CLSD), involves identifying the correct target sentence from among four distractor sentences in the target language, given an original sentence in the source language. \n\nTo better understand how the introduced changes in the distractors influence the semantic similarities cross-and monolingually, we perform a finegrained analysis and correlate the semantic changes with linguistic properties at the level of part-ofspeech tags. \n\nContributions 1. Proposing a new task that emulates cross-lingual semantic search within large text collections, requiring only a set of parallel sentences for dataset creation. 2. Publishing four such datasets within the news domain for the language pair German-French. 3. Demonstrate that within the task, certain embedding models perform better in direct cross-lingual retrieval, while others benefit from using English as a pivot language. 4. Providing insights into the cross-and monolingual semantic representation of multilingual models by a linguistically informed analysis.",
            "score": 0.4780534464065706,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1424
                },
                {
                    "start": 1427,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2132
                },
                {
                    "start": 2133,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 166,
                    "matchedPaperCorpusId": "252907685"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60595703125
        },
        {
            "corpus_id": "269214458",
            "title": "CrossIn: An Efficient Instruction Tuning Approach for Cross-Lingual Knowledge Alignment",
            "text": "We conduct three comprehensive ablation studies to systematically assess the effects of various data formations, the integration of translation data, and the influence of different alignment dataset sizes on the performance of our models, aiming to identify key factors that enhance or inhibit their effectiveness.\n\nData Formulation Comparison. Figure 3 shows the AC3 scores from three tests when the language backbone is the Mistral-7B-v0.We speculate that this is because tasks included in our benchmark focus on understanding and reasoning, and the cross-lingual instruction tuning approach stimulate both of them under a multilingual setting.Additionally, the translations used here may be too basic, especially compared to larger datasets like WikiMatrix.This suggests that improving multilingual knowledge alignment may be better achieved through a mixed-language approach at the sample level rather than by incorporating simple translation data.Essential Cross-Lingual Data Quantities.Figure 5 shows the AC3 score of the LLMs with different quantity of cross-lingual alignment data.It can be shown that adding 5000 alignment data could already achieve a good result of cross-lingual consistency, there are not much improvement trend if we add more data.The observation that only a small amount of cross-lingual alignment data is required to achieve satisfactory consistency in LLMs can be attributed to its efficient learning mechanism.This characteristic allows the model to quickly assimilate and generalize from limited data, making it particularly adept at few-shot learning scenarios.Additionally, the model's pretraining on diverse linguistic corpora might have already equipped it with a foundational understanding of various languages, thereby reducing the need for extensive alignment data to bridge linguistic gaps.This efficient use of data not only demonstrates the model's robustness but also highlights its practicality in situations where data availability is constrained.",
            "score": 0.4779622638993293,
            "section_title": "Ablation Study",
            "char_start_offset": 21970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 440
                },
                {
                    "start": 440,
                    "end": 646
                },
                {
                    "start": 646,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1260
                },
                {
                    "start": 1260,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1596
                },
                {
                    "start": 1596,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "272310620",
            "title": "Exploring Multiple Strategies to Improve Multilingual Coreference Resolution in CorefUD",
            "text": "We also conduct zero-shot cross-lingual experiments in two variants; dataset zero shot, and language zero shot. In dataset zero shot we train the model on all the datasets except one and then we evaluate the model on the excluded dataset. However, this approach is not a true cross-lingual zero-shot test, as multiple datasets exist for some languages, meaning the model was still exposed to the language of the evaluation data during training. To address this, we performed language zero-shot experiments, where we removed all datasets for a particular language from the training data, and then evaluated the model on all datasets in that language. Additionally, we conducted a final experiment focusing on the ParCor corpora, which consists of parallel English and German datasets. This allowed us to perform a true zero-shot evaluation on both ParCor corpora.",
            "score": 0.4779067162392776,
            "section_title": "Zero-shot Cross-lingual Experiments",
            "char_start_offset": 27991,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 862
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "270226445",
            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
            "text": "We evaluate models trained on the xGQA dataset with MaXM (Changpinyo et al., 2023), a recently proposed evaluation-only benchmark for multilingual VQA.The MaXM dataset covers seven different languages: English (en), French (fr), Hindi (hi), Hebrew (iw), Romanian (ro), Thai (th), and Chinese (zh  7. The accuracy of the original English evaluation set is increased in both MERGE and TAG compared to solely using translated samples.The overall scores for the translate-test are also improved with data augmentation.These results indicate that augmenting training data with both human and MT texts is helpful for cross-lingual transfer while maintaining its performance on the original English texts.",
            "score": 0.4779062453118714,
            "section_title": "Experiments with MaXM dataset",
            "char_start_offset": 15544,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 431
                },
                {
                    "start": 431,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 698
                }
            ],
            "ref_mentions": [
                {
                    "start": 57,
                    "end": 82,
                    "matchedPaperCorpusId": "232417173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89453125
        },
        {
            "corpus_id": "253098967",
            "title": "EUR-Lex-Sum: A Multi- and Cross-lingual Dataset for Long-form Summarization in the Legal Domain",
            "text": "For Cross-lingual Summarization (XLS), Wang et al. (2022b) provide an extensive survey on the currently available methods, datasets, and prospects. Resources for XLS can be divided into two primary categories: synthetic datasets and webnative multilingual resources. For the former, samples are created by directly translating summaries from a given source language to a separate target language. Examples include English-Chinese (and vice versa) by Zhu et al. (2019), and an English-German resource (Bai et al., 2021). Both works utilize news articles for data and neural MT systems for the translation. In contrast, there also exist web-native multilingual datasets, where both references and summaries were obtained primarily from parallel website data. Global Voices (Nguyen and Daum\u00e9 III, 2019), XWikis (Perez-Beltrachini and Lapata, 2021), Spektrum (Fatima and Strube, 2021), and CLIDSUM (Wang et al., 2022a) represent instances of datasets for the news, encyclopedic, and dialogue domain, with differing numbers of supported languages. \n\nWe have previously mentioned some of the multilingual summarization resource where multiple languages are covered. MLSUM (Scialom et al., 2020) is based on news articles in six languages, however, without cross-lingual alignments. Similarly without alignments, but larger in scale, is MassiveSum (Varab and Schluter, 2021). XL-Sum Hasan et al. (2021) does provide documentaligned news article, in 44 distinct languages, extracted data from translated articles published by the BBC. In particular, their work also provides translations in several lower-resourced Asian languages. WikiLingua (Ladhak et al., 2020) borders the multi-and cross-lingual domain; some weak alignments exist, but only for English references, and not between languages themselves. \n\n3 The EUR-Lex-Sum Dataset \n\nWe present a novel dataset based on available multilingual document summaries from the EUR-Lex platform.",
            "score": 0.4777992642219901,
            "section_title": "Datasets for Multi-or Cross-lingual Summarization",
            "char_start_offset": 5732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1827
                },
                {
                    "start": 1830,
                    "end": 1934
                }
            ],
            "ref_mentions": [
                {
                    "start": 450,
                    "end": 467,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 500,
                    "end": 518,
                    "matchedPaperCorpusId": "235247840"
                },
                {
                    "start": 771,
                    "end": 799,
                    "matchedPaperCorpusId": "203610372"
                },
                {
                    "start": 808,
                    "end": 844,
                    "matchedPaperCorpusId": "243865371"
                },
                {
                    "start": 855,
                    "end": 880,
                    "matchedPaperCorpusId": "241583774"
                },
                {
                    "start": 1166,
                    "end": 1188,
                    "matchedPaperCorpusId": "216868259"
                },
                {
                    "start": 1341,
                    "end": 1367,
                    "matchedPaperCorpusId": "243865257"
                },
                {
                    "start": 1376,
                    "end": 1395,
                    "matchedPaperCorpusId": "235658519"
                },
                {
                    "start": 1635,
                    "end": 1656,
                    "matchedPaperCorpusId": "222177239"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497802734375
        },
        {
            "corpus_id": "248987294",
            "title": "Multi2WOZ: A Robust Multilingual Dataset and Conversational Pretraining for Task-Oriented Dialog",
            "text": "In this section we describe the construction of the MULTI 2 WOZ dataset, providing also details on inter-translator reliability. We then discuss two concurrent efforts in creating multilingual TOD datasets from MultiWOZ and their properties, and emphasize the aspects that make our MULTI 2 WOZ a more reliable and useful benchmark for evaluating cross-lingual transfer for TOD.",
            "score": 0.4774294446617788,
            "section_title": "Multi WOZ",
            "char_start_offset": 3233,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "247594499",
            "title": "Challenges and Strategies in Cross-Cultural NLP",
            "text": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
            "score": 0.47725431406912344,
            "section_title": "Data Collection",
            "char_start_offset": 25564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 1947,
                    "end": 1965,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 1965,
                    "end": 1984,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1984,
                    "end": 2001,
                    "matchedPaperCorpusId": "237503047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "276575630",
            "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
            "text": "Meta-evaluation datasets enable the development of reliable automatic evaluators. In a RAG setup, the input to the evaluator is composed of a question q (user input), a context c (set of passages automatically retrieved to the question) and an answer a generated by a language model to answer the question based on the context. The end-to-end evaluator then needs to judge the quality of the answer a given the context and the question (c, q). Following previous work (Saad-Falcon et al., 2024;Es et al., 2024) we focus on two quality dimensions: \n\nFaithfulness Is the answer grounded on the context, regardless of your world knowledge? \n\nRelevance Is the answer relevant to the question, regardless of the context? \n\nWe build a multilingual end-to-end metaevaluation RAG (MEMERAG) dataset by extending the MIRACL dataset (Zhang et al., 2023) to include model-generated answers and human-based quality judgements. More precisely, we select relevant question-context pairs, generate answers using various language models and gather expert human annotations on the quality of those answers. Our dataset encompasses 5 languages: English (EN), German (DE), Spanish (ES), French (FR), and Hindi (HI), which represent multiple language families and both high-and low-resource languages. Figure 1 shows examples from the dataset, with LLM-generated answers and coarse-to finegrained human-assigned labels for the faithfulness and relevance dimensions. \n\nThe MIRACL dataset is composed of questions written by humans in their native languages, one or more passages automatically retrieved from the Wikipedia, and human annotations about the relevance of each passage. Building a dataset starting from native questions in each language allows to evaluate RAG pipelines without resorting to (machine) translations, thus avoiding limitations and biases associated with translation. Note, however, that as questions were elicited from native speakers independently across different languages, the resulting data set is not parallel.",
            "score": 0.47675271242981615,
            "section_title": "Dataset Construction",
            "char_start_offset": 7620,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 546
                },
                {
                    "start": 549,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1444
                },
                {
                    "start": 1447,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2020
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 510,
                    "matchedPaperCorpusId": "263152733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76611328125
        },
        {
            "corpus_id": "269004948",
            "title": "Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly",
            "text": "Evaluation work is rapidly updating in the field of cross-lingual knowledge alignment.In the PLM era, many cross-lingual NLP benchmark datasets were proposed to test the models' performance on certain aspects in different languages, such as XCOPA (Ponti et al., 2020) and X-CSQA (Lin et al., 2021) for commonsense reasoning, and X-FACTR (Jiang et al., 2020) and multilingual versions of LAMA (Kassner et al., 2021;Yin et al., 2022;Qi et al., 2023) et al., 2023b;Ahuja et al., 2023).\n\nKnowledge misalignment of language models.Previous work have pointed out the imbalance of multilingual pretrained language models (PLMs) (Pires et al., 2019;Qi et al., 2023).However, since the \"incident multilingualism\" in pertraining increased a lot for LLMs, this conclusion needs to be reevaluated.Zhang et al. (2023c) found that Chat-GPT does not perform consistently on tasks in different languages, while exhibiting a translation-like thinking mode.Wang et al. (2023) concluded that multilingually-trained models have not attained \"balanced multilingual\" capabilities, especially on commonsense or factual knowledge.However, they did not differentiate between the two sources of language misalignment.Qi et al. (2023) further evaluated the cross-lingual consistency of PLMs and the factors affecting it using a rank-based metric.However, an evaluation with all three levels of cross-lingual knowledge alignment considered is yet to be done.",
            "score": 0.47668367163948155,
            "section_title": "Multilingual benchmarks and evaluations.",
            "char_start_offset": 6363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 86,
                    "end": 482
                },
                {
                    "start": 484,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 658
                },
                {
                    "start": 658,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1319
                },
                {
                    "start": 1319,
                    "end": 1430
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 267,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 279,
                    "end": 297,
                    "matchedPaperCorpusId": "235421949"
                },
                {
                    "start": 337,
                    "end": 357,
                    "matchedPaperCorpusId": "222310559"
                },
                {
                    "start": 392,
                    "end": 414,
                    "matchedPaperCorpusId": "231740666"
                },
                {
                    "start": 621,
                    "end": 641,
                    "matchedPaperCorpusId": "174798142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58544921875
        },
        {
            "corpus_id": "274789680",
            "title": "Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models",
            "text": "To evaluate multilingual language model performance, we conducted a comprehensive analysis across classification and translation tasks, testing each of the 14 models in zero-shot and two-shot incontext learning settings (Brown et al., 2020). This dual-task evaluation enabled us to assess model performance across different languages and learning scenarios, providing insights into their effectiveness in handling multilingual data. \n\nFor the classification task, we used the SIB-200 dataset, calculating F1 scores based on model outputs compared to ground truth for each language. \n\nFor the generation task, we translated from various languages to English using the Flores-200 dataset, assessing accuracy with sacreBLEU scores against reference translations (Post, 2018). \n\nTo better understand the factors influencing model performance and to quantify the relationships between input features and performance metrics (F1 and sacreBLEU scores), we applied ten regression models: Linear Regression (Galton, 1886), Random Forest (Breiman, 2001), Decision Tree (Quinlan, 1986), Support Vector Regression (SVR) (Vapnik et al., 1995), Gradient Boosting (Friedman, 2001), XGBoost (Chen and Guestrin, 2016), K-Nearest Neighbors (Fix and Hodges, 1989), Lasso (Tibshirani, 1996), Ridge (Hoerl and Kennard, 1970), and Elastic Net (Zou and Hastie, 2005). \n\nWe split the data into an 80-20 training-test split and assessed each model's performance using Rsquared (R 2 ) and Mean Squared Error (MSE), providing a robust evaluation of predictive accuracy across different language and model configurations. \n\nTo further understand the impact of each feature on model performance, we utilized SHAP (SHapley Additive exPlanations) values, which offer a unified measure of feature importance for each prediction (Lundberg and Lee, 2017). We focused on models that demonstrated strongest predictive performance for each task, and analyzed both individual and aggregated (abstract) features to gain insights into broader categories like geographical, linguistic, and token similarity. This analysis provided a deeper understanding of how these features contribute to overall model performance.",
            "score": 0.47643068943574673,
            "section_title": "Feature Analysis",
            "char_start_offset": 10926,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 1344
                },
                {
                    "start": 1347,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 240,
                    "matchedPaperCorpusId": "235457999"
                },
                {
                    "start": 759,
                    "end": 771,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 998,
                    "end": 1012,
                    "matchedPaperCorpusId": "15253776"
                },
                {
                    "start": 1059,
                    "end": 1074,
                    "matchedPaperCorpusId": "13252401"
                },
                {
                    "start": 1108,
                    "end": 1129,
                    "matchedPaperCorpusId": "52874011"
                },
                {
                    "start": 1222,
                    "end": 1244,
                    "matchedPaperCorpusId": "120323383"
                },
                {
                    "start": 1252,
                    "end": 1270,
                    "matchedPaperCorpusId": "16162039"
                },
                {
                    "start": 1278,
                    "end": 1303,
                    "matchedPaperCorpusId": "28142999"
                },
                {
                    "start": 1321,
                    "end": 1343,
                    "matchedPaperCorpusId": "14134075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "11897561",
            "title": "Cross-lingual tagger evaluation without test data",
            "text": "4 Related work Li et al. (2012) gauge 9 Wiktionaries against gold dictionaries to strengthen the argument for their weakly-supervised tagger. Agi\u0107 et al. (2015) use 10 Wiktionaries to extend a cross-lingual tagger evaluation to languages without test sets, but they do so indiscriminately. Their Wiktionaries range from only 50 to more than 20k random entries. To the best of our knowledge, research on evaluating POS taggers in absence of manually annotated test data is novel to our work. \n\nWe collected 16 new Wiktionaries on top of the 9 provided by Li et al. (2012) for our experiment. Recently, larger Wiktionary datasets4 have been made available, enabling further experiments with cross-lingual tagging. The dataset of Sylak-Glassman et al. (2015) covers more than 300 languages, and includes parts of speech and morphological Plank et al. (2015) discuss how various metrics for evaluating syntactic dependency parsing correlate with human judgments. We suggest that our translation-based metrics might naturally extend to dependency parsing by, e.g., treating an English dependency relation dictionary as a tag dictionary. The strong correlations between labeling (LA) and attachment scores (UAS) in dependency parsing favor our proposal. 5 arrette and Baldridge (2013) build taggers for low-resource languages from just 2 hours of man-ual annotation. Similarly, we show how to reliably evaluate cross-lingual POS taggers by translating as little as 100 most frequent English Wiktionary entries to the target language.",
            "score": 0.4763536543807252,
            "section_title": "Discussion",
            "char_start_offset": 12429,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 31,
                    "matchedPaperCorpusId": "8548302"
                },
                {
                    "start": 142,
                    "end": 160,
                    "matchedPaperCorpusId": "18824729"
                },
                {
                    "start": 554,
                    "end": 570,
                    "matchedPaperCorpusId": "8548302"
                },
                {
                    "start": 727,
                    "end": 755,
                    "matchedPaperCorpusId": "6559983"
                },
                {
                    "start": 835,
                    "end": 854,
                    "matchedPaperCorpusId": "11910890"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "218487087",
            "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
            "text": "In contrast to existing literature that mostly focuses on English, we conduct analyses in multilingual settings. We argue that the bias in multilingual word embeddings can be very different from that in English. One reason is that each language has its own properties. For example, in English, most nouns do not have grammatical gender, while in Spanish, all nouns do. Second, when we do the alignment to get the multilingual word embeddings, the choice of target space may cause bias. Third, when we do transfer learning based on multilingual word arXiv:2005.00699v1 [cs.CL] 2 May 2020 embeddings, the alignment methods, as well as the transfer procedure can potentially influence the bias in downstream tasks. Our experiments confirm that bias exists in the multilingual embeddings and such bias also impacts the cross-lingual transfer learning tasks. We observe that the transfer model based on the multilingual word embeddings shows discrimination against genders. To discern such bias, we perform analysis from both the corpus and the embedding perspectives, showing that both contribute to the bias in transfer learning. Our contributions are summarized as follows: \n\n\u2022 We build datasets for studying the gender bias in multilingual NLP systems. Lauscher and Glava\u0161 (2019) show that there is bias in bilingual word embeddings. However, none of them consider the cross-lingual transfer learning which is an important application of the multilingual word embeddings. To mitigate the bias in word embeddings, various approaches have been proposed (Bolukbasi et al., 2016;Zhao et al., 2018b). In contrast to these methods in English embedding space, we propose to mitigate the bias from the multilingual perspectives. Comparing to Zhou et al. (2019), we show that a different choice of alignment target can help to reduce the bias in multilingual embeddings from both intrinsic and extrinsic perspectives.",
            "score": 0.4763276862881219,
            "section_title": "Introduction",
            "char_start_offset": 1908,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1907
                }
            ],
            "ref_mentions": [
                {
                    "start": 1252,
                    "end": 1278,
                    "matchedPaperCorpusId": "135465247"
                },
                {
                    "start": 1550,
                    "end": 1574,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5712890625
        },
        {
            "corpus_id": "253116659",
            "title": "Beyond English-Centric Bitexts for Better Multilingual Language Representation Learning",
            "text": "In order to further evaluate the cross-lingual transferrability of our model, we follow Hu et al. (2020) and evaluate the cross-lingual transfer gap (the difference between the performance on the English test set and the average test set performance for other languages) for XY-LENT Base . This score indicates how much end task knowledge is not transferred to other languages post fine-tuning, with a smaller gap indicating better transferrability. As seen in Table 4, XY-LENT achieves lower scores on 3 out of 5 tasks, thereby demonstrating strong transferrability.\n\n6.6 Using Training Dynamics to Explore Dataset Quality\n\nSo far we have seen that leveraging X-Y aligned bitexts improves model quality. In this section, we consider the inverse direction: whether training dynamics of representation learning models can be used to identify dataset artifacts. Given these bitext datasets span over 1000 language pairs, a manual inspection of these datasets is extremely hard. Thus an automated method for spot-checking the dataset quality is quite valuable.\n\nTo do so, we first train a model in line with the methodology presented by Zhou et al. (2021) for Distributionally Robust Multilingual Machine Translation. Specifically, we train XY-LENT with the following modified objective:\n\nHere L G and L D refer to the generator and discriminator losses respectively ( \u00a74), P is the joint distribution over the bitext language pairs that we want to estimate (i.e P = p i | 1 \u2264 i \u2264 L \u00d7 L; i p i = 1); and Q is the original training distribution (i.e the probability distribution over the bitexts when the training starts, equal to P * as estimated in \u00a73.2). At a high level, the objective minimizes the training loss over a \u03c7 2 ball around the original training distribution, with the supremum up-weighing language pairs with higher loss values, and down-weighing languages with lower loss values 5 . We train a model with the Distributional Robustness Optimization objective (DRO) using Iterated Best Response strategy, as proposed by Zhou et al. (2021) and resample 10 times throughout the training. We hypothesize that the two extremities (i.e language pairs that are highly upsamp",
            "score": 0.476298715395495,
            "section_title": "Crosslingual Transfer Gap",
            "char_start_offset": 17310,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.385498046875
        },
        {
            "corpus_id": "276249513",
            "title": "Evaluation of Multilingual Image Captioning: How far can we get with CLIP models?",
            "text": "This appendix describes the translation scheme that was used to machine translate the datasets used in our experiments. This scheme is designed to mitigate low-quality translations, or hallucinations generated by the machine translation model, thus providing reliable datasets at the end. We specifically used a large (i.e., 1.2 billion parameters) open-access multilingual machine translation model named M2M100 (Fan et al., 2021), available on the HuggingFace6 model hub. M2M100 was trained on a range of high and low-resource languages from different families and using different scripts, achieving state-of-the-art performance across a diverse set of 100 languages. While machine translated data allows us to assess multilingual captioning metrics, the results will depend not only on the performance of the metrics but also on the quality of the translations. Low-quality translations, or hallucinations generated by the translation model, will impact the caption and break our assumption that human ratings for the English data can be transferred across languages. To address this issue, we propose to use the COMETKiwi (Rei et al., 2022(Rei et al., , 2023) ) machine translation quality estimation metric to control for translation quality, assessing the impact of low quality translations on the observed results. \n\nWe specifically began by translating the VICR dataset, followed by the other English datasets mentioned in Appendix D. VICR features English captions with human ratings and also reference captions originally from the MSCOCO and Flickr8K datasets. For each caption, whether a candidate or a reference, we return 25 translations using a beam search technique with 100 beams. Subsequently, we filtered the candidates with a language checker, to ensure proper translation into the intended language. After the language check, we selected for each instance the translation that scored higher based on a large COMETKiwi model7 .",
            "score": 0.4760452732921089,
            "section_title": "D The Machine Translation Scheme",
            "char_start_offset": 41539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 413,
                    "end": 431,
                    "matchedPaperCorpusId": "224814118"
                },
                {
                    "start": 1126,
                    "end": 1143,
                    "matchedPaperCorpusId": "252222165"
                },
                {
                    "start": 1143,
                    "end": 1165,
                    "matchedPaperCorpusId": "262084083"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "271050875",
            "title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
            "text": "To compare the encoded bias in mono-and multilingual models, first we use automatic translation to translate BBQ (Parrish et al., 2022) and CrowS-Pairs (Nangia et al., 2020) datasets and evaluate the translation quality with manual annotation.Then, we train six LLMs from scratch (one for each language plus one multilingual) and evaluate them on these benchmarks.",
            "score": 0.47571825894332437,
            "section_title": "Approach",
            "char_start_offset": 6322,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 364
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 135,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 152,
                    "end": 173,
                    "matchedPaperCorpusId": "222090785"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "264491136",
            "title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages",
            "text": "We introduce a multilingual culturally-relevant dataset for evaluating 11 dimensions of intrinsic bias in 24 languages using target and attribute pairs for the WEAT metric. Beyond previously-studied dimensions of social bias like gender, we propose new dimensions of human-centered contemporary biases like ableism and sexuality to find strong evidence of bias in language models. We show that bias does not uniformly manifest across languages, but monolingual models do reflect human biases more closely than multilingual ones across multiple methods of extracting contextualized word embeddings. We also find that human translations are better suited for bias studies than automated (machine) translation ones. Finally, our case study on Indian languages reveals that biases in resource-heavy languages like Hindi are not necessarily found in other languages. WEAT, however, is an inconsistent metric for measuring biases, as indicated by a limited number of statistically significant results. In the future, we aim to develop metrics better suited for measuring biases in contextualized embeddings and generative models and explore their effects in real-world downstream applications.",
            "score": 0.47537638067449683,
            "section_title": "Conclusion",
            "char_start_offset": 30697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "277066532",
            "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
            "text": "The development of evaluation datasets for specific languages involves significant human resources. Therefore, a widely used strategy is to apply machine translation, with or without manual postediting (Dargis et al., 2024a). \n\nHuman ability to understand language is general, flexible, and robust (Wang, 2018;Lin and Och, 2004). Hence, human evaluations are typically considered the gold standard in NLG to assess the effectiveness of multilingual models (Clark et al., 2021;Chiang and Lee, 2023), particularly in evaluating their ability to generate text that aligns with diverse linguistic and cultural contexts. \n\nThe use of automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), even though commonly used, often fail to capture cultural nuances, making human evaluation essential for a more comprehensive assessment (Kocmi et al., 2021). Human evaluations are essential for assessing how well multilingual models handle grammatical, syntactical, and contextual differences, particularly in LRLs (Costa-juss\u00e0 et al., 2022). Human raters are better at identifying these nuances, using criteria such as appropriateness, bias detection, and cultural sensitivity (Choenni et al., 2024).",
            "score": 0.4750381935857003,
            "section_title": "Human evaluation of multilingual models",
            "char_start_offset": 9408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 225
                },
                {
                    "start": 228,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1209
                }
            ],
            "ref_mentions": [
                {
                    "start": 202,
                    "end": 224,
                    "matchedPaperCorpusId": "273901276"
                },
                {
                    "start": 310,
                    "end": 328,
                    "matchedPaperCorpusId": "7139779"
                },
                {
                    "start": 694,
                    "end": 705,
                    "matchedPaperCorpusId": "964287"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "246294502",
            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
            "text": "2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research. 4) Our results and additional analyses take stock of the current gaps and challenges in cross-lingual V&L research. 5) To further facilitate replicable research in this area, we re-implement the existing multilingual V&L pretrained encoders in a unified framework (VOLTA; Bugliarello et al. 2021), which also provides access to five English V&L BERTs and 12 V&L tasks. We provide data and code for the evaluation of multilingual V&L models at https://iglue-benchmark.github.io/.",
            "score": 0.4749025136064364,
            "section_title": "Introduction",
            "char_start_offset": 4870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "215744964",
            "title": "BLEU Might Be Guilty but References Are Not Innocent",
            "text": "Machine Translation (MT) quality has greatly improved in recent years. In particular, language pairs with abundant training data have benefited tremendously from neural machine translation techniques (Bahdanau et al., 2015;Gehring et al., 2017;Vaswani et al., 2017). This progress has cast doubt on the reliability of automated metrics, especially in the high accuracy regime. For instance, the WMT English\u2192German evaluation in the last two years had a different top system when looking at automated or human evaluation (Bojar et al., 2018;Barrault et al., 2019). Such discrepancies have also been observed in the past, especially when comparing rule-based and statistical systems (Bojar et al., 2016b;Koehn and Monz, 2006;Callison-Burch et al., 2006). \n\nAutomated evaluations are however of crucial importance, especially for system development. Most decisions for architecture selection, hyperparameter search and data filtering rely on automated evaluation at a pace and scale that would not be sustainable with human evaluations. Automated evaluation (Koehn, 2010;Papineni et al., 2002) typically relies on two crucial ingredients: a metric and a reference translation. Metrics generally measure the quality of a translation by assessing the overlap between the system output and the reference translation. Different overlap metrics have been proposed, aiming to improve correlation between human and automated evaluations. Such metrics ranges from n-gram matching, e.g. BLEU (Papineni et al., 2002), to accounting for synonyms, e.g. METEOR (Banerjee and Lavie, 2005), to considering distributed word representation, e.g. BERTScore (Zhang et al., 2019). Orthogonal to metric quality (Ma et al., 2019), reference quality is also essential in improving correlation between human and automated evaluation. \n\nThis work studies how different reference collection methods impact the reliability of automatic evaluation. It also highlights that the reference sentences typically collected with current (human) translation methodology concentrate in a limited part of the space of target sentences with the same meaning. We show that this part of the space is different from original native target sentences.",
            "score": 0.4744653808625886,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1806
                },
                {
                    "start": 1809,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 223,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 223,
                    "end": 244,
                    "matchedPaperCorpusId": "6728280"
                },
                {
                    "start": 244,
                    "end": 265,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 520,
                    "end": 540,
                    "matchedPaperCorpusId": "53247198"
                },
                {
                    "start": 681,
                    "end": 702,
                    "matchedPaperCorpusId": "9961289"
                },
                {
                    "start": 702,
                    "end": 723,
                    "matchedPaperCorpusId": "7470516"
                },
                {
                    "start": 723,
                    "end": 751,
                    "matchedPaperCorpusId": "263885694"
                },
                {
                    "start": 1068,
                    "end": 1090,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1480,
                    "end": 1503,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1545,
                    "end": 1571,
                    "matchedPaperCorpusId": "7164502"
                },
                {
                    "start": 1687,
                    "end": 1704,
                    "matchedPaperCorpusId": "201742578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "273233755",
            "title": "Linguistically-Informed Multilingual Instruction Tuning: Is There an Optimal Set of Languages to Tune?",
            "text": "The study's methodology has limitations, particularly due to the exclusive use of LoRA (Hu et al. 2021) as a PEFT method and the reliance on the Bactrian-X (Li et al. 2023) dataset, which consists of 52 languages. While this dataset is extensive, it includes automatically translated content, which may introduce inaccuracies. The same issue is present in the evaluation benchmarks, as they also contain automatic translations. Additionally, these evaluations are inherently limited in representing realworld language model usage (Lyu, Wu, and Aji 2024;Biderman et al. 2024). However, we chose this dataset and evaluation benchmarks because, despite these limitations, they remain among the most comprehensive and best available options for multilingual research. The outcomes might differ with a more diverse language dataset, as the language selection process could yield different results. The research was conducted under resource constraints, which limited the extent of training, the impact of prolonged training durations and the applicability of findings to models beyond the 7B scale might differ. Due to these constraints, we conducted our experiments three times (with three different seeds), which affects the statistical robustness of our results and leaves room for improvement in the calculation of confidence intervals and p-values for paired t-tests. Furthermore, our work relies on a relatively straightforward algorithmic approach, hinting at the possibility of achieving different or improved results with more complex methodologies targeting the nature of specific models and their exposition of pretraining corpus languages.",
            "score": 0.47425267122726433,
            "section_title": "Limitations",
            "char_start_offset": 47079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1646
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 102,
                    "matchedPaperCorpusId": "218974093"
                },
                {
                    "start": 530,
                    "end": 553,
                    "matchedPaperCorpusId": "267770168"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6884765625
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "or Multilingual Fine-tuning?\n\nSome studies (Tran and Bisazza, 2019;Dhamecha et al., 2021;Trotta et al., 2021;Barbieri et al., 2022;Portelli et al., 2023) reported that multilingual finetuning could improve a part of NLU task performance more than monolingual tuning alone.On the other hand, several studies (Tsai et al., 2019;Kondratyuk, 2019;R\u00f8nningstad, 2023;Kondratyuk and Straka, 2019) reported that it did not always improve performance in some tasks.We analyzed whether multilingual fine-tuning is effective for commonsense reasoning tasks through mCSQA.We used the whole shuffled training split data in all languages and fine-tuned XLM-R LARGE with the same setting as in Table 10.Table 7 compares the accuracy between monolingual fine-tuning, where tuning and evaluation are in the same language, and multilingual fine-tuning, where tuning is performed for all languages, evaluated for each language's accuracy score.These results show that most languages observed improvements, especially in all cases in Easy sets.However, in Hard sets, some cases observed a decline in performance compared to the monolingual setting.Therefore, while training in a multilingual setting generally promotes  accuracy improvement, multilingual training might lead to the loss of language-specific commonsense information for questions requiring more human commonsense.This analysis complements the previous reports (Dhamecha et al., 2021;Zhang et al., 2023a;Hu et al., 2021;Mueller et al., 2020) on the successes and failures of multilingual training.Furthermore, Table 7 shows the evaluation results of cross-lingual performance in the unseen setting, where the model was not trained on the language for evaluation data.While some languages outperform the monolingual setting, overall results indicate that training with target language data consistently yields better outcomes.This suggests that target language data acts as the secret sauce for enhancing NLU performance.Therefore, it suggests that for language-specific deep knowledge and cultural understanding, language-transfer capability alone is insufficient, and training with datasets focused on language-specific knowledge is necessary.",
            "score": 0.4741226194202345,
            "section_title": "Which is Better: Monolingual Fine-tuning",
            "char_start_offset": 19645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 30,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 456
                },
                {
                    "start": 456,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 925
                },
                {
                    "start": 925,
                    "end": 1024
                },
                {
                    "start": 1024,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1542
                },
                {
                    "start": 1542,
                    "end": 1712
                },
                {
                    "start": 1712,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 1965
                },
                {
                    "start": 1965,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 67,
                    "matchedPaperCorpusId": "204512297"
                },
                {
                    "start": 89,
                    "end": 109,
                    "matchedPaperCorpusId": "237635480"
                },
                {
                    "start": 109,
                    "end": 131,
                    "matchedPaperCorpusId": "248693350"
                },
                {
                    "start": 307,
                    "end": 326,
                    "matchedPaperCorpusId": "202122780"
                },
                {
                    "start": 326,
                    "end": 343,
                    "matchedPaperCorpusId": "201642047"
                },
                {
                    "start": 343,
                    "end": 361,
                    "matchedPaperCorpusId": "258352214"
                },
                {
                    "start": 361,
                    "end": 389,
                    "matchedPaperCorpusId": "102351547"
                },
                {
                    "start": 1429,
                    "end": 1449,
                    "matchedPaperCorpusId": "259376830"
                },
                {
                    "start": 1465,
                    "end": 1486,
                    "matchedPaperCorpusId": "218487708"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "252283924",
            "title": "\u00daFAL CorPipe at CRAC 2022: Effectivity of Multilingual Models for Coreference Resolution",
            "text": "A view on the effectiveness of multilingual models is shown in official ablation results on test data in Table 2, which compares all our three individual/multilingual settings: multilingual as a baseline, individual and best dev, using a base encoder (XLM-R base for the multilingual baseline, bestperforming base encoder for the remaining cases) and a large encoder (RemBERT). The multilingual is superior to individual for all datasets, with the exception of the three largest datasets using a base encoder -we hypothesize that the base encoder does not have sufficient capacity to capture the largest datasets in the multilingual setting, because with a large encoder, also the three largest datasets benefit from the multilingual model. Furthermore, Table 4.C demonstrates that while XLM-R large is the best in the individual settings, RemBERT delivers superior multilingual performance.\n\nMotivated by the improvements of the multilingual models, we considered a setting where 50% of the training data comes from a single dataset and the rest from all other datasets (with logarithmic mixing). Surprisingly, such setting delivers consistently worse performance than the multilingual models (last line of Table 4.C).\n\nThe comparison of logarithmic, uniform, and linear mixing, together with the presence or absence of corpus id, is evaluated in Table 4.D and  Table 4.E. Unexpectedly, neither the mixing rations nor the corpus id have a large effect on the results, which is surprising especially for the linear mixing, where the smallest treebanks are nearly 100 times less frequent than the largest one.",
            "score": 0.4740138016875999,
            "section_title": "Multilingual Models",
            "char_start_offset": 18154,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "261696828",
            "title": "BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models",
            "text": "We selected the tasks for the benchmark based on whether there was a dataset of reasonable quality available for at least 3 of the 4 languages targeted. We selected datasets that contained data originally in the target language as far as possible and where translated datasets were used, we ensured that they were translated by professional translators or that elaborate effort has been shown to ensure that the translation was done accurately. This is important because translated datasets often contain translationese [53] which can differ significantly from natively written text [54,55,56,57].  At the end of the data collection and selection process, we finalized a set of 8 tasks with datasets available for all 4 languages with the exception of Toxicity Detection for which we were unable to find a Tamil dataset of reasonable quality. Table 2 below summarizes the tasks and the details of the respective datasets.",
            "score": 0.4729547371028062,
            "section_title": "Task and dataset curation",
            "char_start_offset": 14464,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 587,
                    "matchedPaperCorpusId": "57174748"
                },
                {
                    "start": 587,
                    "end": 590,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 593,
                    "end": 596,
                    "matchedPaperCorpusId": "207852344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7099609375
        },
        {
            "corpus_id": "269449075",
            "title": "Unknown Script: Impact of Script on Cross-Lingual Transfer",
            "text": "The reported F1-score is averaged over five runs, with a standard deviation ranging between -0.003 and 0.009.transfer (Pires et al., 2019;Conneau et al., 2020), language adapters (Pfeiffer et al., 2020), and extending multilingual models (Conneau et al., 2020;Devlin et al., 2019).\n\nWhile such methods require fewer resources in the target language, they often yield sub-optimal results (Lauscher et al., 2020;Pfeiffer et al., 2021).For example, using even a small amount of labeled data in the target language has been shown to be more effective than a zero-shot transfer (Lauscher et al., 2020).\n\nThe effectiveness of cross-lingual transfer is influenced by several factors, such as language similarity between the source and target languages, the size of the pre-trained model, and the quality and amount of the pre-training and fine-tuning data (Muller et al., 2021;Pires et al., 2019;Cao et al., 2020;Wu and Dredze, 2019).\n\nIn this paper, we explore which factors determine cross-lingual transfer performance for a new language with a unique script.We consider a challenging case where the target language is not part of the pre-trained model, and the script has also not been seen in pre-training.Our analysis targets three main factors: language similarity, tokenization methods, and script.We design an experiment by varying these factors.We evaluate a range of existing monolingual and multilingual models, specifically choosing those trained on typologically related or typologically distant languages.Furthermore, we select models trained using various tokenizers, allowing us to assess how these choices impact cross-lingual performance for a language with a unique script.We focus on two main research questions:\n\n1. To what extent does the script of a source language influence cross-lingual transfer to a new language in monolingual and multilingual models?\n\n2. To what extent does the tokenizer influence cross-lingual transfer to a new language in monolingual and multilingual models?\n\nFigure 1 shows the results of our experiment.",
            "score": 0.47272980063190095,
            "section_title": "Introduction",
            "char_start_offset": 1891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 109,
                    "end": 281
                },
                {
                    "start": 283,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 597
                },
                {
                    "start": 599,
                    "end": 927
                },
                {
                    "start": 929,
                    "end": 1054
                },
                {
                    "start": 1054,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1298
                },
                {
                    "start": 1298,
                    "end": 1347
                },
                {
                    "start": 1347,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1685
                },
                {
                    "start": 1685,
                    "end": 1725
                },
                {
                    "start": 1727,
                    "end": 1872
                },
                {
                    "start": 1874,
                    "end": 2001
                },
                {
                    "start": 2003,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 138,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 138,
                    "end": 159,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 179,
                    "end": 202,
                    "matchedPaperCorpusId": "218470133"
                },
                {
                    "start": 238,
                    "end": 260,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 260,
                    "end": 280,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 387,
                    "end": 410,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 410,
                    "end": 432,
                    "matchedPaperCorpusId": "229923783"
                },
                {
                    "start": 573,
                    "end": 596,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 849,
                    "end": 870,
                    "matchedPaperCorpusId": "225067077"
                },
                {
                    "start": 870,
                    "end": 889,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 906,
                    "end": 926,
                    "matchedPaperCorpusId": "126167342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28662109375
        },
        {
            "corpus_id": "204734128",
            "title": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
            "text": "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
            "score": 0.47244516548111337,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8828125
        },
        {
            "corpus_id": "218487087",
            "title": "Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer",
            "text": "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language. These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language. While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages. In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications. We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives. Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning. We further provide recommendations for using the multilingual word representations for downstream tasks.",
            "score": 0.47226820991461005,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7314453125
        },
        {
            "corpus_id": "236287290",
            "title": "Monolingual and Cross-Lingual Intent Detection without Training Data in Target Languages",
            "text": "The authors in [23] use the multilingual dataset containing annotated utterances in English (~43 thousand), Spanish (~8.6 thousand), and Thai (~5 thousand) and covering 3 domains, 12 intents, and 11 slots. They evaluate crosslingual transfer methods based on 1) translated training data; 2) cross-lingual pre-trained embeddings; 3) multilingual machine translation encoder as contextual word representations. The joint intent detection and slot-filling model at first use a sentence classification model to identify the domain and then a domain-specific model to jointly predict intent and slots. The method architecture has self-attention BiLSTM and Conditional Random Fields (CRF) layers. The method is tested with several types of word embeddings (zero, XLU, encoder, CoVe, multilingual CoVE, and multilingual CoVE + autoencoder) trained by authors and available pre-trained ELMo encoders for Spanish. The authors found that languages with limited data benefit from cross-lingual learning. Despite it, multilingual contextual word representations outperform cross-lingual static embeddings. Due to these findings, the authors have to highlight a need for more refined cross-lingual methods. Another interesting cross-lingual research [24] uses a dataset containing ~6.9 thousand utterances across 16 COVID-19 specific intents in English, Spanish, French, and German languages. The authors explore: 1) monolingual and multilingual model baselines; 2) cross-lingual transfer from English to other languages; 3) zero-shot (in which only English data is used for training and model selection) codeswitching for Spanglish (combining words and idioms from Spanish and English). These authors tested fastText, XLM-R, and ELMo embeddings. Authors prove that lower results are obtained under a zero-shot setting, and XLM-R cross-lingual sentence embeddings significantly outperform their other cross-lingual solutions. Another cross-lingual research [25], for the first time, presents multilingual modeling without degrading perlanguage performance.",
            "score": 0.4722242484763675,
            "section_title": "Related Work",
            "char_start_offset": 16915,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "53110354"
                },
                {
                    "start": 1237,
                    "end": 1241,
                    "matchedPaperCorpusId": "221658006"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "204901567",
            "title": "On the Cross-lingual Transferability of Monolingual Representations",
            "text": "We compared state-of-the-art multilingual representation learning models and a monolingual model that is transferred to new languages at the lexical level. We demonstrated that these models perform comparably on standard zero-shot crosslingual transfer benchmarks, indicating that neither a shared vocabulary nor joint pre-training are necessary in multilingual models. We also showed that a monolingual model trained on a particular language learns some semantic abstractions that are generalizable to other languages in a series of probing experiments. Our results and analysis contradict previous theories and provide new insights into the basis of the generalization abilities of multilingual models. To provide a more comprehensive benchmark to evaluate cross-lingual models, we also released the Cross-lingual Question Answering Dataset (XQuAD).",
            "score": 0.4720317223619578,
            "section_title": "Conclusions",
            "char_start_offset": 24946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 851
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71240234375
        },
        {
            "corpus_id": "270226445",
            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
            "text": "We conduct experiments with all multilingual and monolingual VL models addressed in Bugliarello et al. (2022).Specifically, for multilingual models, MUNITER (Qiu et al., 2022), XUNITER (Qiu et al., 2022), UC 2 (Zhou et al., 2021), and M3 P (Ni et al., 2021) are used.For monolingual Englishonly models, LXMERT (Tan and Bansal, 2019), UNITER (Chen et al., 2020), VILBERT (Lu et al.,   2 Afterward, the source language refers to English.\n\n2019), VisualBERT (Li et al., 2020), and VL-BERT (Su et al., 2019) are used.All models are based on transformer (Vaswani et al., 2017) architecture, and both image and text are fed to the network simultaneously.In addition, we conduct experiments with recently proposed monolingual English VL models -BLIP-2 (Li et al., 2023a), In-structBLIP (Dai et al., 2023), andFLAVA (Singh et al., 2022).More details are in Appendix B.\n\nFor the cross-lingual transfer of multilingual models, the following approaches are considered: zero-shot, translate-train, and translate-test.The zero-shot approach trains a model on the original English training set in the GQA dataset and directly uses it to infer evaluation samples in the target language. 3The translate-train approach trains individual models for each target language on a translated training dataset.The translate-test approach trains a single model on an English training dataset and uses it for the evaluation of target languages along with a translation system.For monolingual models, only the translate-test approach is evaluated.",
            "score": 0.47180317678468214,
            "section_title": "Models",
            "char_start_offset": 5583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 110,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 435
                },
                {
                    "start": 437,
                    "end": 513
                },
                {
                    "start": 513,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1519
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 109,
                    "matchedPaperCorpusId": "40142250"
                },
                {
                    "start": 341,
                    "end": 360,
                    "matchedPaperCorpusId": "252222320"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3076171875
        },
        {
            "corpus_id": "221641003",
            "title": "FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding",
            "text": "Cross-lingual Datasets Cross-lingual language understanding has been investigated for many NLP tasks, where the knowledge learned from a pivot language (e.g., English) is transferred to other languages indirectly, as labeled data in low-resource languages are often scarce. There exist many multilingual corpora for diverse NLP tasks. Nivre et al. (2016) released a collection of multilingual treebanks on universal dependencies for 33 languages. Pan et al. (2017) introduced cross-lingual name tagging and linking for 282 languages. Other multilingual datasets range over tasks such as document classification, natural language inference, information retrieval, paraphrase identification, and summarization (Klementiev, Titov, and Bhattarai 2012;Cer et al. 2 Our code is released at https://github.com/yuwfan/FILTER 2017; Conneau et al. 2018;Sasaki et al. 2018;Yang et al. 2019;Zhu et al. 2019).\n\nMore recent studies on open-domain question answering and machine reading comprehension also introduced cross-lingual datasets, such as MLQA (Lewis et al. 2020), XQuAD (Artetxe, Ruder, and Yogatama 2020), and Ty-DiQA (Clark et al. 2020). Most recently, XTREME (Hu et al. 2020) and XGLUE (Liang et al. 2020) released several datasets across multiple tasks, and set up public leaderboards for evaluating cross-lingual models. In this paper, we work on both XTREME (see Figure 1 for examples) and XGLUE to demonstrate the effectiveness of our proposed method.\n\nCross-lingual Models Most previous work tackles crosslingual problems in two fashions: (i) cross-lingual zeroshot transfer; and (ii) translate-train/test. For cross-lingual zero-shot transfer, models are trained on labeled data in the source language only, and directly evaluated on target languages. Early work focused on training multilingual word embeddings (Mikolov, Le, and Sutskever 2013;Faruqui and Dyer 2014;Xu et al. 2018), while more recent",
            "score": 0.4717771290698758,
            "section_title": "Related Work",
            "char_start_offset": 4969,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 335,
                    "end": 354,
                    "matchedPaperCorpusId": "17954486"
                },
                {
                    "start": 708,
                    "end": 747,
                    "matchedPaperCorpusId": "6758088"
                },
                {
                    "start": 823,
                    "end": 843,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 862,
                    "end": 879,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 879,
                    "end": 895,
                    "matchedPaperCorpusId": "202541473"
                },
                {
                    "start": 1066,
                    "end": 1101,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1115,
                    "end": 1134,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 1158,
                    "end": 1173,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65576171875
        },
        {
            "corpus_id": "248834196",
            "title": "Consistent Human Evaluation of Machine Translation across Language Pairs",
            "text": "We report on two large-scale human evaluation studies to assess the two novel contributions of this work. The first study compares XSTS and its variants against other evaluation methods like Direct Assessment. The second study assesses the effectiveness of our calibration method. \n\nLanguage Pairs We selected languages with the goal to cover both high-resource languages with good machine translation quality and low-resource languages with weaker machine translation quality. The languages also differ in writing system, morphological complexity, and other linguistic dimensions. See the Table 1 for the list of languages in our studies. \n\nSelection of Evaluators Evaluators were selected for each language pair and they evaluated both language directions (English-X and X-English). The evaluators were professional translators who were recruited by a translation agency. They had to have at least three years of translation experience, be native speakers of the language X, high level of English proficiency, and pass through a training process (detailed documentation of the task and training examples). \n\nUser Interface and Training Since we are working with language service providers who subcontract the work to professional translators who differ in their technical setup, we do not always have full control over the way text is presented to them and how they register their evaluations. Throughout our studies, the employed tools vary from simple spreadsheets to a customized annotation tool similar to the one used in WMT evaluations. \n\nMachine Translation Systems Most of the machine translation systems used in this studies were trained in-house with fairseq (Ott et al., 2019) on public data sets at different times in 2020 and 2021, each designed to optimized translation quality given available data and technology. The most recent system, used in the calibration study, is a 100-language multilingual system, similar to the one developed for the WMT 2021 Shared Task (Tran et al., 2021). \n\nTest Set The translated sentences to be evaluated are selected from social media messages and Wikipedia -the later being part of the FLORES test set which comprises close to 200 languages at the time of writing (Guzm\u00e1n et al., 2019). Note that social media messages have the additional challenge of disfluency and creative language variation in the source sentence.",
            "score": 0.4712244508429394,
            "section_title": "Study Design",
            "char_start_offset": 9843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 280
                },
                {
                    "start": 283,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2239
                },
                {
                    "start": 2240,
                    "end": 2371
                }
            ],
            "ref_mentions": [
                {
                    "start": 1671,
                    "end": 1689,
                    "matchedPaperCorpusId": "91184134"
                },
                {
                    "start": 1983,
                    "end": 2002,
                    "matchedPaperCorpusId": "245855870"
                },
                {
                    "start": 2217,
                    "end": 2238,
                    "matchedPaperCorpusId": "263883936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6435546875
        },
        {
            "corpus_id": "270225999",
            "title": "Parrot: Multilingual Visual Instruction Tuning",
            "text": "Ablation study on each component. We conduct an ablation experiment on the multilingual data and the MoE module. As shown in Figure 5a, using multilingual data improves performance in each language. Moreover, the MoE module significantly improves performance, demonstrating the effectiveness of our proposed method. \n\nAblation study on different datasets. As shown in Table 3, it is evident that the inclusion of different multilingual datasets continually improves performance on the MMBench benchmark, and all models with 7B parameters are used for this experiment. This highlights the robustness and scalability of our approach to handling multiple languages effectively. Ablation study on monolingual fine-tuning datasets. The ablation study presented in Table 9 evaluates the performance of different monolingual datasets added incrementally to the baseline dataset LLaVA-1.5-finetune. It highlights the significant impact of adding different multilingual datasets to a baseline model. Each dataset incrementally improves performance in its respective language and, when combined, leads to overall enhanced performance across all evaluated languages. This indicates the robustness and effectiveness of the proposed method in handling multilingual data, making it a scalable solution for multilingual tasks.",
            "score": 0.4712012976246085,
            "section_title": "Ablation Study",
            "char_start_offset": 26617,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.457275390625
        },
        {
            "corpus_id": "258841859",
            "title": "Revisiting Machine Translation for Cross-lingual Classification",
            "text": "As we have just seen, the optimal cross-lingual learning approach is highly task dependent. In this section, we try to characterize the specific factors that explain this different behavior. To that end, we build on the concept of cross-lingual transfer gap, which is defined as the difference in performance between the source language that we have training data in (typically English) and the target language that we are evaluating on (Hu et al., 2020). While prior work has used this as an absolute metric to compare the cross-lingual generalization capabilities of different multilingual models, we argue that such a transfer gap can be attributed to different sources depending on the approach used, which we try to quantify empirically. \n\nIn what follows, we identify the specific sources of transfer gap that each approach is sensitive to ( \u00a74.1), propose a methodology to estimate their impact using a monolingual dataset ( \u00a74.2), and present the estimates that we obtain for various tasks and target languages ( \u00a74.3).",
            "score": 0.47120021733817086,
            "section_title": "Analyzing the variance across tasks",
            "char_start_offset": 16967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 742
                },
                {
                    "start": 745,
                    "end": 1027
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 454,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87451171875
        },
        {
            "corpus_id": "3687074",
            "title": "A resource-light method for cross-lingual semantic textual similarity",
            "text": "To create multilingual versions of this dataset, we translated one sentence of each pair to Spanish, Italian, and Croatian. Definitional sentences of the OnWN-12 dataset vary in length much more than the MSRVid-12 dataset. \n\nFor all of the dataset, we created the missing cross-lingual and monolingual variants by first performing automated machine translation with Google translate and then letting native speakers of target languages fix the errors introduced by machine translation.9 \n\nWe summarize the key information about the datasets in Table 3. We observe that the datasets are mutually very different in terms of average sentence length, with News-16 dataset sentences being on average five times longer than the MSRVid-12 sentences. MulSrc-16 and OnWN-12 datasets have larger within-dataset relative variance in sentence length than News-16 and MSRVid-12. On top of high sentence-length variance, instances in MulSrc-16 also vary in genre. We believe that evaluating STS models on the collection of datasets with such mutually differing properties (sentence length, genre, and both length-and genre-based homogeneity/heterogeneity can provide more insights into the strengths and weaknesses of different STS algorithm variants.",
            "score": 0.47071108070900264,
            "section_title": "Datasets",
            "char_start_offset": 27745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 486
                },
                {
                    "start": 489,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "202558976",
            "title": "MultiFiT: Efficient Multi-lingual Language Model Fine-tuning",
            "text": "This section provides an overview of our experimental setup; see the appendix for full details.\n\nData We evaluate our models on the Multilingual Document Classification Corpus (MLDoc; Schwenk and Li, 2018)    product reviews in four languages. We provide an overview of the datasets in Table 1.\n\nPretraining We pretrain our models on 100M tokens extracted from the Wikipedia of the corresponding language for 10 epochs. As fewer tokens might be available for some languages, we also compare against a version (no wiki) that uses no pretraining. For all models, we fine-tune the LMs on the target data of the same language for 20 epochs. We perform subword tokenization with the unigram language model (Kudo, 2018).\n\nEvaluation settings We compare two settings based on the availability of source and target language data: supervised and zero-shot. In the supervised setting, every model is fine-tuned and evaluated on examples from the target language.\n\nIn the zero-shot setting, every model is fine-tuned on source language examples and evaluated on target language examples. In all cases, we use English as the source language.\n\nBaselines We compare against the state-ofthe-art cross-lingual embedding models LASER (Artetxe and Schwenk, 2018), which uses a large parallel corpus, multilingual BERT (MultiB-   ERT) 10 , and monolingual BERT 11 . We also compare against the best models on each dataset, Mul-tiCCA (Ammar et al., 2016), a cross-lingual word embedding model, and BiDRL (Zhou et al., 2016), which translates source and target data.\n\nOur methods We evaluate our monolingual LMs in the supervised setting (MultiFit) and our LMs fine-tuned with pseudo labels from LASER in the zero-shot setting (pseudo).  forms the comparison methods as the shared embedding space between many languages is overly restrictive. Our monolingual LMs outperform their cross-lingual teacher LASER in almost every setting. When fine-tuned with only 100 target language examples, they are able to outperform all zero-shot approaches except MultiFiT on DE and FR. This calls into question the need",
            "score": 0.47069294801880907,
            "section_title": "Experimental setup",
            "char_start_offset": 6947,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 205,
                    "matchedPaperCorpusId": "21732778"
                },
                {
                    "start": 701,
                    "end": 713,
                    "matchedPaperCorpusId": "13753208"
                },
                {
                    "start": 1484,
                    "end": 1503,
                    "matchedPaperCorpusId": "14043561"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5654296875
        },
        {
            "corpus_id": "273901352",
            "title": "Language Bias in Multilingual Information Retrieval: The Nature of the Beast and Mitigation Methods",
            "text": "These parallel query sets are from the original dataset, not machine-translated, and are based on human-annotated document tags. In repurposing them as queries and using the tags as relevance judgements, we fashion a multilingual IR collection with massively-multilingual parallel query sets. \n\nOur work makes four main contributions:1 1. Novel evaluation metric for fairness under ranking: we propose the mean rank correlation (MRC) score to evaluate language fairness under MLIR, based on the premise that semantically-equivalent queries in different languages should yield consistent document rankings. 2. Novel dataset: we develop the MultiEup-v2 dataset, consisting of semantically parallel queries and multilingual documents, along with demographic attributes. This dataset serves as a benchmark for future fairness research in MLIR. 3. Quantification of language (un)fairness: we analyze language fairness in MLIR across different languages and language families, and find that BM25 exhibits larger language bias than neural retrieval frameworks like mDPR. Additionally, higher-resource languages tend to be associated with higher degrees of language fairness. 4. Proposal of a new ranking bias mitigation method: we propose the language KLdivergence alignment (LaKDA) loss to mitigate language bias in MLIR, demonstrating its effectiveness within the mDPR neural retrieval framework with multilingual text encoders mBERT and XLM-R.",
            "score": 0.4703156408576297,
            "section_title": "Introduction",
            "char_start_offset": 2108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78466796875
        },
        {
            "corpus_id": "248240015",
            "title": "IndicXNLI: Evaluating Multilingual Inference for Indian Languages",
            "text": "One of our work's key limitations is that the dataset IndicXNLI was created by machine translation of the original English XNLI dataset. Although In-dicXNLI is not human translated, it has been carefully evaluated for translation accuracy by a number of natural bilingual Indic speakers (2 for each language). Furthermore, as shown in our research (Table 2), employing automatic assessment measures such as round trip English-English evaluation via back translation and direct Indic-English sentence comparison is effective. In the past, such a metric has been shown to be highly beneficial for comparing without-reference machine translation (Bapna et al., 2022;Huang, 1990;Moon et al., 2020a,b). Furthermore, as did with the Hindi dataset in Appendix E, we might use correlation in the prediction score between human and machine translated sets for evaluating translation quality. \n\nSecond, adapting an existing dataset risks transferring biases and shortcomings from the original XNLI dataset into ours. However, it has been established that XNLI is a typical benchmark for evaluating multilingual and cross-lingual sentence representation, and it has been used to evaluate several multilingual models (Conneau et al., 2020b;Hu et al., 2020b). Morphological analysis of related languages, as well as insights into their performance behavior, may be useful. The authors, however, are not experts in that area, and such an assessment would have been outside the scope of the current work. This study might be expanded to include language groups other than Indian languages such as Indo-European. Third, because of limited resources, the current study did not include large versions of well-known models such as XLM-RoBERTa-Large and MuRIL-Large. However, for IndicBert, mBERT, XLM-RoBERTa, and MuRIL, we assessed model performance in relation to model size (#parameters) in Table 5.",
            "score": 0.4702623542225353,
            "section_title": "Limitations",
            "char_start_offset": 13977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 663,
                    "end": 675,
                    "matchedPaperCorpusId": "2175391"
                },
                {
                    "start": 1205,
                    "end": 1228,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1228,
                    "end": 1245,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "270703301",
            "title": "M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models",
            "text": "(more in Section 6). Finally, to demonstrate that the importance lies in the use of synthetic Evols rather than simply increasing the amount of seed-like data, we sampled an additional 86K IR pairs from the Aya dataset and collection and replace it with synthetic generated Evols and observed that the performance decreases across all benchmarks, especially in MT-Bench and MGSM by 1.1 and 3.38 points respectively (results are shown in Section 9.5). \n\nImportance of multi-turn Evol. The Seed + Evol + MT rows in Table 2 shows performance after adding synthetically generated turns using multi-turn Evol. This boosts performance in MT-Bench evaluations substantially by 0.8 points with the most significant gain of 1.31 and 1.0 points on French and Japanese for Mistral-7B model (Table 12). Adding multi-turn data also helps in multilingual benchmarks as the results consistently improve by 3 \u2212 4% across all evaluations. Additional results are shown in Appendix, Table 12   Table 5: Low-resource evaluation across 10 languages from Flores200. We present BLEU scores for translating each language into every other language. The final score for a language is calculated as the average BLEU score across all its translations to the remaining languages. \n\nComparison with Human-AI generated data in the wild. For completeness, we also include performance comparisons with Human-AI generated datasets collected from voluntary participation, such as OpenAssistant, ShareGPT, and WildChat in  5. However, we would like to point out that these datasets are not focused specifically towards improving multilingual abilities, even though their creation methods lend inadvertently to multilingual data. For holistic comparison, we also include details with the next best performing dataset WildChat in further analysis. \n\nLow-resource languages. Table 4 shows the results on 6 languages. M2Lingual performs better than all the baselines across both Mistral-7B and LLama-3-8B. Specifically, M2Lingual improves the performance by 1.3 and 0.2 on average for both models respectively. We also evaluate cross-lingual machine translation performance on extremely low resource languages as shown in Table 5.",
            "score": 0.4699421585081842,
            "section_title": "Results",
            "char_start_offset": 15070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2190
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4365234375
        },
        {
            "corpus_id": "253420301",
            "title": "Local Structure Matters Most in Most Languages",
            "text": "All experiments are conducted on three pretrained cross-lingual models. The XLM-RoBERTa-Base (Lample and Conneau, 2019), BERT-Base-Multilingual-Cased (Devlin et al., 2019) 2: Plotted are the relations between the different choices of metrics measuring the amount of perturbation and the average performance of all 3 models on all tested datasets. Left is more perturbed, up is better performance. The X-axis of the IDC metric is inverted for clearer comparison. \n\nCanine-S (Clark et al., 2021) model are used. The Canine model is a tokenization-free pretrained model, which lets us isolate the impact of subword destruction on the findings. \n\nThe zero-shot cross-lingual setting (Hu et al., 2020) is used for all experiments. The model is first finetuned on the English version of the dataset and evaluated without further tuning on all target languages. \n\nThe English version on which the model is finetuned is kept unperturbed, while the target language text on which the model is evaluated goes through several perturbations. We perform a total of 43 different perturbations on every task and language and obtain their performance. All models are finetuned on five different random seeds, and all perturbations are performed on five different random seeds, for a total of 25 evaluations for every model on every task, every language present in the tasks, and every perturbation setting. 1  A total of 8 cross-lingual tasks selected from the most popular cross-lingual benchmarks (Hu et al., 2020;Liang et al., 2020;Ponti et al., 2020) covering over 120 languages are used for evaluation. 2 Summary information of the tasks can be found in Table 1. 3",
            "score": 0.46970351932363646,
            "section_title": "Experimental Details",
            "char_start_offset": 5624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1652
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 171,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44091796875
        },
        {
            "corpus_id": "261557946",
            "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
            "text": "Since collecting human relevance labels is laborious and costly, other studies have adopted workarounds to build multilingual datasets. For example, Bonifacio et al. ( 2021) automatically translated the MS MARCO dataset (Bajaj et al., 2018) from English into 13 other languages. However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020;Lembersky et al., 2012;Volansky et al., 2015;Avner et al., 2016;Eetemadi and Toutanova, 2014;Rabinovich and Wintner, 2015) and may lead to training data of questionable value. Alternatively, Sun and Duh (2020) built synthetic bilingual retrieval datasets in a resource called CLIRMatrix based on the parallel structure of Wikipedia that covers 139 languages. Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches.",
            "score": 0.4696263367219294,
            "section_title": "Comparison to Synthetic Datasets",
            "char_start_offset": 6780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1291
                }
            ],
            "ref_mentions": [
                {
                    "start": 367,
                    "end": 387,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 387,
                    "end": 410,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 451,
                    "end": 480,
                    "matchedPaperCorpusId": "15699208"
                },
                {
                    "start": 480,
                    "end": 509,
                    "matchedPaperCorpusId": "940724"
                },
                {
                    "start": 578,
                    "end": 596,
                    "matchedPaperCorpusId": "226262282"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.908203125
        },
        {
            "corpus_id": "225075677",
            "title": "Cross-lingual Machine Reading Comprehension with Language Branch Knowledge Distillation",
            "text": "To verify the effectiveness of our method. We use the following datasets to conduct our experiments. \n\nMLQA A cross-lingual machine reading comprehension benchmark (Lewis et al., 2019). The instances in MLQA cover 7 languages. We evaluate our method on three languages (English, German, Spanish) with translation training method, and also test our method under the setting of zero-shot transfer on the other three languages (Arabic, Hindi, Vietnamese). \n\nXQuAD Another cross-lingual question answering dataset (Artetxe et al., 2019). XQuAD contains instances in 11 languages, and we cover 9 languages in our experiments. Similar to the setting above, we evaluate our method on English, German, Spanish. In addition, we test our method on Arabic, Hindi, Vietnamese, Greek, Russian, and Turkish under the setting of zero-shot transfer.",
            "score": 0.4695266705444181,
            "section_title": "Datasets and Evaluation Metrics",
            "char_start_offset": 18117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 100
                },
                {
                    "start": 103,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76220703125
        },
        {
            "corpus_id": "236171330",
            "title": "Evaluation of contextual embeddings on less-resourced languages",
            "text": "We split our evaluations into two categories: monolingual and cross-lingual. In the monolingual evaluation, we compare fastText, ELMo, monolingual BERTlike models (English, Russian, Finnish, Swedish, Slovene, Croatian, Estonian, and Latvian), trilingual BERT models (FinEst, CroSloEngual, and LitLat BERT), and massively multilingual BERT models (mBERT and XLM-R). The exact choice of compared models depends on the availability of datasets in specific languages. In the cross-lingual setting, we compare cross-lingual maps for ELMo models, massively multilingual BERT models, and trilingual BERT models. The specifics of models for individual tasks are described below.",
            "score": 0.4689098919115845,
            "section_title": "Evaluation settings",
            "char_start_offset": 40622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 670
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.440185546875
        },
        {
            "corpus_id": "202537648",
            "title": "Investigating Multilingual NMT Representations at Scale",
            "text": "Multilingual Neural Machine Translation (NMT) models have yielded large empirical success in transfer learning settings. However, these black-box representations are poorly understood, and their mode of transfer remains elusive. In this work, we attempt to understand massively multilingual NMT representations (with 103 languages) using Singular Value Canonical Correlation Analysis (SVCCA), a representation similarity framework that allows us to compare representations across different languages, layers and models. Our analysis validates several empirical results and long-standing intuitions, and unveils new observations regarding how representations evolve in a multilingual translation model. We draw three major results from our analysis, with implications on cross-lingual transfer learning: (i) Encoder representations of different languages cluster based on linguistic similarity, (ii) Representations of a source language learned by the encoder are dependent on the target language, and vice-versa, and (iii) Representations of high resource and/or linguistically similar languages are more robust when fine-tuning on an arbitrary language pair, which is critical to determining how much cross-lingual transfer can be expected in a zero or few-shot setting. We further connect our findings with existing empirical observations in multilingual NMT and transfer learning.",
            "score": 0.46874609000133477,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.421142578125
        },
        {
            "corpus_id": "236447339",
            "title": "QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension",
            "text": "In the third place we have Russian, which a version of SQuAD [88], a dataset for open-domain QA over Wikidata [152], a boolean QA dataset [105], and datasets for cloze-style commonsense reasoning and multi-choice, multi-hop RC [95]. \n\nThe fourth best resourced language is Japanese, with a Cloze RC dataset [303], a manual translation of a part of SQuAD [13], and a commonsense reasoning resource [212]. \n\nThree more languages have their versions of SQuAD [235]: French [78,144], Vietnamese [210], and Korean [171], \n\nand there are three more small-scale evaluation sets (independently collected for Arabic [205]), human-translated to French [13]). Polish has a small dataset of open-domain questions based on Wikipedia \"Did you know...?\" data [188]. And, to the best of our knowledge, this is it: not even the relatively well-resourced languages like German necessarily have any monolingual QA/RC data. There is more data for individual languages that is part of multilingual benchmarks, but that comes with a different set of issues ( \u00a77.2). \n\nIn the absence of data, the researchers resort to machine translation of English resources. For instance, there is such SQuAD data for Spanish [43], Arabic [205], Italian [70], Korean [158]. However, this has clear limitations: machine translation comes with its own problems and artifacts, and in terms of content even the best translations could differ from the questions that would be \"naturally\" asked by the speakers of different languages. \n\nThe fact that so few languages have many high-quality QA/RC resources reflecting the idiosyncrasies and information needs of the speakers of their languages says a lot about the current distribution of funding for data development, and the NLP community appetite for publishing non-English data at top NLP conferences. There are reports of reviewer bias [248]: such work may be perceived as \"niche\" and low-impact, which makes it look like a natural candidate for second-tier venues 9 , which makes such work hard to pursue for early career researchers.",
            "score": 0.4685702398451248,
            "section_title": "Manuscript submitted to ACM",
            "char_start_offset": 54531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 403
                },
                {
                    "start": 406,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1491
                },
                {
                    "start": 1494,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2047
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "227230829"
                },
                {
                    "start": 307,
                    "end": 312,
                    "matchedPaperCorpusId": "218973931"
                },
                {
                    "start": 397,
                    "end": 402,
                    "matchedPaperCorpusId": "226262292"
                },
                {
                    "start": 456,
                    "end": 461,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "218974030"
                },
                {
                    "start": 491,
                    "end": 496,
                    "matchedPaperCorpusId": "222066698"
                },
                {
                    "start": 607,
                    "end": 612,
                    "matchedPaperCorpusId": "189762189"
                },
                {
                    "start": 744,
                    "end": 749,
                    "matchedPaperCorpusId": "61855958"
                },
                {
                    "start": 1202,
                    "end": 1207,
                    "matchedPaperCorpusId": "189762189"
                },
                {
                    "start": 1217,
                    "end": 1221,
                    "matchedPaperCorpusId": "201827300"
                },
                {
                    "start": 1230,
                    "end": 1235,
                    "matchedPaperCorpusId": "21702843"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "273404117",
            "title": "Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs",
            "text": "Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.",
            "score": 0.4681766235368167,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77294921875
        },
        {
            "corpus_id": "250390644",
            "title": "Bilingual Tabular Inference: A Case Study on Indic Languages",
            "text": "Multilingual, and specifically Cross-Lingual transfer (Deshpande et al., 2021;Patil et al., 2022, and other), has been widely discussed in the context of low resource languages. Several datasets (Conneau et al., 2018;Yang et al., 2019;Ponti et al., 2020;Artetxe et al., 2020;Nivre et al., 2016;Lewis et al., 2021, and others), benchmarks and leaderboards (Hu et al., 2020;Liang et al., 2020;Ruder et al., 2021;Khanuja et al., 2021b, and others), and evaluation frameworks (Tarunesh et al., 2021;K et al., 2021;Srinivasan et al., 2021) have emerged which focus entirely on evaluation of multilingual NLU. Further, multilingual language models have been developed for (a.) Natural Language Understanding (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020;Chi et al., 2021;Chung et al., 2021, and others), (b.) and Natural Language Generation (Xue et al., 2021;Fan et al., 2021, and others). \n\nIndic Resources. Indic NLP, recently, has seen a recent surge in the number of datasets (Ramesh et al., 2022;Roark et al., 2020;Haddow and Kirefu, 2020a;Abadji et al., 2022;Kolluru et al., 2021, and others), multilingual models (Dabre et al., 2021;Kakwani et al., 2020;Khanuja et al., 2021a, and others), toolkits (Arora, 2020;Bhat et al., 2015;Jain et al., 2020, and others), translation systems (Ramesh et al., 2022), and dedicated benchmarks for evaluation (Kakwani et al., 2020;Krishna et al., 2021).",
            "score": 0.4678924347628106,
            "section_title": "Related Work",
            "char_start_offset": 24142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1411
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 235,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 235,
                    "end": 254,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 275,
                    "end": 294,
                    "matchedPaperCorpusId": "17954486"
                },
                {
                    "start": 391,
                    "end": 410,
                    "matchedPaperCorpusId": "233241004"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20068359375
        },
        {
            "corpus_id": "214641214",
            "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
            "text": "We show the main results in Table 2. XLM-R is the best-performing zero-shot transfer model and generally improves upon mBERT significantly. The improvement is smaller, however, for the structured prediction tasks. MMTE achieves performance competitive with mBERT on most tasks, with stronger results on XNLI, POS, and BUCC. \n\nIf a strong MT system is available, translating the training sets provides improvements over using the same model with zero-shot transfer. Translating the test data provides similar benefits compared to translating the training data and is particularly effective for the more complex QA tasks, while being more expensive during inference time. While using an MT system as a black box leads to strong baselines, the MT system could be further improved in the context of data augmentation. \n\nFor the tasks where in-language training data is available, multilingual models trained on in-language data outperform zero-shot transfer models. However, zero-shot transfer models nevertheless outperform multilingual models trained on only 1,000 in-language examples on the complex QA tasks as long as more samples in English are available. For the structured prediction tasks, 1,000 in-language examples enable the model to achieve performance that is similar to being trained on the full labelled dataset, similar to findings for classification (Eisenschlos et al., 2019). Finally, multitask learning on the Translate-train and In-language setting generally improves upon single language training. \n\nCross-lingual transfer gap For a number of representative models, we show the cross-lingual transfer gap, i.e. the difference between the performance on the English test set and all other languages in Table 3. 13 While powerful models such as XLM-R reduce the gap significantly compared to mBERT for challenging tasks such as XQuAD and MLQA, they do not have the same impact on the syntactic structured prediction tasks. On the classification tasks, the transfer",
            "score": 0.4673638658464909,
            "section_title": "Overall results",
            "char_start_offset": 20556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 813
                },
                {
                    "start": 816,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 1981
                }
            ],
            "ref_mentions": [
                {
                    "start": 1364,
                    "end": 1390,
                    "matchedPaperCorpusId": "202558976"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "253098755",
            "title": "Multilingual Auxiliary Tasks Training: Bridging the Gap between Languages for Zero-Shot Transfer of Hate Speech Detection Models",
            "text": "Using a multitask architecture (van der Goot et al., 2021b) on top of a multilingual model, we investigate the impact of auxiliary tasks operating at different sentence linguistics levels (POS Tagging, Named Entity Recognition (NER), Dependency Parsing and Sentiment analysis) on the transfer effectiveness. Using Nozza (2021)'s original set of languages and datasets (hate speech against women and immigrants, from Twitter datasets in English, Italian and Spanish), our main contributions are as follows. \n\n\u2022 Building strictly comparable corpora across languages,2 leading to a thorough evaluation framework, we highlight cases where zeroshot cross-lingual transfer of hate speech detection models fails and diagnose the effect of the choice of the multilingual language model. \u2022 We identify auxiliary tasks with a positive impact on cross-lingual transfer when trained jointly with hate speech detection: sentiment analysis and NER. The impact of syntactic tasks is more mitigated. \u2022 Using the HateCheck test suite (R\u00f6ttger et al., 2021(R\u00f6ttger et al., , 2022)), we identify which hate speech classes of functionalities suffer the most from cross-lingual transfer, highlighting the impact of slurs; and which ones benefit from joint training with multilingual auxiliary tasks.",
            "score": 0.46728777266325694,
            "section_title": "Introduction",
            "char_start_offset": 3515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1278
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 59,
                    "matchedPaperCorpusId": "219124482"
                },
                {
                    "start": 1017,
                    "end": 1038,
                    "matchedPaperCorpusId": "229923551"
                },
                {
                    "start": 1038,
                    "end": 1063,
                    "matchedPaperCorpusId": "249889128"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5625
        },
        {
            "corpus_id": "268357672",
            "title": "CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in Korean",
            "text": "To investigate how LMs can comprehend or generate text in other languages, there have been efforts to construct multilingual datasets.For instance, XGLUE (Liang et al., 2020) encompasses 100 languages that can be employed for both pre-training and evaluating cross-lingual tasks.XTREME (Hu et al., 2020) introduces an evaluation framework for cross-lingual benchmarks, while MEGA (Ahuja et al., 2023) focuses on assessing LLMs, providing 16 NLP datasets ranging from low-resource to high-resource languages.In addition, datasets that primarily focus on specific target languages, such as Chinese, Indian, and African languages, have been introduced (Huang et al., 2023;Doddapaneni et al., 2023;Adebara et al., 2023).\n\nThe popularity of commonsense datasets has increased because they reflect a wide array of sociocultural knowledge shared by humans (Liu and Singh, 2004).These datasets incorporate everyday concepts, such as CommonsenseQA (Talmor et al., 2019), scientific knowledge like ARC (Clark et al., 2018), and simple arithmetic reasoning like GSM8K (Cobbe et al., 2021).These datasets can be seen as a representation of general and practical knowledge that aligns with human intentions.Consequently, certain datasets incorporate or employ translated portions from English datasets (Seo et al., 2022), potentially overlooking subtle linguistic or cultural differences that may not be apparent to the audience (Tandon et al., 2018).Lee et al. 2023a demonstrated that language models fail to capture biases in different languages due to their cultural insensitivity, which can have societal impacts (Tamkin et al., 2021).Furthermore, Ma et al. 2022 emphasized the importance of cultural background and showed that integrating cultural knowledge can improve models performance.These findings illustrate the need for cultural evaluation datasets.However, building a cultural evaluation dataset from scratch is challenging since it entails significant time and resources while relying on translated datasets fails to incorporate cultural knowledge in different languages.",
            "score": 0.46726276635031605,
            "section_title": "Multilingual and Commonsense",
            "char_start_offset": 4062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 279
                },
                {
                    "start": 279,
                    "end": 507
                },
                {
                    "start": 507,
                    "end": 716
                },
                {
                    "start": 718,
                    "end": 871
                },
                {
                    "start": 871,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1626
                },
                {
                    "start": 1626,
                    "end": 1781
                },
                {
                    "start": 1781,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 174,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 849,
                    "end": 870,
                    "matchedPaperCorpusId": "266028051"
                },
                {
                    "start": 939,
                    "end": 960,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 1289,
                    "end": 1307,
                    "matchedPaperCorpusId": "250562882"
                },
                {
                    "start": 1416,
                    "end": 1437,
                    "matchedPaperCorpusId": "3523957"
                },
                {
                    "start": 1639,
                    "end": 1653,
                    "matchedPaperCorpusId": "247762223"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "16794216",
            "title": "Is Machine Translation Getting Better over Time?",
            "text": "There are several reasons why the assessment of MT quality is difficult. Ideally, each judge should be a native speaker of the target language, while at the same time being highly competent in the source language. Genuinely bilingual people are rare, however. As a result, judges are often people with demonstrated skills in the target language, and a working knowledge -often self-assessedof the source language. Adding to the complexity is the discipline that is required: the task is cognitively difficult and time-consuming when done properly. The judge is, in essence, being asked to decide if the supplied translations are what they would have generated if they were asked to do the same translation. \n\nThe assessment task itself is typically structured as follows: the source segment (a sentence or a phrase), plus five alternative translations and a \"reference\" translation are displayed. The judge is then asked to assign a rank order to the five translations, from best to worst. A set of pairwise preferences are then inferred, and used to generate system rankings, without any explicit formation of stand-alone system \"scores\". \n\nThis structure introduces the risk that judges will only compare translations against the reference translation. Certainly, judges will vary in the degree they rely on the reference translation, which will in turn impact on inter-judge inconsistency. For instance, even when expert judges do assessments, it is possible that they use the reference translation as a substitute for reading the source input, or do not read the source input at all. And if crowd-sourcing is used, can we really expect high proportions of workers to put the additional effort into reading and understanding the source input when a reference translation (probably in their native language) is displayed? In response to this potential variability in how annotators go about the assessment task, we trial assessments of adequacy in which the source input is not displayed to human judges. We structure assessments as a monolingual task and pose them in such a way that the focus is on comparing the meaning of reference translations and system outputs. 2 e therefore ask human judges to assess the degree to which the system output conveys the same meaning as the reference translation. In this way, we focus the human judge indirectly on the question we wish to answer when assessing MT: does the translation convey the meaning of the source?",
            "score": 0.4670649965627091,
            "section_title": "Monolingual Human Evaluation",
            "char_start_offset": 6636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2304
                },
                {
                    "start": 2305,
                    "end": 2461
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "260155190",
            "title": "Towards Bridging the Digital Language Divide",
            "text": "The diversity-diminishing bias towards the English language and Anglo-Saxon cultures is also found in other domains that are well-known to be diverse across languages: family relationships, school systems, etc. \n\nApplying the definition from Section 2, we compute the linguistic bias of the lexical models of four multilingual lexical databases: the first and second versions of the Open Multilingual Wordnet (OMW, OMW2) [11,12], IndoWord-Net (IWN) [6], and BabelNet (BN) [30]. The figures were not computed from the actual contents of the databases-that are necessarily incomplete and thus are not representative of their structural properties-but rather from synthetic data and a gold-standard cross-lingual mapping dataset, both obtained from [19]. Coverages are theoretical in the sense that they represent the percentage of cross-lingual mapping relationships that each model is able to represent with respect to the gold standard mappings. The latter consist of three mapping relation types: equivalent meaning, broader/narrower meaning, and lexical untranslatability. \n\n\u2022 technology t: multilingual lexical databases {OMW, OMW2, IWN, BN}; \n\n\u2022 operation o t : cross-lingual mapping of word meanings; \n\n\u2022 languages L: {English, French, Italian, Chinese, Hindi, Tamil, Malayalam, Hungarian, Mongolian}; \n\n\u2022 utterances U : 32 concepts from six linguistically diverse domains, lexicalised by the languages above as 160 word senses and 128 lexical gaps; \n\n\u2022 performance Perf: defined as the coverage (recall) of gold-standard crosslingual mappings that the lexical database is able to express. \n\nFigure 2 shows the results. We are not concerned with absolute coverage values but rather with how coverage varies across languages. OMW shows a marked bias towards European languages and English in particular (68% coverage) while Asian languages are mapped suboptimally (49-51%), with a bias of b OMW = 6.53%. This is explained by the fact that pivot concepts in OMW are limited to the meanings of English words.",
            "score": 0.4670028754662201,
            "section_title": "Bias in Language Technology",
            "char_start_offset": 15326,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1074
                },
                {
                    "start": 1077,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1306
                },
                {
                    "start": 1309,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2010
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "267930957"
                },
                {
                    "start": 425,
                    "end": 428,
                    "matchedPaperCorpusId": "218974121"
                },
                {
                    "start": 472,
                    "end": 476,
                    "matchedPaperCorpusId": "2085726"
                },
                {
                    "start": 746,
                    "end": 750,
                    "matchedPaperCorpusId": "256105656"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310302734375
        },
        {
            "corpus_id": "234679313",
            "title": "A cost-benefit analysis of cross-lingual transfer methods",
            "text": "Our main contribution is to evaluate crosslingual transfer methods while also considering their financial and computational costs. In addition, we show that automatically translating question answering datasets is not trivial and propose a new method for translating them. Finally, while exploring the best cross-lingual methods, we achieved the state-of-the-art in two datasets in a low-resource language, thus showing that our methodology is sound. \n\nRecent work shows that multilingual models fine-tuned on a specific task perform well on the same task on a different language never seen during fine-tuning (Pires et al., 2019a;Artetxe et al., 2019;Phang et al., 2020;Conneau and Lample, 2019). This zero-shot crosslingual ability allows one to use these models on tasks in languages in which annotated data is scarce. Their effectiveness is quite surprising because they are not generally pretrained in any cross-lingual objective. This behavior fostered several studies that aimed to understand and explore it. For example, Wu and Dredze (2019) explored the cross-lingual potential of multilingual BERT (mBERT) as a zero-shot language transfer model for NLP tasks such as named-entity recognition (NER) and parsing. They further observed that mBERT performs better in languages that share many subwords. Pires et al. (2019b) have shown that mBERT has good zero-shot cross-lingual transfer performance on NER and POS tagging tasks. Artetxe et al. (2019) concluded that neither shared subwords vocabulary nor joint training across multiple languages are necessary to obtain cross-lingual abilities. They have shown that monolingual models are also capable of performing cross-lingual transfer. K et al. (2019) used mBERT to study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives on the generalization ability of crosslingual language models. The experiments were conducted in three typologically different languages and they concluded that the lexical overlap between languages contributes little to the cross-lingual success, while the depth of the network plays an important role.",
            "score": 0.46668713139845314,
            "section_title": "Introduction",
            "char_start_offset": 2103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 1309,
                    "end": 1329,
                    "matchedPaperCorpusId": "174798142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.430908203125
        },
        {
            "corpus_id": "219026125",
            "title": "A Review on Multi-Lingual Sentiment Analysis by Machine Learning Methods",
            "text": "This experiment proves the inadequacy of individual methods and the strength of combinatorial models. Meng et al. [31] uses a Cross-Lingual Mixture Model which gives better coverage of vocabulary, using a Naive Bayes classifier, both in case of availability or unavailability of labelled data in the target language, i.e. Chinese. The method can do better by expansion of vocabulary. Balahur and Turchi [32,33] performed experiments on French, German and Spanish data using Machine Translation on three different engines, and employed an SVM classifier, to prove that machine translation gives comparable results to native corpus in the target language. The results are restricted by translation quality and the introduction of noise by addition of translated data together. \n\nDemirtas and Pechenizkiy [34] explore the effectiveness of Machine Translation of Turkish data and claim that increase in size of training data doesn't necessarily improve on accuracy of the classification, using a Naive Bayes classifier, and increase in accuracy can rather be brought about by semi-supervised learning. It doesn't show significant improvement in accuracy with subsequent iterations, which can be worked upon. Volkova et al. [35] explored gender differences on Twitter data, both in Spanish and Russian. The results are obtained using various classifiers and compared, showing noticeable improvement by including gender demographics. Better results can be achieved by including age, user preferences, etc. These are the major works in 2013. \n\n2014 has been vital for multi-lingual and cross-lingual sentiment analysis. Lin et al. [36] developed a Cross-Lingual Joint Aspect Sentiment model that simultaneously checks aspect-based opinion expression in both languages. They have worked on hotel reviews in a variety of languages, used SVM classifiers, and found it to be more effective with higher accuracy. The method can perform better with availability of a good amount of data. Hajmohammadi et al. [37] used a cotraining approach using active learning in combination with semi-supervised learning to provide and enriched training data by picking best samples from unlabelled data, on book review datasets of three languages, to provide an improved performance.",
            "score": 0.4666101865926877,
            "section_title": "Multi-lingual and Cross-lingual sentiment analysis using Machine Learning Techniques",
            "char_start_offset": 21416,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 1974
                },
                {
                    "start": 1975,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 118,
                    "matchedPaperCorpusId": "7612427"
                },
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "14292133"
                },
                {
                    "start": 407,
                    "end": 410,
                    "matchedPaperCorpusId": "5492940"
                },
                {
                    "start": 802,
                    "end": 806,
                    "matchedPaperCorpusId": "3912960"
                },
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "2166300"
                },
                {
                    "start": 1624,
                    "end": 1628,
                    "matchedPaperCorpusId": "9982061"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5810546875
        },
        {
            "corpus_id": "276647791",
            "title": "Few-Shot Multilingual Open-Domain QA from 5 Examples",
            "text": "We evaluate FSMODQA on various datasets, including cross-lingual and monolingual retrieval, and multilingual open-domain QA. We observe notable improvements over competitive few-shot baselines, with +5.1% gain on retrieval and +8.4% gain on multilingual open-domain QA. To further test FSMODQA's language adaptation ability, we conduct zero-shot adaptation experiments using our cross-lingual prompting strategy on fifteen languages. This adaptation improves performance in both monolingual retrieval and multilingual QA significantly, achieving results that are superior or comparable to strong translation-based methods. 2",
            "score": 0.46650266032612775,
            "section_title": "Introduction",
            "char_start_offset": 3627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "236428949",
            "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
            "text": "Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios. We evaluate F1 and EM scores over the questions with answer annotations from two datasets, following the common evaluation practice in open QA (Lee et al., 2019). \n\nXOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data). XOR-TYDI QA offers both training and evaluation data. \n\nMKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA. MKQA has evaluation data only; XOR-TYDI QA and MKQA have five languages in common.",
            "score": 0.4664826650761462,
            "section_title": "Datasets and Knowledge Sources",
            "char_start_offset": 11704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1173
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 274,
                    "matchedPaperCorpusId": "173990818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "269214448",
            "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
            "text": ") more than artifacts in a different language in the source-language RM. Suppose, for example, that the target-language RM assigns higher rewards when the generation contains certain targetlanguage words (due to bias in the RM training data). A different-language policy model is unlikely to exploit this, as it rarely generates these words, but a same-language policy model may. This hypothesis is consistent with our observed patterns. First, there are many fewer cases of cross-lingual reward optimization outperforming the monolingual setting when measured by the finetuned target-language RM evaluator than the prompted LM evaluators (Figure 3): under this hypothesis, the finetuned evaluator RMs would be more susceptible to such artifacts and (incorrectly) assign higher scores in the monolingual settings. The underperformance of the translate-train baseline ( \u00a75.1) also provides weak evidence: in princi-Figure 4: Alignment effectiveness, compared to the target-language SFT model judged by PaLM-2-L, and the 95% confidence interval across validation instances. \"source\u2192target\" denotes a source-language RM driving alignment in the target language. Cross-lingual alignment is generally effective, sometimes outperforming monolingual alignment. RL is hard to train for OpenAssistant, in line with what its authors found (K\u00f6pf et al., 2023). \n\nple, a source-language RM and a source-translatedinto-target-language RM should capture the same reward signal, as they are derived from the same data source, and would lead to similar downstream performance. However, the former is less susceptible to reward over-optimization due to the language mismatch, leading to better performance, though this is confounded by translation quality. \n\nCorroborating this hypothesis, we also find that when used monolingually, the RMs behave more like a bag-of-word (BoW) model. We take each of the 6 summarization RMs and infer on the validation set of each dataset in each language (Table 1).",
            "score": 0.46614433896443463,
            "section_title": "2023",
            "char_start_offset": 12734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54052734375
        },
        {
            "corpus_id": "253098629",
            "title": "Multilingual Multimodal Learning with Machine Translated Text",
            "text": "Our paper investigates the benefits and limitations of machine translated data towards multilingual multimodal learning. In doing so, we solely rely on the M2M-100 model (Fan et al., 2021). This is a large, multi-to-multi translation system, which proved to be easy to use. Our analyses and results are based on the performance of this model. It would be instructive to investigate how the expected performance of translation systems6 affects (i) the proportion of sentences with high 'badness' scores, and (ii) the resulting performance of the multilingual multimodal systems. Moreover, while machine translating a large corpus is a cheaper effort than manually translating the data or scraping it from the web, there is still a one-time effort required to translate the data before using it for training new models. Therefore, we release our multilingual pretraining and fine-tuning datasets. \n\nFrom an experimental angle, although the proposed framework can be applied to any existing architecture, we only evaluate a single model due to computational constraints. \n\nWe would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones. \n\nFinally, a core limitation of the overall translate data framework is that it centers English as the source language. For example, this means only concepts mentioned in English captions can be grounded across languages (Liu et al., 2021), and hence some non-English concepts might never be modelled. However, we show that machine translating data provides a strong starting point that can effortlessly be integrated in a pipeline, upon which language-specific annotations can be added.",
            "score": 0.46598169581600146,
            "section_title": "Limitations",
            "char_start_offset": 27304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 170,
                    "end": 188,
                    "matchedPaperCorpusId": "224814118"
                },
                {
                    "start": 1648,
                    "end": 1666,
                    "matchedPaperCorpusId": "238198104"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8955078125
        },
        {
            "corpus_id": "274789680",
            "title": "Beyond Data Quantity: Key Factors Driving Performance in Multilingual Language Models",
            "text": "Multilingual language models (MLLMs) are crucial for handling text across various languages, yet they often show performance disparities due to differences in resource availability and linguistic characteristics. While the impact of pre-train data percentage and model size on performance is well-known, our study reveals additional critical factors that significantly influence MLLM effectiveness. Analyzing a wide range of features, including geographical, linguistic, and resource-related aspects, we focus on the SIB-200 dataset for classification and the Flores-200 dataset for machine translation, using regression models and SHAP values across 204 languages. Our findings identify token similarity and country similarity as pivotal factors, alongside pre-train data and model size, in enhancing model performance. Token similarity facilitates cross-lingual transfer, while country similarity highlights the importance of shared cultural and linguistic contexts. These insights offer valuable guidance for developing more equitable and effective multilingual language models, particularly for underrepresented languages.",
            "score": 0.46591226678062453,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "253762032",
            "title": "From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?",
            "text": "One of the strengths of the COMET metric is its multilinguality -the model has seen over 30 language pairs during training. To assess its quality as a summary evaluation tool for non-English data, we evaluated it on the Multi_SummEval dataset (Koto et al., 2021). With only two system outputs annotated (along the Focus and Coverage dimensions), the size of the resource is not sufficient for reporting system-level correlations. Thus, we report the summary-level (segment-level) Pearson correlations. \n\nFor a fair comparison, we wanted to train the COMES model variant using the multilingual data. Due to the lack of sufficient resources, we fall back on using automatic machine translation to translate the English annotated data. This approach has proven successful for e.g. Question Answering (Lewis et al., 2020b;Mackov\u00e1 and Straka, 2020). We limit our analysis to the subset of languages from Multi_SummEval that originates from the MLSUM (Scialom et al., 2020) corpus. We have translated SummEval into German, French, Russian, Turkish and Spanish using the uni-directional models provided by the Helsinki-NLP group (Tiedemann, 2020) and used the data (together with the original SummEval) to train a multilingual COMES model (COMES_MT_ML). \n\nOur findings indicate that in the summary-level evaluation, the original COMET metric is superior to any other variant considered, clearly outperforming the reference-less variant COMET_QE. Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by Braun et al. (2022), which indicate that summary evaluations do not survive translation. On this dataset, even the best performing COMET is still inferior to both ROUGE and BERTScore. Considering, however, the relatively small size of the dataset (270 instances per language, outputs from two systems) we believe that the question about COMET/COMES usefulness for multilingual and summary-level evaluation is still open. For the full results, see Table 8 in Appendix.",
            "score": 0.46486934474358976,
            "section_title": "Non-English data",
            "char_start_offset": 10373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 501
                },
                {
                    "start": 504,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 243,
                    "end": 262,
                    "matchedPaperCorpusId": "235313819"
                },
                {
                    "start": 797,
                    "end": 818,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 818,
                    "end": 843,
                    "matchedPaperCorpusId": "220347499"
                },
                {
                    "start": 945,
                    "end": 967,
                    "matchedPaperCorpusId": "216868259"
                },
                {
                    "start": 1122,
                    "end": 1139,
                    "matchedPaperCorpusId": "222310761"
                },
                {
                    "start": 1592,
                    "end": 1611,
                    "matchedPaperCorpusId": "237532546"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "258584732",
            "title": "Multilingual text categorization and sentiment analysis: a comparative analysis of the utilization of multilingual approaches for classifying twitter data",
            "text": "Under the scopes of this research work two different datasets were used as the core objects in which the models are utilized and evaluated. At first, the MARC dataset was used to train and evaluate the models on a widely used and tested large multilingual corpus and evaluate their accuracy and cross-lingual transfer inference.\n\nThe initial dataset on which the introduced models were trained and evaluated is a collection of Amazon reviews specifically designed to aid research in multilingual text classification tasks [56]. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each one the records represented into the different language datasets contains an anonymized ''review ID'', an anonymized ''product ID'', an anonymized ''reviewer ID'', the review text as ''review_body'', the ''review_title'', the star rating as ''stars'' attribute, the ''language'' on which the review is written, and the coarse-grained ''product_category'', e.g., ''furniture'', ''sports,'' etc. A respective snapshot with corresponding data from 3 languages, i.e., English, German, and Spanish, is depicted in Fig. 3.\n\nMoreover, the corpus is balanced across a 5-star rating system (1 ''highly negative'' -5 ''highly positive''), so each star rating constitutes 20% of the reviews in each language. For each of the five examined language, there are 200,000, 5000 and 5000 reviews in the training, development, and test sets, respectively. The maximum number of reviews per reviewer is 20 and the maximum number of reviews per product is 20. All reviews are truncated after 2000 characters, and all reviews are at least 20 characters long. Moreover, it should be noted that the number of different product categories represented into the different language datasets are 31 with uneven distributions and occurrences in each of these datasets, as also depicted in Fig. 4. The proposed models were trained and applied on this multilingual dataset to test their accuracy on the identified class, with regard to the multilingual text catego",
            "score": 0.46475993642961305,
            "section_title": "Datasets",
            "char_start_offset": 26590,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 526,
                    "matchedPaperCorpusId": "222141483"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35107421875
        },
        {
            "corpus_id": "208547653",
            "title": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation",
            "text": "We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen 2010), which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation. 1\n\nDatasets. The statistics of Europarl and MultiUN corpora are summarized in Table 1. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr\u2192Es and De\u2192Fr. For distant language pair Ro\u2192De, we extract 1,000 overlapping sentences from new-stest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) (Sennrich, Haddow, and Birch 2015).\n\nFor MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data Experimental Details. We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the attn drop = 0 (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions (Gu et al. 2019). For the model initialization, we use",
            "score": 0.46469971543828253,
            "section_title": "Experiments Setup",
            "char_start_offset": 15904,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 181,
                    "matchedPaperCorpusId": "13222584"
                },
                {
                    "start": 1229,
                    "end": 1263,
                    "matchedPaperCorpusId": "8030425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6337890625
        },
        {
            "corpus_id": "246294502",
            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
            "text": "We also release machine-translated versions of the test sets to enable the evaluation of 'translate test' cross-lingual transfer. \n\nBy virtue of the newly created IGLUE benchmark, we also run the first systematic comparative evaluation of cuttingedge multilingually pretrained V&L architectures (Ni et al., 2021;Zhou et al., 2021;Liu et al., 2021), as well as a series of representative monolingual V&L models combined with machine translation (Lu et al., 2019;Chen et al., 2020, inter alia) across a range of diverse V&L tasks and languages. \n\nThis evaluation offers new insights on the models' core strengths and current limitations. Foreshadowing, we showcase large gaps between performance in English and transfer performance, where (to a limited extent) the gaps are more prominent in lower-resource languages and languages more distant from English. In addition, leveraging target-language in-task few shots is remarkably arduous: whereas in previous text-only experiments (Lauscher et al., 2020), ceteris paribus there are huge gains of few-shot scenarios over their zero-shot counterparts, we demonstrate that current V&L models often require thousands of examples before showing signs of improvement. Finally, performance also seems correlated with task difficulty: for instance, NLI shows the smallest gaps between English and other languages and QA benefits the most from few-shot adaptation. On the other hand, visually grounded reasoning and cross-modal retrieval appear to be harder in both respects. \n\nContributions. 1) In order to guide and inspire more work in the area of multilingual V&L research, we present a first evaluation benchmark for cross-lingual transfer learning for V&L tasks, spanning 20 languages, 5 datasets, and 4 different tasks. 2) In the process of benchmark creation, we complement existing datasets with new training and evaluation data in several languages to increase diversity and enable few-shot learning, and introduce a first multilingual dataset for visually grounded cross-lingual NLI. 3) We conduct systematic evaluations of representative V&L architectures in zero-shot and few-shot cross-lingual transfer scenarios, offering standard data splits and empirical baselines for future research.",
            "score": 0.4645556586766535,
            "section_title": "Introduction",
            "char_start_offset": 3104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 132,
                    "end": 542
                },
                {
                    "start": 545,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1514
                },
                {
                    "start": 1517,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 330,
                    "matchedPaperCorpusId": "232478575"
                },
                {
                    "start": 330,
                    "end": 347,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 444,
                    "end": 461,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 461,
                    "end": 478,
                    "matchedPaperCorpusId": "216080982"
                },
                {
                    "start": 979,
                    "end": 1002,
                    "matchedPaperCorpusId": "226262344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "243865167",
            "title": "Wino-X: Multilingual Winograd Schemas for Commonsense Reasoning and Coreference Resolution",
            "text": "Since our work introduces a novel resource, we include a Data Statement (Bender and Friedman, 2018) as a concise overview of its provenance and construction. We hope this will motivate the research community to adopt the dataset for projects relating to cross-lingual natural language understanding by increasing transparency.\n\nA. CURATION RATIONALE: We discuss the filtering criteria applied to WinoGrande samples and their translations in \u00a72.2 and \u00a7A.2. In enforcing conservative selection criteria, our aim is to ensure grammaticality of the semi-automatically constructed samples and to minimize the percentage of undecidable or disfluent instances.\n\nB. LANGUAGE VARIETY: The collected dataset contains English, German, French, and Russian sentences. English sentences were authored by human crowd-workers, while translations into other languages were obtained from an online translation service. Since (Sakaguchi et al., 2020) do not provide demographics of workers involved in data collection, we cannot report on the dominant variety of English. Due to their origin, translations into DE, FR, and RU are likely to exhibit features of neural translationese (Graham et al., 2020).\n\nC. SPEAKER DEMOGRAPHIC: N/A D. ANNOTATOR DEMOGRAPHIC: We appropriate this section to summarize the demographics of raters involved in evaluating the dataset quality, as detailed in \u00a72.2. Of the 6 annotators involved (two per language pair), all were bilingual speakers with native or native-like proficiency in both English and German / French / Russian. All six were of European origin, between 25-35 years of age, and held a graduate degree. Four of the raters identified as female and two as male.\n\nE. SPEECH SITUATION: The dataset was constructed semi-automatically using scripts distributed in the project's repository. Raters submitted their judgments in the course of a single week and had the opportunity to contact the primary author with clarifying questions.\n\nF. TEXT CHARACTERISTICS: Wino-X contains a collection of cross-lingual and multilingual Winograd schemas for the study of coreference resolution and commonsense reasoning in NMT models and MLLMs. Due to the relative simplicity of scenarios described by the schemas, it is highly unlikely for the dataset to have significant ethical implications.\n\nG Yuqing",
            "score": 0.46436245511891777,
            "section_title": "Ethical Considerations",
            "char_start_offset": 25277,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436767578125
        },
        {
            "corpus_id": "246430787",
            "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
            "text": "We evaluate and compare the baselines for each task along the following axes: (i) different multilingual pretrained models; (ii) cross-lingual transfer approaches; (iii) in-domain versus cross-domain. \n\nMultilingual Pretrained Models. For crosslingual transfer based on multilingual pretrained models, we abide by the standard procedure where the entire set of encoder parameters and the taskspecific classifier head are fine-tuned. We evaluate the following pretrained language models: (i) for NLU and DST, we use the Base variants of multilingual BERT (mBERT; Devlin et al., 2019) and XLM on RoBERTa (XLM-R; Conneau et al., 2020); for intent detection and slot labelling, we evaluate both a model that jointly learns the two tasks (Xu et al., 2020) as well as separate task-specific models; (ii) for E2E modelling, we use multilingual T5 (mT5; Xue et al., 2021), a sequence-to-sequence model, as it demonstrated to be the strongest baseline for cross-lingual dialogue generation (Lin et al., 2021). \n\nCross-lingual Transfer. We focus on two standard methods of cross-lingual transfer: (i) transfer based on multilingual pretrained models and (ii) translatetest (Hu et al., 2020). In (i), a Transformer-based encoder is pretrained on multiple languages with a language modelling objective, yielding strong cross-lingual representations that enable zero-shot model transfer. In (ii), test data in a target language are translated into English via a translation system. To this end, we compare translations obtained via Google Translate (GTr) 7 and MarianMT (Junczys-Dowmunt et al., 2018). \n\nFor end-to-end training, we set up two additional cross-lingual baselines, similar to Lin et al. (2021). In few-shot fine-tuning (FF), after the model is trained on the source language data (English), it is further fine-tuned on a small number of target language dialogues.",
            "score": 0.46427283198720676,
            "section_title": "Baselines and Experimental Setup",
            "char_start_offset": 28253,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 1000
                },
                {
                    "start": 1003,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 582,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 610,
                    "end": 631,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 733,
                    "end": 750,
                    "matchedPaperCorpusId": "216641842"
                },
                {
                    "start": 1163,
                    "end": 1180,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1557,
                    "end": 1587,
                    "matchedPaperCorpusId": "4623739"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "256598350",
            "title": "Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval",
            "text": "Cross-lingual Zero-shot Transfer. This setting is most widely adopted for the evaluation of multilingual scenarios with English as the source language, as many tasks only have labeled train data available in English. Concretely, the models are fine-tuned on English labeled data and then evaluated on the test data in the target languages. It also facilitates evaluation as models only need to be trained once and can be evaluated on all other languages. For Mr. TyDi dataset, the original paper adopt the Natural Questions data (Kwiatkowski et al., 2019) for fine-tuning while later Zhang et al. (2022) suggests fine-tuning on MS MARCO for better results, so we fine-tune on MARCO when compared with best-reported results and on NQ otherwise. For XOR Retrieve, we fine-tune on NQ dataset as the original paper (Asai et al., 2021a). For Mewsli-X and LAReQA, we follow the settings in XTREME-R, where Mewsli-X on a predefined set of English-only mention-entity pairs and LAReQA on the English QA pairs from SQuAD v1.1 train set. \n\nMulti-lingual Fine-tune. For the tasks where multi-lingual training data is available, we additionally compare the performance when jointly trained on the combined training data of all languages. Following the setting of Mr. TyDi and XOR Retrieve, we pre-fine-tune models as in Cross-lingual Zero-shot Transfer and then fine-tune on multi-lingual data if available. For the Mewsli-X and LAReQA, there is no available multi-lingual labeled data.",
            "score": 0.4640456618724992,
            "section_title": "EVALUATION SETTINGS",
            "char_start_offset": 19612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1027
                },
                {
                    "start": 1030,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1474
                }
            ],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 603,
                    "matchedPaperCorpusId": "237213465"
                },
                {
                    "start": 811,
                    "end": 831,
                    "matchedPaperCorpusId": "225040672"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.603515625
        },
        {
            "corpus_id": "257532371",
            "title": "PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning",
            "text": "In our study, an evaluation set is created by directly annotating languages other than English to remove the bias of the machine translation (MT) model in the evaluation phase. However, in the training phase, the dataset size is too large to annotate directly in multiple languages other than English. Therefore, the pre-training set and the fine-tuning set are translated into other languages by utilizing the MT model, so we have no choice but to depend on the performance of the MT model and avoid model bias. \n\nRecently, Reimers and Gurevych (2019) released the MultiLingual CLIP based on the ViT-B/32 CLIP model * . We constructed an MCS with this model as the backbone and measured human correlation using M-CapEval1k. The results showed a slightly lower correlation compared to the MCS with our implemented Multilingual Text Encoder, but were still similar (average Kendall score for 5 languages: Sentenceformer based MCS -0.249, our MCS -0.255). However, as this model is a large-scale model, it has the disadvantage of being slow in inference speed as an automatic metric. We conducted finetuning with our more lightweight model with high human correlation. In future work, if experiments are conducted with this large-scale model, additional analysis on our proposed methodology and new metrics can be done through a variety of experimental results and interpretations. Furthermore, trends in large-scale models can also be confirmed.",
            "score": 0.4639200959300265,
            "section_title": "Limitations Model bias of machine translation in training",
            "char_start_offset": 28484,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1379
                },
                {
                    "start": 1380,
                    "end": 1444
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80078125
        },
        {
            "corpus_id": "277467567",
            "title": "Do Chinese models speak Chinese languages?",
            "text": "Datasets that allow comparable evaluations across a range of high-and low-resource languages are scarce, but we identify three datasets that allow us to quantify model performance on multilingual data through three experiments. These experiments include both task-agnostic and task-dependent evaluations. \n\nExperiment 1: Information Parity We conduct a task-agnostic evaluation utilizing the FLORES+ benchmark for multilingual machine translation (Goyal et al., 2022;NLLB Team et al., 2024). The benchmark has 997 English samples sentences from Wiki sources. 7 These sentences are translated into target languages by native speakers and professional translators. This high quality dataset includes parallel translations across around 200 language variants, including many low-resource languages. Example English sentences are in appendix C. \n\nTo measure multilingual capabilities, we calculate Information Parity (IP), an index proposed by Tsvetkov & Kipnis (2024). IP aims to measure the comparative efficiency of representing the same information in a reference language R to a target language L. Suppose a text input in the reference language text R is translated to a target language text L , and NLL(text) is the sum of negative log-likelihood of a text input (refer to Appendix B for formal definition), IP is defined by: With base models, Chinese models have higher accuracy than Western models in most languages except Burmese, Lao, Jingpho, and Tibetan. But both groups are similar on their instructiontuned models across all languages. We notice that DeepSeek-R1-Qwen is more competitive with its custom chat template (See Appendix Figure 7). Since we apply a consistent prompt across all models for comparability, we exclude DeepSeek-R1-Qwen from the grouped bar results. Instead, we highlight its performance with the more effective chat template using a green triangle in (a). \n\nWe use English as the reference language for text R because all models have been shown to be good at it. Higher IP score means higher efficiency in language-agnostic information representation. In other words, language input text L with higher IP(text L ) score means closer alignment with English reference input text R .",
            "score": 0.4638364736926044,
            "section_title": "Data and Experiments",
            "char_start_offset": 10303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 840
                },
                {
                    "start": 843,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 1996
                },
                {
                    "start": 1997,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 447,
                    "end": 467,
                    "matchedPaperCorpusId": "235358129"
                },
                {
                    "start": 467,
                    "end": 490,
                    "matchedPaperCorpusId": "270282859"
                },
                {
                    "start": 940,
                    "end": 964,
                    "matchedPaperCorpusId": "274060463"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61767578125
        },
        {
            "corpus_id": "220936592",
            "title": "Multilingual Translation with Extensible Multilingual Pretraining and Finetuning",
            "text": "To demonstrate the impact of multilingual finetuning on additional languages, we create the ML50 Benchmark. ML50 standardizes the training and evaluation schemes across 50 different languages, from extremely low resource languages like Xhosa and Gujarati to high resource languages like French and German. The full list of languages is shown in Table 3. We group the languages into five categories based on the amount of available training data: more than 10M pairs (8 languages), 1M to 10M pairs (5 languages), 100k to 1M pairs (17 languages), 10K to 100K pairs (13 languages), and finally, less than 10K pairs of training data (5 languages). ML50 includes languages in N language families, from Germanic and Romance languages to Indic and African ones. Many additional languages we contribute are lower resource, compared to the languages in the original mBART. \n\nTraining Data We gather parallel data between English and 49 other languages to form ML50, to enable the training of machine translation models. We select these 49 languages based on the amount of parallel and monolingual data to cover languages with different amount of resources and under different language families. The quantity of available monolingual data is relevant for pretraining, so we want to ensure there is a sufficient amount. All of the data is publicly available, such as WMT, IWSLT, WAT, TED, and other published research works. For training data, each language pair can include multiple sources. We simply concatenate them together and remove duplicated source-target sentence pairs for each language pair. We use fasttext (Joulin et al., 2017) to perform language identification on both source and target sentences, and we remove sentences pairs if either source or target sentence is not predicted as expected language. We further filter out training data that match to any source or target side sentences in evaluation datasets. Compared to other datasets such as OPUS100, the ML50 benchmark contains around 4 times more training data. The full list of languages, data sources, and amount of resulting data can be found in Table 6 in the Appendix. \n\nEvaluation Data To ensure high quality evaluation of languages covered in ML50, we include publicly available, widely used evaluation sets.",
            "score": 0.46379494881755323,
            "section_title": "ML50 Benchmark",
            "char_start_offset": 18988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2136
                },
                {
                    "start": 2139,
                    "end": 2278
                }
            ],
            "ref_mentions": [
                {
                    "start": 1609,
                    "end": 1630,
                    "matchedPaperCorpusId": "1210515"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "267414730",
            "title": "Distractor Generation Through Text-to-Text Transformer Models",
            "text": "Although multilingual models tend to have lower performance in comparison to their monolingual counterparts [54], the significant parameter difference gives our approach a likely advantage in terms of result quality. However, this advantage may be partially mitigated by the fact that our training dataset is derived from machine translation, potentially compromising its integrity compared to the original English version. As it can be seen in Table 1, our model outperforms their approach. Nevertheless, it would be interesting to have a comparison with the equiv-alent size model in English in order to compare the results for checking how much it affects the use of a multilingual model for a single language task. It would be also interesting to study whether the automated translated datasets have a notable impact in the final generation of the models. \n\nAcross these various comparisons, a recurring concern is the insufficient ability of traditional metrics such as BLEU and ROUGE to fully encapsulate the semantic quality of the generated text [36] especially in nuanced tasks such as distractor generation. These traditional metrics, based on word-overlapping n-grams, often fail to capture the semantic richness or contextuality of the text. They are also unable to account for synonyms and paraphrases [55], limiting their effectiveness for tasks where semantic fidelity is critical. In line with Rodriguez-Torrealba et al. [38], we also argue for more sophisticated evaluation metrics such as cosine similarity. Unlike conventional metrics, cosine similarity can assess semantic alignments, even when the words used are different. As a result, it serves as a more robust measure of distractor quality, capturing semantic relevance in a way that word-based metrics cannot.",
            "score": 0.46372821531930664,
            "section_title": "V. DISCUSSION",
            "char_start_offset": 28892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 1054,
                    "end": 1058,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1315,
                    "end": 1319,
                    "matchedPaperCorpusId": "53079743"
                },
                {
                    "start": 1437,
                    "end": 1441,
                    "matchedPaperCorpusId": "251033945"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74658203125
        },
        {
            "corpus_id": "269362363",
            "title": "IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages",
            "text": "\u2022 A qualitative analysis for assessing the gaps in current language technologies and define potential directions of future research. \n\nAs evident from the lists above, our benchmark provides a broad-coverage over languages with respect to their resourcedness, allowing users to evaluate language models on relatively highresource languages such as Hindi and extremely low-resource languages such as Manipuri in Meitei script on a single benchmark. \n\nTo curate the evaluation datasets for our benchmark, we use the following existing datasets as the source: CrossSum (Bhattacharjee et al., 2023) for cross-lingual summarization, FLO-RES (NLLB- Team et al., 2022) for machine translation, XQuAD (Artetxe et al., 2020) for multilingual QA, and XoRQA (Asai et al., 2021) for cross-lingual QA. From each of these datasets we select a subset of English examples to be a part of our benchmark, and then collect professional human translations for these examples in all target Indic languages. Some target languages are already covered by the source datasets in which case we re-purpose this existing data and only collect translations for the remaining languages. We also collect and release a small amount of training and validation data making possible evaluation of training techniques like fine-tuning, parameter-efficient training, in-context learning, and others. Why extend existing benchmarks? We chose to collect human translations of existing benchmarks as opposed to creating evaluation data from scratch due to various reasons: \n\n\u2022 Translation-based extension of existing benchmark results in multi-way parallel data, allowing researchers to attribute performance due to task knowledge vs. language understanding, and measure cross-lingual generalization  Sum (Bhattacharjee et al., 2023), a dataset for crosslingual summarization, which in turn is derived from XL-Sum (Hasan et al., 2021b). CrossSum contains multi-way parallel data in 45 languages where BBC news articles as source in a language are paired with corresponding summaries in other languages. Based on their matching criteria, different languages have different amount of sourcetarget pairs.",
            "score": 0.4635959374108899,
            "section_title": "Introduction",
            "char_start_offset": 3587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 447
                },
                {
                    "start": 450,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "247958074",
            "title": "Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval",
            "text": "State-of-the-art neural (re)rankers are notoriously data-hungry which \u2013 given the lack of large-scale training data in languages other than English \u2013 makes them rarely used in multilingual and cross-lingual retrieval settings. Current approaches therefore commonly transfer rankers trained on English data to other languages and cross-lingual setups by means of multilingual encoders: they fine-tune all parameters of pretrained massively multilingual Transformers (MMTs, e.g., multilingual BERT) on English relevance judgments, and then deploy them in the target language(s). In this work, we show that two parameter-efficient approaches to cross-lingual transfer, namely Sparse Fine-Tuning Masks (SFTMs) and Adapters, allow for a more lightweight and more effective zero-shot transfer to multilingual and cross-lingual retrieval tasks. We first train language adapters (or SFTMs) via Masked Language Modelling and then train retrieval (i.e., reranking) adapters (SFTMs) on top, while keeping all other parameters fixed. At inference, this modular design allows us to compose the ranker by applying the (re)ranking adapter (or SFTM) trained with source language data together with the language adapter (or SFTM) of a target language. We carry out a large scale evaluation on the CLEF-2003 and HC4 benchmarks and additionally, as another contribution, extend the former with queries in three new languages: Kyrgyz, Uyghur and Turkish. The proposed parameter-efficient methods outperform standard zero-shot transfer with full MMT fine-tuning, while being more modular and reducing training times. The gains are particularly pronounced for low-resource languages, where our approaches also substantially outperform the competitive machine translation-based rankers.",
            "score": 0.46338595165381546,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "247694098",
            "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU",
            "text": "We use the pre-trained versions of base RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020). For all datasets, we report accuracy as the main evaluation metric across three random seeds, on the following settings. \n\nIn-Distribution (ID) and Out-Of-Distribution (OOD): We first fine-tune a monolingual (English) model on a target dataset and evaluate on their ID test set, e.g. train RoBERTa on MNLI, and evaluate on MNLI-M validation set. We also evaluate it on an OOD dataset, e.g. NLI Diagnostics. \n\nZero-Shot (ZS): Constitutes the zero-shot crosslingual transfer setting. In particular, we train a multilingual model on the same dataset, e.g. XLM-RoBERTa on (English only) MNLI and evaluate it on a zero-shot cross-lingual set, e.g. XNLI test set (Hu et al., 2020). \n\nIn all experiments, we select the best check-point based on the English validation set performance. When reporting significance tests we use the Approximate Randomization test with all seeds (Noreen, 1989). More details about experimental settings can be found in Appendix C.3.",
            "score": 0.46321685492762843,
            "section_title": "Evaluation Settings",
            "char_start_offset": 14693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1056
                }
            ],
            "ref_mentions": [
                {
                    "start": 77,
                    "end": 99,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 758,
                    "end": 775,
                    "matchedPaperCorpusId": "214641214"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375244140625
        },
        {
            "corpus_id": "246294502",
            "title": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
            "text": "The main zero-shot results, averaged across languages, are reported in Table 4. In order to measure the gap between cross-lingual transfer and supervised learning, we also provide the results for the same models trained and evaluated on English test data in Table 3. The performance metric is accuracy for all tasks except cross-modal retrieval, which uses Recall@1.12 Finally, for completeness, the full results subdivided by language are detailed across Appendix C.2. Next, we compare these scores across different dimensions. \n\nTransfer Method. The results in Multilingual Models. Even within each transfer method, we observe considerable variance among individual multilingual multimodal models. For instance, for zero-shot model transfer via multilingual encoders, UC 2 is consistently better across the board (except for WIT) and surpasses the other models by a large margin, especially in German, French, Japanese and Mandarin-languages in which the authors had translated CC and pretrained UC 2 on (Appendix C.2). Remarkably, this tendency is less accentuated in MaRVL, where pretraining Mandarin multimodal data is still insufficient to tackle the out-of-distribution nature of MaRVL's culture-specific concepts (Table 15 in Appendix C.2). Nonetheless, these results prove how the simple 'translate pretrain' approach of UC 2 can be an effective baseline for multilingual transfer in multimodal pretrained models. As for 'translate test' transfer, no clear winner emerges across the monolingual models, whereas mUNITER generally performs better among multilingual models limited to cross-modal retrieval tasks. We also note that massively multilingual models (top rows) are often on a par with their monolingual counterparts (bottom rows). \n\nTransfer Gap. Comparing the results on the English test set (Table 3) with those averaged across the other languages (Table 4) reveals an extremely large gap in performance due to cross-lingual transfer. Considering the best multilingual encoders for each task, the gap is 14.8 points for XVNLI, 26.5 for xGQA, 14.6 for MaRVL, 24.2/23.0",
            "score": 0.46316272136293335,
            "section_title": "Zero-shot Learning",
            "char_start_offset": 22147,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 528
                },
                {
                    "start": 531,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "261682140",
            "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
            "text": "In this paper, we spot the cross-lingual inconsistency problem for multilingual foundation models. \n\nTo better evaluate this aspect, we collect and propose two new datasets with respective metrics. In this section, we provide more analysis on the crosslingual consistency study and the effectiveness of s in the proposed Consitency and AC3 metrics. \n\nThe evaluation results for Cross-LogiQA dataset is depicted in Figure 7. As the leading opensource multilingual model, BLOOMZ outperforms other open-source models and ChatGPT in terms of consistency but falls short in achieving high performance in specific languages. This suggests that BLOOMZ provides more consistent answers across languages, possibly due to its training on a more balanced multilingual corpus and fine-tuning with multilingual instruction data to improve crosslanguage alignment. As observed in Figure 6, the performance is better for higher-resource languages such as English, Chinese, and Spanish compared to lower-resource languages like Indonesian, Vietnamese, and Malay. GPT4 surpasses other models in both \"Accuracy\" and \"Consistency,\" highlighting significant potential for further enhancement of all other models. \n\nIn Section 3.3, AC3 score is presented, taking into account both accuracy and consistency scores. We have one tolerance hyperparameter s which requires the answers to be consistent across s languages to be rewarded in consistency score. We deploy s = 3 as the default hyperparameter in above experiments. Here, we conduct more systematic study of s with its effect on the final scores. \n\nThe results on Cross-MMLU and Cross-LogiQA are shown in Figure 8 and 9. As s increases, the consistency score has dropped dramatically for all models. Among all three models, the performance of GPT4 drops the least, indicating a robust consistency alignment across languages. Even for Chat-GPT, when s = 6, the consistency score downgrades to around 26% on both Cross-MMLU and Cross-LogiQA datasets. It indicates that only 26% of the cases that ChatGPT is selecting the same answer for the same question across six languages.",
            "score": 0.4631507211013932,
            "section_title": "C Cross-Lingual Consistency",
            "char_start_offset": 31187,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2108
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66162109375
        },
        {
            "corpus_id": "218487791",
            "title": "On the Limitations of Cross-lingual Encoders as Exposed by Reference-Free Machine Translation Evaluation",
            "text": "Existing semantically-motivated metrics for reference-free evaluation of MT systems have so far displayed rather poor correlation with human estimates of translation quality. In this work, we investigate a range of reference-free metrics based on cutting-edge models for inducing cross-lingual semantic representations: cross-lingual (contextualized) word embeddings and cross-lingual sentence embeddings. We have identified some scenarios in which these metrics fail, prominently their inability to punish literal word-by-word translations (the so-called \"translationese\"). We have investigated two different mechanisms for mitigating this undesired phenomenon: (1) an additional (weakly-supervised) cross-lingual alignment step, reducing the mismatch between representations of mutual translations, and (2) language modeling (LM) on the target side, which is inherently equipped to punish \"unnatural\" sentences in the target language. We show that the reference-free coupling of cross-lingual similarity scores with the target-side language model surpasses the reference-based BLEU in segment-level MT evaluation. We believe our results have two relevant implications. First, they portray the viability of referencefree MT evaluation and warrant wider research efforts in this direction. Second, they indicate that reference-free MT evaluation may be the most challenging (\"adversarial\") evaluation task for multilingual text encoders as it uncovers some of their shortcomings-prominently, the inability to capture semantically non-sensical word-by-word translations or paraphrases-which remain hidden in their common evaluation scenarios. \n\nWe release our metrics under the name XMover-Score publicly: https://github.com/AIPHES/",
            "score": 0.462874394592357,
            "section_title": "Conclusion",
            "char_start_offset": 30441,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 1731
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65771484375
        },
        {
            "corpus_id": "259859119",
            "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
            "text": "For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets. Similar to previous approach, we perform both zero-shot and sequential fine-tuning for translated datasets and report the results in table 5.  4: Comparison of monolingual fine-tuning vs multilingual fine-tuning for all datasets. Four source language models, XLM_ENG, XLM_HI, XLM_IT and XLM_KOR are the fine-tuned XLM-R models on English, Hindi, Italian and Korean datasets respectively. Last four columns correspond to sequential fine-tuning of all datasets using source language models. Best F1-scores are shown in bold.",
            "score": 0.4627094743671982,
            "section_title": "MTL based on Translation",
            "char_start_offset": 18270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 897
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90283203125
        },
        {
            "corpus_id": "273501835",
            "title": "A Novel Interpretability Metric for Explaining Bias in Language Models: Applications on Multilingual Models from Southeast Asia",
            "text": "We evaluate a wide range of models to compare biased behavior across different levels of PLM properties. First, we evaluate both masked and causal PLMs as both (especially the latter) are currently pushing the state-of-the-art in terms of language modeling performance. We also evaluate both English-only models and multilingual models in order to analyze whether a pattern or relationship exists between model multilingualism and bias. Among multilingual models, we also compare bias across models trained on languages worldwide and those trained particularly on Southeast Asian datasets. Table 2 summarizes the models evaluated and their properties.",
            "score": 0.4626274912648247,
            "section_title": "Models",
            "char_start_offset": 10015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 436
                },
                {
                    "start": 437,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 651
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6376953125
        },
        {
            "corpus_id": "267320768",
            "title": "A Cross-Language Investigation into Jailbreak Attacks in Large Language Models",
            "text": "To develop a multilingual question dataset, we introduce a semantic-preserving algorithm. This algorithm starts with an English corpus and produces outputs that maintain high semantic fidelity in the target languages. Our approach centers around the utilization of a state-of-the-art (SOTA) machine translation service, specifically Microsoft Translate, known for its reliability and accuracy. This service is employed to translate our initial set of English questions into their counterparts in eight different languages. \n\nTo ensure the precision and reliability of these translations, a critical step in our process involves filtering the translated data. This is executed through a similarity-based data filtering algorithm, as detailed in Algorithm 1. This algorithm plays a pivotal role in maintaining the integrity of the dataset by ensuring that the translated questions closely mirror the original English questions in terms of semantic content. ). This enables us to assess the semantic fidelity of the translation by calculating the similarity between the original English questions and their re-translated English counterparts (line 6). Our goal is to retain those translations that demonstrate high similarity, thus ensuring semantic consistency. \n\nFor the purpose of measuring sentence similarity in our study, we employ the pre-trained model all-MiniLM-L6-v2. This model is renowned for its effectiveness in generating sentence embeddings, particularly useful for semantic searches. The similarity between sentences is quantified using Cosine-Similarity, a widely accepted method for comparing vector-based representations of text. The similarity metric for the sentences can be expressed as follows: \n\nAlgorithm 1 operates by evaluating whether the calculated similarity scores for each translation exceed a predetermined threshold (line 7). If the similarity score for any language falls below this threshold, the algorithm excludes that particular corpus entry and moves on to the next one (line 8-11). Conversely, if all the languages exhibit similarity scores that meet or surpass the threshold for a given corpus entry, the algorithm includes that corpus in the dataset (line [12][13]. This approach ensures that only corpus entries maintaining a consistent and high level of semantic similarity across all translations are selected for further analysis. Such a method significantly bolsters the reliability and validity of our multilingual dataset by rigorously filtering out entries with potential semantic discrepancies. Threshold Selection.",
            "score": 0.4625358591880265,
            "section_title": "Semantic-preserving Multilingual Dataset Construction.",
            "char_start_offset": 21960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1715
                },
                {
                    "start": 1718,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2206
                },
                {
                    "start": 2207,
                    "end": 2375
                },
                {
                    "start": 2376,
                    "end": 2544
                },
                {
                    "start": 2545,
                    "end": 2565
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "255537290",
            "title": "Zero-shot cross-lingual transfer language selection using linguistic similarity",
            "text": "In the field of NLP, sentiment analysis is one of the most active research areas [47]. The recent research in sentiment analysis, as with many other NLP tasks, has mainly focused on using deep neural networks and pretrained language models [48,49,50,51,52]. The popularization of multilingual transformer models has made it possible to utilize cross-lingual transfer in order to train models for low-resource languages. \n\nRasooli et al. \n\n[53] used a set of 16 languages from different language families, namely Indo-European, Turkic, Afro-Asiatic, Uralic, and Sino-Tibetan, to learn a sentiment analysis model. Their experiments showed that for most target languages the best result can be obtained by leveraging from multiple source languages at the same time. Also, datasets of a similar genre and domain tended to yield higher results when compared to out-of-domain and dissimilar genres. Pelicon et al. [54] used zero-shot cross-lingual transfer to classify Croatian news articles with an mBERT model fine-tuned using Slovene data with good results. In addition, Kumar et al. [55] used XLM-R and performed crosslingual transfer from English to Hindi. Their model compared favorably to the used benchmarks and gives an effective solution to the analysis of sentiments in a resource-poor scenario. \n\nThe majority of the sentiment analysis datasets used in this research consists of product reviews, as we attempted to keep the domain the same throughout the languages. However, for some languages, we were unable to find such data, most notably Croatian, which consists of news articles. We also had to adjust the labels of some of the datasets so that they would match among all of the languages. Training and evaluation splits were retained from original datasets if possible, otherwise datasets were split to 80% training and 20% evaluation. \n\nFor this research we used the Multilingual Amazon Reviews Corpus [56], which covers English, Japanese and German. The dataset contains over 200,000 reviews for each language collected between 2015 and 2019. The reviews are labeled from one to five stars.",
            "score": 0.46232791483987834,
            "section_title": "Sentiment Analysis",
            "char_start_offset": 21914,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2104
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39013671875
        },
        {
            "corpus_id": "241583550",
            "title": "SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining",
            "text": "Results for data augmentation are shown in Table 7. When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning. \n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages. \n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval. \n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average.",
            "score": 0.46220783344723304,
            "section_title": "Data Augmentation",
            "char_start_offset": 25601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 51
                },
                {
                    "start": 52,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1087
                },
                {
                    "start": 1090,
                    "end": 1220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "5071247",
            "title": "Automatic Language Identification in Texts: A Survey",
            "text": "specific results across datasets.\n\nAnother challenge in standardizing datasets for LI is the great deal of variation that can exist between data in the same language. We examine this in greater detail in Section 10.2, where we discuss how the same language can use a number of different orthographies, can be digitized using a number of different encodings, and may also exist in transliterated forms. The issue of variation within a language complicates the development of standardized datasets, due to challenges in determining which variants of a language should be Reference Type Source Baldwin and Lui (2010b) Multilingual (81) Government Documents, News Texts, Wikipedia https://github.com/varh1i/language_detection/tree/master/src/main/resources/naacl2010-langid Baldwin and Lui (2010a) Multilingual (74) Wikipedia (synthetic multilingual docs) http://people. eng.unimelb.edu.au/tbaldwin/etc/altw2010-langid.tgz Vatanen et al. (2010 Multiglingual ( (Baldwin and Lui, 2010b), that LI research is often motivated by target applications (see Section 8), and that domain-specific information can be used to improve accuracy (see Section 10.9), it is often unsound to use a generic LI dataset to develop a language identifier for a particular domain.\n\nA third challenge in standardizing datasets for LI is the cost of obtaining correctlylabeled data. Manual labeling of data is usually prohibitively expensive, as it requires access to native speakers of all languages that the dataset aims to include. Large quantities of raw text data are available from sources such as web crawls or Wikipedia, but this data is frequently mislabeled (e.g. most non-English Wikipedias still include some Englishlanguage documents). In constructing corpora from such resources, it is common to use some form of automatic LI, but this makes such corpora unsuitable for evaluation purposes as they are biased towards documents that can be correctly identified by automatic systems (Lui and Baldwin, 2014). Future work in this area could investigate other means of ensuring",
            "score": 0.4621633461421496,
            "section_title": "Corpora Used for LI Evaluation",
            "char_start_offset": 116573,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 591,
                    "end": 614,
                    "matchedPaperCorpusId": "10845076"
                },
                {
                    "start": 770,
                    "end": 793,
                    "matchedPaperCorpusId": "15218055"
                },
                {
                    "start": 956,
                    "end": 980,
                    "matchedPaperCorpusId": "10845076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30615234375
        },
        {
            "corpus_id": "248157267",
            "title": "The Impact of Cross-Lingual Adjustment of Contextual Word Representations on Zero-Shot Transfer",
            "text": "We evaluate effectiveness of an existing approach to cross-lingual adjustment of mBERT (Cao et al., 2020) using four typologically different languages (Spanish, Russian, Vietnamese, and Hindi) and four IR/NLP tasks (XSR, QA, NLI, and NER). The original mBERT is being compared to mBERT \"adjusted\" with a help of a small parallel corpus. The cross-lingual adjustment of mBERT improves NLI in four target languages; NER, XSR, and cross-lingual QA in three languages (though some cross-lingual QA gains are not statistically significant). However, in the case of mono-lingual QA performance never improves and sometimes degrades. We believe our results support a conjecture that the cross-lingual adjustment of Cao et al. (2020) is more beneficial for cross-lingual tasks. \n\nInspired by the analysis of histograms of distances, we obtain additional improvement on NLI using continual learning. Our study contributes to a better understanding of cross-lingual transfer capabilities of large multilingual language models and identifies limitations of the cross-lingual adjustment in various IR and NLP tasks.",
            "score": 0.4619678348183318,
            "section_title": "Conclusion",
            "char_start_offset": 27032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1103
                }
            ],
            "ref_mentions": [
                {
                    "start": 87,
                    "end": 105,
                    "matchedPaperCorpusId": "211069110"
                },
                {
                    "start": 708,
                    "end": 725,
                    "matchedPaperCorpusId": "211069110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.447509765625
        },
        {
            "corpus_id": "257771427",
            "title": "Model and Evaluation: Towards Fairness in Multilingual Text Classification",
            "text": "Research on multilingualism has proliferated in order to enable models to be applied to different languages and to gain new knowledge from different languages. Mutuvi et al. (2020) proposed that text classification models tend to perform differently across different languages, more particularly when the data is highly imbalanced in different languages in the dataset. To solve the performance deviation of different languages, there are three main solution strategies: (1) Construct more balanced multilingual datasets: For example, Ponti et al. (2020) proposed a dataset for causal commonsense reasoning in 11 languages XCOPA, Zhao et al. (2020) proposed a multilingual dataset for bias analysis. \n\n(2) Use multilingual pre-trained language models: Wu and Dredze (2019) explored the performance of mBERT in performing five NLP tasks on 15 languages and found that it effectively learned good multilingual representations. (3) Cross-language transfer techniques: Cross-language techniques migrate modeling approaches that perform well in high resource languages to low resource languages. Webster and Pitler (2020) used a crosslingual pivoting technique to improve the performance of a multilingual machine translation system. On the other hand, Nooralahzadeh et al. (2020) used meta-learning for cross-lingual text classification model migration, etc.",
            "score": 0.46180555002722956,
            "section_title": "Multilingual Text Classification Research",
            "char_start_offset": 6008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1354
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 180,
                    "matchedPaperCorpusId": "227230342"
                },
                {
                    "start": 535,
                    "end": 554,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 630,
                    "end": 648,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1248,
                    "end": 1275,
                    "matchedPaperCorpusId": "212415221"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "270379589",
            "title": "MINERS: Multilingual Language Models as Semantic Retrievers",
            "text": "We gauge LMs' robustness to various text inputs with three different evaluation settings: \n\n\u2022 Monolingual (Mono): We measure the individual language performance using the same language as train and test sets. Acc. LinCE MT (Aguilar et al., 2020) 2 Bitext Retrieval F1 LinCE SA (Patwa et al., 2020) 2 Sentiment Analysis \n\nAcc. PHINC (Srivastava and Singh, 2020) 2 Bitext Retrieval F1 vice versa, and for retrieval-based classification and ICL classification, we take CS texts as input and predict their labels. \n\n\u2022 Cross-lingual (XL): We measure the performance of multilingual datasets with one language as the source language and the rest as target languages. For detailed information, please refer to Table 7 in the Appendix. \n\n\u2022 Cross-lingual Code-switching (XL CS): We tackle a more challenging scenario by evaluating CS data within a cross-lingual context. (Wang et al., 2024), multilingual MPNet BASE v2 (Song et al., 2020), multilingual MiniLM L12-E384 (Wang et al., 2020), Glot-500 (ImaniGooghari et al., 2023), XLM-R BASE , XLM-R LARGE (Conneau and Lample, 2019), and two commercial embedding APIs: Cohere-Embedv3 (embed-multilingual-v3.0) and OpenAI-Embedv3 (text-embedding-3-large).",
            "score": 0.46179260881063844,
            "section_title": "Settings",
            "char_start_offset": 9723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 92,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 318
                },
                {
                    "start": 321,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 245,
                    "matchedPaperCorpusId": "218581853"
                },
                {
                    "start": 277,
                    "end": 297,
                    "matchedPaperCorpusId": "221090332"
                },
                {
                    "start": 332,
                    "end": 360,
                    "matchedPaperCorpusId": "215827471"
                },
                {
                    "start": 910,
                    "end": 929,
                    "matchedPaperCorpusId": "215827489"
                },
                {
                    "start": 960,
                    "end": 979,
                    "matchedPaperCorpusId": "211296536"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31884765625
        },
        {
            "corpus_id": "270719963",
            "title": "Evaluating Neural Networks\u2019 Ability to Generalize against Adversarial Attacks in Cross-Lingual Settings",
            "text": "Finally, they show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance.\n\nIn a recent paper, Artetxe, Labaka, and Agirre [9] show that translation can introduce artifacts that can have a notable impact on existing cross-lingual models.For example, they show that translating the premise and the hypothesis independently from the XNLI dataset can reduce the lexical overlap between them, which current models are highly sensitive to.The authors also show that some previous findings in cross-lingual transfer learning need to be reconsidered in light of this phenomenon.Based on the insights gained, they also improved the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.These findings suggest that it is important to be aware of the potential for bias when using machine translation and to take steps to mitigate it.\n\nOne of the earliest contributions in this domain was the introduction of multilingual word embeddings, which map words from different languages into a shared vector space.Mikolov et al. [10] pioneered this approach with their work on bilingual word embeddings, which facilitated knowledge transfer from resource-rich languages to those with fewer resources.Following this, cross-lingual word embeddings have been extended to multiple languages, as demonstrated by the work on polyglot embeddings by Al-Rfou et al. [11], which leveraged unsupervised learning on large multilingual corpora.\n\nThe advent of transformer-based models, such as BERT [12], revolutionized NLP by learning deep contextualized representations.Adapting these models for multilingual purposes, Conneau et al. [4] introduced XLM-Roberta, a robust cross-lingual model trained on 100 languages, showing that a single model could perform well on several cross-lingual benchmarks.\n\nThe impact of surface-level patterns in data on model performance has also been examined.Geirhos et al. [13] highlighted that neural networks often exploit such patterns for prediction, which can be problematic when these patterns do not generalize well across languages.",
            "score": 0.46178266441672594,
            "section_title": "Related Works",
            "char_start_offset": 10335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 128,
                    "end": 289
                },
                {
                    "start": 289,
                    "end": 486
                },
                {
                    "start": 486,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 931
                },
                {
                    "start": 933,
                    "end": 1104
                },
                {
                    "start": 1104,
                    "end": 1290
                },
                {
                    "start": 1290,
                    "end": 1521
                },
                {
                    "start": 1523,
                    "end": 1649
                },
                {
                    "start": 1649,
                    "end": 1879
                },
                {
                    "start": 1881,
                    "end": 1970
                },
                {
                    "start": 1970,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 1447,
                    "end": 1451,
                    "matchedPaperCorpusId": "11332377"
                },
                {
                    "start": 1985,
                    "end": 1989,
                    "matchedPaperCorpusId": "215786368"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72705078125
        },
        {
            "corpus_id": "267617144",
            "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
            "text": "Improved performance was shown when smaller cross-lingual models were fine-tuned with data generated by LLMs. Some recently released datasets focus on specialized language domains such as law (Niklaus et al., 2023), education (Zhang et al., 2023c), or healthcare (Wang et al., 2023). \n\nThese corpora often suffer from inadequate data quality and require extensive cleaning (Abadji et al., 2022;Kreutzer et al., 2022). Task-specific datasets (Ponti et al., 2020;Conneau et al., 2018) are often smaller in scale but offer higher quality data targeted at a specific model capability such as cross-lingual understanding and transfer learning. This type of data is crucial for evaluating and enhancing the performance of models in diverse linguistic contexts. Such datasets are aggregated in multilingual benchmarks (Hu et al., 2020;Ruder et al., 2021;Cahyawijaya et al., 2021). Recently, (Ruder et al., 2023) released XTREME-UP, which covers data in 88 under-represented languages across 9 user-centric technologies. \n\nNo Language Left Behind (NLLB-Team et al., 2022) open-sourced bitext, mined bitext, and data generated using back-translation in 200+ languages specifically for text-to-text translation. While Seamless4MT (Barrault et al., 2023) released the metadata of SeamlessAlign, an open multimodal translation dataset, there are relatively fewer works for data creation/curation in lowresource languages. (Cahyawijaya et al., 2023) introduced NusaCrowd, a standardized collection of 137 datasets covering 19 Indonesian local languages in text, speech, and image modalities. Our work differs from previous datasets as we create a large-scale instruction-tuning dataset spanning hundreds of different tasks, yet retain high quality by involving human annotation and rigorous quality control across the entire data creation process.",
            "score": 0.46167186977439245,
            "section_title": "B.1 Multilingual datasets",
            "char_start_offset": 38947,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1834
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 394,
                    "matchedPaperCorpusId": "246015576"
                },
                {
                    "start": 811,
                    "end": 828,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 847,
                    "end": 872,
                    "matchedPaperCorpusId": "233289621"
                },
                {
                    "start": 1410,
                    "end": 1436,
                    "matchedPaperCorpusId": "254853901"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5263671875
        },
        {
            "corpus_id": "235683062",
            "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA",
            "text": "We evaluate XLM-E on the XTREME (Hu et al., 2020b) benchmark, which is a multilingual multitask benchmark for evaluating cross-lingual understanding. The XTREME benchmark contains seven cross-lingual understanding tasks, namely part-of-speech tagging on the Universal Dependencies v2.5 (Zeman et al., 2019), NER named entity recognition on the Wikiann (Pan et al., 2017;Rahimi et al., 2019) dataset, cross-lingual natural language inference on XNLI (Conneau et al., 2018), cross-lingual paraphrase adversaries from word scrambling (PAWS-X; Yang et al. 2019a), and cross-lingual question answering on MLQA , XQuAD (Artetxe et al., 2020), and TyDiQA-GoldP (Clark et al., 2020a).\n\nBaselines We compare our XLM-E model with the cross-lingual language models pretrained with multilingual text, i.e., Multilingual BERT  Table 1: Evaluation results on XTREME cross-lingual understanding tasks. We consider the cross-lingual transfer setting, where models are only fine-tuned on the English training data but evaluated on all target languages. Results with \"*\" are taken from (Hu et al., 2020b). Results of XLM-E and XLM-R base are averaged over five runs. Results We use the cross-lingual transfer setting for the evaluation on XTREME (Hu et al., 2020b), where the models are first fine-tuned with the English training data and then evaluated on the target languages. In Table 1, we report the accuracy, F1, or Exact-Match (EM) scores on the XTREME cross-lingual understanding tasks. The results are averaged over all target languages and five runs with different random seeds. We divide the pretrained models into two categories, i.e., the models pretrained on multilingual corpora, and the models pretrained on both multilingual corpora and parallel corpora. For the first setting, we pretrain XLM-E with only the",
            "score": 0.461639588165642,
            "section_title": "Cross-lingual Understanding",
            "char_start_offset": 11744,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 352,
                    "end": 370,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 370,
                    "end": 390,
                    "matchedPaperCorpusId": "153313061"
                },
                {
                    "start": 449,
                    "end": 471,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 540,
                    "end": 558,
                    "matchedPaperCorpusId": "201698093"
                },
                {
                    "start": 613,
                    "end": 635,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 654,
                    "end": 675,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49267578125
        },
        {
            "corpus_id": "226282329",
            "title": "Low-Resource Adaptation of Neural NLP Models",
            "text": "Cross-lingual learning has a fairly short history in NLP, and has mainly been restricted to traditional NLP tasks, such as PoS tagging and parsing. In contrast to these tasks, which have seen much cross-lingual attention Bjerva, 2017;de Lhoneux et al., 2018), there has been relatively little work on cross-lingual NLU, partly due to a lack of benchmark datasets. Existing work has mainly been focused on NLI (Agi\u0107 and Schluter, 2018;Conneau et al., 2018), and to a lesser degree on RE (Faruqui and Kumar, 2015;Verga et al., 2016) and QA (Lewis et al., 2020;Abdou et al., 2019). Previous research generally reports that cross-lingual learning is challenging and that it is hard to beat a machine translation baseline (e.g., Conneau et al. (2018)). Such a baseline is (for instance) suggested by Faruqui and Kumar (2015), where the text in the target language is automatically translated to English. For many language pairs, a machine translation model may be available, which can be used to obtain data in the target language. To evaluate the impact of using such data, in much of previous research work, the English training data is translated into the target language using a machine translation system. Then, the model is fine-tuned on the translated data and evaluated on the test set of target languages and reported as a TRANSLATE-TRAIN baseline. Alternatively, after fine-tuning the model on the English training data, a TRANSLATE-TEST baseline is introduced by evaluating the model on the test data that is translated from the target language to English using the machine translation system.\n\nIn this chapter, we show that our meta-learning based framework can achieve competitive performance compared to a machine translation baseline (for XNLI), and propose a method that requires no training instances for the target task in the target language.",
            "score": 0.46119786146199937,
            "section_title": "Cross-Lingual NLU",
            "char_start_offset": 230001,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 234,
                    "end": 258,
                    "matchedPaperCorpusId": "52111211"
                },
                {
                    "start": 409,
                    "end": 434,
                    "matchedPaperCorpusId": "3659314"
                },
                {
                    "start": 434,
                    "end": 455,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 486,
                    "end": 511,
                    "matchedPaperCorpusId": "1387162"
                },
                {
                    "start": 511,
                    "end": 530,
                    "matchedPaperCorpusId": "9206785"
                },
                {
                    "start": 558,
                    "end": 577,
                    "matchedPaperCorpusId": "199577725"
                },
                {
                    "start": 724,
                    "end": 745,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 795,
                    "end": 819,
                    "matchedPaperCorpusId": "1387162"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406494140625
        },
        {
            "corpus_id": "19211851",
            "title": "Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices",
            "text": "and English are closely related languages and also share significant amounts of cultural knowledge. Future work should also investigate multilingual image description with language pairs that are more distantly related both linguistically and culturally.\n\nThe studies presented here can be seen as an examination of how to evaluate image description: what is an appropriate gold standard to evaluate against? This is a particularly important question to ask when developing a dataset for a new language, since the available resources constrain the direction of future research. The first study indicates that there is no a priori reason to discount (image-aware) translation as a source of high-quality image descriptions. It also means that image description systems (either based on multimodal MT or cross-linguistic methods) can use translations as a gold standard in evaluation. It will, however, remain important to be aware of potential cultural differences, for example, by developing methods for identifying cases in which translation is inappropriate.\n\nThe second study is relevant to whether text-only 'image-blind' translations are an appropriate gold standard for evaluating multimodal MT, specifically evaluation using automatic metrics like BLEU or Meteor. The post-editing resulted in only a small number of words being changed, albeit often with significant semantic impact. The minor changes meant that the difference between using text-only and imageaware reference translations led to only minor differences in system evaluations using Meteor. Rather than concluding that text-only translations may be used in evaluation, we take these findings to indicate that automatic metrics should not be used for multimodal MT, since these metrics are not sufficiently sensitive to the information provided by the image (i.e., the difference between pre-and post-edited translations). The difficulties in evaluating multimodal MT are similar to those faced in evaluating discourse-level MT, where small changes (e.g., pronoun choice) often have significant semantic consequences. Given the inappropriateness of word-overlap metrics such as BLEU, the discourse-level MT community has developed sub-tasks focussed on specific translation problems that require discourse awareness, such as pronoun prediction (Hardmeier et al. 2015). Attempts in multimodal MT to create a similar test set of ambiguous instances that require image information, such as the Ambiguous COCO dataset, are a promising future direction.\n\nThe",
            "score": 0.4606175177681068,
            "section_title": "Conclusion",
            "char_start_offset": 39928,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80126953125
        },
        {
            "corpus_id": "253762071",
            "title": "NTREX-128 \u2013 News Test References for MT Evaluation of 128 Languages",
            "text": "Test data has to be of a high-enough quality level to be useful. We specified two main requirements: 1) we require translations which are performed by native speakers of the respective target language who are bilingual in English; and 2) reference translations should not be created based on post-editing MT output. \n\nOur translation provider, as part of their translation process, performed quality assurance before delivery of the test set files. Upon receipt of the files we then sent them out to human evaluation via source-based direct assessment (src-DA), as implemented in the Appraise framework (Federmann, 2018). To avoid potential bias, annotation work was performed by an independent vendor. \n\nAs the result of the human evaluation process, we obtain segment-level quality scores based on the assessment of bilingual annotators who are native speakers of the respective target language. Scores range from 0 \u2212 100 and express the 'quality of the semantic transfer' between source and target language. This focuses more on adequacy than on fluency but, based on previous research findings, we consider this an acceptable trade-off. \n\nSegments with scores < 25 are deemed defective, while any score in the [25, 50) range is considered suspect. We return any segments with a score < 50 to the translation vendor for repairs. We have found that this method allows us to check quality for all translated segments; it scales well to thousands of segments with acceptable cost. As a side effect we have observed an increased level of quality control on the translation provider's side as they have understood that we will routinely verify their translation output for the full data sets, instead of random samples.",
            "score": 0.46042012575288505,
            "section_title": "Quality Assurance",
            "char_start_offset": 2158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1717
                }
            ],
            "ref_mentions": [
                {
                    "start": 603,
                    "end": 620,
                    "matchedPaperCorpusId": "52013208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "277467881",
            "title": "Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?",
            "text": "We acknowledge the ethical considerations inherent in working with language data, particularly for low-resource languages. Efforts were made to ensure that the collected datasets comply with ethical guidelines and respect intellectual property rights, including securing appropriate permissions where applicable. Moreover, we recognize the importance of promoting linguistic diversity and avoiding potential biases in our models. Consequently, the datasets and models developed in this research aim to be inclusive and accessible, thereby supporting equitable representation and technological advancement for low-resource language communities. All of our data processing and storage fulfil the requirements from GDPR. FLORES-200 (team et al., 2022) is a benchmark dataset specifically designed for low-resource and multilingual machine translation, serving as an extended version of FLORES-101 (Goyal et al., 2021). It covers 200 languages and consists of sentences extracted from 842 web articles, with an average length of approximately 21 words. These sentences are divided into three datasets: dev, devtest, and a hidden test set. Since we require additional evaluation metrics, we use devtest as our set of tests in this study. In our paper, we primarily evaluate the translation performance of all 200 languages into English. However, in the subsequent model training, we focus solely on the Luxembourgish-English language pair for training and testing. \n\nThe VAL 300 validation set was constructed using 300 pieces of official news content from July 2024 as the source data. Corresponding ground truth in Luxembourgish was generated using Chat-GPT, followed by dictionary-based verification to ensure validity. Additionally, we extracted 30 samples from the dataset and engaged Luxembourgish-English bilingual speakers to perform quality assessment.",
            "score": 0.4604129772700979,
            "section_title": "Ethics Statement",
            "char_start_offset": 22532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1856
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4580078125
        },
        {
            "corpus_id": "237838256",
            "title": "Cross-lingual transfer of sentiment classifiers",
            "text": "Another type of cross-lingual transfer is possible if we increase the training sets with instances from several related and unrelated languages. We conduct two sets of experiments in this scenario. In the first setting, reported in Table 6, we constructed the training set in each experiment with instances from several languages and 70% of the target language dataset. The remaining 30% of target language instances are used as the testing set. In the second setting, reported in Table 7, we merge all other languages and 70% of the target language into a joint training set. We compare the LASER approach, mBERT, and also CSE BERT, as Slovene and Croatian are involved in some combinations. \n\nTable 6 shows a gap between learning models using the expanded datasets and models with only target language data. The decrease is more extensive for both BERT models (on average around 10%) than for the LASER approach (the decrease is on average 3% for F F \u203e 1 and 5% for CA). These results indicate that the tested expansion of datasets was unsuccessful, i.e. the provided amount of training instances in the target language was already sufficient for successful learning. The additional instances from other languages in the transformed space are likely to be of lower quality than the native instances and therefore decrease the performance. Note. We compare the LASER approach, mBERT, and CSE BERT. As the upper bound, we give results of the LASER approach trained on only the target language. \n\nThe results in Table 7, where we test the expansion of the training set (consisting of 70% of the dataset in the target language) with all other languages, show that using many languages and significant enlargement of datasets is also not successful. The two improvements in the LASER approach over using only target language are limited to a single metric (F 1 in case of We studied state-of-the-art approaches to the cross-lingual transfer of Twitter sentiment prediction models: mappings of words into the common vector space using the LASER library and two multilingual BERT variants (mBERT and trilingual CSE BERT). Our empirical evaluation is based on relatively large datasets of labelled tweets from 13 European languages. We first tested the success of these text representations in a monolingual setting.",
            "score": 0.45971522732666825,
            "section_title": "Increasing datasets with several languages",
            "char_start_offset": 27155,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2226
                },
                {
                    "start": 2227,
                    "end": 2310
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.462646484375
        },
        {
            "corpus_id": "270379819",
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "text": "We present the Multilingual Bias Benchmark for Question-answering (MBBQ), consisting of a hand-checked translation of the English BBQ dataset into Dutch, Spanish, and Turkish,   and a parallel control set to measure task performance independent from bias.MBBQ covers stereotypes from 6 bias categories that are commonly held across all 4 languages, allowing for an investigation of cross-lingual stereotypes, with differences that are due to inconsistencies in model behavior across languages rather than cultural shifts.In this paper, we evaluated 7 LLMs on the MBBQ dataset.Our results show that 1) the ability of generative LLMs to answer multiple choice questions significantly differs across languages, 2) for the less accurate models, the extent to which they exhibit stereotypical behavior significantly differs across languages, and 3) the biases of a generative LLM differ across bias categories.Based on our findings, we recommend evaluating model bias across different bias categories, rather than reporting on the bias of a model as a whole, and separating measurements of model bias from their performance, especially cross-lingually.We hope that our work will spark further research in the direction of multilingual debiasing, to ensure that these models do not exhibit biased behavior regardless of the language used to prompt them.",
            "score": 0.45965084434356784,
            "section_title": "Conclusion",
            "char_start_offset": 27691,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 521
                },
                {
                    "start": 521,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1147
                },
                {
                    "start": 1147,
                    "end": 1347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "261394577",
            "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
            "text": "The absence of high-quality, parallel evaluation benchmarks is a major obstacle in assessing the text comprehension capabilities of multilingual models. NLP datasets with high language coverage do exist, such as FLORES-200 (NLLB et al., 2022), but they primarily focus on machine translation. Popular multilingual evaluation benchmarks, such as multilingual question answering (Lewis et al., 2020;Clark et al., 2020), natural language inference (NLI) (Conneau et al., 2018), and summarization (Ladhak et al., 2020;Hasan et al., 2021), altogether only cover around 30 languages. And while understanding and generative text services are used across the globe in 100+ languages, the lack of labeled data provides a major obstacle to building functional systems in most languages. \n\nSimultaneously, large language models (LLMs) have become increasingly popular. Certain LLMs, like BLOOM (Scao et al., 2022), are trained on multilingual data and tout their innate multilingual capabilities. Others like GPT-3 (Brown et al., 2020) and LLAMA (Touvron et al., 2023a) have demonstrated multilingual competence despite their training data being predominantly in English. Even so, LLMs benefit from pretraining data that is linguistically diverse, intentionally or not, as well as from cross-lingual transfer (Zoph et al., 2016;Artetxe et al., 2020;Muller et al., 2021b). But how multilingual are these models really? Beyond LLMs, significant scientific progress needs to be made before NLP systems can be built effectively and efficiently in low-resource languages. Many modeling techniques are being presented as languageagnostic but have only truly been evaluated in a small number of languages (Bender, 2011), risking not being applicable to diverse typologically phenomena (Bender, 2009). We believe that largescale, parallel, and discriminative datasets are crucial for studying the multilingual capabilities of such models and understanding how the technological disparity between high-and low-resource languages is evolving.",
            "score": 0.45963246794165724,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 2021
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 397,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 397,
                    "end": 416,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "222177239"
                },
                {
                    "start": 514,
                    "end": 533,
                    "matchedPaperCorpusId": "235658519"
                },
                {
                    "start": 1004,
                    "end": 1024,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1298,
                    "end": 1317,
                    "matchedPaperCorpusId": "16631020"
                },
                {
                    "start": 1317,
                    "end": 1338,
                    "matchedPaperCorpusId": "204901567"
                },
                {
                    "start": 1338,
                    "end": 1359,
                    "matchedPaperCorpusId": "231718746"
                },
                {
                    "start": 1687,
                    "end": 1701,
                    "matchedPaperCorpusId": "317797"
                },
                {
                    "start": 1767,
                    "end": 1781,
                    "matchedPaperCorpusId": "5449971"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "237364084",
            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
            "text": "With the introduction of the Transformer architecture, a variety of research was done on pretraining strategies and models. As a result of this, several multilingual pretrained models have emerged in recent years. These models extend the progress on NLP to a wide range of languages and investigate whether using a diverse set of languages during language model pretraining is beneficial when finetuning on downstream tasks. Models such as mBERT (Devlin et al., 2018), XLM (Lample and Conneau, 2019), XLM-R (Conneau et al., 2019), mT5 (Xue et al., 2020), and mMiniLM (Wang et al., 2020) achieved performance improvements in cross-lingual tasks, showing to be competitive with strong monolingual models. This evidence motivated Isbister et al. (2021) to investigate whether using automatically translated datasets is sufficient in these tasks and there is no need to train monolingual models on various languages. \n\nThere are several successful attempts in the literature reporting the use of dataset translation techniques. Rodrigues et al. (2019) demonstrated that translating sentences to English in order to use an English-based pretrained model outperforms its multilingual counterpart for semantic similarity and textual entailment tasks. In other studies, the dataset translation goal is not to replace multilingual models, but to provide more training data for downstream tasks that can be used in finetuning. Carrino et al. (2019) proposed a method called Translate Align Retrieve for automatically translating the SQuAD dataset. Using the translated resource, they finetuned a multilingual BERT, and show significant improvements over previous works. In a related approach, Ara\u00fajo et al. (2020) demonstrated that using machine translation to translate examples to English in order to use an English monolingual model culminates in better results when compared to monolingual models in languages other than English.",
            "score": 0.4590878307322016,
            "section_title": "Related Work",
            "char_start_offset": 4305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 1024,
                    "end": 1047,
                    "matchedPaperCorpusId": "214676745"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "271050875",
            "title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
            "text": "For the languages with two annotators, we also evaluated the inter-annotator agreement by Cohen's Kappa (Cohen, 1968) as shown in Table 3.\n\nOn average, the translation quality of DeepL was rated better, with a higher margin for French and German.In terms of Cohen's \u03ba, we see for Meta-Translator a moderate agreement and for DeepL a fair agreement.\n\nThe bias was rated by the annotators in a translation sample as equal to the English original in most cases.In a few instances, the annotators found no bias in either the CrowS-pairs sample or the trans-  lation.This highlights a potential weakness of the CrowS-pairs dataset.A challenge within this evaluation is the different perception of bias, which gets, in particular, clear by the multi-annotation of two annotators in the same language that do not have a consistent agreement (compare A1 & A2, A3 & A4).Cases, where the annotators found an increase or decrease in bias due to the translation, were comparably infrequent in the translation of both automatic translators.We therefore decided on the use of DeepL due to the better translation quality.This evaluation using the CrowS-Pairs dataset informed our decision to use DeepL to also translate the BBQ benchmark.",
            "score": 0.4590542404568969,
            "section_title": "Translation and Annotation",
            "char_start_offset": 11161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 140,
                    "end": 246
                },
                {
                    "start": 246,
                    "end": 348
                },
                {
                    "start": 350,
                    "end": 458
                },
                {
                    "start": 458,
                    "end": 562
                },
                {
                    "start": 562,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 861
                },
                {
                    "start": 861,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1223
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 116,
                    "matchedPaperCorpusId": "29694079"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "253098917",
            "title": "Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings",
            "text": "We perform 1-to-1 annotation projection in two directions: Translate-Train: We translate the English train and development data to the target language. We project the gold labels from the English data to the translated dataset. We then train a sequence labelling model using only the automatically generated dataset for the target language. Translate-Test: We translate the target language test set to English. We then use a model trained in the English gold-labelled data to label the translated test set. Finally, we project the labelled sequences back to the target language. \n\nThese two data-based cross-lingual transfer approaches are compared with the zero-shot method in which a fine-tuned model using English goldlabelled data is evaluated by generating predictions in the target language. Finally, we also fine-tuned language models on the gold-labelled data, which would constitute the upper-bound in our experimental setting.",
            "score": 0.4588943127771814,
            "section_title": "Experimental Setup",
            "char_start_offset": 13286,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 936
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80322265625
        },
        {
            "corpus_id": "221246333",
            "title": "MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
            "text": "multilingual dataset which contains compositional representations that allow complex nested queries.\n\n\u2022 We build strong benchmarks on the released MTOP dataset using state-of-the-art multilingual pre-trained models for both flat and compositional representations. We demonstrate the effectiveness of our approaches by achieving new state-of-the-art result on exist-arXiv:2008.09335v2 [cs.CL] 27 Jan 2021\n\ning multilingual task-oriented semantic parsing datasets.\n\n\u2022 We demonstrate strong performance on zeroshot cross-lingual transfer using automatic translation and alignment, combined with a proposed distant supervision approach. We achieve 67.2% exact match accuracy (averaged across 5 languages) without using any target language data compared to best inlanguage model performance of 77.7%.",
            "score": 0.45856629932529025,
            "section_title": "Introduction",
            "char_start_offset": 1966,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76904296875
        },
        {
            "corpus_id": "253098629",
            "title": "Multilingual Multimodal Learning with Machine Translated Text",
            "text": "Existing work has tackled this problem by either jointly training on English multimodal data and multilingual text-only data (Liu et al., 2021;Ni et al., 2021), pretraining with a private dataset of multilingual captioned images (Jain et al., 2021), or machine translating multimodal pretraining data (Zhou et al., 2021). \n\nIn this paper, we further investigate the potential of machine translated text for both fine-tuning and pretraining across four diverse V&L tasks. 1 he overarching motivation is that machine translation is an inexpensive approach to producing large amounts of multilingual text compared to collecting data from humans, or scraping high-quality imagecaption pairs from the web. Having access to thou-sands of data points in a target language might indeed be necessary to improve cross-lingual performance in downstream tasks (Bugliarello et al., 2022). As such, translating fine-tuning data into multiple languages may be a compelling approach towards downstream task success. Moreover, if this can be achieved through machine translated text, it raises the question of whether we can also pretrain on many millions of multilingual translated examples. Motivated by the initial experiments of Zhou et al. (2021), we test this hypothesis further, on more languages and more tasks, reporting more nuanced results from large-scale translated text. \n\nOverall, we show that machine translation can provide inexpensive and impressive improvements when fine-tuning models for multilingual multimodal tasks. Moreover, translation-based pretraining leads to significant gains in zero-shot crosslingual transfer over existing approaches. However, we find mixed results when combining this with multilingual fine-tuning. There are still opportunities to realise further benefits from machine translated text, which may be found through more compute-intensive pretraining. \n\nContributions. 1) We present the TD-MML framework to narrow the gap between English and non-English languages in multimodal research. 2) In the process of translation-based pretraining, we present a reliable strategy to filter out bad translations. 3) We conduct systematic evaluations in zero-shot and machine translated scenarios, and show the benefits that can be gained from simply having more data in the target languages.",
            "score": 0.45855287747513496,
            "section_title": "Introduction",
            "char_start_offset": 1641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1883
                },
                {
                    "start": 1886,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2313
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 143,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 143,
                    "end": 159,
                    "matchedPaperCorpusId": "219303834"
                },
                {
                    "start": 229,
                    "end": 248,
                    "matchedPaperCorpusId": "244119621"
                },
                {
                    "start": 301,
                    "end": 320,
                    "matchedPaperCorpusId": "232478575"
                },
                {
                    "start": 1216,
                    "end": 1234,
                    "matchedPaperCorpusId": "232478575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81005859375
        },
        {
            "corpus_id": "248779950",
            "title": "Natural Language Processing for Multilingual Task-Oriented Dialogue",
            "text": "\u2022 Current evaluation protocols in TOD; \n\n\u2022 Automatic vs. human-centered evaluation in multilingual setups: pros and cons; \u2022 How to evaluate language-specific phenomena and fluency; \u2022 Difficulties in evaluation and current gaps in evaluation resources. \n\nPart V: Open Challenges and Research Directions in Multilingual TOD (40 minutes) \n\nIn the concluding Part V, we will discuss the main open challenges impeding the development of TOD systems and reflect on the promising avenues for further progress. First, we will advocate for linguistically motivated design of multilingual TOD datasets focusing on linguistic diversity and idiomacity. To fulfill their role as gauges of model performance across languages (Hu et al., 2020;Liang et al., 2020), multilingual datasets should (i) maximise diversity along the dimensions of language family, geographic area, and typological features (Ponti et al., 2020). as well as (ii) adequately represent the linguistic and extra-linguistic (e.g., world knowledge, cultural references) properties of selected languages (rather than replicating dialogue structures, topics, and entities from a resource-rich source language). We will discuss first attempts at cultural adaptation for dialogue (Majewska et al., 2022). Second, we will outline how existing strategies for dealing with data scarcity can be borrowed from other NLP tasks to benefit multilingual and cross-lingual TOD NLU (Ponti et al., 2019;Hedderich et al., 2021). Third, we will emphasise the importance of user-centered evaluation as a way of assessing the fluency of generated responses and guiding improvements in ToD systems across different languages. Finally, we will discuss the significance of developments in multilingual ASR and TTS as keys to the ultimate success of multilingual ToD on a wide scale, and the potential of integrating speech-based and text-based modules in future research. Topics: \n\n\u2022 Recommendations for creation of future multilingual TOD datasets: linguistic diversity and idiomacity, low-resource languages, expansion to new domains; \u2022 Coping with low-resource scenarios: methods and lessons learned from other NLP tasks and applications; source selection for multi-source transfer and multilingual training;",
            "score": 0.4583510510345598,
            "section_title": "Motivation and Objectives",
            "char_start_offset": 5776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 38
                },
                {
                    "start": 41,
                    "end": 251
                },
                {
                    "start": 254,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1910
                },
                {
                    "start": 1913,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 711,
                    "end": 728,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 728,
                    "end": 747,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 884,
                    "end": 904,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1421,
                    "end": 1441,
                    "matchedPaperCorpusId": "202772914"
                },
                {
                    "start": 1441,
                    "end": 1464,
                    "matchedPaperCorpusId": "225062337"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5830078125
        },
        {
            "corpus_id": "276421759",
            "title": "Language Barriers: Evaluating Cross-Lingual Performance of CNN and Transformer Architectures for Speech Quality Estimation",
            "text": "Ideally, a comparative analysis would require datasets that differ only in language while remaining identical in all other aspects-such as recording quality, introduced degradations, and subjective rating conditions. This would involve speech samples recorded at uniform quality (e.g., 48 kHz), processed with identical degradation pipelines, and evaluated by native listeners under consistent rating protocols. \n\nUnfortunately, such standardized datasets are currently unavailable. Instead, we utilize a diverse dataset compiled for the ITU-T P.SAMD initiative, which includes speech samples across multiple languages, raters, environments, and degradation types. While this dataset provides valuable insights into cross-lingual variability, it suffers from inconsistencies in degradation conditions, as data collection and processing were conducted independently across different laboratories. To ensure a meaningful comparison, we restrict our analysis to languages with at least 1,000 samples, as summarized in Table 1. Our final dataset comprises over 49,000 training samples, with Dutch being the smallest subset (1,035 samples). While this dataset is not strictly comparable across languages, we hypothesize that the extensive sample size per language offsets some of the inconsistencies inherent to the data. \n\nThis study systematically evaluates the factors influencing cross-lingual generalization in speech quality models. Specifically, we examine: \n\n\u2022 Source Material: The impact of speaker characteristics, recording environments, and linguistic variations. \n\n\u2022 Degradations: Differences in speech distortion types and their severity across datasets. \n\n\u2022 Model Architecture: Comparing CNN-based (NISQA) and Transformer-based (AST) approaches. \n\n\u2022 Subjective Evaluations: Variability in MOS ratings across linguistic groups. \n\n\u2022 Dataset Composition: The influence of dataset structure, including single-speaker vs. conversational speech. \n\nBy analyzing these factors, we aim to provide deeper insights into how architectural differences between CNNs and Transformers influence speech quality modeling, the extent to which languagespecific biases affect model predictions, and strategies to enhance the cross-lingual robustness of objective speech quality assessment models.",
            "score": 0.45830382692699845,
            "section_title": "Introduction",
            "char_start_offset": 2163,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1316
                },
                {
                    "start": 1319,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1459
                },
                {
                    "start": 1462,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1663
                },
                {
                    "start": 1666,
                    "end": 1755
                },
                {
                    "start": 1758,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 2285
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73681640625
        },
        {
            "corpus_id": "273901276",
            "title": "Evaluating Open-Source LLMs in Low-Resource Languages: Insights from Latvian High School Exams",
            "text": "Recent advances on LLMs have led to impressive gains on NLU benchmarks, starting from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) with 10 tasks related to different NLU problems, followed by MMLU (Hendrycks et al., 2021) which covers nearly 60 subjects (including STEM: science, technology, engineering and mathematics), and Bigbench (Srivastava et al., 2023) with more than 200 tasks, as well as many other benchmarks. \n\nAgain, many well-known benchmarks are available only in English and other widely spoken languages. Google researchers addressed the need for a highly multilingual benchmark when the first transformer-based LLMs appeared by introducing the Cross-lingual Transfer Evaluation of Multilingual Encoders (XTREME) benchmark (Hu et al., 2020) which is used to evaluate cross-lingual generalization capabilities of multilingual representations. Although XTREME covers 40 typologically diverse languages, spanning 12 language families, Baltic languages are not included in this benchmark. Similarly, the dataset for the evaluation of multilingual LLMs developed by Okapi (Lai et al., 2023), in which the English part was translated with the help of ChatGPT, covers 26 languages except the Baltic languages (the \"smallest\" language is Danish with 6 million speakers, followed by Slovak with 7 million speakers). \n\nThe development of test sets for specific languages involves significant human resources. Therefore a widely used strategy is to apply machine translation, with or without manual postediting. Recently, this approach was chosen to translate the MMLU and COPA (Gordon et al., 2012) datasets into Latvian.1 Evaluation of OpenAI ChatGPT 3.5 Turbo and Google Gemini 1.0 Pro on the machine translated MMLU dataset shows that performance of these LLMs for Latvian is worse when compared to English (Bakanovs, 2024). It should be noted, that this dataset is not manually post-edited, and machine translation most likely has introduced some errors which can result in generating wrong answers.",
            "score": 0.45817992964753385,
            "section_title": "Related Work",
            "char_start_offset": 1763,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 110,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 125,
                    "end": 144,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 211,
                    "end": 235,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 349,
                    "end": 374,
                    "matchedPaperCorpusId": "263625818"
                },
                {
                    "start": 1098,
                    "end": 1116,
                    "matchedPaperCorpusId": "260334562"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "274149965",
            "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
            "text": "In this section we list some limitations of our work. \n\nLanguages included. Our primary focus is on English-German language training, as these two Germanic family languages share linguistic similarities (Lewis et al., 2015). German is one of the most well-represented languages in the mC4 dataset, facilitating model comparisons with varying amounts of German and English data. Furthermore, the availability of extensive public resources for German, including translation systems and translated evaluation data, further supports our emphasis on this language pair. We experimented with seven additional datasets including French, Spanish, Italian, Portuguese, Korean, Japanese, and Chinese. However, we note that there are many other languages within mC4 and more broadly which can benefit from having auxiliary English data for pretraining. Due to limited evaluation benchmarks and availability of target language data for comparison, we leave investigation for truly low-resource languages to future work. \n\nEvaluation data. Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models. Model size. Finally, this work studies three model sizes up to 3B models. We note that there are many standard benchmarks that can be evaluated at 1B-3B scale, however many more benchmarks and patterns can appear at larger model sizes. It is important future work to evaluate whether the results extend to larger scales including evaluating potential \"emergent behaviors\" as well as risks at larger scales (Wei et al., 2022).",
            "score": 0.4580743205049682,
            "section_title": "Limitations",
            "char_start_offset": 32229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9091796875
        },
        {
            "corpus_id": "264832929",
            "title": "From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities",
            "text": "VLP datasets are usually large-scale datasets of image-text pairs automatically extracted from online sources. As quality Subjective human opinion Subjective answers, controversial topics assurance is difficult for such large-scale datasets, it often leads to image-text misalignment issues and data redundancy [275]. VLP datasets are also primarily English-based and translations of native English answers are usually erroneous [124]. Current training strategies relying on multilingual captioning [276] use human-annotated captions to overcome annotation artifacts. However, cross-lingual performance is still low compared to its native English counterparts [124]. Potential research directions may include introducing non-English VLP datasets or improving cross-lingual capabilities by enhancing cross-lingual model architecture.",
            "score": 0.4580522238626589,
            "section_title": "VLP Dataset",
            "char_start_offset": 89571,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44580078125
        },
        {
            "corpus_id": "233481097",
            "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
            "text": "We consider three evaluation benchmarks. For cross-lingual understanding, we use cross-lingual natural language inference and question answering, and use the GLUE benchmark to evaluate the English performance.\n\nCross-lingual Natural Language Inference.\n\nThe XNLI dataset (Conneau et al., 2018) comes with ground-truth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency.\n\nCross-lingual Question Answering. We use the MLQA and XQuad benchmark from  and Artetxe et al. (2019), which extends the English SQuAD benchmark to more languages.\n\nWe report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English.\n\nThe English GLUE Benchmark. Finally, we evaluate the English performance of our model on the GLUE benchmark (Wang et al., 2018) which gathers multiple classification tasks, such as MNLI (Williams et al., 2017), SST-2 (Socher et al., 2013), or QNLI (Rajpurkar et al., 2018).",
            "score": 0.4579765947182617,
            "section_title": "Evaluation",
            "char_start_offset": 4890,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1443,
                    "end": 1466,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1474,
                    "end": 1495,
                    "matchedPaperCorpusId": "990233"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9072265625
        },
        {
            "corpus_id": "267412633",
            "title": "Analyzing the Evaluation of Cross-Lingual Knowledge Transfer in Multilingual Language Models",
            "text": "The Paraphrase Identification task evaluates a model's understanding of the semantic similarity between two sentences (Wang et al., 2018). Paraphrase Adversaries from Word Scrambling (PAWS) is a challenging dataset for this task, where both sentences in each example have high word overlap (Zhang et al., 2019). PAWS-X is a multilingual benchmark and extends this dataset to six languages beyond English using professionally translated validation and test sets (Yang et al., 2019). We employ a similar evaluation setup as described for NLI to assess the cross-lingual capability of multilingual models on the semantic similarity task. Since not all instances are translated into all six other languages in the dataset, we only consider parallel sentences for the evaluation, resulting in the exclusion of a small number of examples (less than 0.5% on average) from the test sets. \n\nThe performance of the fine-tuned models on PAWS-X is presented in Table 2, and the detailed results can be found in Figures 5-7. In the within language setup, where the sentences in every instance are from the same language, the models demonstrate the successful knowledge transfer across languages, as indicated by their relatively high performance compared to English. However, in the across language setting, which tests the models' cross-lingual ability in a more challenging scenario, similar to the findings in XNLI, there is Table 3: Zero-shot F1 scores of fine-tuned models for the QA task using the within and across language evaluation approaches. All the models struggle with the across setup, especially for mBERT and XLM-r, where we observe more than 50% drop in their performance, challenging the extent of their cross-linguality. \n\na significant drop in performance for the models. These results show that comprehending information from two different languages and making semantic-based decisions pose challenges for multilingual models, particularly for non-Latin script languages, where their performance is noticeably affected.",
            "score": 0.45790200572271655,
            "section_title": "Paraphrase Identification",
            "char_start_offset": 13824,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2028
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 137,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 290,
                    "end": 310,
                    "matchedPaperCorpusId": "91184042"
                },
                {
                    "start": 461,
                    "end": 480,
                    "matchedPaperCorpusId": "201698093"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "268681702",
            "title": "Can Machine Translation Bridge Multilingual Pretraining and Cross-lingual Transfer Learning?",
            "text": "The successes of pretrained multilingual language models (LM) on cross-lingual tasks have been underscored time and time again (Wu and Dredze, 2019, e.g.,), and appear all the more surprising that they are often pretrained on datasets comprising multiple languages, without explicit cross-lingual supervision (cf., for instance, Liu et al., 2020, though explicit supervision also exists, Xue et al., 2021).Explicit alignments such as linear mapping (Wang et al., 2019) andL2 alignment (Cao et al., 2020) between source and target languages do not necessarily improve the quality of cross-lingual representations (Wu and Dredze, 2020).\n\nThis is somewhat at odds with expectations from earlier studies in machine translation (MT).In particular, MT systems have historically connected with the concept of an interlingua-a languageindependent representation space that MT systems can leverage to perform translation (Masterman, 1961;Lu et al., 2018).As such, MT models are expected to pick up on language-independent semantic features (Tiedemann, 2018)-though in practice, this shared representation space can be in a tradeoff relationship with performance, which benefits from a greater separability of source language representations (Chen et al., 2023, e.g.).\n\nResearch questions This paper investigates whether machine translation as a learning objective can improve performances on zero-shot cross-lingual transfer downstream tasks.We expect that MT objectives, as they provide explicit cross-lingual alignments, should benefit cross-lingual transfer tasks.This paper, therefore, focuses on comparing the cross-lingual abilities of publicly available pretrained models-both MT models trained from scratch and multilingual LMs where pretraining has been continued with an MT objective.We attempt to establish whether MT training objectives implicitly foster cross-lingual alignment:\n\n(i) Do models (re)trained with the MT objective develop cross-lingual representations?\n\n(ii) Do they generalize well on cross-lingual tasks?\n\n(iii) Which factors impact their performances?",
            "score": 0.4576636912677462,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 406
                },
                {
                    "start": 406,
                    "end": 634
                },
                {
                    "start": 636,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1258
                },
                {
                    "start": 1260,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1558
                },
                {
                    "start": 1558,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1882
                },
                {
                    "start": 1884,
                    "end": 1970
                },
                {
                    "start": 1972,
                    "end": 2024
                },
                {
                    "start": 2026,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 472,
                    "matchedPaperCorpusId": "202578048"
                },
                {
                    "start": 912,
                    "end": 929,
                    "matchedPaperCorpusId": "36728371"
                },
                {
                    "start": 929,
                    "end": 945,
                    "matchedPaperCorpusId": "5067886"
                },
                {
                    "start": 1031,
                    "end": 1048,
                    "matchedPaperCorpusId": "9098487"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.315673828125
        },
        {
            "corpus_id": "19211851",
            "title": "Assessing multilingual multimodal image description: Studies of native speaker preferences and translator choices",
            "text": ", 'tailgating' (elaborate picnicking around the back of a vehicle, usually associated with a sports event) is a popular activity in the USA that is obscure to German and Dutch speakers ( van Miltenburg, Elliott and Vossen 2017). A description of an image depicting tailgating thus needs to be phrased differently depending on the audience, which varies with language.\n\nThe two scenarios outlined above have been codified as a multimodal translation task and a description generation task, respectively, as part of the multimodal machine translation (MMT) shared task held at the Conference for Machine Translation in 2016 and 2017 Elliott et al. 2017). The two tasks use different training data: the multimodal translation task is based on images with parallel translations of descriptions, while in the cross-lingual image description task, the training data consists of images with independently authored descriptions in multiple languages. Evaluation also proceeds differently, since multimodal translation is evaluated as translation, based on the faithfulness of the target language description to the source language description, while cross-lingual image description is evaluated based on the similarity of the generated description to multiple reference target-language descriptions, collected independently.\n\nIn this paper, we re-evaluate and test the assumptions, outlined above, behind the multimodal translation and description generation tasks. First, we assess whether the division into two separate tasks, one based on flexible description generation in the cross-lingual scenario and one focussed on literal translations, is actually necessary for the image description setting: do they result in measurably different descriptions? In particular, do target language speakers prefer descriptions created in their own language over translations from a different language? Note that the humangenerated target language descriptions constitute an upper bound, in terms of quality: automatically generated descriptions based on source-and target-language training data, in the form of either translations or independent descriptions, are expected to perform less well. If, for example, German speakers do not differentiate between German descriptions and translations into German, this has important implications for multilingual multimodal NLP in the cross-lingual, non-translation, setting.\n\nSecond, we examine the role of visual information in multimodal translation. Again, we take an approach based on human performance, but here we study how human translators use images during translation. Professional translators first translate",
            "score": 0.4575853521002059,
            "section_title": "Introduction",
            "char_start_offset": 4077,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 631,
                    "end": 650,
                    "matchedPaperCorpusId": "10643243"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54248046875
        },
        {
            "corpus_id": "270285718",
            "title": "mCSQA: Multilingual Commonsense Reasoning Dataset with Unified Creation Strategy by Language Models and Humans",
            "text": "Can you choose the correct answer in Table 1?Each choice is semantically very close, making it difficult for non-native speakers to distinguish them.However, native speakers who have languagespecific commonsense and knowledge can choose the most plausible choice considering subtle nuances.Despite the need to consider different backgrounds for each language, the datasets to evaluate the natural language understanding (NLU) capabilities of language models (LMs) are mostly for Table 1: Examples require language-specific knowledge.\n\nThey cannot be solved without such knowledge, as the translations consolidate the nuances into a single word.\n\na few major languages such as English, and thus, many languages lack such datasets.When focusing on the cross-lingual capability of LMs, datasets created from scratch in multiple languages are lim-ited, and currently, evaluations mostly use datasets created through translation.However, as can be seen from the example in Table 1, datasets created through translation cannot accurately evaluate language-specific commonsense or knowledge.Therefore, it is necessary to create datasets for each language from scratch, but the manual creation of such datasets is limited by the availability of annotators and financial costs.\n\nTo tackle this problem, as shown in Figure 1, we propose a method to efficiently create multilingual NLU datasets from multilingual resources by replacing some of the manual annotation processes with generative multilingual LMs.In this study, we focus on CommonsenseQA (CSQA) (Talmor et al., 2019), a dataset for evaluating commonsense reasoning capabilities within NLU evaluations.CSQA is a major commonsense reasoning Question-Answering dataset manually created from the multilingual knowledge base Concept-Net (Speer et al., 2017).However, due to such limitations, CSQA has been created from scratch only in English and Japanese, JCommonsenseQA (JCSQA) (Kurihara et al., 2022).Therefore, we create a Multilingual CommonsenseQA (mCSQA) that extends CSQA to eight languages 1 using our proposed method.",
            "score": 0.457541289898561,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 45,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 290
                },
                {
                    "start": 290,
                    "end": 533
                },
                {
                    "start": 535,
                    "end": 644
                },
                {
                    "start": 646,
                    "end": 729
                },
                {
                    "start": 729,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1268
                },
                {
                    "start": 1270,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1652
                },
                {
                    "start": 1652,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 1546,
                    "end": 1567,
                    "matchedPaperCorpusId": "53296520"
                },
                {
                    "start": 1783,
                    "end": 1803,
                    "matchedPaperCorpusId": "15206880"
                },
                {
                    "start": 1926,
                    "end": 1949,
                    "matchedPaperCorpusId": "249687727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "256461245",
            "title": "Discovering Low-rank Subspaces for Language-agnostic Multilingual Representations",
            "text": "While Tatoeba reveals the cross-lingual transferability across English-centric language pairs, it is restricted to monolingual pools (i.e., the set of candidates is restricted to certain language). Therefore, it fails to thoroughly evaluate whether texts with a similar semantic meaning are grouped together in the latent space, regardless of their languages. \n\nWith that in mind, we further examine the alignment methods on LAReQA (Roy et al., 2020), a challenging cross-lingual answer retrieval task. Unlike Tatoeba, the targets of LAReQA must be retrieved from a large multilingual candidate pool. It consists of two sub-datasets, XQuAD-R and MLQA-R, whose candidate pool covers 11 and 7 languages respectively. \n\nWe follow Yang et al. (2021) to evaluate the alignment methods on two models, mBERT (En-En) and mBERT (X-X). Specifically, mBERT (En-En) finetunes the original mBERT model on the English QA pairs collected from the SQuAD v1.1 dataset. mBERT (X-X) employs the same training procedure but with an extended dataset where each sample is translated into the 11 XQuAD languages. Since all positive samples for finetuning are within the same language as the question query, both models exhibit strong self-language bias while preserving the weak alignment property. For evaluation, we use the dot product of embeddings to score a QA pair, which accords with the finetuning protocol. The retrieval performance is measured by mean Average Precision (mAP). \n\nTable 2 reports our LAReQA results. We can observe that applying LSAR again results in signification improvements, nearly doubling mAP of mBERT (X-X) on XQuAD-R. Since in the candidate pool each language has one of the relevant answers, better retrieval performances directly indicate better language agnosticism. Centered and LIR (k = 1) also show impressive performances, suggesting that in weakly aligned multilingual sys- tems, the mean embeddings and principal components do encode language-specific signals. But for LIR, it is shown that removing the first principal component consistently leads to the best performance.",
            "score": 0.4575344611106823,
            "section_title": "Language-agonstic Answer Retrieval",
            "char_start_offset": 17223,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 359
                },
                {
                    "start": 362,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1979
                },
                {
                    "start": 1980,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 432,
                    "end": 450,
                    "matchedPaperCorpusId": "215745460"
                },
                {
                    "start": 727,
                    "end": 745,
                    "matchedPaperCorpusId": "237485496"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80029296875
        },
        {
            "corpus_id": "258999460",
            "title": "Improving Polish to English Neural Machine Translation with Transfer Learning: Effects of Data Volume and Language Similarity",
            "text": "In this section, we describe the methodology we used to study the impact of data volume and language similarity on transfer learning in machine translation. \n\nWe fine-tuned the multilingual BART (mBART) (Tang et al., 2020) model for the Polish-English translation task. mBART is a pre-trained language model developed by Facebook AI Research (FAIR) that is designed to improve machine translation and other sequence-to-sequence tasks across multiple languages. It is based on the BERT architecture and is trained on a diverse set of languages. mBART has achieved state-of-the-art performance on various machine translation benchmarks and has shown promising results in crosslingual transfer learning tasks. \n\nThe fine-tuning is done using the OPUS-100 corpus. It is a large-scale parallel corpus consisting of more than 100 million sentences in over 100 languages (Zhang et al., 2020). The corpus is designed to facilitate research on multilingual natural language processing, including machine translation, cross-lingual information retrieval, and language modeling. The data is collected from various sources, including web pages, books, and subtitles, and the text is aligned at the sentence level to create parallel corpora for each language pair. Being one of the largest open parallel corpora available, the Opus-100 corpus has become a widely used benchmark dataset for multilingual machine translation and has been used in a number of studies exploring various approaches to multilingual natural language processing. \n\nTo evaluate the impact of data volume and the use of related languages, we propose five different models. First, we use a baseline model finetuned only on Polish. The other four models are trained in the same manner as Zoph et al. (Zoph et al., 2016) in a parent-child configuration. We finetune a parent model first in other languages in a translation task to English. We swap the training corpus and fine-tuning is then continued on these models on the Polish to English task. \n\nThe composition of the parent models varies in terms of language similarity, with the first parent model using Czech, a West Slavic language similar to Polish. The second model is fine-tuned in Russian, which is an East Slavic language, a slightly more distant cousin to Polish and Czech.",
            "score": 0.45745122013479167,
            "section_title": "Methods",
            "char_start_offset": 7964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 159,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1524
                },
                {
                    "start": 1527,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2005
                },
                {
                    "start": 2008,
                    "end": 2167
                },
                {
                    "start": 2168,
                    "end": 2296
                }
            ],
            "ref_mentions": [
                {
                    "start": 864,
                    "end": 884,
                    "matchedPaperCorpusId": "216144650"
                },
                {
                    "start": 1758,
                    "end": 1776,
                    "matchedPaperCorpusId": "16631020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "195886574",
            "title": "Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges",
            "text": "Metrics for automatic quality evaluation (Papineni et al., 2002) have been critical to the rapid progress in machine translation, by making evaluation fast, cheap and reproducible. For multilingual NMT, new concerns arise due to the multiobjective nature of the problem and the inherent quality trade-offs between languages. \n\nInter-language quality trade-offs arise due to various decisions made while constructing, training and selecting a model. When constructing the model, the vocabulary may be constructed to favor a certain script or group of languages, or the language specific parameters may be unevenly distributed across languages. During training, the optimization settings or the rate at which training data from different languages are sampled strongly influence the eventual performance on different languages. Finally, when selecting a checkpoint 10 , the model may perform better on high resource languages at later checkpoints but may have regressed on low resource languages by that time due to over-training (or under-training for the opposing case). Each of these choices naturally may favor certain languages over others. \n\nTo choose between the aforementioned tradeoffs, we need a translation quality metric that is 10 Certain snapshot of the model parameters. \n\nboth effective and comparable across languages. This in and of itself is a hard problem with an ever growing list of hundreds of metrics to choose from. Oftentimes these metrics vary in their effectiveness across languages; WMT shared tasks (Ma et al., 2018b) report that the specific language, dataset, and system significantly affect which metric has the strongest correlation with human ratings. When metrics are sufficiently effective across languages they are not always comparable. N-gram based metrics (Papineni et al., 2002;Doddington, 2002;Wang et al., 2016;Popovi\u0107, 2015) that measure lexical overlap require tokenization which is highly affected by language specific factors such as alphabet size and morphological complexity. In fact, even within the same language, tokenization discrepancies pose significant challenges to reliable evaluation (Post, 2018). Embedding based approaches (Stanojevic and Sima'an, 2014) may be language agnostic and help address these issues. \n\nEqually important to choosing a metric is choosing an evaluation set.",
            "score": 0.4572092858304557,
            "section_title": "Evaluation",
            "char_start_offset": 49171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1070
                },
                {
                    "start": 1071,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2269
                },
                {
                    "start": 2272,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 64,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1527,
                    "end": 1545,
                    "matchedPaperCorpusId": "53246643"
                },
                {
                    "start": 1795,
                    "end": 1818,
                    "matchedPaperCorpusId": "11080756"
                },
                {
                    "start": 1818,
                    "end": 1835,
                    "matchedPaperCorpusId": "263836548"
                },
                {
                    "start": 1835,
                    "end": 1853,
                    "matchedPaperCorpusId": "15421052"
                },
                {
                    "start": 1853,
                    "end": 1867,
                    "matchedPaperCorpusId": "15349458"
                },
                {
                    "start": 2183,
                    "end": 2213,
                    "matchedPaperCorpusId": "17807266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5517578125
        },
        {
            "corpus_id": "258715321",
            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
            "text": "Based on the collected dataset, we report baseline results on two subtasks: (a) candidate ranking, which selects the best English candidate that contains the information to answer the non-English question; (b) answer generation, which generates a natural-sounding non-English answer to present to the user based on the selected English candidate. We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability. The translation-based approach is the most effective for candidate ranking while the multilingual-finetuning works the best for answer generation. Nonetheless, on both tasks, there is a substantial gap between the English-based and cross-lingual performances. In the following, we first elaborate on the problem formulation for the cross-lingual PQA task ( \u00a72), then explain the xPQA data collection process ( \u00a73), and present experiment results ( \u00a75.2) and conclusions ( \u00a76).",
            "score": 0.45716914378098533,
            "section_title": "Answer:",
            "char_start_offset": 2499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89208984375
        },
        {
            "corpus_id": "231979145",
            "title": "MUDES: Multilingual Detection of Offensive Spans",
            "text": "A large model; en-large which is more accurate, but has a low efficiency regarding space and time. The base model; en-base is efficient, but has a comparatively low accuracy than the en-large model. \n\nMultilingual models -The motivation behind the use of multilingual models comes from recent work (Ranasinghe and Zampieri, 2020) which used transfer learning and cross lingual embeddings. This study showed that cross-lingual transformers like XLM-R (Conneau et al., 2019) can be trained on an English dataset and have the model weights saved to detect offensive language in other languages outperforming monolingual models trained on the target language dataset. We used a similar methodology but for the token classification architecture instead. We used XLM-R cross-lingual transformer model (Conneau et al., 2019) as the Transformer in Figure 1 on TSDTrain and carried out evaluations on the Danish and Greek datasets. We release two multilingual models; multilingualbase based on XLM-R base model and multilinguallarge based on XLM-R large model.",
            "score": 0.4569421350300025,
            "section_title": "Methodology",
            "char_start_offset": 8407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1051
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 329,
                    "matchedPaperCorpusId": "210510414"
                },
                {
                    "start": 450,
                    "end": 472,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 795,
                    "end": 817,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35595703125
        },
        {
            "corpus_id": "274437132",
            "title": "SailCompass: Towards Reproducible and Robust Evaluation for Southeast Asian Languages",
            "text": "Existing English multi-task benchmarks, such as GLUE [42] and SuperGLUE [41], have undoubtedly stimulated the growth in research interest and efforts on the transfer learning ability of language models across diverse tasks. However, the development of multilingual benchmarks has significantly lagged behind that of English-dominant benchmarks. To fill this gap, XTREME [18] contributed a comprehensive multilingual multi-task benchmark for evaluating cross-lingual transfer learning across 40 languages with 9 datasets. MTG [9] included four human-annotated text generation datasets in five languages to support both training and test scenarios. \n\nXGLUE [23] expanded the task scope of the multilingual benchmark to encompass both natural language understanding and generation tasks. SeaEval [43] provided a benchmark for multilingual foundation models, additionally considering the cultural understanding ability of models, but the majority of the datasets were still based on high-resource languages, such as English and Chinese. The aforementioned benchmark either neglects tasks in Southeast Asian languages (e.g., MTG and XGLUE), or only contains a limited subset of tasks or languages from Southeast Asia (e.g., XTREME and SeaEval). \n\nRecent multilingual benchmarks like MEGA [1] and BUFFET [5], narrow the datasets that built from scratch in native languages and the models that specific designed to Southeast Asia (SEA). This limits our ability to draw comprehensive and systematic conclusions for SEA large language \n\nPlease follow the given examples, read the context, and answer the question.",
            "score": 0.4569221837072656,
            "section_title": "B Related Work on Multilingual Benchmarks",
            "char_start_offset": 24014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1239
                },
                {
                    "start": 1242,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1604
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 57,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 370,
                    "end": 374,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 525,
                    "end": 528,
                    "matchedPaperCorpusId": "237091323"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 793,
                    "end": 797,
                    "matchedPaperCorpusId": "261682140"
                },
                {
                    "start": 1283,
                    "end": 1286,
                    "matchedPaperCorpusId": "257663467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5673828125
        },
        {
            "corpus_id": "276107976",
            "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study",
            "text": "Our results caution against over reliance on machine translation in cross-language MF applications and raise broader methodology concerns for its reliability in measuring other cross-language human values. \n\nResearchers should also be cautious with local language lexicons-they perform worse than the machine translation for cross-language MF measurement in some cases. Given the inherent limitations of lexicon-based methods and the extensive resources required to develop them, they are inefficient for cross-language MF measurement, particularly at the document level where semantic complexity is higher. However, culturally specific moral lexicons may still pro-  Our evaluation with multilingual language models and LLMs also reveals several key implications. Above all, fine-tuning LLMs is more data-efficient than fine-tuning multilingual language models for cross-language MF measurement. In the CCV benchmarking dataset, XLM-T requires on average more than 2,000 local-language annotated records (20 batches) to achieve strong performance (i.e., F 1 = 0.75). In contrast, Llama3.1-8b can reach comparable performance with only English data machine-translated to Chinese and thus no local language labeling. \n\nIn addition, data augmentation shows strong potential in enhancing LLMs' performance on culturally distinct MF values. For example, Llama3.1-8b notably improves on \"loyalty\" when fine-tuned with English data machine-translated to Chinese (F 1 = 0.69), compared to its performance with the original English training data (F 1 = 0.57). Such data augmentation may serve as a practical and cost-effective way to improve model performance in cross-language measurements, especially in cases where mass-labeling of local-language data is infeasible due to the resource-intensive process of creating high-quality language-specific labeled datasets. \n\nOur findings also suggest that larger or more advanced LLMs such as Llama3.1-70b may perform better for measuring MFs in non-English corpora. That is, updating the base LLM to a more power model with enhanced multilingual capabilities could further improve the effectiveness of the LLM approach for cross-language MF measurement.",
            "score": 0.45669674771614943,
            "section_title": "Discussion and Limitations",
            "char_start_offset": 33045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8623046875
        },
        {
            "corpus_id": "237513912",
            "title": "On the Universality of Deep Contextual Language Models",
            "text": "There are over 7000 languages in the world. \n\nThere is an increased demand for multilingual systems as information technologies penetrate more lives globally. The largest available LMs include mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020a), andmT5 (Xue et al., 2020) serve 104, 100, and 101 languages respectively. It is clear that they are far from universal in terms of language coverage compared to the number of languages in the world. Further, there is an expectation that massive multilingual LMs will perform equally, or at least reasonably, well on all the languages they serve. \n\nThe limited availability of evaluation benchmarks is a major bottleneck in knowing how LMs perform across the languages they are pre-trained on. Table 1 shows that the largest benchmark, XTREME (Hu et al., 2020), covers less than half of the total number of languages that these LMs are trained on. Moreover, other than datasets for syntactic tasks like NER (Rahimi et al., 2019), and POS (Nivre et al., 2016), the largest available semantic task dataset, XNLI (Conneau et al., 2018), covers only 15 languages. Although LMs are tested on individual tasks or languages that may not be covered in these benchmarks, overall, there 2 Balanced version 3 Each task may cover only a subset of languages is a considerable reliance on standard benchmarks to make modelling choices. Thus, how well LMs perform in the untested languages remains unanswered. \n\nThere is a great disparity in performance across the languages that are tested through these benchmarks. A general observation is that the performance of low resource languages continues to be lower than high resource languages. The extent to which cross-lingual transfer helps improve performance varies across languages. Studies that empirically support these claims are: \n\nCross-script transfer is not equally good across languages in mBERT. Ahmad et al. 2019a find that cross-script cross-lingual transfer is effective in the case of Hindi and Urdu, whereas this is not observed between English and Japanese. \n\nWord order differences across language leads to worse cross-lingual transfer (Ahmad et al., 2019a).",
            "score": 0.4566425451585602,
            "section_title": "Language",
            "char_start_offset": 6557,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 43
                },
                {
                    "start": 46,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1446
                },
                {
                    "start": 1449,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1822
                },
                {
                    "start": 1825,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 228,
                    "end": 256,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 795,
                    "end": 812,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 959,
                    "end": 980,
                    "matchedPaperCorpusId": "153313061"
                },
                {
                    "start": 990,
                    "end": 1010,
                    "matchedPaperCorpusId": "17954486"
                },
                {
                    "start": 1062,
                    "end": 1084,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 1894,
                    "end": 1912,
                    "matchedPaperCorpusId": "67856712"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "paperId": "bdcf6e6bf085723de0ab43f6f1079c4db0d667ec",
            "corpusId": 277479215,
            "title": "Multilingual Table Question Answering for Low-Resource Languages via Selective Question Translation and Cross-Lingual Adaptation",
            "venue": "International Conference on Big Data and Smart Computing",
            "year": 2025,
            "referenceCount": 25,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigComp64353.2025.00057?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigComp64353.2025.00057, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275937920",
                    "name": "Sanghyun Cho"
                },
                {
                    "authorId": "2110187863",
                    "name": "Minho Kim"
                },
                {
                    "authorId": "2306073098",
                    "name": "Hye-Lynn Kim"
                },
                {
                    "authorId": "2296495081",
                    "name": "Jung-Hun Lee"
                },
                {
                    "authorId": "2275995189",
                    "name": "Hyuk-Chul Kwon"
                }
            ],
            "abstract": "Table question answering datasets have been released in various table sources and diverse tasks. However, most of these table question answering datasets are constructed in resource-rich languages like English. For low-resource languages, it is necessary to create new datasets or translate existing ones to train models on table question answering tasks. This process requires substantial costs and manpower. Table data, unlike natural text, is structured in a way that is less affected by word order and language-specific characteristics. In this paper, we propose a method to minimize the costs associated with translating and applying new table question answering datasets. By leveraging the characteristics of table data, we suggest translating only the question data of the question answering table datasets into multiple languages. To minimize the performance degradation caused by using the native table data of the original dataset and not directly utilizing the table data in the target language, we applied adaptation methods using contrastive learning and adversarial training. To enhance the reasoning capabilities required for table question answering without diminishing performance in languages other than the one used for pre-training, we further pre-trained a RoBERTa-based multilingual encoder on table data using SQL data. The proposed cross-lingual adaptation demonstrated performance improvements in most of the languages evaluated. Additionally, the pre-training using SQL data significantly improved performance across all languages, even though the pre-training table data consisted of English data.",
            "corpus_id": "277479215",
            "text": "Table question answering datasets have been released in various table sources and diverse tasks. However, most of these table question answering datasets are constructed in resource-rich languages like English. For low-resource languages, it is necessary to create new datasets or translate existing ones to train models on table question answering tasks. This process requires substantial costs and manpower. Table data, unlike natural text, is structured in a way that is less affected by word order and language-specific characteristics. In this paper, we propose a method to minimize the costs associated with translating and applying new table question answering datasets. By leveraging the characteristics of table data, we suggest translating only the question data of the question answering table datasets into multiple languages. To minimize the performance degradation caused by using the native table data of the original dataset and not directly utilizing the table data in the target language, we applied adaptation methods using contrastive learning and adversarial training. To enhance the reasoning capabilities required for table question answering without diminishing performance in languages other than the one used for pre-training, we further pre-trained a RoBERTa-based multilingual encoder on table data using SQL data. The proposed cross-lingual adaptation demonstrated performance improvements in most of the languages evaluated. Additionally, the pre-training using SQL data significantly improved performance across all languages, even though the pre-training table data consisted of English data.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6396484375
        },
        {
            "paperId": "d732be98e4004d66f6b2bd0190555730080c22c7",
            "corpusId": 273350862,
            "title": "A Cross-Lingual Statutory Article Retrieval Dataset for Taiwan Legal Studies",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 17,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.11450, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312073459",
                    "name": "Yen-Hsiang Wang"
                },
                {
                    "authorId": "2325951160",
                    "name": "Feng-Dian Su"
                },
                {
                    "authorId": "2325950166",
                    "name": "Tzu-Yu Yeh"
                },
                {
                    "authorId": "2312006774",
                    "name": "Yao-Chung Fan"
                }
            ],
            "abstract": "This paper introduces a cross-lingual statutory article retrieval (SAR) dataset designed to enhance legal information retrieval in multilingual settings. Our dataset features spoken-language-style legal inquiries in English, paired with corresponding Chinese versions and relevant statutes, covering all Taiwanese civil, criminal, and administrative laws. This dataset aims to improve access to legal information for non-native speakers, particularly for foreign nationals in Taiwan. We propose several LLM-based methods as baselines for evaluating retrieval effectiveness, focusing on mitigating translation errors and improving cross-lingual retrieval performance. Our work provides a valuable resource for developing inclusive legal information retrieval systems.",
            "corpus_id": "273350862",
            "text": "This paper introduces a cross-lingual statutory article retrieval (SAR) dataset designed to enhance legal information retrieval in multilingual settings. Our dataset features spoken-language-style legal inquiries in English, paired with corresponding Chinese versions and relevant statutes, covering all Taiwanese civil, criminal, and administrative laws. This dataset aims to improve access to legal information for non-native speakers, particularly for foreign nationals in Taiwan. We propose several LLM-based methods as baselines for evaluating retrieval effectiveness, focusing on mitigating translation errors and improving cross-lingual retrieval performance. Our work provides a valuable resource for developing inclusive legal information retrieval systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.45849609375
        },
        {
            "paperId": "6c815248c431a23d2be29003dce430d6a8c2de0c",
            "corpusId": 268254629,
            "title": "A Comparative Analysis of Transformers for Multilingual Neural Machine Translation",
            "venue": "Conference Information and Communication Technology",
            "year": 2023,
            "referenceCount": 26,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CICT59886.2023.10455324?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CICT59886.2023.10455324, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290096677",
                    "name": "Prasanna Kumar"
                },
                {
                    "authorId": "2285905523",
                    "name": "Avuthu Avinash Reddy"
                },
                {
                    "authorId": "2290119449",
                    "name": "Siva Jyothi"
                },
                {
                    "authorId": "2290092928",
                    "name": "Natha Reddy"
                },
                {
                    "authorId": "2280749398",
                    "name": "Vemireddy Anvitha"
                }
            ],
            "abstract": "Multilingual neural machine translation (NMT) has emerged as a promising solution to break down language barriers and promote cross-lingual communication. Transformer-based NMT models have gained significant attention due to their ability to learn contextual information and their effectiveness in handling long-range dependencies. The transformer model, with its self-attention and cross-attention mechanisms, fundamentally alters machine translation when compared to other neural machine translation models and traditional machine translation models. Token alignments between source and target sentences are successfully modelled by these mechanisms. This paper deals with the performance of a transformer-based multilingual NMT model that uses a shared encoder and decoder. We conducted experiments on the publicly available Multi30k dataset for translation jobs involving German, French, and Czech into English. In comparison of models, the Transformer model has performed better against the Convolutional SeqtoSeq and Attention-Based Seq2Seq models. We have evaluated our model on the metric Bilingual Evaluation Understudy (BLEU), where the transformer achieved 36.67, 48.47, and 33.66 for DE, FR, and CZ to EN translations.",
            "corpus_id": "268254629",
            "text": "Multilingual neural machine translation (NMT) has emerged as a promising solution to break down language barriers and promote cross-lingual communication. Transformer-based NMT models have gained significant attention due to their ability to learn contextual information and their effectiveness in handling long-range dependencies. The transformer model, with its self-attention and cross-attention mechanisms, fundamentally alters machine translation when compared to other neural machine translation models and traditional machine translation models. Token alignments between source and target sentences are successfully modelled by these mechanisms. This paper deals with the performance of a transformer-based multilingual NMT model that uses a shared encoder and decoder. We conducted experiments on the publicly available Multi30k dataset for translation jobs involving German, French, and Czech into English. In comparison of models, the Transformer model has performed better against the Convolutional SeqtoSeq and Attention-Based Seq2Seq models. We have evaluated our model on the metric Bilingual Evaluation Understudy (BLEU), where the transformer achieved 36.67, 48.47, and 33.66 for DE, FR, and CZ to EN translations.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.4755859375
        },
        {
            "paperId": "8a4466e72db7bc7f5a95ac341843377176d35c36",
            "corpusId": 201668305,
            "title": "Translate and Label! An Encoder-Decoder Approach for Cross-lingual Semantic Role Labeling",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2019,
            "referenceCount": 62,
            "citationCount": 22,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D19-1056.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1908.11326, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46578259",
                    "name": "Angel Daza"
                },
                {
                    "authorId": "143876555",
                    "name": "A. Frank"
                }
            ],
            "abstract": "We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.",
            "corpus_id": "201668305",
            "text": "We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.477294921875
        },
        {
            "paperId": "b4437037f30c7a51bcd34510313bbaf3f89c1d4a",
            "corpusId": 245390093,
            "title": "Cross-lingual timeline summarization",
            "venue": "International Conference on Artificial Intelligence and Knowledge Engineering",
            "year": 2021,
            "referenceCount": 36,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/AIKE52691.2021.00014?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/AIKE52691.2021.00014, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1692878",
                    "name": "Luca Cagliero"
                },
                {
                    "authorId": "147574346",
                    "name": "Moreno La Quatra"
                },
                {
                    "authorId": "1712027",
                    "name": "P. Garza"
                },
                {
                    "authorId": "1750105",
                    "name": "Elena Baralis"
                }
            ],
            "abstract": "Timeline summarization methods analyze times-tamped, topic-specific news article collections to select the key dates representing the event flow and to extract the most relevant per-date content. Existing approaches are all tailored to a single language. Hence, they are unable to combine topic-related content available in different languages. Enriching news timelines with multilingual content is particularly useful for (i) summarizing complex events, whose main facets are covered differently by media sources from different countries, and (ii) generating news timelines in low-resource languages, for which there is a lack of news content in the target language.This paper presents three alternative approaches to address cross-lingual timeline summarization. They combine state-of-the-art extractive summarization methods with machine translation steps at different stages of the timeline generation process. The paper also proposes novel Rouge-based evaluation metrics customized for cross-lingual timeline summarization with a twofold aim: (i) quantifying the ability of the cross-lingual process to enhance available content extraction in the target language and (ii) estimating summarizer effectiveness in conveying additional content from other languages. A new multilingual timeline benchmark dataset has been generated to allow a thorough analysis of the factors that mainly influence summarization performance.",
            "corpus_id": "245390093",
            "text": "Timeline summarization methods analyze times-tamped, topic-specific news article collections to select the key dates representing the event flow and to extract the most relevant per-date content. Existing approaches are all tailored to a single language. Hence, they are unable to combine topic-related content available in different languages. Enriching news timelines with multilingual content is particularly useful for (i) summarizing complex events, whose main facets are covered differently by media sources from different countries, and (ii) generating news timelines in low-resource languages, for which there is a lack of news content in the target language.This paper presents three alternative approaches to address cross-lingual timeline summarization. They combine state-of-the-art extractive summarization methods with machine translation steps at different stages of the timeline generation process. The paper also proposes novel Rouge-based evaluation metrics customized for cross-lingual timeline summarization with a twofold aim: (i) quantifying the ability of the cross-lingual process to enhance available content extraction in the target language and (ii) estimating summarizer effectiveness in conveying additional content from other languages. A new multilingual timeline benchmark dataset has been generated to allow a thorough analysis of the factors that mainly influence summarization performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.434814453125
        },
        {
            "paperId": "d167e9dcc2eb013091809f61c9c35bd2dc48f4ef",
            "corpusId": 277781617,
            "title": "X-Guard: Multilingual Guard Agent for Content Moderation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 54,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.08848, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "117783847",
                    "name": "Bibek Upadhayay"
                },
                {
                    "authorId": "2355357197",
                    "name": "Ph.D Vahid Behzadan"
                },
                {
                    "authorId": "2330143099",
                    "name": "Minjia Wang"
                },
                {
                    "authorId": "2290132976",
                    "name": "Pingping Lin"
                },
                {
                    "authorId": "2330243810",
                    "name": "Siqi Cai"
                },
                {
                    "authorId": "2320696072",
                    "name": "Shengnan An"
                },
                {
                    "authorId": "2330239532",
                    "name": "Shengjie Ma"
                },
                {
                    "authorId": "2277149729",
                    "name": "Zeqi Lin"
                },
                {
                    "authorId": "2355366812",
                    "name": "Congrui Huang"
                },
                {
                    "authorId": "2355357619",
                    "name": "Alexander Wei"
                },
                {
                    "authorId": "3033269",
                    "name": "Nika Haghtalab"
                },
                {
                    "authorId": "2288262460",
                    "name": "Jacob Steinhardt"
                },
                {
                    "authorId": "2355357191",
                    "name": "Jailbroken"
                },
                {
                    "authorId": "2253952872",
                    "name": "Jason Wei"
                },
                {
                    "authorId": "2275277634",
                    "name": "Xuezhi Wang"
                },
                {
                    "authorId": "2265994815",
                    "name": "Dale Schuurmans"
                },
                {
                    "authorId": "40377863",
                    "name": "Maarten Bosma"
                },
                {
                    "authorId": "2291173007",
                    "name": "Fei Xia"
                },
                {
                    "authorId": "2253469026",
                    "name": "E. Chi"
                },
                {
                    "authorId": "2261982078",
                    "name": "V. Quoc"
                },
                {
                    "authorId": "2286991163",
                    "name": "Denny Le"
                },
                {
                    "authorId": "2321896602",
                    "name": "Zhou"
                },
                {
                    "authorId": "2355361016",
                    "name": "Zeming Wei"
                },
                {
                    "authorId": "2286566741",
                    "name": "Yifei Wang"
                },
                {
                    "authorId": "2264494159",
                    "name": "Ang Li"
                },
                {
                    "authorId": "2283768780",
                    "name": "Yichuan Mo"
                },
                {
                    "authorId": "2257103809",
                    "name": "Yisen Wang"
                },
                {
                    "authorId": "2355808381",
                    "name": "Xiaofei Wen"
                },
                {
                    "authorId": "2355357143",
                    "name": "Wenjie Wenxuan Zhou"
                },
                {
                    "authorId": "2355402819",
                    "name": "Jacky Mo"
                },
                {
                    "authorId": "2355357365",
                    "name": "Thinkguard"
                },
                {
                    "authorId": "2354109310",
                    "name": "Zhen Xiang"
                },
                {
                    "authorId": "2306335663",
                    "name": "Linzhi Zheng"
                },
                {
                    "authorId": "2306072975",
                    "name": "Yanjie Li"
                },
                {
                    "authorId": "2284689881",
                    "name": "Junyuan Hong"
                },
                {
                    "authorId": "2355360969",
                    "name": "Qinbin Li"
                },
                {
                    "authorId": "2307738654",
                    "name": "Han Xie"
                },
                {
                    "authorId": "2288889632",
                    "name": "Jiawei Zhang"
                },
                {
                    "authorId": "2155965725",
                    "name": "Zidi Xiong"
                },
                {
                    "authorId": "2316271348",
                    "name": "An Yang"
                },
                {
                    "authorId": "2257101724",
                    "name": "Baosong Yang"
                },
                {
                    "authorId": "2321681508",
                    "name": "Beichen Zhang"
                },
                {
                    "authorId": "2321578848",
                    "name": "Binyuan Hui"
                },
                {
                    "authorId": "2327961765",
                    "name": "Bo Zheng"
                },
                {
                    "authorId": "2325918197",
                    "name": "Bowen Yu"
                },
                {
                    "authorId": "2311714296",
                    "name": "Chengyuan Li"
                },
                {
                    "authorId": "2004587660",
                    "name": "Dayiheng Liu"
                },
                {
                    "authorId": "2313916188",
                    "name": "Wenjun Zeng"
                },
                {
                    "authorId": "2314065426",
                    "name": "Yuchi Liu"
                },
                {
                    "authorId": "2291067498",
                    "name": "Ryan Mullins"
                },
                {
                    "authorId": "2291065300",
                    "name": "Ludovic Peran"
                },
                {
                    "authorId": "2314081561",
                    "name": "Joe Fernandez"
                },
                {
                    "authorId": "2313918004",
                    "name": "Hamza Harkous"
                },
                {
                    "authorId": "2285784924",
                    "name": "Karthik Narasimhan"
                }
            ],
            "abstract": "Large Language Models (LLMs) have rapidly become integral to numerous applications in critical domains where reliability is paramount. Despite significant advances in safety frameworks and guardrails, current protective measures exhibit crucial vulnerabilities, particularly in multilingual contexts. Existing safety systems remain susceptible to adversarial attacks in low-resource languages and through code-switching techniques, primarily due to their English-centric design. Furthermore, the development of effective multilingual guardrails is constrained by the scarcity of diverse cross-lingual training data. Even recent solutions like Llama Guard-3, while offering multilingual support, lack transparency in their decision-making processes. We address these challenges by introducing X-Guard agent, a transparent multilingual safety agent designed to provide content moderation across diverse linguistic contexts. X-Guard effectively defends against both conventional low-resource language attacks and sophisticated code-switching attacks. Our approach includes: curating and enhancing multiple open-source safety datasets with explicit evaluation rationales; employing a jury of judges methodology to mitigate individual judge LLM provider biases; creating a comprehensive multilingual safety dataset spanning 132 languages with 5 million data points; and developing a two-stage architecture combining a custom-finetuned mBART-50 translation module with an evaluation X-Guard 3B model trained through supervised finetuning and GRPO training. Our empirical evaluations demonstrate X-Guard's effectiveness in detecting unsafe content across multiple languages while maintaining transparency throughout the safety evaluation process. Our work represents a significant advancement in creating robust, transparent, and linguistically inclusive safety systems for LLMs and its integrated systems.",
            "corpus_id": "277781617",
            "text": "Large Language Models (LLMs) have rapidly become integral to numerous applications in critical domains where reliability is paramount. Despite significant advances in safety frameworks and guardrails, current protective measures exhibit crucial vulnerabilities, particularly in multilingual contexts. Existing safety systems remain susceptible to adversarial attacks in low-resource languages and through code-switching techniques, primarily due to their English-centric design. Furthermore, the development of effective multilingual guardrails is constrained by the scarcity of diverse cross-lingual training data. Even recent solutions like Llama Guard-3, while offering multilingual support, lack transparency in their decision-making processes. We address these challenges by introducing X-Guard agent, a transparent multilingual safety agent designed to provide content moderation across diverse linguistic contexts. X-Guard effectively defends against both conventional low-resource language attacks and sophisticated code-switching attacks. Our approach includes: curating and enhancing multiple open-source safety datasets with explicit evaluation rationales; employing a jury of judges methodology to mitigate individual judge LLM provider biases; creating a comprehensive multilingual safety dataset spanning 132 languages with 5 million data points; and developing a two-stage architecture combining a custom-finetuned mBART-50 translation module with an evaluation X-Guard 3B model trained through supervised finetuning and GRPO training. Our empirical evaluations demonstrate X-Guard's effectiveness in detecting unsafe content across multiple languages while maintaining transparency throughout the safety evaluation process. Our work represents a significant advancement in creating robust, transparent, and linguistically inclusive safety systems for LLMs and its integrated systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.5966796875
        },
        {
            "paperId": "1665977dd646ed8ebbbaf848b0aa694e7fee32ed",
            "corpusId": 222310469,
            "title": "XL-WiC: A Multilingual Benchmark for Evaluating Semantic Contextualization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "referenceCount": 67,
            "citationCount": 65,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.584.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.06478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3106437",
                    "name": "Alessandro Raganato"
                },
                {
                    "authorId": "40438851",
                    "name": "Tommaso Pasini"
                },
                {
                    "authorId": "1387447871",
                    "name": "Jos\u00e9 Camacho-Collados"
                },
                {
                    "authorId": "1717641",
                    "name": "Mohammad Taher Pilehvar"
                }
            ],
            "abstract": "The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at this https URL.",
            "corpus_id": "222310469",
            "text": "The ability to correctly model distinct meanings of a word is crucial for the effectiveness of semantic representation techniques. However, most existing evaluation benchmarks for assessing this criterion are tied to sense inventories (usually WordNet), restricting their usage to a small subset of knowledge-based representation techniques. The Word-in-Context dataset (WiC) addresses the dependence on sense inventories by reformulating the standard disambiguation task as a binary classification problem; but, it is limited to the English language. We put forward a large multilingual benchmark, XL-WiC, featuring gold standards in 12 new languages from varied language families and with different degrees of resource availability, opening room for evaluation scenarios such as zero-shot cross-lingual transfer. We perform a series of experiments to determine the reliability of the datasets and to set performance baselines for several recent contextualized multilingual models. Experimental results show that even when no tagged instances are available for a target language, models trained solely on the English data can attain competitive performance in the task of distinguishing different meanings of a word, even for distant languages. XL-WiC is available at this https URL.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7373046875
        },
        {
            "paperId": "04a19344921ff7338344ba860dc51ec2bc288468",
            "corpusId": 218855673,
            "title": "Encoder-Decoder Model for Semantic Role Labeling",
            "venue": "",
            "year": 2020,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.11588/DATA/TOI9NQ?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.11588/DATA/TOI9NQ, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46578259",
                    "name": "Angel Daza"
                }
            ],
            "abstract": "Abstract (Daza & Frank 2019): We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.",
            "corpus_id": "218855673",
            "text": "Abstract (Daza & Frank 2019): We propose a Cross-lingual Encoder-Decoder model that simultaneously translates and generates sentences with Semantic Role Labeling annotations in a resource-poor target language. Unlike annotation projection techniques, our model does not need parallel data during inference time. Our approach can be applied in monolingual, multilingual and cross-lingual settings and is able to produce dependency-based and span-based SRL annotations. We benchmark the labeling performance of our model in different monolingual and multilingual settings using well-known SRL datasets. We then train our model in a cross-lingual setting to generate new SRL labeled data. Finally, we measure the effectiveness of our method by using the generated data to augment the training basis for resource-poor languages and perform manual evaluation to show that it produces high-quality sentences and assigns accurate semantic role annotations. Our proposed architecture offers a flexible method for leveraging SRL data in multiple languages.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.419677734375
        }
    ],
    "quotes": {
        "cost": 0.242328,
        "quotes": [
            {
                "idx": 0,
                "key": "[196174566 | Liu et al. | 2019 | Citations: 91]",
                "snippets": "The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 678,
                        "end": 1165,
                        "sentence_offsets": [
                            {
                                "start": 678,
                                "end": 804
                            },
                            {
                                "start": 805,
                                "end": 981
                            },
                            {
                                "start": 982,
                                "end": 1165
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[207880568 | Conneau et al. | 2019 | Citations: 6589]",
                "snippets": "Cross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Evaluation",
                        "pdf_hash": "",
                        "start": 241,
                        "end": 1164,
                        "sentence_offsets": [
                            {
                                "start": 171,
                                "end": 290
                            },
                            {
                                "start": 293,
                                "end": 341
                            },
                            {
                                "start": 342,
                                "end": 457
                            },
                            {
                                "start": 458,
                                "end": 596
                            },
                            {
                                "start": 597,
                                "end": 678
                            },
                            {
                                "start": 679,
                                "end": 1140
                            },
                            {
                                "start": 1141,
                                "end": 1217
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Cross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[215548041 | Artetxe et al. | 2020 | Citations: 120]",
                "snippets": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models...Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)...In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204901567 | Artetxe et al. | 2019 | Citations: 799]": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
                    "[52271711 | Conneau et al. | 2018 | Citations: 1388]": "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 616,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 119
                            },
                            {
                                "start": 120,
                                "end": 332
                            },
                            {
                                "start": 333,
                                "end": 617
                            }
                        ],
                        "ref_mentions": [
                            "52271711",
                            "204901567"
                        ],
                        "quote": "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1973,
                        "end": 2415,
                        "sentence_offsets": [
                            {
                                "start": 1973,
                                "end": 2101
                            },
                            {
                                "start": 2102,
                                "end": 2413
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)"
                    },
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 621,
                        "end": 1092,
                        "sentence_offsets": [
                            {
                                "start": 621,
                                "end": 793
                            },
                            {
                                "start": 794,
                                "end": 1089
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[215754572 | Fei et al. | 2020 | Citations: 108]",
                "snippets": "Translated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann et al., 2016)(Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[202541341 | Zhang et al. | 2019 | Citations: 37]": "Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods.",
                    "[7805664 | Tiedemann et al. | 2016 | Citations: 56]": "How do we parse the languages for which no treebanks are available? This contribution addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt to make use of resource-rich source language treebanks to build and adapt models for the under-resourced target languages. We outline the benefits, and indicate the drawbacks of the current major approaches. We emphasize synthetic treebanking: the automatic creation of target language treebanks by means of annotation projection and machine translation. We present competitive results in cross-lingual dependency parsing using a combination of various techniques that contribute to the overall success of the method. We further include a detailed discussion about the impact of part-of-speech label accuracy on parsing results that provide guidance in practical applications of cross-lingual methods for truly under-resourced languages."
                },
                "metadata": [
                    {
                        "section_title": "Cross-Lingual Transfer from English",
                        "pdf_hash": "",
                        "start": 1413,
                        "end": 2241,
                        "sentence_offsets": [
                            {
                                "start": 1413,
                                "end": 1431
                            },
                            {
                                "start": 1432,
                                "end": 1557
                            },
                            {
                                "start": 1558,
                                "end": 1768
                            },
                            {
                                "start": 1769,
                                "end": 1859
                            },
                            {
                                "start": 1860,
                                "end": 1993
                            },
                            {
                                "start": 1994,
                                "end": 2241
                            }
                        ],
                        "ref_mentions": [
                            "7805664",
                            "202541341"
                        ],
                        "quote": "Translated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann et al., 2016)(Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[227231710 | Glavas et al. | 2020 | Citations: 8]",
                "snippets": "We first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vulic et al., 2019). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;(Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[196183388 | Vulic et al. | 2019 | Citations: 16]": "Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs.",
                    "[7665329 | Camacho-Collados et al. | 2017 | Citations: 155]": "This paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish. High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: http://alt.qcri.org/semeval2017/task2/"
                },
                "metadata": [
                    {
                        "section_title": "Data",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1011,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 158
                            },
                            {
                                "start": 159,
                                "end": 291
                            },
                            {
                                "start": 292,
                                "end": 410
                            },
                            {
                                "start": 411,
                                "end": 617
                            },
                            {
                                "start": 618,
                                "end": 739
                            },
                            {
                                "start": 740,
                                "end": 915
                            },
                            {
                                "start": 916,
                                "end": 1011
                            }
                        ],
                        "ref_mentions": [
                            "196183388",
                            "7665329"
                        ],
                        "quote": "We first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vulic et al., 2019). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;(Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[233481097 | Goyal et al. | 2021 | Citations: 129]",
                "snippets": "We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Evaluation",
                        "pdf_hash": "",
                        "start": 533,
                        "end": 987,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[235097287 | Wang et al. | 2021 | Citations: 28]",
                "snippets": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data Augmentation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 311,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 133
                            },
                            {
                                "start": 134,
                                "end": 312
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019)."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[235313293 | Razumovskaia et al. | 2021 | Citations: 34]",
                "snippets": "These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Outlook for Multilingual ToD Datasets",
                        "pdf_hash": "",
                        "start": 127,
                        "end": 1231,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[235683093 | Turc et al. | 2021 | Citations: 58]",
                "snippets": "In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Datasets",
                        "pdf_hash": "",
                        "start": 409,
                        "end": 781,
                        "sentence_offsets": [
                            {
                                "start": 409,
                                "end": 571
                            },
                            {
                                "start": 572,
                                "end": 781
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[236318351 | Ponti et al. | 2021 | Citations: 18]",
                "snippets": "While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[236428949 | Asai et al. | 2021 | Citations: 73]",
                "snippets": "Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios...XOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data)...MKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Datasets and Knowledge Sources",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 111,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 112
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios"
                    },
                    {
                        "section_title": "Datasets and Knowledge Sources",
                        "pdf_hash": "",
                        "start": 278,
                        "end": 747,
                        "sentence_offsets": [
                            {
                                "start": 278,
                                "end": 290
                            },
                            {
                                "start": 291,
                                "end": 520
                            },
                            {
                                "start": 521,
                                "end": 747
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "XOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data)"
                    },
                    {
                        "section_title": "Datasets and Knowledge Sources",
                        "pdf_hash": "",
                        "start": 804,
                        "end": 1091,
                        "sentence_offsets": [
                            {
                                "start": 804,
                                "end": 809
                            },
                            {
                                "start": 810,
                                "end": 958
                            },
                            {
                                "start": 959,
                                "end": 1090
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "MKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[237364084 | Bonifacio et al. | 2021 | Citations: 113]",
                "snippets": "More recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO...We evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 181,
                        "end": 1096,
                        "sentence_offsets": [
                            {
                                "start": 181,
                                "end": 460
                            },
                            {
                                "start": 461,
                                "end": 581
                            },
                            {
                                "start": 582,
                                "end": 807
                            },
                            {
                                "start": 808,
                                "end": 946
                            },
                            {
                                "start": 947,
                                "end": 1097
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "More recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1842,
                        "end": 2076,
                        "sentence_offsets": [
                            {
                                "start": 1832,
                                "end": 2075
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[241583550 | Jacqmin et al. | 2021 | Citations: 2]",
                "snippets": "When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning.\n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages.\n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval.\n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Data Augmentation",
                        "pdf_hash": "",
                        "start": 41,
                        "end": 1206,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 51
                            },
                            {
                                "start": 52,
                                "end": 104
                            },
                            {
                                "start": 105,
                                "end": 384
                            },
                            {
                                "start": 387,
                                "end": 513
                            },
                            {
                                "start": 514,
                                "end": 733
                            },
                            {
                                "start": 734,
                                "end": 895
                            },
                            {
                                "start": 896,
                                "end": 985
                            },
                            {
                                "start": 988,
                                "end": 1087
                            },
                            {
                                "start": 1090,
                                "end": 1220
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning.\n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages.\n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval.\n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[246430787 | Majewska et al. | 2022 | Citations: 28]",
                "snippets": "The results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation....the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets....Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Translation versus Outline-Based",
                        "pdf_hash": "",
                        "start": 1089,
                        "end": 1784,
                        "sentence_offsets": [
                            {
                                "start": 1062,
                                "end": 1259
                            },
                            {
                                "start": 1260,
                                "end": 1382
                            },
                            {
                                "start": 1383,
                                "end": 1549
                            },
                            {
                                "start": 1550,
                                "end": 1781
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets"
                    },
                    {
                        "section_title": "Translation versus Outline-Based",
                        "pdf_hash": "",
                        "start": 2039,
                        "end": 2363,
                        "sentence_offsets": [
                            {
                                "start": 2039,
                                "end": 2227
                            },
                            {
                                "start": 2228,
                                "end": 2320
                            },
                            {
                                "start": 2321,
                                "end": 2362
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[247594499 | Hershcovich et al. | 2022 | Citations: 182]",
                "snippets": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture...Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations...Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218470125 | Ponti et al. | 2020 | Citations: 327]": "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL.",
                    "[237503047 | Yin et al. | 2021 | Citations: 56]": "Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models\u2019 ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR.",
                    "[238198104 | Liu et al. | 2021 | Citations: 180]": "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."
                },
                "metadata": [
                    {
                        "section_title": "Data Collection",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 174,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 175
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture"
                    },
                    {
                        "section_title": "Data Collection",
                        "pdf_hash": "",
                        "start": 691,
                        "end": 1107,
                        "sentence_offsets": [
                            {
                                "start": 691,
                                "end": 812
                            },
                            {
                                "start": 813,
                                "end": 962
                            },
                            {
                                "start": 963,
                                "end": 1107
                            }
                        ],
                        "ref_mentions": [
                            "238198104"
                        ],
                        "quote": "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations"
                    },
                    {
                        "section_title": "Data Collection",
                        "pdf_hash": "",
                        "start": 1694,
                        "end": 2105,
                        "sentence_offsets": [
                            {
                                "start": 1694,
                                "end": 1842
                            },
                            {
                                "start": 1843,
                                "end": 2104
                            }
                        ],
                        "ref_mentions": [
                            "238198104",
                            "218470125",
                            "237503047"
                        ],
                        "quote": "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[248780386 | Ahuja et al. | 2022 | Citations: 16]",
                "snippets": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 741,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 113
                            },
                            {
                                "start": 114,
                                "end": 428
                            },
                            {
                                "start": 429,
                                "end": 592
                            },
                            {
                                "start": 593,
                                "end": 741
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[252968346 | Oh et al. | 2022 | Citations: 9]",
                "snippets": "Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;(Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[215548041 | Artetxe et al. | 2020 | Citations: 120]": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1437,
                        "end": 1879,
                        "sentence_offsets": [
                            {
                                "start": 1437,
                                "end": 1672
                            },
                            {
                                "start": 1673,
                                "end": 1879
                            }
                        ],
                        "ref_mentions": [
                            "215548041"
                        ],
                        "quote": "Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;(Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[253098629 | Qiu et al. | 2022 | Citations: 15]",
                "snippets": "We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitations",
                        "pdf_hash": "",
                        "start": 1070,
                        "end": 1426,
                        "sentence_offsets": [
                            {
                                "start": 1070,
                                "end": 1217
                            },
                            {
                                "start": 1218,
                                "end": 1426
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[253762032 | Krubinski et al. | 2022 | Citations: 3]",
                "snippets": "Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by (Iskender et al., 2021), which indicate that summary evaluations do not survive translation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237532546 | Iskender et al. | 2021 | Citations: 4]": "The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at https://github.com/PrimerAI/primer-research/."
                },
                "metadata": [
                    {
                        "section_title": "Non-English data",
                        "pdf_hash": "",
                        "start": 1439,
                        "end": 1680,
                        "sentence_offsets": [
                            {
                                "start": 1439,
                                "end": 1551
                            },
                            {
                                "start": 1552,
                                "end": 1680
                            }
                        ],
                        "ref_mentions": [
                            "237532546"
                        ],
                        "quote": "Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by (Iskender et al., 2021), which indicate that summary evaluations do not survive translation."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[258557740 | Hennig et al. | 2023 | Citations: 8]",
                "snippets": "The MultiTACRED dataset we present in this work covers 12 languages from 9 language families. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances....Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;(Ozaki et al., 2021)Yarmohammadi et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233189585 | Ozaki et al. | 2021 | Citations: 7]": "This paper describes the first report on cross-lingual transfer for semantic dependency parsing. We present the insight that there are twodifferent kinds of cross-linguality, namely sur-face level and mantic level, and try to cap-ture both kinds of cross-linguality by combin-ing annotation projection and model transferof pre-trained language models. Our exper-iments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound."
                },
                "metadata": [
                    {
                        "quote": "The MultiTACRED dataset we present in this work covers 12 languages from 9 language families. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 45,
                        "end": 325,
                        "sentence_offsets": [],
                        "ref_mentions": [
                            "233189585"
                        ],
                        "quote": ".Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;(Ozaki et al., 2021)Yarmohammadi et al., 2021)."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[258715321 | Shen et al. | 2023 | Citations: 8]",
                "snippets": "We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Answer:",
                        "pdf_hash": "",
                        "start": 347,
                        "end": 661,
                        "sentence_offsets": [
                            {
                                "start": 347,
                                "end": 473
                            },
                            {
                                "start": 474,
                                "end": 661
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[258841859 | Artetxe et al. | 2023 | Citations: 38]",
                "snippets": "Finally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Sources of cross-lingual transfer gap",
                        "pdf_hash": "",
                        "start": 1869,
                        "end": 2167,
                        "sentence_offsets": [
                            {
                                "start": 1869,
                                "end": 2081
                            },
                            {
                                "start": 2082,
                                "end": 2342
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Finally, there can also be an inherent distribution mismatch across languages in the benchmark itself (e.g., the source language training data and the target language evaluation data having different properties). This can be due to annotation artifacts in multilingual datasets, in particular those"
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[258865558 | Asai et al. | 2023 | Citations: 62]",
                "snippets": "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[216914383 | Artetxe et al. | 2020 | Citations: 72]": "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.",
                    "[218470125 | Ponti et al. | 2020 | Citations: 327]": "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."
                },
                "metadata": [
                    {
                        "section_title": "Design Principles",
                        "pdf_hash": "",
                        "start": 885,
                        "end": 1368,
                        "sentence_offsets": [
                            {
                                "start": 885,
                                "end": 923
                            },
                            {
                                "start": 924,
                                "end": 1087
                            },
                            {
                                "start": 1088,
                                "end": 1241
                            },
                            {
                                "start": 1242,
                                "end": 1368
                            }
                        ],
                        "ref_mentions": [
                            "218470125",
                            "216914383"
                        ],
                        "quote": "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[259859119 | Sahoo et al. | 2023 | Citations: 1]",
                "snippets": "For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "MTL based on Translation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 374,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 100
                            },
                            {
                                "start": 101,
                                "end": 263
                            },
                            {
                                "start": 264,
                                "end": 374
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[261242630 | Ranaldi et al. | 2023 | Citations: 34]",
                "snippets": "XQUAD and MLQA focus on understanding questions and answers through translation into different languages...Cross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages...Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204901567 | Artetxe et al. | 2019 | Citations: 799]": "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
                    "[252917648 | Suzgun et al. | 2022 | Citations: 1138]": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves."
                },
                "metadata": [
                    {
                        "section_title": "Benchmarks",
                        "pdf_hash": "",
                        "start": 201,
                        "end": 305,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 288
                            },
                            {
                                "start": 289,
                                "end": 395
                            }
                        ],
                        "ref_mentions": [
                            "252917648"
                        ],
                        "quote": "XQUAD and MLQA focus on understanding questions and answers through translation into different languages"
                    },
                    {
                        "section_title": "Benchmarks",
                        "pdf_hash": "",
                        "start": 1392,
                        "end": 1720,
                        "sentence_offsets": [
                            {
                                "start": 1392,
                                "end": 1652
                            },
                            {
                                "start": 1653,
                                "end": 1720
                            }
                        ],
                        "ref_mentions": [
                            "204901567"
                        ],
                        "quote": "Cross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages"
                    },
                    {
                        "section_title": "Benchmarks",
                        "pdf_hash": "",
                        "start": 1345,
                        "end": 1671,
                        "sentence_offsets": [
                            {
                                "start": 1274,
                                "end": 1389
                            },
                            {
                                "start": 1392,
                                "end": 1652
                            },
                            {
                                "start": 1653,
                                "end": 1720
                            }
                        ],
                        "ref_mentions": [
                            "204901567"
                        ],
                        "quote": "Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[261557946 | Zhang et al. | 2023 | Citations: 77]",
                "snippets": "However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020)(Lembersky et al., 2011)Volansky et al., 2015;Avner et al., 2016;(Eetemadi et al., 2014)(Rabinovich et al., 2015) and may lead to training data of questionable value...Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[15699208 | Eetemadi et al. | 2014 | Citations: 17]": "Distinct properties of translated text have been the subject of research in linguistics for many year (Baker, 1993). In recent years computational methods have been developed to empirically verify the linguistic theories about translated text (Baroni and Bernardini, 2006). While many characteristics of translated text are more apparent in comparison to the original text, most of the prior research has focused on monolingual features of translated and original text. The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level, rather than using monolingual statistics at the document level. We show that these bilingual features outperform the monolingual features used in prior work (Kurokawa et al., 2009) for the task of classifying translation direction.",
                    "[212657414 | Clark et al. | 2020 | Citations: 612]": "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation.",
                    "[940724 | Rabinovich et al. | 2015 | Citations: 55]": "Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy."
                },
                "metadata": [
                    {
                        "section_title": "Comparison to Synthetic Datasets",
                        "pdf_hash": "",
                        "start": 279,
                        "end": 561,
                        "sentence_offsets": [
                            {
                                "start": 279,
                                "end": 562
                            }
                        ],
                        "ref_mentions": [
                            "212657414",
                            "11280500",
                            "15699208",
                            "940724"
                        ],
                        "quote": "However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020)(Lembersky et al., 2011)Volansky et al., 2015;Avner et al., 2016;(Eetemadi et al., 2014)(Rabinovich et al., 2015) and may lead to training data of questionable value"
                    },
                    {
                        "section_title": "Comparison to Synthetic Datasets",
                        "pdf_hash": "",
                        "start": 746,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 746,
                                "end": 919
                            },
                            {
                                "start": 920,
                                "end": 1011
                            },
                            {
                                "start": 1012,
                                "end": 1145
                            },
                            {
                                "start": 1146,
                                "end": 1291
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[264405758 | Jin et al. | 2023 | Citations: 76]",
                "snippets": "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], and ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text?",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[248887528 | Verma et al. | 2022 | Citations: 14]": "Advances in Natural Language Processing (NLP) have revolutionized the way researchers and practitioners address crucial societal problems. Large language models are now the standard to develop state-of-the-art solutions for text detection and classification tasks. However, the development of advanced computational techniques and resources is disproportionately focused on the English language, sidelining a majority of the languages spoken globally. While existing research has developed better multilingual and monolingual language models to bridge this language disparity between English and non-English languages, we explore the promise of incorporating the information contained in images via multimodal machine learning. Our comparative analyses on three detection tasks focusing on crisis information, fake news, and emotion recognition, as well as five high-resource non-English languages, demonstrate that: (a) detection frameworks based on pre-trained large language models like BERT and multilingual-BERT systematically perform better on the English language compared against non-English languages, and (b) including images via multimodal learning bridges this performance gap. We situate our findings with respect to existing work on the pitfalls of large language models, and discuss their theoretical and practical implications.",
                    "[4623739 | Junczys-Dowmunt et al. | 2018 | Citations: 718]": "We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."
                },
                "metadata": [
                    {
                        "section_title": "A DETAILS OF DATASET CONSTRUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1081,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 107
                            },
                            {
                                "start": 108,
                                "end": 320
                            },
                            {
                                "start": 323,
                                "end": 339
                            },
                            {
                                "start": 340,
                                "end": 523
                            },
                            {
                                "start": 524,
                                "end": 619
                            },
                            {
                                "start": 620,
                                "end": 703
                            },
                            {
                                "start": 704,
                                "end": 891
                            },
                            {
                                "start": 894,
                                "end": 906
                            },
                            {
                                "start": 907,
                                "end": 979
                            },
                            {
                                "start": 980,
                                "end": 992
                            },
                            {
                                "start": 993,
                                "end": 1083
                            }
                        ],
                        "ref_mentions": [
                            "4623739",
                            "248887528"
                        ],
                        "quote": "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], and ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text?"
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[267413041 | Agrawal et al. | 2024 | Citations: 3]",
                "snippets": "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[268819377 | Xu et al. | 2024 | Citations: 43]",
                "snippets": "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Bias Benchmark",
                        "pdf_hash": "",
                        "start": 2002,
                        "end": 2150,
                        "sentence_offsets": [
                            {
                                "start": 2002,
                                "end": 2150
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[270226445 | Park et al. | 2024 | Citations: 3]",
                "snippets": "We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes...Evaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer...In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Varying NMT and Pivot Languages",
                        "pdf_hash": "",
                        "start": 1068,
                        "end": 1429,
                        "sentence_offsets": [
                            {
                                "start": 1068,
                                "end": 1224
                            },
                            {
                                "start": 1224,
                                "end": 1429
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Evaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer"
                    },
                    {
                        "section_title": "Varying NMT and Pivot Languages",
                        "pdf_hash": "",
                        "start": 1586,
                        "end": 1710,
                        "sentence_offsets": [
                            {
                                "start": 1586,
                                "end": 1710
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[270379819 | Neplenbroek et al. | 2024 | Citations: 8]",
                "snippets": "MBBQ covers stereotypes from 6 bias categories that are commonly held across all 4 languages, allowing for an investigation of cross-lingual stereotypes, with differences that are due to inconsistencies in model behavior across languages rather than cultural shifts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Conclusion",
                        "pdf_hash": "",
                        "start": 255,
                        "end": 521,
                        "sentence_offsets": [
                            {
                                "start": 255,
                                "end": 521
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "MBBQ covers stereotypes from 6 bias categories that are commonly held across all 4 languages, allowing for an investigation of cross-lingual stereotypes, with differences that are due to inconsistencies in model behavior across languages rather than cultural shifts."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[270380088 | Etxaniz et al. | 2024 | Citations: 11]",
                "snippets": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 964,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 267
                            },
                            {
                                "start": 268,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 569
                            },
                            {
                                "start": 572,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 886
                            },
                            {
                                "start": 887,
                                "end": 964
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[273323209 | Thellmann et al. | 2024 | Citations: 10]",
                "snippets": "Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012)...An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). Those languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[246680398 | Meng et al. | 2022 | Citations: 235]": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 414,
                        "end": 786,
                        "sentence_offsets": [
                            {
                                "start": 414,
                                "end": 601
                            },
                            {
                                "start": 602,
                                "end": 787
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012)"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 779,
                        "end": 1611,
                        "sentence_offsets": [
                            {
                                "start": 602,
                                "end": 787
                            },
                            {
                                "start": 790,
                                "end": 1008
                            },
                            {
                                "start": 1009,
                                "end": 1074
                            },
                            {
                                "start": 1075,
                                "end": 1135
                            },
                            {
                                "start": 1136,
                                "end": 1286
                            },
                            {
                                "start": 1287,
                                "end": 1460
                            },
                            {
                                "start": 1463,
                                "end": 1559
                            },
                            {
                                "start": 1560,
                                "end": 1680
                            }
                        ],
                        "ref_mentions": [
                            "246680398"
                        ],
                        "quote": "An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). Those languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[273323302 | Yang et al. | 2024 | Citations: 7]",
                "snippets": "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;(Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "D.3 HOW TO AVOID TRANSLATIONESE BIAS IN MULTILINGUAL BENCHMARKS EVALUATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1179,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 224
                            },
                            {
                                "start": 225,
                                "end": 359
                            },
                            {
                                "start": 360,
                                "end": 503
                            },
                            {
                                "start": 506,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 856
                            },
                            {
                                "start": 859,
                                "end": 1047
                            },
                            {
                                "start": 1048,
                                "end": 1179
                            }
                        ],
                        "ref_mentions": [
                            "271244246"
                        ],
                        "quote": "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;(Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[273532366 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "B. Evaluation Tasks",
                        "pdf_hash": "",
                        "start": 1131,
                        "end": 1559,
                        "sentence_offsets": [
                            {
                                "start": 1131,
                                "end": 1263
                            },
                            {
                                "start": 1264,
                                "end": 1443
                            },
                            {
                                "start": 1444,
                                "end": 1559
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation."
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[274149965 | Seto et al. | 2024 | Citations: 1]",
                "snippets": "Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitations",
                        "pdf_hash": "",
                        "start": 1027,
                        "end": 1731,
                        "sentence_offsets": [
                            {
                                "start": 1027,
                                "end": 1165
                            },
                            {
                                "start": 1166,
                                "end": 1276
                            },
                            {
                                "start": 1277,
                                "end": 1405
                            },
                            {
                                "start": 1406,
                                "end": 1523
                            },
                            {
                                "start": 1524,
                                "end": 1731
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[276421738 | Barth et al. | 2025 | Citations: 0]",
                "snippets": "Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Challenge 3: Cultural Bias",
                        "pdf_hash": "",
                        "start": 329,
                        "end": 1304,
                        "sentence_offsets": [
                            {
                                "start": 329,
                                "end": 444
                            },
                            {
                                "start": 445,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 820
                            },
                            {
                                "start": 821,
                                "end": 943
                            },
                            {
                                "start": 946,
                                "end": 1018
                            },
                            {
                                "start": 1019,
                                "end": 1193
                            },
                            {
                                "start": 1194,
                                "end": 1304
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[276575630 | Bland'on et al. | 2025 | Citations: 0]",
                "snippets": "Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993)Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[270562911 | Chen et al. | 2024 | Citations: 3]": "Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks.",
                    "[57174748 | Baker et al. | 1993 | Citations: 1061]": "The rise of corpus linguistics has serious implications for any discipline in which language plays a major role. This paper explores the impact that the availability of corpora is likely to have on the study of translation as an empirical phenomenon. It argues that the techniques and methodology developed in the field of corpus linguistics will have a direct impact on the emerging discipline of translation studies, particularly with respect to its theoretical and descriptive branches. The nature of this impact is discussed in some detail and brief reference is made to some of the applications of corpus techniques in the applied branch of the dis\u00ad cipline."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 227,
                        "end": 697,
                        "sentence_offsets": [
                            {
                                "start": 227,
                                "end": 578
                            },
                            {
                                "start": 579,
                                "end": 697
                            }
                        ],
                        "ref_mentions": [
                            "57174748",
                            "270562911"
                        ],
                        "quote": "Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993)Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[277993848 | Wu et al. | 2025 | Citations: 5]",
                "snippets": "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)...Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221516475 | Hendrycks et al. | 2020 | Citations: 4568]": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
                    "[252735112 | Shi et al. | 2022 | Citations: 369]": "We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",
                    "[260334562 | Lai et al. | 2023 | Citations: 150]": "A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has left a significant gap for fine-tuned LLMs based on RLHF in diverse languages and raised important questions on how RLHF can boost the performance of multilingual instruction tuning. To overcome this issue, we present Okapi, the first system with instruction-tuned LLMs based on RLHF for multiple languages. Okapi introduces instruction and response-ranked data in 26 diverse languages to facilitate the experiments and development of future multilingual LLM research. We also present benchmark datasets to enable the evaluation of generative LLMs in multiple languages. Our experiments demonstrate the advantages of RLHF for multilingual instruction over SFT for different base models and datasets. Our framework and resources are released at https://github.com/nlp-uoregon/Okapi.",
                    "[263829211 | Koto et al. | 2023 | Citations: 46]": "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.",
                    "[271245129 | Yuksel et al. | 2024 | Citations: 14]": "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                },
                "metadata": [
                    {
                        "section_title": "Domain",
                        "pdf_hash": "",
                        "start": 282,
                        "end": 911,
                        "sentence_offsets": [
                            {
                                "start": 282,
                                "end": 360
                            },
                            {
                                "start": 361,
                                "end": 569
                            },
                            {
                                "start": 570,
                                "end": 664
                            },
                            {
                                "start": 665,
                                "end": 912
                            }
                        ],
                        "ref_mentions": [
                            "252735112",
                            "260334562",
                            "221516475",
                            "263829211",
                            "271245129"
                        ],
                        "quote": "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)"
                    },
                    {
                        "quote": "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.034371,
        "cot": "For the query on comparing translation-based multilingual evaluation datasets with natively collected datasets, I need to structure dimensions that will provide a comprehensive answer.\n\n1. \"Introduction/Background\" - This should provide context about multilingual evaluation datasets, why they're important, and the two main approaches (translation-based vs. natively collected). This will be in synthesis format to give readers a coherent foundation.\n\n2. \"Methods of Creating Multilingual Evaluation Datasets\" - This dimension will explain the different approaches to creating multilingual datasets, focusing on translation-based methods versus native collection methods. Synthesis format is appropriate to explain the methodological differences and nuances.\n\n3. \"Effectiveness Comparison\" - This dimension will compare the performance and effectiveness of translation-based versus natively collected datasets. This should be synthesis format to properly analyze the complex trade-offs and results from multiple studies.\n\n4. \"Translation Artifacts and Biases\" - This dimension will specifically address the biases and artifacts introduced through translation processes. Synthesis format is best to explain the nuanced issues with translationese and other artifacts.\n\n5. \"Impact on Cross-lingual Evaluation Reliability\" - This dimension will directly address how the dataset creation method affects the reliability of cross-lingual evaluations. Synthesis format is appropriate to properly explain the implications for evaluation validity.\n\n6. \"Recommendations and Best Practices\" - This final dimension will synthesize recommendations from the literature on when to use each approach and how to mitigate potential issues. Synthesis format allows for cohesive presentation of these recommendations.\n\nThese dimensions together will provide a comprehensive answer to the query, covering both effectiveness and bias aspects while focusing on the impact on cross-lingual evaluation reliability.",
        "plan": {
            "Introduction/Background (synthesis)": [],
            "Methods of Creating Multilingual Evaluation Datasets (synthesis)": [
                2,
                4,
                6,
                11,
                14,
                15,
                22,
                24,
                25,
                26,
                31,
                32,
                38
            ],
            "Effectiveness Comparison (synthesis)": [
                0,
                1,
                3,
                5,
                6,
                9,
                11,
                12,
                20,
                23,
                29
            ],
            "Translation Artifacts and Biases (synthesis)": [
                2,
                7,
                8,
                13,
                16,
                17,
                18,
                19,
                21,
                25,
                27,
                28,
                29,
                30,
                33,
                34,
                37
            ],
            "Impact on Cross-lingual Evaluation Reliability (synthesis)": [
                0,
                7,
                10,
                13,
                14,
                15,
                17,
                22,
                27,
                28,
                31,
                35,
                36,
                38
            ],
            "Recommendations and Best Practices (synthesis)": [
                2,
                14,
                17,
                22,
                31,
                36,
                37,
                38
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Multilingual evaluation datasets are crucial for assessing cross-lingual capabilities of NLP models, but their creation methods can significantly impact their quality and reliability. Translation-based datasets differ fundamentally from datasets natively collected in target languages, raising important questions about their comparative effectiveness and potential biases. (LLM Memory)",
                "text": "\nMultilingual evaluation datasets serve as critical benchmarks for assessing the cross-lingual capabilities of natural language processing (NLP) models. As AI systems increasingly aim to serve users across diverse linguistic backgrounds, the quality and representativeness of these evaluation datasets directly impact our understanding of models' true multilingual abilities. However, the methods used to create these datasets vary significantly, with two primary approaches emerging: translation-based datasets (where content is originally created in one language and translated to others) and natively collected datasets (where content is originally authored in each target language). <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe distinction between these approaches is not merely methodological but has profound implications for evaluation validity. Translation-based datasets offer practical advantages, including cost-effectiveness, consistent content across languages, and scalability to many languages simultaneously. Yet, they may introduce translation artifacts, cultural misalignments, and fail to capture natural language patterns specific to each language community. These limitations raise fundamental questions about whether assessments based on translated content accurately reflect a model's performance on naturally occurring text in target languages. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe research community has increasingly recognized this tension, prompting investigations into how these different dataset creation methodologies affect evaluation outcomes. Understanding these differences is essential not only for interpreting current benchmark results but also for designing more robust evaluation frameworks that can reliably assess models across the linguistic diversity of the world's languages. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Methods of Creating Multilingual Evaluation Datasets",
                "tldr": "Two primary approaches exist for creating multilingual evaluation datasets: translation-based methods (converting existing English datasets into other languages) and native collection methods (creating datasets directly in target languages). Each approach offers distinct advantages and limitations that significantly impact the quality and representativeness of the resulting evaluations. (23 sources)",
                "text": "\nMultilingual evaluation datasets are typically created through one of two main approaches: translation of existing resources or direct collection in target languages. Translation-based approaches, which dominate the multilingual benchmarking landscape, involve converting English datasets into other languages either through machine translation or human translators <Paper corpusId=\"52271711\" paperTitle=\"(Conneau et al., 2018)\" isShortName></Paper> <Paper corpusId=\"204901567\" paperTitle=\"(Artetxe et al., 2019)\" isShortName></Paper>. This approach offers several practical advantages, including cost-effectiveness, time efficiency, and scalability across multiple languages simultaneously <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper>. Additionally, translation creates parallel evaluation sets that provide a meaningful measure of the cross-lingual transfer gap between different models <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n\nProminent examples of translation-based evaluation resources include XNLI (for natural language inference), XQuAD (for question answering), and XCOPA (for commonsense reasoning) <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261242630\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. These datasets typically begin with carefully selected English examples and translate them to ensure comparable concept coverage across languages <Paper corpusId=\"227231710\" paperTitle=\"(Glavas et al., 2020)\" isShortName></Paper> <Paper corpusId=\"196183388\" paperTitle=\"(Vulic et al., 2019)\" isShortName></Paper>. For instance, XQuAD comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators <Paper corpusId=\"204901567\" paperTitle=\"(Artetxe et al., 2019)\" isShortName></Paper>.\n\nTranslation quality varies significantly depending on the method employed. Machine translation offers rapid scaling but may introduce quality issues, particularly for low-resource languages <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246680398\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. Some researchers have evaluated different translation approaches, including Google Translate, MarianMT, and even large language models like ChatGPT, using fluency and meaning preservation metrics to ensure quality <Paper corpusId=\"264405758\" paperTitle=\"(Jin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"4623739\" paperTitle=\"(Junczys-Dowmunt et al., 2018)\" isShortName></Paper>.\n\nDespite their prevalence, translation-based datasets have significant limitations. They often contain translation artifacts or \"translationese\" - linguistic features that make translated text distinguishable from naturally occurring text in the target language <Paper corpusId=\"261557946\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"212657414\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper> <Paper corpusId=\"940724\" paperTitle=\"(Rabinovich et al., 2015)\" isShortName></Paper>. These artifacts can lead to unreliable evaluation results, particularly when models are both trained and evaluated on translated data <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper> <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.\n\nThe alternative approach involves direct collection of data in target languages, creating natively authored datasets without translation intermediaries. TyDi QA exemplifies this method, featuring questions written directly by speakers of 11 typologically diverse languages <Paper corpusId=\"212657414\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper>. Similarly, region-specific evaluations like IndoMMLU for Indonesian <Paper corpusId=\"263829211\" paperTitle=\"(Koto et al., 2023)\" isShortName></Paper> and TurkishMMLU <Paper corpusId=\"271245129\" paperTitle=\"(Yuksel et al., 2024)\" isShortName></Paper> contain questions written by curriculum experts that reflect local educational standards and cultural contexts.\n\nNative collection approaches better capture cultural nuances and linguistic particularities but require significantly more resources, especially for gathering expert annotations in multiple languages <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>. However, they avoid cultural biases introduced through translation and produce more authentic evaluations of model capabilities in target languages <Paper corpusId=\"238198104\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237503047\" paperTitle=\"(Yin et al., 2021)\" isShortName></Paper>.\n\nSome researchers have suggested hybrid approaches as a practical compromise. For instance, translating only high-quality evaluation data while using native data for training represents a middle ground that balances practicality with cultural appropriateness <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>. Others recommend ensuring consistent test sets across languages by either using original annotations in all languages or translating from non-English languages to avoid English-centric biases <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n\nRecent studies have demonstrated that localized benchmarks show significantly higher alignment with local human judgments (0.68 correlation) compared to translated counterparts (0.47), underscoring the importance of creating culturally and linguistically tailored evaluation datasets <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. This finding has motivated an increasing focus on developing native evaluation benchmarks that better represent local cultural knowledge and linguistic phenomena <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Conneau et al., 2018)",
                        "snippets": [
                            "State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."
                        ],
                        "paper": {
                            "corpus_id": 52271711,
                            "title": "XNLI: Evaluating Cross-lingual Sentence Representations",
                            "authors": [
                                {
                                    "authorId": "2480903",
                                    "name": "Alexis Conneau"
                                },
                                {
                                    "authorId": "1830914",
                                    "name": "Guillaume Lample"
                                },
                                {
                                    "authorId": "1905713",
                                    "name": "Ruty Rinott"
                                },
                                {
                                    "authorId": "81840293",
                                    "name": "Adina Williams"
                                },
                                {
                                    "authorId": "3644767",
                                    "name": "Samuel R. Bowman"
                                },
                                {
                                    "authorId": "144518416",
                                    "name": "Holger Schwenk"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                }
                            ],
                            "year": 2018,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1388
                        },
                        "score": 0
                    },
                    {
                        "id": "(Artetxe et al., 2019)",
                        "snippets": [
                            "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators."
                        ],
                        "paper": {
                            "corpus_id": 204901567,
                            "title": "On the Cross-lingual Transferability of Monolingual Representations",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 799
                        },
                        "score": 0
                    },
                    {
                        "id": "(Thellmann et al., 2024)",
                        "snippets": [
                            "Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012)",
                            "An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). Those languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."
                        ],
                        "paper": {
                            "corpus_id": 273323209,
                            "title": "Towards Multilingual LLM Evaluation for European Languages",
                            "authors": [
                                {
                                    "authorId": "2269539",
                                    "name": "Klaudia Thellmann"
                                },
                                {
                                    "authorId": "2325726089",
                                    "name": "Bernhard Stadler"
                                },
                                {
                                    "authorId": "2258551638",
                                    "name": "Michael Fromm"
                                },
                                {
                                    "authorId": "2258550838",
                                    "name": "Jasper Schulze Buschhoff"
                                },
                                {
                                    "authorId": "2325726070",
                                    "name": "Alex Jude"
                                },
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2258552277",
                                    "name": "Johannes Leveling"
                                },
                                {
                                    "authorId": "2347259072",
                                    "name": "Nicolas Flores-Herr"
                                },
                                {
                                    "authorId": "2330411614",
                                    "name": "Joachim K\u00f6hler"
                                },
                                {
                                    "authorId": "2330412312",
                                    "name": "Ren\u00e9 J\u00e4kel"
                                },
                                {
                                    "authorId": "2258668067",
                                    "name": "Mehdi Ali"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.9345703125
                    },
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models",
                            "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)",
                            "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Etxaniz et al., 2024)",
                        "snippets": [
                            "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets."
                        ],
                        "paper": {
                            "corpus_id": 270380088,
                            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                            "authors": [
                                {
                                    "authorId": "2226458991",
                                    "name": "Julen Etxaniz"
                                },
                                {
                                    "authorId": "2481918",
                                    "name": "Gorka Azkune"
                                },
                                {
                                    "authorId": "2260104163",
                                    "name": "A. Soroa"
                                },
                                {
                                    "authorId": "2251043402",
                                    "name": "Oier L\u00f3pez de Lacalle"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 11
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Ranaldi et al., 2023)",
                        "snippets": [
                            "XQUAD and MLQA focus on understanding questions and answers through translation into different languages",
                            "Cross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages",
                            "Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4."
                        ],
                        "paper": {
                            "corpus_id": 261242630,
                            "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations",
                            "authors": [
                                {
                                    "authorId": "2008183566",
                                    "name": "Leonardo Ranaldi"
                                },
                                {
                                    "authorId": "2199247500",
                                    "name": "Giulia Pucci"
                                },
                                {
                                    "authorId": "145528474",
                                    "name": "A. Freitas"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 34
                        },
                        "score": 0.92578125
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."
                        ],
                        "paper": {
                            "corpus_id": 258865558,
                            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "35871436",
                                    "name": "Sneha Kudugunta"
                                },
                                {
                                    "authorId": "2118211280",
                                    "name": "Xinyan Velocity Yu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "1557386977",
                                    "name": "Machel Reid"
                                },
                                {
                                    "authorId": "2073587169",
                                    "name": "Yulia Tsvetkov"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 62
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Ponti et al., 2020)",
                        "snippets": [
                            "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."
                        ],
                        "paper": {
                            "corpus_id": 218470125,
                            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "50383712",
                                    "name": "Qianchu Liu"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 327
                        },
                        "score": 0
                    },
                    {
                        "id": "(Glavas et al., 2020)",
                        "snippets": [
                            "We first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vulic et al., 2019). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;(Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets."
                        ],
                        "paper": {
                            "corpus_id": 227231710,
                            "title": "SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment",
                            "authors": [
                                {
                                    "authorId": "2472657",
                                    "name": "Goran Glavas"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                },
                                {
                                    "authorId": "2029669151",
                                    "name": "Simone Paolo Ponzetto"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Workshop on Semantic Evaluation",
                            "n_citations": 8
                        },
                        "score": 0.89306640625
                    },
                    {
                        "id": "(Vulic et al., 2019)",
                        "snippets": [
                            "Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs."
                        ],
                        "paper": {
                            "corpus_id": 196183388,
                            "title": "Multilingual and Cross-Lingual Graded Lexical Entailment",
                            "authors": [
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "1801255",
                                    "name": "Simone Paolo Ponzetto"
                                },
                                {
                                    "authorId": "2472657",
                                    "name": "Goran Glavas"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 16
                        },
                        "score": 0
                    },
                    {
                        "id": "(Meng et al., 2022)",
                        "snippets": [
                            "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class."
                        ],
                        "paper": {
                            "corpus_id": 246680398,
                            "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding",
                            "authors": [
                                {
                                    "authorId": "145391513",
                                    "name": "Yu Meng"
                                },
                                {
                                    "authorId": "3488341",
                                    "name": "Jiaxin Huang"
                                },
                                {
                                    "authorId": "49891156",
                                    "name": "Yu Zhang"
                                },
                                {
                                    "authorId": "2111759643",
                                    "name": "Jiawei Han"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 235
                        },
                        "score": 0
                    },
                    {
                        "id": "(Jin et al., 2023)",
                        "snippets": [
                            "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], and ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text?"
                        ],
                        "paper": {
                            "corpus_id": 264405758,
                            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
                            "authors": [
                                {
                                    "authorId": "2087723977",
                                    "name": "Yiqiao Jin"
                                },
                                {
                                    "authorId": "32556330",
                                    "name": "Mohit Chandra"
                                },
                                {
                                    "authorId": "145816931",
                                    "name": "Gaurav Verma"
                                },
                                {
                                    "authorId": "49995036",
                                    "name": "Yibo Hu"
                                },
                                {
                                    "authorId": "2256118804",
                                    "name": "Munmun De Choudhury"
                                },
                                {
                                    "authorId": "2254102372",
                                    "name": "Srijan Kumar"
                                }
                            ],
                            "year": 2023,
                            "venue": "The Web Conference",
                            "n_citations": 76
                        },
                        "score": 0.8759765625
                    },
                    {
                        "id": "(Junczys-Dowmunt et al., 2018)",
                        "snippets": [
                            "We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."
                        ],
                        "paper": {
                            "corpus_id": 4623739,
                            "title": "Marian: Fast Neural Machine Translation in C++",
                            "authors": [
                                {
                                    "authorId": "1733933",
                                    "name": "Marcin Junczys-Dowmunt"
                                },
                                {
                                    "authorId": "3272639",
                                    "name": "Roman Grundkiewicz"
                                },
                                {
                                    "authorId": "3407177",
                                    "name": "Tomasz Dwojak"
                                },
                                {
                                    "authorId": "152378023",
                                    "name": "Hieu T. Hoang"
                                },
                                {
                                    "authorId": "1702066",
                                    "name": "Kenneth Heafield"
                                },
                                {
                                    "authorId": "1410712818",
                                    "name": "Tom Neckermann"
                                },
                                {
                                    "authorId": "1745715",
                                    "name": "F. Seide"
                                },
                                {
                                    "authorId": "50655248",
                                    "name": "Ulrich Germann"
                                },
                                {
                                    "authorId": "8129718",
                                    "name": "Alham Fikri Aji"
                                },
                                {
                                    "authorId": "3444222",
                                    "name": "Nikolay Bogoychev"
                                },
                                {
                                    "authorId": "145644643",
                                    "name": "Andr\u00e9 F. T. Martins"
                                },
                                {
                                    "authorId": "2539211",
                                    "name": "Alexandra Birch"
                                }
                            ],
                            "year": 2018,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 718
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020)(Lembersky et al., 2011)Volansky et al., 2015;Avner et al., 2016;(Eetemadi et al., 2014)(Rabinovich et al., 2015) and may lead to training data of questionable value",
                            "Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches."
                        ],
                        "paper": {
                            "corpus_id": 261557946,
                            "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
                            "authors": [
                                {
                                    "authorId": "2118895402",
                                    "name": "Xinyu Crystina Zhang"
                                },
                                {
                                    "authorId": "47583894",
                                    "name": "Nandan Thakur"
                                },
                                {
                                    "authorId": "2166106776",
                                    "name": "Odunayo Ogundepo"
                                },
                                {
                                    "authorId": "2023642",
                                    "name": "Ehsan Kamalloo"
                                },
                                {
                                    "authorId": "1419474794",
                                    "name": "David Alfonso-Hermelo"
                                },
                                {
                                    "authorId": "2238110973",
                                    "name": "Xiaoguang Li"
                                },
                                {
                                    "authorId": "1688015",
                                    "name": "Qun Liu"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 77
                        },
                        "score": 0.908203125
                    },
                    {
                        "id": "(Clark et al., 2020)",
                        "snippets": [
                            "Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation."
                        ],
                        "paper": {
                            "corpus_id": 212657414,
                            "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
                            "authors": [
                                {
                                    "authorId": "144797264",
                                    "name": "J. Clark"
                                },
                                {
                                    "authorId": "2890423",
                                    "name": "Eunsol Choi"
                                },
                                {
                                    "authorId": "123052390",
                                    "name": "Michael Collins"
                                },
                                {
                                    "authorId": "2758616",
                                    "name": "Dan Garrette"
                                },
                                {
                                    "authorId": "15652489",
                                    "name": "T. Kwiatkowski"
                                },
                                {
                                    "authorId": "48942032",
                                    "name": "Vitaly Nikolaev"
                                },
                                {
                                    "authorId": "52578817",
                                    "name": "J. Palomaki"
                                }
                            ],
                            "year": 2020,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 612
                        },
                        "score": 0
                    },
                    {
                        "id": "(Rabinovich et al., 2015)",
                        "snippets": [
                            "Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy."
                        ],
                        "paper": {
                            "corpus_id": 940724,
                            "title": "Unsupervised Identification of Translationese",
                            "authors": [
                                {
                                    "authorId": "2653682",
                                    "name": "Ella Rabinovich"
                                },
                                {
                                    "authorId": "2524073",
                                    "name": "S. Wintner"
                                }
                            ],
                            "year": 2015,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 55
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."
                        ],
                        "paper": {
                            "corpus_id": 248780386,
                            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "NLPPOWER",
                            "n_citations": 16
                        },
                        "score": 0.95751953125
                    },
                    {
                        "id": "(Koto et al., 2023)",
                        "snippets": [
                            "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."
                        ],
                        "paper": {
                            "corpus_id": 263829211,
                            "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
                            "authors": [
                                {
                                    "authorId": "2789148",
                                    "name": "Fajri Koto"
                                },
                                {
                                    "authorId": "2256987672",
                                    "name": "Nurul Aisyah"
                                },
                                {
                                    "authorId": "49404498",
                                    "name": "Haonan Li"
                                },
                                {
                                    "authorId": "2256987316",
                                    "name": "Timothy Baldwin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 46
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yuksel et al., 2024)",
                        "snippets": [
                            "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                        ],
                        "paper": {
                            "corpus_id": 271245129,
                            "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
                            "authors": [
                                {
                                    "authorId": "2311698426",
                                    "name": "Arda Yuksel"
                                },
                                {
                                    "authorId": "1999179692",
                                    "name": "Abdullatif K\u00f6ksal"
                                },
                                {
                                    "authorId": "2126865294",
                                    "name": "Lutfi Kerem cSenel"
                                },
                                {
                                    "authorId": "2311700614",
                                    "name": "Anna Korhonen"
                                },
                                {
                                    "authorId": "2130001188",
                                    "name": "Hinrich Schutze"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hershcovich et al., 2022)",
                        "snippets": [
                            "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture",
                            "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations",
                            "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."
                        ],
                        "paper": {
                            "corpus_id": 247594499,
                            "title": "Challenges and Strategies in Cross-Cultural NLP",
                            "authors": [
                                {
                                    "authorId": "2064295987",
                                    "name": "Daniel Hershcovich"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "49568895",
                                    "name": "Heather Lent"
                                },
                                {
                                    "authorId": "3295381",
                                    "name": "Miryam de Lhoneux"
                                },
                                {
                                    "authorId": "30671790",
                                    "name": "Mostafa Abdou"
                                },
                                {
                                    "authorId": "6547490",
                                    "name": "Stephanie Brandl"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "2093582149",
                                    "name": "Laura Cabello Piqueras"
                                },
                                {
                                    "authorId": "2125376289",
                                    "name": "Ilias Chalkidis"
                                },
                                {
                                    "authorId": "1717462692",
                                    "name": "Ruixiang Cui"
                                },
                                {
                                    "authorId": "50110151",
                                    "name": "Constanza Fierro"
                                },
                                {
                                    "authorId": "82259306",
                                    "name": "Katerina Margatina"
                                },
                                {
                                    "authorId": "1660797358",
                                    "name": "Phillip Rust"
                                },
                                {
                                    "authorId": "1700187",
                                    "name": "Anders S\u00f8gaard"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 182
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."
                        ],
                        "paper": {
                            "corpus_id": 238198104,
                            "title": "Visually Grounded Reasoning across Languages and Cultures",
                            "authors": [
                                {
                                    "authorId": "144097210",
                                    "name": "Fangyu Liu"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                },
                                {
                                    "authorId": "50638196",
                                    "name": "Nigel Collier"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 180
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yin et al., 2021)",
                        "snippets": [
                            "Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models\u2019 ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR."
                        ],
                        "paper": {
                            "corpus_id": 237503047,
                            "title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "144508458",
                                    "name": "Da Yin"
                                },
                                {
                                    "authorId": "2108904535",
                                    "name": "Liunian Harold Li"
                                },
                                {
                                    "authorId": "3407296",
                                    "name": "Ziniu Hu"
                                },
                                {
                                    "authorId": "3157053",
                                    "name": "Nanyun Peng"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 56
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)",
                            "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations."
                        ],
                        "paper": {
                            "corpus_id": 277993848,
                            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                            "authors": [
                                {
                                    "authorId": "2327995656",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "2258789284",
                                    "name": "Weixuan Wang"
                                },
                                {
                                    "authorId": "2349802912",
                                    "name": "Sinuo Liu"
                                },
                                {
                                    "authorId": "2331767275",
                                    "name": "Huifeng Yin"
                                },
                                {
                                    "authorId": "2356794252",
                                    "name": "Xintong Wang"
                                },
                                {
                                    "authorId": "2331686159",
                                    "name": "Yu Zhao"
                                },
                                {
                                    "authorId": "2266387313",
                                    "name": "Chenyang Lyu"
                                },
                                {
                                    "authorId": "2302342302",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.91796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Effectiveness Comparison",
                "tldr": "Studies comparing translation-based and natively collected multilingual evaluation datasets show mixed results, with translation-based methods offering scalability advantages but suffering from translation artifacts that affect performance. The effectiveness of each approach varies significantly depending on the task, language pairs, and evaluation setup, with domain transferability sometimes proving more crucial than language transferability. (13 sources)",
                "text": "\nComparative analyses of translation-based and natively collected multilingual evaluation datasets reveal significant differences in their effectiveness across various tasks and languages. Translation-based approaches demonstrate considerable utility for cross-lingual transfer, with studies showing that models trained on translated datasets often outperform source-language baselines. For instance, in semantic role labeling tasks, translated training data led to an average F1 score increase of 6.7 points compared to source-only baselines, demonstrating that \"corpus translation is one effective way for cross-lingual SRL\" <Paper corpusId=\"215754572\" paperTitle=\"(Fei et al., 2020)\" isShortName></Paper> <Paper corpusId=\"7805664\" paperTitle=\"(Tiedemann et al., 2016)\" isShortName></Paper> <Paper corpusId=\"202541341\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.\n\nHowever, the effectiveness of translation-based methods varies considerably across languages and depends on several factors. Liu et al. found that performance on translated question-answering datasets \"depends on not only how similar the target language and English are, but also how difficult the question set of the target language is\" <Paper corpusId=\"196174566\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. Additionally, translation-based methods often struggle with named entities, which can significantly impact performance in knowledge-intensive tasks <Paper corpusId=\"196174566\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nSeveral evaluation frameworks have been developed to assess translation-based approaches, including translate-test (where evaluation sets are translated to English and a single English model is used) and translate-train (where English training data is translated to target languages) <Paper corpusId=\"207880568\" paperTitle=\"(Conneau et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233481097\" paperTitle=\"(Goyal et al., 2021)\" isShortName></Paper>. These methods serve as critical baselines for cross-lingual transfer assessment, with translate-train-all (training on all translated datasets) often producing strong results <Paper corpusId=\"233481097\" paperTitle=\"(Goyal et al., 2021)\" isShortName></Paper>.\n\nInterestingly, recent research has uncovered a counterintuitive phenomenon: models trained on translated texts sometimes outperform those trained on human-authored texts when evaluated on translate-test sets <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>. This suggests that translation artifacts may actually benefit performance when both training and evaluation involve similar translation processes. However, on original English evaluation sets, models trained on human texts typically perform best <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of translation-based approaches also varies by task type. For sentiment analysis on the SemEval dataset, augmenting training data with translations consistently improved performance over multilingual baselines, with translating source language data to all target languages (Tr S\u2192all) proving most effective <Paper corpusId=\"241583550\" paperTitle=\"(Jacqmin et al., 2021)\" isShortName></Paper>. However, this advantage did not generalize to in-house datasets, highlighting the context-dependent nature of translation effectiveness <Paper corpusId=\"241583550\" paperTitle=\"(Jacqmin et al., 2021)\" isShortName></Paper>.\n\nDespite their utility, translation-based approaches face significant limitations. Translation errors can percolate to the classification phase, and maximum-likelihood translations may lack sufficient expressiveness to capture nuanced meanings <Paper corpusId=\"236318351\" paperTitle=\"(Ponti et al., 2021)\" isShortName></Paper>. These issues can compromise evaluation reliability, particularly for complex semantic tasks.\n\nA critical consideration in evaluating these approaches is domain transferability. Studies show that cross-lingual rankers trained on Wikipedia-based QA datasets generalize poorly to product domains, performing worse than multilingual rankers trained on in-domain English data <Paper corpusId=\"258715321\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper>. This suggests that \"domain transferability is even more crucial than language transferability\" in some applications <Paper corpusId=\"258715321\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper>.\n\nTranslation-based approaches have proven particularly valuable for creating multilingual versions of important benchmark datasets. For example, the multilingual MS MARCO passage ranking dataset (mMARCO) demonstrated that \"multilingual models finetuned on translated datasets achieve superior effectiveness to models finetuned on the original English version alone\" in zero-shot scenarios on the Mr. TyDi dataset <Paper corpusId=\"237364084\" paperTitle=\"(Bonifacio et al., 2021)\" isShortName></Paper>. Similarly, machine translation has been applied as cross-lingual data augmentation for bias detection in Hindi, Italian, and Korean datasets <Paper corpusId=\"259859119\" paperTitle=\"(Sahoo et al., 2023)\" isShortName></Paper> and has shown improvements on multilingual benchmarks <Paper corpusId=\"235097287\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\nOverall, while translation-based approaches offer practical advantages for cross-lingual evaluation, their effectiveness compared to natively collected datasets varies significantly by language, task, and evaluation methodology. The ideal approach depends on specific research goals, available resources, and the languages being studied.",
                "citations": [
                    {
                        "id": "(Fei et al., 2020)",
                        "snippets": [
                            "Translated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann et al., 2016)(Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model."
                        ],
                        "paper": {
                            "corpus_id": 215754572,
                            "title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus",
                            "authors": [
                                {
                                    "authorId": "46959445",
                                    "name": "Hao Fei"
                                },
                                {
                                    "authorId": "2678094",
                                    "name": "Meishan Zhang"
                                },
                                {
                                    "authorId": "145628086",
                                    "name": "Donghong Ji"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 108
                        },
                        "score": 0.94775390625
                    },
                    {
                        "id": "(Tiedemann et al., 2016)",
                        "snippets": [
                            "How do we parse the languages for which no treebanks are available? This contribution addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt to make use of resource-rich source language treebanks to build and adapt models for the under-resourced target languages. We outline the benefits, and indicate the drawbacks of the current major approaches. We emphasize synthetic treebanking: the automatic creation of target language treebanks by means of annotation projection and machine translation. We present competitive results in cross-lingual dependency parsing using a combination of various techniques that contribute to the overall success of the method. We further include a detailed discussion about the impact of part-of-speech label accuracy on parsing results that provide guidance in practical applications of cross-lingual methods for truly under-resourced languages."
                        ],
                        "paper": {
                            "corpus_id": 7805664,
                            "title": "Synthetic Treebanking for Cross-Lingual Dependency Parsing",
                            "authors": [
                                {
                                    "authorId": "143675545",
                                    "name": "J. Tiedemann"
                                },
                                {
                                    "authorId": "1806948",
                                    "name": "Zeljko Agic"
                                }
                            ],
                            "year": 2016,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 56
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2019)",
                        "snippets": [
                            "Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods."
                        ],
                        "paper": {
                            "corpus_id": 202541341,
                            "title": "Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank",
                            "authors": [
                                {
                                    "authorId": "2678094",
                                    "name": "Meishan Zhang"
                                },
                                {
                                    "authorId": "2096969631",
                                    "name": "Zhang Meishan"
                                },
                                {
                                    "authorId": "2068672157",
                                    "name": "Zhang Yue"
                                },
                                {
                                    "authorId": "31060997",
                                    "name": "Fu Guo-hong"
                                }
                            ],
                            "year": 2019,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 37
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is."
                        ],
                        "paper": {
                            "corpus_id": 196174566,
                            "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
                            "authors": [
                                {
                                    "authorId": "46701066",
                                    "name": "Jiahua Liu"
                                },
                                {
                                    "authorId": "2427350",
                                    "name": "Yankai Lin"
                                },
                                {
                                    "authorId": "49293587",
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "1753344",
                                    "name": "Maosong Sun"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 91
                        },
                        "score": 0.9013671875
                    },
                    {
                        "id": "(Conneau et al., 2019)",
                        "snippets": [
                            "Cross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project."
                        ],
                        "paper": {
                            "corpus_id": 207880568,
                            "title": "Unsupervised Cross-lingual Representation Learning at Scale",
                            "authors": [
                                {
                                    "authorId": "2480903",
                                    "name": "Alexis Conneau"
                                },
                                {
                                    "authorId": "40267343",
                                    "name": "Kartikay Khandelwal"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "113810201",
                                    "name": "Vishrav Chaudhary"
                                },
                                {
                                    "authorId": "2293203",
                                    "name": "Guillaume Wenzek"
                                },
                                {
                                    "authorId": "144204682",
                                    "name": "Francisco Guzm\u00e1n"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                },
                                {
                                    "authorId": "40511414",
                                    "name": "Myle Ott"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1759422",
                                    "name": "Veselin Stoyanov"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 6589
                        },
                        "score": 0.8798828125
                    },
                    {
                        "id": "(Goyal et al., 2021)",
                        "snippets": [
                            "We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency."
                        ],
                        "paper": {
                            "corpus_id": 233481097,
                            "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling",
                            "authors": [
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "3048577",
                                    "name": "Jingfei Du"
                                },
                                {
                                    "authorId": "40511414",
                                    "name": "Myle Ott"
                                },
                                {
                                    "authorId": "4168565",
                                    "name": "Giridhar Anantharaman"
                                },
                                {
                                    "authorId": "2480903",
                                    "name": "Alexis Conneau"
                                }
                            ],
                            "year": 2021,
                            "venue": "Workshop on Representation Learning for NLP",
                            "n_citations": 129
                        },
                        "score": 0.9072265625
                    },
                    {
                        "id": "(Park et al., 2024)",
                        "snippets": [
                            "We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes",
                            "Evaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer",
                            "In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts."
                        ],
                        "paper": {
                            "corpus_id": 270226445,
                            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
                            "authors": [
                                {
                                    "authorId": "1484057728",
                                    "name": "chaeHun Park"
                                },
                                {
                                    "authorId": "2110019663",
                                    "name": "Ko-tik Lee"
                                },
                                {
                                    "authorId": "2151202457",
                                    "name": "Hyesu Lim"
                                },
                                {
                                    "authorId": "2293573807",
                                    "name": "Jaeseok Kim"
                                },
                                {
                                    "authorId": "2305630071",
                                    "name": "Junmo Park"
                                },
                                {
                                    "authorId": "2293447208",
                                    "name": "Yu-Jung Heo"
                                },
                                {
                                    "authorId": "2293430063",
                                    "name": "Du-Seong Chang"
                                },
                                {
                                    "authorId": "2260653165",
                                    "name": "Jaegul Choo"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.89453125
                    },
                    {
                        "id": "(Jacqmin et al., 2021)",
                        "snippets": [
                            "When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning.\n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages.\n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval.\n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average."
                        ],
                        "paper": {
                            "corpus_id": 241583550,
                            "title": "SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining",
                            "authors": [
                                {
                                    "authorId": "66957859",
                                    "name": "L\u00e9o Jacqmin"
                                },
                                {
                                    "authorId": "2483876",
                                    "name": "Gabriel Marzinotto"
                                },
                                {
                                    "authorId": "2101678",
                                    "name": "Justyna Gromada"
                                },
                                {
                                    "authorId": "1736936",
                                    "name": "Ewelina Szczekocka"
                                },
                                {
                                    "authorId": "2611308",
                                    "name": "Robert Kolodynski"
                                },
                                {
                                    "authorId": "1722225",
                                    "name": "G\u00e9raldine Damnati"
                                }
                            ],
                            "year": 2021,
                            "venue": "WNUT",
                            "n_citations": 2
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Ponti et al., 2021)",
                        "snippets": [
                            "While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation."
                        ],
                        "paper": {
                            "corpus_id": 236318351,
                            "title": "Modelling Latent Translations for Cross-Lingual Transfer",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "3422710",
                                    "name": "Julia Kreutzer"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Shen et al., 2023)",
                        "snippets": [
                            "We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability."
                        ],
                        "paper": {
                            "corpus_id": 258715321,
                            "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages",
                            "authors": [
                                {
                                    "authorId": "2562211",
                                    "name": "Xiaoyu Shen"
                                },
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "36126076",
                                    "name": "B. Byrne"
                                },
                                {
                                    "authorId": "1786911",
                                    "name": "A. Gispert"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.89208984375
                    },
                    {
                        "id": "(Bonifacio et al., 2021)",
                        "snippets": [
                            "More recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO",
                            "We evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone."
                        ],
                        "paper": {
                            "corpus_id": 237364084,
                            "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset",
                            "authors": [
                                {
                                    "authorId": "2003019597",
                                    "name": "L. Bonifacio"
                                },
                                {
                                    "authorId": "1472908948",
                                    "name": "Israel Campiotti"
                                },
                                {
                                    "authorId": "1809633",
                                    "name": "R. Lotufo"
                                },
                                {
                                    "authorId": "143744603",
                                    "name": "Rodrigo Nogueira"
                                }
                            ],
                            "year": 2021,
                            "venue": "",
                            "n_citations": 113
                        },
                        "score": 0.89111328125
                    },
                    {
                        "id": "(Sahoo et al., 2023)",
                        "snippets": [
                            "For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets."
                        ],
                        "paper": {
                            "corpus_id": 259859119,
                            "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages",
                            "authors": [
                                {
                                    "authorId": "2119502119",
                                    "name": "Nihar Ranjan Sahoo"
                                },
                                {
                                    "authorId": "2166204873",
                                    "name": "Niteesh Mallela"
                                },
                                {
                                    "authorId": "145532184",
                                    "name": "P. Bhattacharyya"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.90283203125
                    },
                    {
                        "id": "(Wang et al., 2021)",
                        "snippets": [
                            "In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019)."
                        ],
                        "paper": {
                            "corpus_id": 235097287,
                            "title": "Practical Transformer-based Multilingual Text Classification",
                            "authors": [
                                {
                                    "authorId": "2108725400",
                                    "name": "Cindy Wang"
                                },
                                {
                                    "authorId": "2339397",
                                    "name": "Michele Banko"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.86669921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Translation Artifacts and Biases",
                "tldr": "Translation-based multilingual datasets often contain subtle artifacts that make translated text systematically different from naturally authored text, leading to evaluation bias. These \"translationese\" features can create artificially inflated performance metrics when models are both trained and evaluated on translated data, resulting in unreliable assessments of true cross-lingual capabilities. (19 sources)",
                "text": "\nTranslation-based approaches to creating multilingual evaluation datasets introduce distinct linguistic patterns known as \"translationese\" or translation artifacts that make translated text systematically different from naturally authored text in target languages <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper> <Paper corpusId=\"261557946\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. These artifacts appear in both human and machine translations and can manifest as differences in lexical characteristics, including the frequency of functional words and pronouns <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper>. Such patterns make translated datasets less representative of authentic language use in real-world contexts, particularly for dialogue and culturally-specific content <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper> <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nTranslation quality significantly affects the reliability of cross-lingual evaluations, with high-resource languages appearing to outperform low-resource languages partly due to better translation systems rather than actual model capabilities <Paper corpusId=\"235683093\" paperTitle=\"(Turc et al., 2021)\" isShortName></Paper>. This bias is particularly problematic because it creates an artificially optimistic estimation of models' cross-lingual abilities <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper>. Recent work has empirically demonstrated this effect, finding that sentence representations of translated data are unnaturally more similar to their English sources than independently generated examples in the target language <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nThe impact of translation artifacts on evaluation extends to various NLP tasks. For natural language inference (NLI), independent translation of premises and hypotheses can reduce the lexical overlap between them, affecting models that are highly sensitive to such patterns <Paper corpusId=\"252968346\" paperTitle=\"(Oh et al., 2022)\" isShortName></Paper> <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Similarly, in summarization, quality evaluations have been found not to \"survive translation,\" suggesting that translated summaries may be assessed differently than their original counterparts <Paper corpusId=\"253762032\" paperTitle=\"(Krubinski et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237532546\" paperTitle=\"(Iskender et al., 2021)\" isShortName></Paper>.\n\nTranslation inconsistencies disproportionately affect low-resource languages, as demonstrated in the XNLI dataset. Researchers have found poor agreement between human-translated test instances and the original English labels they were supposed to inherit, particularly for languages like Hindi and Urdu <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>. These inconsistencies can be identified by measuring performance gaps between evaluations on human-translated versus machine-translated target text across multiple languages <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nAn intriguing finding is that models trained on translated texts often outperform those trained on human-authored texts when evaluated on translate-test sets <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>. This suggests that translation artifacts may actually benefit performance when both training and evaluation involve similar translation processes, further confirming that relying solely on translated evaluation data can yield misleading results <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>. However, when evaluated on original English datasets, models trained on human-authored texts typically perform best <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>.\n\nWhile translation-based multilingual datasets are valuable for enabling cross-language comparisons, they introduce simpler syntax and lexical choices that differ distributionally from native data and may not reflect native users' preferences <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270562911\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"57174748\" paperTitle=\"(Baker et al., 1993)\" isShortName></Paper>. For relation extraction tasks, researchers have identified specific error categories in translation and alignment that can affect model performance, including pronoun-dropping, coordination, and compounding issues <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\nTo mitigate these biases, researchers have proposed several approaches. One recommendation is to ensure consistent test sets across languages by either using original annotations in all languages or translating from non-English sources to avoid English-centric biases <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Others suggest using large language models for self-translation to construct training data, as this can reduce translationese effects and produce more natural translations <Paper corpusId=\"273323302\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Additionally, complementing translation-based benchmarks with natively authored multilingual benchmarks can provide a more comprehensive evaluation of cross-lingual capabilities <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, machine-translated data has shown benefits for classification and structured prediction tasks <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233189585\" paperTitle=\"(Ozaki et al., 2021)\" isShortName></Paper>. However, researchers emphasize the importance of using target-language originating evaluation data in multimodal setups rather than translated data to avoid the risk of fitting to translationese <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>. This is particularly crucial when evaluating large language models that are expected to serve diverse linguistic communities <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models",
                            "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)",
                            "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Zhang et al., 2023)",
                        "snippets": [
                            "However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020)(Lembersky et al., 2011)Volansky et al., 2015;Avner et al., 2016;(Eetemadi et al., 2014)(Rabinovich et al., 2015) and may lead to training data of questionable value",
                            "Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches."
                        ],
                        "paper": {
                            "corpus_id": 261557946,
                            "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages",
                            "authors": [
                                {
                                    "authorId": "2118895402",
                                    "name": "Xinyu Crystina Zhang"
                                },
                                {
                                    "authorId": "47583894",
                                    "name": "Nandan Thakur"
                                },
                                {
                                    "authorId": "2166106776",
                                    "name": "Odunayo Ogundepo"
                                },
                                {
                                    "authorId": "2023642",
                                    "name": "Ehsan Kamalloo"
                                },
                                {
                                    "authorId": "1419474794",
                                    "name": "David Alfonso-Hermelo"
                                },
                                {
                                    "authorId": "2238110973",
                                    "name": "Xiaoguang Li"
                                },
                                {
                                    "authorId": "1688015",
                                    "name": "Qun Liu"
                                },
                                {
                                    "authorId": "2066076226",
                                    "name": "Mehdi Rezagholizadeh"
                                },
                                {
                                    "authorId": "145580839",
                                    "name": "Jimmy J. Lin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 77
                        },
                        "score": 0.908203125
                    },
                    {
                        "id": "(Razumovskaia et al., 2021)",
                        "snippets": [
                            "These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level."
                        ],
                        "paper": {
                            "corpus_id": 235313293,
                            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
                            "authors": [
                                {
                                    "authorId": "66879943",
                                    "name": "E. Razumovskaia"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 34
                        },
                        "score": 0.8642578125
                    },
                    {
                        "id": "(Majewska et al., 2022)",
                        "snippets": [
                            "The results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation",
                            ".the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets",
                            ".Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable."
                        ],
                        "paper": {
                            "corpus_id": 246430787,
                            "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
                            "authors": [
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "66879943",
                                    "name": "E. Razumovskaia"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.9873046875
                    },
                    {
                        "id": "(Turc et al., 2021)",
                        "snippets": [
                            "In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer."
                        ],
                        "paper": {
                            "corpus_id": 235683093,
                            "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer",
                            "authors": [
                                {
                                    "authorId": "1388156275",
                                    "name": "Iulia Turc"
                                },
                                {
                                    "authorId": "2110237268",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "144154709",
                                    "name": "Jacob Eisenstein"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2021,
                            "venue": "arXiv.org",
                            "n_citations": 58
                        },
                        "score": 0.90625
                    },
                    {
                        "id": "(Oh et al., 2022)",
                        "snippets": [
                            "Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;(Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test."
                        ],
                        "paper": {
                            "corpus_id": 252968346,
                            "title": "Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks",
                            "authors": [
                                {
                                    "authorId": "46299565",
                                    "name": "Jaehoon Oh"
                                },
                                {
                                    "authorId": "2051385328",
                                    "name": "Jongwoo Ko"
                                },
                                {
                                    "authorId": "70509252",
                                    "name": "Se-Young Yun"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 9
                        },
                        "score": 0.92724609375
                    },
                    {
                        "id": "(Krubinski et al., 2022)",
                        "snippets": [
                            "Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by (Iskender et al., 2021), which indicate that summary evaluations do not survive translation."
                        ],
                        "paper": {
                            "corpus_id": 253762032,
                            "title": "From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?",
                            "authors": [
                                {
                                    "authorId": "2040867021",
                                    "name": "Mateusz Krubi\u0144ski"
                                },
                                {
                                    "authorId": "1758528",
                                    "name": "Pavel Pecina"
                                }
                            ],
                            "year": 2022,
                            "venue": "EVAL4NLP",
                            "n_citations": 3
                        },
                        "score": 0.89892578125
                    },
                    {
                        "id": "(Iskender et al., 2021)",
                        "snippets": [
                            "The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at https://github.com/PrimerAI/primer-research/."
                        ],
                        "paper": {
                            "corpus_id": 237532546,
                            "title": "Does Summary Evaluation Survive Translation to Other Languages?",
                            "authors": [
                                {
                                    "authorId": "147675294",
                                    "name": "Neslihan Iskender"
                                },
                                {
                                    "authorId": "2753049",
                                    "name": "Oleg V. Vasilyev"
                                },
                                {
                                    "authorId": "1912566",
                                    "name": "Tim Polzehl"
                                },
                                {
                                    "authorId": "1995240446",
                                    "name": "John Bohannon"
                                },
                                {
                                    "authorId": "2058899254",
                                    "name": "Sebastian Moller"
                                }
                            ],
                            "year": 2021,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 4
                        },
                        "score": 0
                    },
                    {
                        "id": "(Agrawal et al., 2024)",
                        "snippets": [
                            "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."
                        ],
                        "paper": {
                            "corpus_id": 267413041,
                            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2282542641",
                                    "name": "Ashish Agrawal"
                                },
                                {
                                    "authorId": "2187454108",
                                    "name": "Barah Fazili"
                                },
                                {
                                    "authorId": "144859542",
                                    "name": "P. Jyothi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Park et al., 2024)",
                        "snippets": [
                            "We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes",
                            "Evaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer",
                            "In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts."
                        ],
                        "paper": {
                            "corpus_id": 270226445,
                            "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering",
                            "authors": [
                                {
                                    "authorId": "1484057728",
                                    "name": "chaeHun Park"
                                },
                                {
                                    "authorId": "2110019663",
                                    "name": "Ko-tik Lee"
                                },
                                {
                                    "authorId": "2151202457",
                                    "name": "Hyesu Lim"
                                },
                                {
                                    "authorId": "2293573807",
                                    "name": "Jaeseok Kim"
                                },
                                {
                                    "authorId": "2305630071",
                                    "name": "Junmo Park"
                                },
                                {
                                    "authorId": "2293447208",
                                    "name": "Yu-Jung Heo"
                                },
                                {
                                    "authorId": "2293430063",
                                    "name": "Du-Seong Chang"
                                },
                                {
                                    "authorId": "2260653165",
                                    "name": "Jaegul Choo"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.89453125
                    },
                    {
                        "id": "(Bland'on et al., 2025)",
                        "snippets": [
                            "Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993)Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 276575630,
                            "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2346978280",
                                    "name": "Mar'ia Andrea Cruz Bland'on"
                                },
                                {
                                    "authorId": "114938178",
                                    "name": "Jayasimha Talur"
                                },
                                {
                                    "authorId": "2346980693",
                                    "name": "Bruno Charron"
                                },
                                {
                                    "authorId": "2343588002",
                                    "name": "Dong Liu"
                                },
                                {
                                    "authorId": "39674628",
                                    "name": "Saab Mansour"
                                },
                                {
                                    "authorId": "2346980678",
                                    "name": "Marcello Federico"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks."
                        ],
                        "paper": {
                            "corpus_id": 270562911,
                            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
                            "authors": [
                                {
                                    "authorId": "143616669",
                                    "name": "Pinzhen Chen"
                                },
                                {
                                    "authorId": "2307212816",
                                    "name": "Simon Yu"
                                },
                                {
                                    "authorId": "2307224542",
                                    "name": "Zhicheng Guo"
                                },
                                {
                                    "authorId": "2259100",
                                    "name": "B. Haddow"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0
                    },
                    {
                        "id": "(Baker et al., 1993)",
                        "snippets": [
                            "The rise of corpus linguistics has serious implications for any discipline in which language plays a major role. This paper explores the impact that the availability of corpora is likely to have on the study of translation as an empirical phenomenon. It argues that the techniques and methodology developed in the field of corpus linguistics will have a direct impact on the emerging discipline of translation studies, particularly with respect to its theoretical and descriptive branches. The nature of this impact is discussed in some detail and brief reference is made to some of the applications of corpus techniques in the applied branch of the dis\u00ad cipline."
                        ],
                        "paper": {
                            "corpus_id": 57174748,
                            "title": "'Corpus Linguistics and Translation Studies: Implications and Applications'",
                            "authors": [
                                {
                                    "authorId": "103983301",
                                    "name": "Mona Baker"
                                },
                                {
                                    "authorId": "46880608",
                                    "name": "G. Francis"
                                },
                                {
                                    "authorId": "1404600818",
                                    "name": "E. Tognini-Bonelli"
                                }
                            ],
                            "year": 1993,
                            "venue": "",
                            "n_citations": 1061
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hennig et al., 2023)",
                        "snippets": [
                            "The MultiTACRED dataset we present in this work covers 12 languages from 9 language families. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances",
                            ".Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;(Ozaki et al., 2021)Yarmohammadi et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 258557740,
                            "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset",
                            "authors": [
                                {
                                    "authorId": "36943315",
                                    "name": "Leonhard Hennig"
                                },
                                {
                                    "authorId": "143680610",
                                    "name": "Philippe E. Thomas"
                                },
                                {
                                    "authorId": "145733288",
                                    "name": "Sebastian M\u00f6ller"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 8
                        },
                        "score": 0.90185546875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;(Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 273323302,
                            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
                            "authors": [
                                {
                                    "authorId": "2218735807",
                                    "name": "Wen Yang"
                                },
                                {
                                    "authorId": "2237788942",
                                    "name": "Junhong Wu"
                                },
                                {
                                    "authorId": "2188130997",
                                    "name": "Chen Wang"
                                },
                                {
                                    "authorId": "2064100826",
                                    "name": "Chengqing Zong"
                                },
                                {
                                    "authorId": "2303798059",
                                    "name": "Jiajun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 7
                        },
                        "score": 0.931640625
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation."
                        ],
                        "paper": {
                            "corpus_id": 273532366,
                            "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
                            "authors": [
                                {
                                    "authorId": "2327246234",
                                    "name": "Junhua Liu"
                                },
                                {
                                    "authorId": "2325881142",
                                    "name": "Bin Fu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92822265625
                    },
                    {
                        "id": "(Ozaki et al., 2021)",
                        "snippets": [
                            "This paper describes the first report on cross-lingual transfer for semantic dependency parsing. We present the insight that there are twodifferent kinds of cross-linguality, namely sur-face level and mantic level, and try to cap-ture both kinds of cross-linguality by combin-ing annotation projection and model transferof pre-trained language models. Our exper-iments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound."
                        ],
                        "paper": {
                            "corpus_id": 233189585,
                            "title": "Project-then-Transfer: Effective Two-stage Cross-lingual Transfer for Semantic Dependency Parsing",
                            "authors": [
                                {
                                    "authorId": "36904068",
                                    "name": "Hiroaki Ozaki"
                                },
                                {
                                    "authorId": "29347584",
                                    "name": "Gaku Morio"
                                },
                                {
                                    "authorId": "1379579811",
                                    "name": "Terufumi Morishita"
                                },
                                {
                                    "authorId": "2468213",
                                    "name": "Toshinori Miyoshi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 7
                        },
                        "score": 0
                    },
                    {
                        "id": "(Qiu et al., 2022)",
                        "snippets": [
                            "We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."
                        ],
                        "paper": {
                            "corpus_id": 253098629,
                            "title": "Multilingual Multimodal Learning with Machine Translated Text",
                            "authors": [
                                {
                                    "authorId": "3383271",
                                    "name": "Chen Qiu"
                                },
                                {
                                    "authorId": "3095774",
                                    "name": "Dan Onea\u0163\u0103"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."
                        ],
                        "paper": {
                            "corpus_id": 268819377,
                            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                            "authors": [
                                {
                                    "authorId": "2257136845",
                                    "name": "Yuemei Xu"
                                },
                                {
                                    "authorId": "2258334185",
                                    "name": "Ling Hu"
                                },
                                {
                                    "authorId": "2294513520",
                                    "name": "Jiayi Zhao"
                                },
                                {
                                    "authorId": "2294361104",
                                    "name": "Zihan Qiu"
                                },
                                {
                                    "authorId": "2294363807",
                                    "name": "Yuqi Ye"
                                },
                                {
                                    "authorId": "2294933103",
                                    "name": "Hanwen Gu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Comput. Sci.",
                            "n_citations": 43
                        },
                        "score": 0.92724609375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact on Cross-lingual Evaluation Reliability",
                "tldr": "The reliability of cross-lingual evaluation is significantly compromised when using translation-based datasets due to factors such as translation artifacts, cultural misalignment, and uneven translation quality across languages. These limitations can lead to overly optimistic performance estimates that do not accurately reflect models' true capabilities when serving users in different languages and cultural contexts. (18 sources)",
                "text": "\nThe impact of dataset creation methodology on cross-lingual evaluation reliability extends beyond mere methodological preferences to fundamentally shape our understanding of multilingual model capabilities. Translation-based evaluation approaches introduce systematic biases that can significantly distort performance assessments, often producing excessively optimistic estimates of models' cross-lingual abilities <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper> <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>. This \"unnaturally skewed\" evaluation data creates unreliable benchmarks that may not reflect real-world performance in target languages <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nA key factor undermining evaluation reliability is the presence of translation inconsistencies, which disproportionately affect low-resource languages. Studies of the XNLI dataset revealed poor agreement between human-translated test instances and their original English labels, particularly for languages like Hindi and Urdu <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>. These inconsistencies can be identified by measuring performance gaps between evaluations on human-translated versus machine-translated target text across languages <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nTranslation quality significantly influences evaluation outcomes, with high-resource languages appearing to outperform low-resource languages partly due to better translation systems rather than actual model capabilities <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper>. This creates artificial performance hierarchies that may not reflect true linguistic competence. Additionally, translation accuracy varies by task type and content domain, with name entities posing particular challenges for translation-based methods in question answering tasks <Paper corpusId=\"196174566\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nCultural representation presents another significant challenge for evaluation reliability. Translated benchmarks often introduce biases stemming from cultural differences <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>. These datasets typically reflect the source culture rather than authentically representing the target culture, compromising their validity for evaluating models intended to serve diverse linguistic communities <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>. Cultural biases cannot be mitigated through automatic translation alone; they require either human annotators to evaluate and address cultural sensitivities or the development of benchmarks based on regional resources <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nThe limitations of translation-based evaluation are particularly evident in task-oriented dialogue (TOD) systems, where researchers have demonstrated that models perform better on translation-based evaluation sets than on more natural, natively generated examples <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>. This finding reveals that \"using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications\" <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nTo address these reliability concerns, researchers have proposed several approaches. Using target-language originating evaluation data rather than translated data is critical, particularly in multimodal setups where \"fitting to translationese is a risk when using translation data at training time\" <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>. This risk can only be identified if evaluation data does not also contain translations, especially automatically generated ones <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>.\n\nCreating culturally and linguistically tailored evaluation datasets represents another important strategy. Recent research has demonstrated that localized benchmarks show significantly higher alignment with local human judgments (0.68 correlation) compared to their translated counterparts (0.47) <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. This finding has motivated an increasing focus on developing native evaluation benchmarks that better represent local cultural knowledge <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>.\n\nA balanced approach involves combining translation-based and natively collected evaluation data. Projects like XOR-TYDI QA exemplify this hybrid methodology, incorporating both in-language data (where answers are extracted from Wikipedia in the question's language) and cross-lingual data (where English spans are translated to target languages) <Paper corpusId=\"236428949\" paperTitle=\"(Asai et al., 2021)\" isShortName></Paper>. Similarly, researchers have suggested that human translation or original data from target cultures, at least for high-quality evaluation data, represents a practical middle ground that \"allows us to judge the success of cross-lingual transfer in a culturally appropriate way\" <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper> <Paper corpusId=\"237503047\" paperTitle=\"(Yin et al., 2021)\" isShortName></Paper> <Paper corpusId=\"238198104\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>.\n\nAs multilingual evaluation evolves, researchers increasingly recognize that the reliability of cross-lingual assessments depends on addressing both translation artifacts and cultural representation. While translation-based datasets offer valuable parallel structures for cross-language comparisons, they must be complemented with natively authored resources to provide a comprehensive and authentic evaluation of models' cross-lingual capabilities <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216914383\" paperTitle=\"(Artetxe et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. Without such complementary approaches, evaluations risk failing to detect cultural biases or distribution shifts that affect real-world model performance <Paper corpusId=\"274149965\" paperTitle=\"(Seto et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Razumovskaia et al., 2021)",
                        "snippets": [
                            "These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level."
                        ],
                        "paper": {
                            "corpus_id": 235313293,
                            "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems",
                            "authors": [
                                {
                                    "authorId": "66879943",
                                    "name": "E. Razumovskaia"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                }
                            ],
                            "year": 2021,
                            "venue": "Journal of Artificial Intelligence Research",
                            "n_citations": 34
                        },
                        "score": 0.8642578125
                    },
                    {
                        "id": "(Majewska et al., 2022)",
                        "snippets": [
                            "The results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation",
                            ".the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets",
                            ".Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable."
                        ],
                        "paper": {
                            "corpus_id": 246430787,
                            "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
                            "authors": [
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "66879943",
                                    "name": "E. Razumovskaia"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 28
                        },
                        "score": 0.9873046875
                    },
                    {
                        "id": "(Agrawal et al., 2024)",
                        "snippets": [
                            "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."
                        ],
                        "paper": {
                            "corpus_id": 267413041,
                            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2282542641",
                                    "name": "Ashish Agrawal"
                                },
                                {
                                    "authorId": "2187454108",
                                    "name": "Barah Fazili"
                                },
                                {
                                    "authorId": "144859542",
                                    "name": "P. Jyothi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.962890625
                    },
                    {
                        "id": "(Liu et al., 2019)",
                        "snippets": [
                            "The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is."
                        ],
                        "paper": {
                            "corpus_id": 196174566,
                            "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset",
                            "authors": [
                                {
                                    "authorId": "46701066",
                                    "name": "Jiahua Liu"
                                },
                                {
                                    "authorId": "2427350",
                                    "name": "Yankai Lin"
                                },
                                {
                                    "authorId": "49293587",
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "1753344",
                                    "name": "Maosong Sun"
                                }
                            ],
                            "year": 2019,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 91
                        },
                        "score": 0.9013671875
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."
                        ],
                        "paper": {
                            "corpus_id": 268819377,
                            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                            "authors": [
                                {
                                    "authorId": "2257136845",
                                    "name": "Yuemei Xu"
                                },
                                {
                                    "authorId": "2258334185",
                                    "name": "Ling Hu"
                                },
                                {
                                    "authorId": "2294513520",
                                    "name": "Jiayi Zhao"
                                },
                                {
                                    "authorId": "2294361104",
                                    "name": "Zihan Qiu"
                                },
                                {
                                    "authorId": "2294363807",
                                    "name": "Yuqi Ye"
                                },
                                {
                                    "authorId": "2294933103",
                                    "name": "Hanwen Gu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Comput. Sci.",
                            "n_citations": 43
                        },
                        "score": 0.92724609375
                    },
                    {
                        "id": "(Etxaniz et al., 2024)",
                        "snippets": [
                            "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets."
                        ],
                        "paper": {
                            "corpus_id": 270380088,
                            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                            "authors": [
                                {
                                    "authorId": "2226458991",
                                    "name": "Julen Etxaniz"
                                },
                                {
                                    "authorId": "2481918",
                                    "name": "Gorka Azkune"
                                },
                                {
                                    "authorId": "2260104163",
                                    "name": "A. Soroa"
                                },
                                {
                                    "authorId": "2251043402",
                                    "name": "Oier L\u00f3pez de Lacalle"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 11
                        },
                        "score": 0.94482421875
                    },
                    {
                        "id": "(Barth et al., 2025)",
                        "snippets": [
                            "Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."
                        ],
                        "paper": {
                            "corpus_id": 276421738,
                            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                            "authors": [
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2302558975",
                                    "name": "Georg Rehm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90966796875
                    },
                    {
                        "id": "(Qiu et al., 2022)",
                        "snippets": [
                            "We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."
                        ],
                        "paper": {
                            "corpus_id": 253098629,
                            "title": "Multilingual Multimodal Learning with Machine Translated Text",
                            "authors": [
                                {
                                    "authorId": "3383271",
                                    "name": "Chen Qiu"
                                },
                                {
                                    "authorId": "3095774",
                                    "name": "Dan Onea\u0163\u0103"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)",
                            "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations."
                        ],
                        "paper": {
                            "corpus_id": 277993848,
                            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                            "authors": [
                                {
                                    "authorId": "2327995656",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "2258789284",
                                    "name": "Weixuan Wang"
                                },
                                {
                                    "authorId": "2349802912",
                                    "name": "Sinuo Liu"
                                },
                                {
                                    "authorId": "2331767275",
                                    "name": "Huifeng Yin"
                                },
                                {
                                    "authorId": "2356794252",
                                    "name": "Xintong Wang"
                                },
                                {
                                    "authorId": "2331686159",
                                    "name": "Yu Zhao"
                                },
                                {
                                    "authorId": "2266387313",
                                    "name": "Chenyang Lyu"
                                },
                                {
                                    "authorId": "2302342302",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.91796875
                    },
                    {
                        "id": "(Asai et al., 2021)",
                        "snippets": [
                            "Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios",
                            "XOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data)",
                            "MKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA."
                        ],
                        "paper": {
                            "corpus_id": 236428949,
                            "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "2118211280",
                                    "name": "Xinyan Velocity Yu"
                                },
                                {
                                    "authorId": "11348687",
                                    "name": "Jungo Kasai"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 73
                        },
                        "score": 0.91357421875
                    },
                    {
                        "id": "(Hershcovich et al., 2022)",
                        "snippets": [
                            "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture",
                            "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations",
                            "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."
                        ],
                        "paper": {
                            "corpus_id": 247594499,
                            "title": "Challenges and Strategies in Cross-Cultural NLP",
                            "authors": [
                                {
                                    "authorId": "2064295987",
                                    "name": "Daniel Hershcovich"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "49568895",
                                    "name": "Heather Lent"
                                },
                                {
                                    "authorId": "3295381",
                                    "name": "Miryam de Lhoneux"
                                },
                                {
                                    "authorId": "30671790",
                                    "name": "Mostafa Abdou"
                                },
                                {
                                    "authorId": "6547490",
                                    "name": "Stephanie Brandl"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "2093582149",
                                    "name": "Laura Cabello Piqueras"
                                },
                                {
                                    "authorId": "2125376289",
                                    "name": "Ilias Chalkidis"
                                },
                                {
                                    "authorId": "1717462692",
                                    "name": "Ruixiang Cui"
                                },
                                {
                                    "authorId": "50110151",
                                    "name": "Constanza Fierro"
                                },
                                {
                                    "authorId": "82259306",
                                    "name": "Katerina Margatina"
                                },
                                {
                                    "authorId": "1660797358",
                                    "name": "Phillip Rust"
                                },
                                {
                                    "authorId": "1700187",
                                    "name": "Anders S\u00f8gaard"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 182
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Ponti et al., 2020)",
                        "snippets": [
                            "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."
                        ],
                        "paper": {
                            "corpus_id": 218470125,
                            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "50383712",
                                    "name": "Qianchu Liu"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 327
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yin et al., 2021)",
                        "snippets": [
                            "Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models\u2019 ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR."
                        ],
                        "paper": {
                            "corpus_id": 237503047,
                            "title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "144508458",
                                    "name": "Da Yin"
                                },
                                {
                                    "authorId": "2108904535",
                                    "name": "Liunian Harold Li"
                                },
                                {
                                    "authorId": "3407296",
                                    "name": "Ziniu Hu"
                                },
                                {
                                    "authorId": "3157053",
                                    "name": "Nanyun Peng"
                                },
                                {
                                    "authorId": "2782886",
                                    "name": "Kai-Wei Chang"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 56
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."
                        ],
                        "paper": {
                            "corpus_id": 238198104,
                            "title": "Visually Grounded Reasoning across Languages and Cultures",
                            "authors": [
                                {
                                    "authorId": "144097210",
                                    "name": "Fangyu Liu"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                },
                                {
                                    "authorId": "50638196",
                                    "name": "Nigel Collier"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 180
                        },
                        "score": 0
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."
                        ],
                        "paper": {
                            "corpus_id": 258865558,
                            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "35871436",
                                    "name": "Sneha Kudugunta"
                                },
                                {
                                    "authorId": "2118211280",
                                    "name": "Xinyan Velocity Yu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "1557386977",
                                    "name": "Machel Reid"
                                },
                                {
                                    "authorId": "2073587169",
                                    "name": "Yulia Tsvetkov"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 62
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Artetxe et al._1, 2020)",
                        "snippets": [
                            "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."
                        ],
                        "paper": {
                            "corpus_id": 216914383,
                            "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                },
                                {
                                    "authorId": "3255091",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 72
                        },
                        "score": 0
                    },
                    {
                        "id": "(Seto et al., 2024)",
                        "snippets": [
                            "Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models."
                        ],
                        "paper": {
                            "corpus_id": 274149965,
                            "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
                            "authors": [
                                {
                                    "authorId": "2324783252",
                                    "name": "Skyler Seto"
                                },
                                {
                                    "authorId": "41096186",
                                    "name": "Maartje ter Hoeve"
                                },
                                {
                                    "authorId": "2331509164",
                                    "name": "He Bai"
                                },
                                {
                                    "authorId": "3344220",
                                    "name": "Natalie Schluter"
                                },
                                {
                                    "authorId": "2529182",
                                    "name": "David Grangier"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9091796875
                    },
                    {
                        "id": "(Ahuja et al., 2022)",
                        "snippets": [
                            "Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."
                        ],
                        "paper": {
                            "corpus_id": 248780386,
                            "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages",
                            "authors": [
                                {
                                    "authorId": "52154863",
                                    "name": "Kabir Ahuja"
                                },
                                {
                                    "authorId": "34725175",
                                    "name": "Sandipan Dandapat"
                                },
                                {
                                    "authorId": "3010457",
                                    "name": "Sunayana Sitaram"
                                },
                                {
                                    "authorId": "143990839",
                                    "name": "M. Choudhury"
                                }
                            ],
                            "year": 2022,
                            "venue": "NLPPOWER",
                            "n_citations": 16
                        },
                        "score": 0.95751953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recommendations and Best Practices",
                "tldr": "To improve multilingual evaluation reliability, researchers recommend complementing translation-based datasets with natively collected resources, using consistent test sets across languages, and prioritizing cultural sensitivity. Best practices include creating culturally tailored evaluation datasets for high-stakes assessments, using target-language originating data for evaluation, and implementing hybrid approaches that balance practicality with authenticity. (15 sources)",
                "text": "\nDespite the limitations of translation-based approaches to multilingual evaluation, researchers have proposed several evidence-based recommendations to improve cross-lingual assessment reliability. These recommendations aim to balance practical constraints with the need for culturally appropriate and linguistically authentic evaluation.\n\nA fundamental recommendation is to complement translation-based benchmarks with natively authored multilingual benchmarks to provide a more comprehensive evaluation of cross-lingual capabilities <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper>. This dual approach acknowledges that while translation-based datasets enable valuable cross-language comparisons, they often contain \"translationese\" phenomena that make them distributionally different from native data <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270562911\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"57174748\" paperTitle=\"(Baker et al., 1993)\" isShortName></Paper>.\n\nFor translation-based evaluation datasets, researchers strongly recommend ensuring consistent test sets across languages. This can be achieved through: (1) using original annotations in all languages, (2) using original annotations in a non-English language and translating them into English and other languages, or (3) if translating from English, doing so at the document level to minimize translation inconsistencies <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. These approaches help address the English-centric bias common in multilingual benchmarks.\n\nCultural representation requires particular attention in multilingual evaluation. Researchers emphasize that cultural biases cannot be mitigated through automatic translation alone and recommend two primary approaches: (1) using human annotators to evaluate cultural sensitivity in translated content, or (2) developing benchmarks based on regional resources that inherently reflect local knowledge <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>. The latter approach has been implemented in benchmarks like Include, which uses local exam sources rather than translating benchmarks with inherent cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nThe importance of target-language originating evaluation data is consistently emphasized, particularly for multimodal setups. Researchers warn that \"fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones\" <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>. This recommendation reflects growing concerns about the reliability of assessments based solely on translated content.\n\nA pragmatic middle-ground approach involves using human translation or original data from target cultures, at least for high-quality evaluation data. This balanced strategy \"allows us to judge the success of cross-lingual transfer in a culturally appropriate way\" <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper> <Paper corpusId=\"238198104\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. It acknowledges resource constraints while prioritizing culturally sensitive assessment where it matters most.\n\nRecent empirical evidence strongly supports these recommendations. Research has demonstrated that localized benchmarks show significantly higher alignment with local human judgments (0.68 correlation) compared to their translated counterparts (0.47 correlation) <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. This substantial difference in correlation underscores the importance of creating culturally and linguistically tailored evaluation datasets rather than relying solely on translations.\n\nThe development of native evaluation benchmarks has already begun to address these concerns, with resources like IndoMMLU for Indonesian <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263829211\" paperTitle=\"(Koto et al., 2023)\" isShortName></Paper> and TurkishMMLU <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271245129\" paperTitle=\"(Yuksel et al., 2024)\" isShortName></Paper> providing culturally appropriate assessments inspired by established methodologies like MMLU <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>. These resources reflect a growing recognition that multilingual evaluation must extend beyond translation to include native cultural knowledge and linguistic patterns.\n\nUltimately, researchers recommend being transparent about the creation process of multilingual datasets and their potential limitations <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>. This includes acknowledging when datasets might exhibit undesired biases such as translation artifacts or unnatural topic distributions <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216914383\" paperTitle=\"(Artetxe et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. This transparency helps users of these datasets make informed decisions about their applicability to specific research questions and use cases.",
                "citations": [
                    {
                        "id": "(Bland'on et al., 2025)",
                        "snippets": [
                            "Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993)Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks."
                        ],
                        "paper": {
                            "corpus_id": 276575630,
                            "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2346978280",
                                    "name": "Mar'ia Andrea Cruz Bland'on"
                                },
                                {
                                    "authorId": "114938178",
                                    "name": "Jayasimha Talur"
                                },
                                {
                                    "authorId": "2346980693",
                                    "name": "Bruno Charron"
                                },
                                {
                                    "authorId": "2343588002",
                                    "name": "Dong Liu"
                                },
                                {
                                    "authorId": "39674628",
                                    "name": "Saab Mansour"
                                },
                                {
                                    "authorId": "2346980678",
                                    "name": "Marcello Federico"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.97509765625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks."
                        ],
                        "paper": {
                            "corpus_id": 270562911,
                            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
                            "authors": [
                                {
                                    "authorId": "143616669",
                                    "name": "Pinzhen Chen"
                                },
                                {
                                    "authorId": "2307212816",
                                    "name": "Simon Yu"
                                },
                                {
                                    "authorId": "2307224542",
                                    "name": "Zhicheng Guo"
                                },
                                {
                                    "authorId": "2259100",
                                    "name": "B. Haddow"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0
                    },
                    {
                        "id": "(Baker et al., 1993)",
                        "snippets": [
                            "The rise of corpus linguistics has serious implications for any discipline in which language plays a major role. This paper explores the impact that the availability of corpora is likely to have on the study of translation as an empirical phenomenon. It argues that the techniques and methodology developed in the field of corpus linguistics will have a direct impact on the emerging discipline of translation studies, particularly with respect to its theoretical and descriptive branches. The nature of this impact is discussed in some detail and brief reference is made to some of the applications of corpus techniques in the applied branch of the dis\u00ad cipline."
                        ],
                        "paper": {
                            "corpus_id": 57174748,
                            "title": "'Corpus Linguistics and Translation Studies: Implications and Applications'",
                            "authors": [
                                {
                                    "authorId": "103983301",
                                    "name": "Mona Baker"
                                },
                                {
                                    "authorId": "46880608",
                                    "name": "G. Francis"
                                },
                                {
                                    "authorId": "1404600818",
                                    "name": "E. Tognini-Bonelli"
                                }
                            ],
                            "year": 1993,
                            "venue": "",
                            "n_citations": 1061
                        },
                        "score": 0
                    },
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models",
                            "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)",
                            "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.9716796875
                    },
                    {
                        "id": "(Barth et al., 2025)",
                        "snippets": [
                            "Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."
                        ],
                        "paper": {
                            "corpus_id": 276421738,
                            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                            "authors": [
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2302558975",
                                    "name": "Georg Rehm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.90966796875
                    },
                    {
                        "id": "(Qiu et al., 2022)",
                        "snippets": [
                            "We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."
                        ],
                        "paper": {
                            "corpus_id": 253098629,
                            "title": "Multilingual Multimodal Learning with Machine Translated Text",
                            "authors": [
                                {
                                    "authorId": "3383271",
                                    "name": "Chen Qiu"
                                },
                                {
                                    "authorId": "3095774",
                                    "name": "Dan Onea\u0163\u0103"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 15
                        },
                        "score": 0.8955078125
                    },
                    {
                        "id": "(Hershcovich et al., 2022)",
                        "snippets": [
                            "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture",
                            "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations",
                            "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."
                        ],
                        "paper": {
                            "corpus_id": 247594499,
                            "title": "Challenges and Strategies in Cross-Cultural NLP",
                            "authors": [
                                {
                                    "authorId": "2064295987",
                                    "name": "Daniel Hershcovich"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "49568895",
                                    "name": "Heather Lent"
                                },
                                {
                                    "authorId": "3295381",
                                    "name": "Miryam de Lhoneux"
                                },
                                {
                                    "authorId": "30671790",
                                    "name": "Mostafa Abdou"
                                },
                                {
                                    "authorId": "6547490",
                                    "name": "Stephanie Brandl"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "2093582149",
                                    "name": "Laura Cabello Piqueras"
                                },
                                {
                                    "authorId": "2125376289",
                                    "name": "Ilias Chalkidis"
                                },
                                {
                                    "authorId": "1717462692",
                                    "name": "Ruixiang Cui"
                                },
                                {
                                    "authorId": "50110151",
                                    "name": "Constanza Fierro"
                                },
                                {
                                    "authorId": "82259306",
                                    "name": "Katerina Margatina"
                                },
                                {
                                    "authorId": "1660797358",
                                    "name": "Phillip Rust"
                                },
                                {
                                    "authorId": "1700187",
                                    "name": "Anders S\u00f8gaard"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 182
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Ponti et al., 2020)",
                        "snippets": [
                            "In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."
                        ],
                        "paper": {
                            "corpus_id": 218470125,
                            "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning",
                            "authors": [
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "1666177566",
                                    "name": "Goran Glavavs"
                                },
                                {
                                    "authorId": "46963731",
                                    "name": "Olga Majewska"
                                },
                                {
                                    "authorId": "50383712",
                                    "name": "Qianchu Liu"
                                },
                                {
                                    "authorId": "1747849",
                                    "name": "Ivan Vulic"
                                },
                                {
                                    "authorId": "145762466",
                                    "name": "A. Korhonen"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 327
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2021)",
                        "snippets": [
                            "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."
                        ],
                        "paper": {
                            "corpus_id": 238198104,
                            "title": "Visually Grounded Reasoning across Languages and Cultures",
                            "authors": [
                                {
                                    "authorId": "144097210",
                                    "name": "Fangyu Liu"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "3381663",
                                    "name": "E. Ponti"
                                },
                                {
                                    "authorId": "145732771",
                                    "name": "Siva Reddy"
                                },
                                {
                                    "authorId": "50638196",
                                    "name": "Nigel Collier"
                                },
                                {
                                    "authorId": "50369944",
                                    "name": "Desmond Elliott"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 180
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)",
                            "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations."
                        ],
                        "paper": {
                            "corpus_id": 277993848,
                            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                            "authors": [
                                {
                                    "authorId": "2327995656",
                                    "name": "Minghao Wu"
                                },
                                {
                                    "authorId": "2258789284",
                                    "name": "Weixuan Wang"
                                },
                                {
                                    "authorId": "2349802912",
                                    "name": "Sinuo Liu"
                                },
                                {
                                    "authorId": "2331767275",
                                    "name": "Huifeng Yin"
                                },
                                {
                                    "authorId": "2356794252",
                                    "name": "Xintong Wang"
                                },
                                {
                                    "authorId": "2331686159",
                                    "name": "Yu Zhao"
                                },
                                {
                                    "authorId": "2266387313",
                                    "name": "Chenyang Lyu"
                                },
                                {
                                    "authorId": "2302342302",
                                    "name": "Longyue Wang"
                                },
                                {
                                    "authorId": "2305289815",
                                    "name": "Weihua Luo"
                                },
                                {
                                    "authorId": "2304530663",
                                    "name": "Kaifu Zhang"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.91796875
                    },
                    {
                        "id": "(Koto et al., 2023)",
                        "snippets": [
                            "Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."
                        ],
                        "paper": {
                            "corpus_id": 263829211,
                            "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU",
                            "authors": [
                                {
                                    "authorId": "2789148",
                                    "name": "Fajri Koto"
                                },
                                {
                                    "authorId": "2256987672",
                                    "name": "Nurul Aisyah"
                                },
                                {
                                    "authorId": "49404498",
                                    "name": "Haonan Li"
                                },
                                {
                                    "authorId": "2256987316",
                                    "name": "Timothy Baldwin"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 46
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yuksel et al., 2024)",
                        "snippets": [
                            "Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                        ],
                        "paper": {
                            "corpus_id": 271245129,
                            "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish",
                            "authors": [
                                {
                                    "authorId": "2311698426",
                                    "name": "Arda Yuksel"
                                },
                                {
                                    "authorId": "1999179692",
                                    "name": "Abdullatif K\u00f6ksal"
                                },
                                {
                                    "authorId": "2126865294",
                                    "name": "Lutfi Kerem cSenel"
                                },
                                {
                                    "authorId": "2311700614",
                                    "name": "Anna Korhonen"
                                },
                                {
                                    "authorId": "2130001188",
                                    "name": "Hinrich Schutze"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 14
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hendrycks et al., 2020)",
                        "snippets": [
                            "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
                        ],
                        "paper": {
                            "corpus_id": 221516475,
                            "title": "Measuring Massive Multitask Language Understanding",
                            "authors": [
                                {
                                    "authorId": "3422872",
                                    "name": "Dan Hendrycks"
                                },
                                {
                                    "authorId": "90909974",
                                    "name": "Collin Burns"
                                },
                                {
                                    "authorId": "104444594",
                                    "name": "Steven Basart"
                                },
                                {
                                    "authorId": "1380103052",
                                    "name": "Andy Zou"
                                },
                                {
                                    "authorId": "16787428",
                                    "name": "Mantas Mazeika"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "5164568",
                                    "name": "J. Steinhardt"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4568
                        },
                        "score": 0
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."
                        ],
                        "paper": {
                            "corpus_id": 258865558,
                            "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "35871436",
                                    "name": "Sneha Kudugunta"
                                },
                                {
                                    "authorId": "2118211280",
                                    "name": "Xinyan Velocity Yu"
                                },
                                {
                                    "authorId": "3443287",
                                    "name": "Terra Blevins"
                                },
                                {
                                    "authorId": "1821892",
                                    "name": "Hila Gonen"
                                },
                                {
                                    "authorId": "1557386977",
                                    "name": "Machel Reid"
                                },
                                {
                                    "authorId": "2073587169",
                                    "name": "Yulia Tsvetkov"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 62
                        },
                        "score": 0.9326171875
                    },
                    {
                        "id": "(Artetxe et al._1, 2020)",
                        "snippets": [
                            "We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."
                        ],
                        "paper": {
                            "corpus_id": 216914383,
                            "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2884561",
                                    "name": "Sebastian Ruder"
                                },
                                {
                                    "authorId": "1755465",
                                    "name": "Dani Yogatama"
                                },
                                {
                                    "authorId": "3255091",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 72
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.243225
    }
}