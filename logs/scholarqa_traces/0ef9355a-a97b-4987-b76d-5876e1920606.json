{
    "query": "How does the attention masking mechanism differ between causal decoder-only, non-causal (prefix) decoder-only, and encoder-decoder language models?",
    "user_id": "lib_user",
    "task_id": "0ef9355a-a97b-4987-b76d-5876e1920606",
    "timestamp": "2025-06-24T01:02:23.265516",
    "n_retrieval": 256,
    "n_retrieved": 270,
    "n_candidates": 28,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.37867799999999996,
    "decomposed_query": {
        "rewritten_query": "Differences in attention masking mechanisms between causal decoder-only, non-causal (prefix) decoder-only, and encoder-decoder language models.",
        "keyword_query": "attention masking causal decoder-only non-causal prefix encoder-decoder language models",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.00999,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "venue": "Applied Sciences",
            "year": 2024,
            "reference_count": 108,
            "citation_count": 80,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2076-3417/14/5/2074/pdf?version=1709291698",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/app14052074?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/app14052074, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289385425",
                    "name": "Rajvardhan Patil"
                },
                {
                    "authorId": "117730513",
                    "name": "Venkat Gudivada"
                }
            ],
            "abstract": "Natural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.",
            "corpus_id": 268157336,
            "sentences": [
                {
                    "corpus_id": "268157336",
                    "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                    "text": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
                    "score": 0.9933250455227525,
                    "section_title": "Prefix (Non-Causal) Language Model",
                    "char_start_offset": 19013,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 94
                        },
                        {
                            "start": 95,
                            "end": 243
                        },
                        {
                            "start": 246,
                            "end": 424
                        },
                        {
                            "start": 425,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 988
                        },
                        {
                            "start": 989,
                            "end": 1034
                        },
                        {
                            "start": 1035,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1329
                        },
                        {
                            "start": 1330,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1595
                        },
                        {
                            "start": 1596,
                            "end": 1798
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97265625
                }
            ],
            "relevance_judgement": 0.97265625,
            "relevance_judgment_input_expanded": "# Title: A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)\n# Venue: Applied Sciences\n# Authors: Rajvardhan Patil, Venkat Gudivada\n## Abstract\nNatural language processing (NLP) has significantly transformed in the last decade, especially in the field of language modeling. Large language models (LLMs) have achieved SOTA performances on natural language understanding (NLU) and natural language generation (NLG) tasks by learning language representation in self-supervised ways. This paper provides a comprehensive survey to capture the progression of advances in language models. In this paper, we examine the different aspects of language models, which started with a few million parameters but have reached the size of a trillion in a very short time. We also look at how these LLMs transitioned from task-specific to task-independent to task-and-language-independent architectures. This paper extensively discusses different pretraining objectives, benchmarks, and transfer learning methods used in LLMs. It also examines different finetuning and in-context learning techniques used in downstream tasks. Moreover, it explores how LLMs can perform well across many domains and datasets if sufficiently trained on a large and diverse dataset. Next, it discusses how, over time, the availability of cheap computational power and large datasets have improved LLM\u2019s capabilities and raised new challenges. As part of our study, we also inspect LLMs from the perspective of scalability to see how their performance is affected by the model\u2019s depth, width, and data size. Lastly, we provide an empirical comparison of existing trends and techniques and a comprehensive analysis of where the field of LLM currently stands.\n## Prefix (Non-Causal) Language Model\nPrefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
            "reference_string": "[268157336 | Patil et al. | 2024 | Citations: 80]"
        },
        {
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 14,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.02656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1658871094",
                    "name": "P. Suganthan"
                },
                {
                    "authorId": "2165469946",
                    "name": "Fedor Moiseev"
                },
                {
                    "authorId": "2348489099",
                    "name": "Le Yan"
                },
                {
                    "authorId": "2261361394",
                    "name": "Junru Wu"
                },
                {
                    "authorId": "2348507846",
                    "name": "Jianmo Ni"
                },
                {
                    "authorId": "2348488953",
                    "name": "Jay Han"
                },
                {
                    "authorId": "1954563",
                    "name": "I. Zitouni"
                },
                {
                    "authorId": "1727837",
                    "name": "Enrique Alfonseca"
                },
                {
                    "authorId": "2348422460",
                    "name": "Xuanhui Wang"
                },
                {
                    "authorId": "2349772191",
                    "name": "Zhe Dong"
                }
            ],
            "abstract": "Decoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.",
            "corpus_id": 276771845,
            "sentences": [
                {
                    "corpus_id": "276771845",
                    "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                    "text": "Pre-trained generative models often employ three types of attention mask patterns: bidirectional, causal, and prefix masking (Figures 3 and 4 in Raffel et al. (2020)). For our focus on non-generative tasks, we limited the explorations to causal attention and bidirectional attention. \n\nBidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence. \n\nGiven that Gemma models are pre-trained with causal attention, we investigated the impact of both bidirectional and causal attention masks during fine-tuning to maximize the performance of Gemma Encoder models.",
                    "score": 0.7418955335720548,
                    "section_title": "Attention Masking",
                    "char_start_offset": 4030,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 283
                        },
                        {
                            "start": 286,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 592
                        },
                        {
                            "start": 595,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1028
                        },
                        {
                            "start": 1031,
                            "end": 1137
                        },
                        {
                            "start": 1138,
                            "end": 1233
                        },
                        {
                            "start": 1234,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1538
                        },
                        {
                            "start": 1541,
                            "end": 1751
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 145,
                            "end": 165,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 347,
                            "end": 368,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92333984375
                },
                {
                    "corpus_id": "276771845",
                    "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                    "text": "To investigate the impact of attention masking strategies on Gemma Encoder performance, we evaluated both causal and bidirectional masking using the GLUE benchmark. Table 6 provides a comparison of the GLUE average scores for Gemma-2 2B and 9B models under both masking conditions. For the bidirectional masking results, we report the optimal performance achieved across different pooling strategies, consistent with the methodology in Table 5. \n\nA key observation from our experiments is the superior performance of bidirectional masking compared to causal masking. This pattern is consistently observed across the majority of tasks within the GLUE benchmark, detailed in Section A.2 for different model sizes. This finding is particularly noteworthy given that the Gemma decoder's pretraining process utilizes solely causal masking. It suggests that employing bidirectional masking, at finetuning time, can still enhance performance on encoder-related tasks, even with a causally pretrained decoder and limited finetuning data. Furthermore, our results demonstrate that when causal masking is employed, last-token pooling exhibits significantly better performance than alternative pooling methods. This observation aligns with the inherent nature of causal masking, where the last token possesses a comprehensive contextual understanding due to its attention over the entire preceding sequence. This is analogous to the pretraining objective of decoderbased language models, where the embedding of the last token serves as the basis for predicting the subsequent token.",
                    "score": 0.5310019991518748,
                    "section_title": "Attention Masking",
                    "char_start_offset": 19050,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 164
                        },
                        {
                            "start": 165,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 444
                        },
                        {
                            "start": 447,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 711
                        },
                        {
                            "start": 712,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1029
                        },
                        {
                            "start": 1030,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1396
                        },
                        {
                            "start": 1397,
                            "end": 1571
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80224609375
                }
            ],
            "relevance_judgement": 0.92333984375,
            "relevance_judgment_input_expanded": "# Title: Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks\n# Venue: arXiv.org\n# Authors: P. Suganthan, Fedor Moiseev, Le Yan, Junru Wu, Jianmo Ni, Jay Han, I. Zitouni, Enrique Alfonseca, Xuanhui Wang, Zhe Dong\n## Abstract\nDecoder-based transformers, while revolutionizing language modeling and scaling to immense sizes, have not completely overtaken encoder-heavy architectures in natural language processing. Specifically, encoder-only models remain dominant in tasks like classification, regression, and ranking. This is primarily due to the inherent structure of decoder-based models, which limits their direct applicability to these tasks. In this paper, we introduce Gemma Encoder, adapting the powerful Gemma decoder model to an encoder architecture, thereby unlocking its potential for a wider range of non-generative applications. To optimize the adaptation from decoder to encoder, we systematically analyze various pooling strategies, attention mechanisms, and hyperparameters (e.g., dropout rate). Furthermore, we benchmark Gemma Encoder against established approaches on the GLUE benchmarks, and MS MARCO ranking benchmark, demonstrating its effectiveness and versatility.\n## Attention Masking\nPre-trained generative models often employ three types of attention mask patterns: bidirectional, causal, and prefix masking (Figures 3 and 4 in Raffel et al. (2020)). For our focus on non-generative tasks, we limited the explorations to causal attention and bidirectional attention. \n\nBidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence. \n\nGiven that Gemma models are pre-trained with causal attention, we investigated the impact of both bidirectional and causal attention masks during fine-tuning to maximize the performance of Gemma Encoder models.\n...\nTo investigate the impact of attention masking strategies on Gemma Encoder performance, we evaluated both causal and bidirectional masking using the GLUE benchmark. Table 6 provides a comparison of the GLUE average scores for Gemma-2 2B and 9B models under both masking conditions. For the bidirectional masking results, we report the optimal performance achieved across different pooling strategies, consistent with the methodology in Table 5. \n\nA key observation from our experiments is the superior performance of bidirectional masking compared to causal masking. This pattern is consistently observed across the majority of tasks within the GLUE benchmark, detailed in Section A.2 for different model sizes. This finding is particularly noteworthy given that the Gemma decoder's pretraining process utilizes solely causal masking. It suggests that employing bidirectional masking, at finetuning time, can still enhance performance on encoder-related tasks, even with a causally pretrained decoder and limited finetuning data. Furthermore, our results demonstrate that when causal masking is employed, last-token pooling exhibits significantly better performance than alternative pooling methods. This observation aligns with the inherent nature of causal masking, where the last token possesses a comprehensive contextual understanding due to its attention over the entire preceding sequence. This is analogous to the pretraining objective of decoderbased language models, where the embedding of the last token serves as the basis for predicting the subsequent token.",
            "reference_string": "[276771845 | Suganthan et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "venue": "Journal of machine learning research",
            "year": 2019,
            "reference_count": 134,
            "citation_count": 20336,
            "influential_citation_count": 2274,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.10683, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                },
                {
                    "authorId": "1846258",
                    "name": "Noam M. Shazeer"
                },
                {
                    "authorId": "145625142",
                    "name": "Adam Roberts"
                },
                {
                    "authorId": "3844009",
                    "name": "Katherine Lee"
                },
                {
                    "authorId": "46617804",
                    "name": "Sharan Narang"
                },
                {
                    "authorId": "1380243217",
                    "name": "Michael Matena"
                },
                {
                    "authorId": "2389316",
                    "name": "Yanqi Zhou"
                },
                {
                    "authorId": "2157338362",
                    "name": "Wei Li"
                },
                {
                    "authorId": "35025299",
                    "name": "Peter J. Liu"
                }
            ],
            "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
            "corpus_id": 204838007,
            "sentences": [
                {
                    "corpus_id": "204838007",
                    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "text": "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe first model structure we consider is an an encoder-decoder Transformer, which consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of Figure 4.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern and appends a special \"classification\" token to the input. BERT's output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-",
                    "score": 0.5695582616857068,
                    "section_title": "Model Structures",
                    "char_start_offset": 39750,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91650390625
                },
                {
                    "corpus_id": "204838007",
                    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                    "text": "dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nat the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence. The self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output. An attention matrix for this masking pattern is shown in Figure 3, middle.\n\nThe decoder in an encoder-decoder Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model's predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction Radford et al., 2018;Al-Rfou et al., 2019). This constitutes the second model structure we consider. A schematic of this architecture is shown in Figure 4, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method (Radford et al., 2018).\n\nLanguage models are typically used for compression or sequence generation (Graves, 2013). However, they can also be used in the text-to-text framework simply by concate",
                    "score": 0.5951206243280238,
                    "section_title": "Model Structures",
                    "char_start_offset": 41573,
                    "sentence_offsets": [],
                    "ref_mentions": [
                        {
                            "start": 1642,
                            "end": 1663,
                            "matchedPaperCorpusId": "52004855"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.865234375
                }
            ],
            "relevance_judgement": 0.91650390625,
            "relevance_judgment_input_expanded": "# Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n# Venue: Journal of machine learning research\n# Authors: Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu\n## Abstract\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.\n## Model Structures\nA major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe first model structure we consider is an an encoder-decoder Transformer, which consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of Figure 4.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern and appends a special \"classification\" token to the input. BERT's output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-\n...\ndark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nat the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence. The self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output. An attention matrix for this masking pattern is shown in Figure 3, middle.\n\nThe decoder in an encoder-decoder Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model's predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction Radford et al., 2018;Al-Rfou et al., 2019). This constitutes the second model structure we consider. A schematic of this architecture is shown in Figure 4, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method (Radford et al., 2018).\n\nLanguage models are typically used for compression or sequence generation (Graves, 2013). However, they can also be used in the text-to-text framework simply by concate",
            "reference_string": "[204838007 | Raffel et al. | 2019 | Citations: 20336]"
        },
        {
            "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 42,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.07276, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "150270469",
                    "name": "Qijiong Liu"
                },
                {
                    "authorId": "2290237904",
                    "name": "Jieming Zhu"
                },
                {
                    "authorId": "2147259441",
                    "name": "Lu Fan"
                },
                {
                    "authorId": "2265936086",
                    "name": "Zhou Zhao"
                },
                {
                    "authorId": "2187512110",
                    "name": "Xiao-Ming Wu"
                }
            ],
            "abstract": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tail or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. In this way, it preserves the item's semantics within these tokens and ensures that semantically similar items are represented by similar tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing generative recommendation methods typically involve multiple sub-models for embedding, quantization, and recommendation, leading to an overly complex system. In this paper, we propose to streamline the semantic tokenization and generative recommendation process with a unified framework, dubbed STORE, which leverages a single large language model (LLM) for both tasks. Specifically, we formulate semantic tokenization as a text-to-token task and generative recommendation as a token-to-token task, supplemented by a token-to-text reconstruction task and a text-to-token auxiliary task. All these tasks are framed in a generative manner and trained using a single LLM backbone. Extensive experiments have been conducted to validate the effectiveness of our STORE framework across various recommendation tasks and datasets. We will release the source code and configurations for reproducible research.",
            "corpus_id": 272593363,
            "sentences": [
                {
                    "corpus_id": "272593363",
                    "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM",
                    "text": "where   is a special token representing the -th self-supervised task: it is a reconstruction task when 0 <  \u2264  and a generation task when  <  \u2264 . For example, this could be: \"reconstruct_title: title: Yellowstone tourist injured ...\" Cascaded Attention Mask. Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking. \n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty. \n\nPost-pretraining. As depicted in Figure 3, we replicate each item content  times to create  training samples. Each sample is tailored to a specific reconstruction or generation task   . \n\nWe will employ low-rank adaptation (LoRA) [8], a parameterefficient fine-tuning approach, to train the language model. Additionally, we will freeze the pretrained word embeddings while tuning the dense token embeddings and task embeddings. \n\nTo perform the filling operation on the placeholder block, we will conduct dual forward propagation: first to capture the output from the token block to fill the placeholder, and then to tune the language model with the next token prediction task: \u00e2,+1 = argmax  \u2208W  ( | ,1 ,  ,2 , . . . ,  , ), \n\noptimized using cross-entropy loss, where W denotes the token vocabulary and  , denotes the -th token of the attribute a  .",
                    "score": 0.5263825258916119,
                    "section_title": "Dense Tokenizer",
                    "char_start_offset": 12504,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 258
                        },
                        {
                            "start": 259,
                            "end": 448
                        },
                        {
                            "start": 449,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 748
                        },
                        {
                            "start": 751,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1097
                        },
                        {
                            "start": 1098,
                            "end": 1160
                        },
                        {
                            "start": 1163,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1272
                        },
                        {
                            "start": 1273,
                            "end": 1348
                        },
                        {
                            "start": 1351,
                            "end": 1469
                        },
                        {
                            "start": 1470,
                            "end": 1590
                        },
                        {
                            "start": 1593,
                            "end": 1880
                        },
                        {
                            "start": 1881,
                            "end": 1888
                        },
                        {
                            "start": 1891,
                            "end": 2014
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.833984375
                }
            ],
            "relevance_judgement": 0.833984375,
            "relevance_judgment_input_expanded": "# Title: STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM\n# Venue: arXiv.org\n# Authors: Qijiong Liu, Jieming Zhu, Lu Fan, Zhou Zhao, Xiao-Ming Wu\n## Abstract\nTraditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tail or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. In this way, it preserves the item's semantics within these tokens and ensures that semantically similar items are represented by similar tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing generative recommendation methods typically involve multiple sub-models for embedding, quantization, and recommendation, leading to an overly complex system. In this paper, we propose to streamline the semantic tokenization and generative recommendation process with a unified framework, dubbed STORE, which leverages a single large language model (LLM) for both tasks. Specifically, we formulate semantic tokenization as a text-to-token task and generative recommendation as a token-to-token task, supplemented by a token-to-text reconstruction task and a text-to-token auxiliary task. All these tasks are framed in a generative manner and trained using a single LLM backbone. Extensive experiments have been conducted to validate the effectiveness of our STORE framework across various recommendation tasks and datasets. We will release the source code and configurations for reproducible research.\n## Dense Tokenizer\nwhere   is a special token representing the -th self-supervised task: it is a reconstruction task when 0 <  \u2264  and a generation task when  <  \u2264 . For example, this could be: \"reconstruct_title: title: Yellowstone tourist injured ...\" Cascaded Attention Mask. Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking. \n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty. \n\nPost-pretraining. As depicted in Figure 3, we replicate each item content  times to create  training samples. Each sample is tailored to a specific reconstruction or generation task   . \n\nWe will employ low-rank adaptation (LoRA) [8], a parameterefficient fine-tuning approach, to train the language model. Additionally, we will freeze the pretrained word embeddings while tuning the dense token embeddings and task embeddings. \n\nTo perform the filling operation on the placeholder block, we will conduct dual forward propagation: first to capture the output from the token block to fill the placeholder, and then to tune the language model with the next token prediction task: \u00e2,+1 = argmax  \u2208W  ( | ,1 ,  ,2 , . . . ,  , ), \n\noptimized using cross-entropy loss, where W denotes the token vocabulary and  , denotes the -th token of the attribute a  .",
            "reference_string": "[272593363 | Liu et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Segment-Based Attention Masking for GPTs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.18487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "121254633",
                    "name": "Shahar Katz"
                },
                {
                    "authorId": "2186740854",
                    "name": "Liran Ringel"
                },
                {
                    "authorId": "2335566528",
                    "name": "Yaniv Romano"
                },
                {
                    "authorId": "2284763723",
                    "name": "Lior Wolf"
                }
            ],
            "abstract": "Modern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial\"prefill\"phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved.",
            "corpus_id": 274992300,
            "sentences": [
                {
                    "corpus_id": "274992300",
                    "title": "Segment-Based Attention Masking for GPTs",
                    "text": "Causal attention, as an autoregressive mechanism, restricts information flow to propagate only from earlier tokens to later ones. While this design is essential during the autoregressive decoding phase, it is unnecessarily restrictive during the prefill phase, in which the entire prompt is available at once. Specifically, causal masking prevents the model from leveraging information from later tokens in the prompt, introducing a constraint in the attention computation. \n\nFor instance, consider the following example from the commonsense reasoning task ARC-Challenge (Clark et al., 2018): Please choose the correct answer to the question: Giant sloths lived in the Americas and the Caribbean during the Ice Age... Most of these sloths disappeared... some of these sloths lived alongside humans. What is the most likely reason that these last giant sloths became extinct? Answer1: disease... Answer4: humans as predators...\" \n\nTo correctly answer this question, the model must infer a specific information from the prompt (that human lived next to giant sloths) and ignore other (the fact that sloths lived through the Ice Age is a distracting detail). In a standard autoregressive model with causal masking, the text is read unidirectionally. This means the model cannot contextualize the final question while reading the initial sentences. Its success relies solely on its ability to memorize the prompt, as it cannot revisit earlier parts of the text when processing subsequent information. In contrast, humans readers can revisit earlier sentences or questions to focus on relevant details and build a coherent understanding. \n\nThe unidirectional nature of GPTs with causal masking imposes a limitation on their capabilities to integrate context from the entire prompt. To address this limitation, we propose Masked Attention by Segment (MAS), which adapts the attention mechanism to process full prompts more effectively. Similar to encoder-decoder, MAS operates in two modes: i. Prefill Phase: MAS removes the strict causal masking within each input prompt, allowing tokens to attend to both earlier and later tokens in the same block.",
                    "score": 0.5586576236393535,
                    "section_title": "Method",
                    "char_start_offset": 10843,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 129
                        },
                        {
                            "start": 130,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 473
                        },
                        {
                            "start": 476,
                            "end": 798
                        },
                        {
                            "start": 799,
                            "end": 874
                        },
                        {
                            "start": 875,
                            "end": 927
                        },
                        {
                            "start": 930,
                            "end": 1155
                        },
                        {
                            "start": 1156,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1632
                        },
                        {
                            "start": 1635,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1929
                        },
                        {
                            "start": 1930,
                            "end": 2144
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79931640625
                },
                {
                    "corpus_id": "274992300",
                    "title": "Segment-Based Attention Masking for GPTs",
                    "text": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.",
                    "score": 0.5676424168617182,
                    "section_title": "Related Work",
                    "char_start_offset": 3399,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 343
                        },
                        {
                            "start": 344,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 575
                        },
                        {
                            "start": 578,
                            "end": 799
                        },
                        {
                            "start": 800,
                            "end": 1046
                        },
                        {
                            "start": 1047,
                            "end": 1185
                        },
                        {
                            "start": 1188,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1681
                        },
                        {
                            "start": 1684,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 2097
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 595,
                            "end": 616,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 950,
                            "end": 970,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1278,
                            "end": 1299,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55615234375
                }
            ],
            "relevance_judgement": 0.79931640625,
            "relevance_judgment_input_expanded": "# Title: Segment-Based Attention Masking for GPTs\n# Venue: arXiv.org\n# Authors: Shahar Katz, Liran Ringel, Yaniv Romano, Lior Wolf\n## Abstract\nModern Language Models (LMs) owe much of their success to masked causal attention, the backbone of Generative Pre-Trained Transformer (GPT) models. Although GPTs can process the entire user prompt at once, the causal masking is applied to all input tokens step-by-step, mimicking the generation process. This imposes an unnecessary constraint during the initial\"prefill\"phase when the model processes the input prompt and generates the internal representations before producing any output tokens. In this work, attention is masked based on the known block structure at the prefill phase, followed by the conventional token-by-token autoregressive process after that. For example, in a typical chat prompt, the system prompt is treated as one block, and the user prompt as the next one. Each of these is treated as a unit for the purpose of masking, such that the first tokens in each block can access the subsequent tokens in a non-causal manner. Then, the model answer is generated in the conventional causal manner. This Segment-by-Segment scheme entails no additional computational overhead. When integrating it into models such as Llama and Qwen, state-of-the-art performance is consistently achieved.\n## Related Work\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.\n\n## Method\nCausal attention, as an autoregressive mechanism, restricts information flow to propagate only from earlier tokens to later ones. While this design is essential during the autoregressive decoding phase, it is unnecessarily restrictive during the prefill phase, in which the entire prompt is available at once. Specifically, causal masking prevents the model from leveraging information from later tokens in the prompt, introducing a constraint in the attention computation. \n\nFor instance, consider the following example from the commonsense reasoning task ARC-Challenge (Clark et al., 2018): Please choose the correct answer to the question: Giant sloths lived in the Americas and the Caribbean during the Ice Age... Most of these sloths disappeared... some of these sloths lived alongside humans. What is the most likely reason that these last giant sloths became extinct? Answer1: disease... Answer4: humans as predators...\" \n\nTo correctly answer this question, the model must infer a specific information from the prompt (that human lived next to giant sloths) and ignore other (the fact that sloths lived through the Ice Age is a distracting detail). In a standard autoregressive model with causal masking, the text is read unidirectionally. This means the model cannot contextualize the final question while reading the initial sentences. Its success relies solely on its ability to memorize the prompt, as it cannot revisit earlier parts of the text when processing subsequent information. In contrast, humans readers can revisit earlier sentences or questions to focus on relevant details and build a coherent understanding. \n\nThe unidirectional nature of GPTs with causal masking imposes a limitation on their capabilities to integrate context from the entire prompt. To address this limitation, we propose Masked Attention by Segment (MAS), which adapts the attention mechanism to process full prompts more effectively. Similar to encoder-decoder, MAS operates in two modes: i. Prefill Phase: MAS removes the strict causal masking within each input prompt, allowing tokens to attend to both earlier and later tokens in the same block.",
            "reference_string": "[274992300 | Katz et al. | 2024 | Citations: 1]"
        },
        {
            "title": "WPN: An Unlearning Method Based on N-pair Contrastive Learning in Language Models",
            "venue": "European Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.09459, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316443362",
                    "name": "Guitao Chen"
                },
                {
                    "authorId": "2316551557",
                    "name": "Yunshen Wang"
                },
                {
                    "authorId": "2316544717",
                    "name": "Hongye Sun"
                },
                {
                    "authorId": "2316516363",
                    "name": "Guang Chen"
                }
            ],
            "abstract": "Generative language models (LMs) offer numerous advantages but may produce inappropriate or harmful outputs due to the harmful knowledge acquired during pre-training. This knowledge often manifests as undesirable correspondences, such as\"harmful prompts\"leading to\"harmful outputs,\"which our research aims to mitigate through unlearning techniques.However, existing unlearning methods based on gradient ascent can significantly impair the performance of LMs. To address this issue, we propose a novel approach called Weighted Positional N-pair (WPN) Learning, which leverages position-weighted mean pooling within an n-pair contrastive learning framework. WPN is designed to modify the output distribution of LMs by eliminating specific harmful outputs (e.g., replacing toxic responses with neutral ones), thereby transforming the model's behavior from\"harmful prompt-harmful output\"to\"harmful prompt-harmless response\".Experiments on OPT and GPT-NEO LMs show that WPN effectively reduces the proportion of harmful responses, achieving a harmless rate of up to 95.8\\% while maintaining stable performance on nine common benchmarks (with less than 2\\% degradation on average). Moreover, we provide empirical evidence to demonstrate WPN's ability to weaken the harmful correspondences in terms of generalizability and robustness, as evaluated on out-of-distribution test sets and under adversarial attacks.",
            "corpus_id": 271903802,
            "sentences": [
                {
                    "corpus_id": "271903802",
                    "title": "WPN: An Unlearning Method Based on N-pair Contrastive Learning in Language Models",
                    "text": "In the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence. Empirically, we can take the text representation of the last token vS as the embedding of the entire text sequence. Alternatively, we can use the mean pooling method, averaging the vector representations of all tokens as the sequence embedding. The methods of last-token pooling and mean pooling can be respectively expressed as h lP = vS and hmP = 1 S S i=1 vi, where S is the sequence length, v = (v1, v2, ..., vS) is the hidden state vector of the last hidden layer of LMs after the input text data x, with a dimension of (SeqLen, dim). \n\nHowever, last token pooling discards the preceding tokens, resulting in a loss of semantic information. Mean pooling simply aggregates the vector representations of all tokens, a method nearly identical to how models with encoders obtain text representations. Therefore, to obtain a richer text vector representation from decoder-only LMs, we employ a position-weighted mean pooling method [23]. Specifically, the vector representation of x, hwP , is: \n\nwhere the definition of wi is as follows: \n\nThe position-weighted mean pooling method can give higher weights to subsequent tokens, which is consistent with the causal attention masking mechanism. We will show the comparative results of these three pooling methods in the Further Analysis section.",
                    "score": 0.5167436872793989,
                    "section_title": "Position-weighted Mean Pooling",
                    "char_start_offset": 10862,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 248
                        },
                        {
                            "start": 249,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 788
                        },
                        {
                            "start": 791,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1050
                        },
                        {
                            "start": 1051,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1242
                        },
                        {
                            "start": 1245,
                            "end": 1286
                        },
                        {
                            "start": 1289,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1542
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79248046875
                }
            ],
            "relevance_judgement": 0.79248046875,
            "relevance_judgment_input_expanded": "# Title: WPN: An Unlearning Method Based on N-pair Contrastive Learning in Language Models\n# Venue: European Conference on Artificial Intelligence\n# Authors: Guitao Chen, Yunshen Wang, Hongye Sun, Guang Chen\n## Abstract\nGenerative language models (LMs) offer numerous advantages but may produce inappropriate or harmful outputs due to the harmful knowledge acquired during pre-training. This knowledge often manifests as undesirable correspondences, such as\"harmful prompts\"leading to\"harmful outputs,\"which our research aims to mitigate through unlearning techniques.However, existing unlearning methods based on gradient ascent can significantly impair the performance of LMs. To address this issue, we propose a novel approach called Weighted Positional N-pair (WPN) Learning, which leverages position-weighted mean pooling within an n-pair contrastive learning framework. WPN is designed to modify the output distribution of LMs by eliminating specific harmful outputs (e.g., replacing toxic responses with neutral ones), thereby transforming the model's behavior from\"harmful prompt-harmful output\"to\"harmful prompt-harmless response\".Experiments on OPT and GPT-NEO LMs show that WPN effectively reduces the proportion of harmful responses, achieving a harmless rate of up to 95.8\\% while maintaining stable performance on nine common benchmarks (with less than 2\\% degradation on average). Moreover, we provide empirical evidence to demonstrate WPN's ability to weaken the harmful correspondences in terms of generalizability and robustness, as evaluated on out-of-distribution test sets and under adversarial attacks.\n## Position-weighted Mean Pooling\nIn the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence. Empirically, we can take the text representation of the last token vS as the embedding of the entire text sequence. Alternatively, we can use the mean pooling method, averaging the vector representations of all tokens as the sequence embedding. The methods of last-token pooling and mean pooling can be respectively expressed as h lP = vS and hmP = 1 S S i=1 vi, where S is the sequence length, v = (v1, v2, ..., vS) is the hidden state vector of the last hidden layer of LMs after the input text data x, with a dimension of (SeqLen, dim). \n\nHowever, last token pooling discards the preceding tokens, resulting in a loss of semantic information. Mean pooling simply aggregates the vector representations of all tokens, a method nearly identical to how models with encoders obtain text representations. Therefore, to obtain a richer text vector representation from decoder-only LMs, we employ a position-weighted mean pooling method [23]. Specifically, the vector representation of x, hwP , is: \n\nwhere the definition of wi is as follows: \n\nThe position-weighted mean pooling method can give higher weights to subsequent tokens, which is consistent with the causal attention masking mechanism. We will show the comparative results of these three pooling methods in the Further Analysis section.",
            "reference_string": "[271903802 | Chen et al. | 2024 | Citations: 1]"
        },
        {
            "title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07093, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325264641",
                    "name": "Zhe Li"
                },
                {
                    "authorId": "2268513823",
                    "name": "Weihao Yuan"
                },
                {
                    "authorId": "2281417686",
                    "name": "Yisheng He"
                },
                {
                    "authorId": "73555520",
                    "name": "Lingteng Qiu"
                },
                {
                    "authorId": "2325327454",
                    "name": "Shenhao Zhu"
                },
                {
                    "authorId": "2268724718",
                    "name": "Xiaodong Gu"
                },
                {
                    "authorId": "2323499860",
                    "name": "Weichao Shen"
                },
                {
                    "authorId": "2218391810",
                    "name": "Yuan Dong"
                },
                {
                    "authorId": "2268646063",
                    "name": "Zilong Dong"
                },
                {
                    "authorId": "2261796256",
                    "name": "Laurence T. Yang"
                }
            ],
            "abstract": "Language plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP's motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.",
            "corpus_id": 273233776,
            "sentences": [
                {
                    "corpus_id": "273233776",
                    "title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning",
                    "text": "Unlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;Zhang et al., 2023a) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness. Therefore, we predict the masked regions autoregressively: Given a text query T -for instance, \"A person is walking in a circle\"-our primary objective is to rank motions from a comprehensive database based on their semantic alignment with the textual input. The ultimate aim is to retrieve the motion that exhibits the highest correspondence with the provided textual description, effectively bridging the gap between disparate modalities-text and motion. We hypothesize that a model exhibiting better alignment between these modalities will inherently possess improved cross-modal retrieval capabilities, and can serve as a more reasonable R-Precision evaluator.",
                    "score": 0.5651876206356716,
                    "section_title": "AUTOREGRESSIVE GENERATION",
                    "char_start_offset": 18980,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 552
                        },
                        {
                            "start": 555,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 890
                        },
                        {
                            "start": 891,
                            "end": 1000
                        },
                        {
                            "start": 1001,
                            "end": 1160
                        },
                        {
                            "start": 1161,
                            "end": 1280
                        },
                        {
                            "start": 1281,
                            "end": 1437
                        },
                        {
                            "start": 1438,
                            "end": 1695
                        },
                        {
                            "start": 1696,
                            "end": 1893
                        },
                        {
                            "start": 1894,
                            "end": 2101
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 218,
                            "end": 238,
                            "matchedPaperCorpusId": "255942203"
                        },
                        {
                            "start": 980,
                            "end": 999,
                            "matchedPaperCorpusId": "232134936"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78857421875
                }
            ],
            "relevance_judgement": 0.78857421875,
            "relevance_judgment_input_expanded": "# Title: LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning\n# Venue: arXiv.org\n# Authors: Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, Laurence T. Yang\n## Abstract\nLanguage plays a vital role in the realm of human motion. Existing methods have largely depended on CLIP text embeddings for motion generation, yet they fall short in effectively aligning language and motion due to CLIP's pretraining on static image-text pairs. This work introduces LaMP, a novel Language-Motion Pretraining model, which transitions from a language-vision to a more suitable language-motion latent space. It addresses key limitations by generating motion-informative text embeddings, significantly enhancing the relevance and semantics of generated motion sequences. With LaMP, we advance three key tasks: text-to-motion generation, motion-text retrieval, and motion captioning through aligned language-motion representation learning. For generation, we utilize LaMP to provide the text condition instead of CLIP, and an autoregressive masked prediction is designed to achieve mask modeling without rank collapse in transformers. For retrieval, motion features from LaMP's motion transformer interact with query tokens to retrieve text features from the text transformer, and vice versa. For captioning, we finetune a large language model with the language-informative motion features to develop a strong motion captioning model. In addition, we introduce the LaMP-BertScore metric to assess the alignment of generated motions with textual descriptions. Extensive experimental results on multiple datasets demonstrate substantial improvements over previous methods across all three tasks. The code of our method will be made public.\n## AUTOREGRESSIVE GENERATION\nUnlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;Zhang et al., 2023a) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness. Therefore, we predict the masked regions autoregressively: Given a text query T -for instance, \"A person is walking in a circle\"-our primary objective is to rank motions from a comprehensive database based on their semantic alignment with the textual input. The ultimate aim is to retrieve the motion that exhibits the highest correspondence with the provided textual description, effectively bridging the gap between disparate modalities-text and motion. We hypothesize that a model exhibiting better alignment between these modalities will inherently possess improved cross-modal retrieval capabilities, and can serve as a more reasonable R-Precision evaluator.",
            "reference_string": "[273233776 | Li et al. | 2024 | Citations: 10]"
        },
        {
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "venue": "International Conference on Machine Learning",
            "year": 2022,
            "reference_count": 86,
            "citation_count": 175,
            "influential_citation_count": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.05832",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2204.05832, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2135734748",
                    "name": "Thomas Wang"
                },
                {
                    "authorId": "145625142",
                    "name": "Adam Roberts"
                },
                {
                    "authorId": "80424302",
                    "name": "Daniel Hesslow"
                },
                {
                    "authorId": "1379806208",
                    "name": "Teven Le Scao"
                },
                {
                    "authorId": "3351938",
                    "name": "Hyung Won Chung"
                },
                {
                    "authorId": "46181066",
                    "name": "Iz Beltagy"
                },
                {
                    "authorId": "143945447",
                    "name": "Julien Launay"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                }
            ],
            "abstract": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",
            "corpus_id": 248118752,
            "sentences": [
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Transformer. Virtually all state-of-the-art LLMs are based on the Transformer architecture [Vaswani et al., 2017]. Due to its ubiquity, we only highlight a few relevant high-level characteristics. The main architectural unit of the Transformer is a Transformer block, which consists of (at minimum) multi-headed self attention [Cheng et al., 2016], layer normalization [Ba et al., 2016], a dense twolayer feedforward network, and residual connections [He et al., 2016]. A Transformer stack is a sequence of such blocks. In NLP applications, the Transformer ingests and outputs tokens. Since being introduced by Vaswani et al. [2017], various architectural variants of the Transformer have been proposed. A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider. \n\nEncoder-decoder. As originally proposed, the Transformer consisted of two stacks: an encoder and a decoder. The encoder is fed the sequence of input tokens and outputs a sequence of vectors of the same length as the input. Then, the decoder autoregressively predicts the target sequence, token by token, conditioned on the output of the encoder. To achieve this conditioning, the decoder includes cross-attention layers in each of its blocks, allowing the decoder to also attend to the output of the encoder. The self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right). \n\nWe hereafter refer to this architecture as the encoder-decoder (ED) . Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020].",
                    "score": 0.5196015578736635,
                    "section_title": "Architectures",
                    "char_start_offset": 8455,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 12
                        },
                        {
                            "start": 13,
                            "end": 114
                        },
                        {
                            "start": 115,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 703
                        },
                        {
                            "start": 704,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 976
                        },
                        {
                            "start": 979,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1086
                        },
                        {
                            "start": 1087,
                            "end": 1201
                        },
                        {
                            "start": 1202,
                            "end": 1324
                        },
                        {
                            "start": 1325,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1683
                        },
                        {
                            "start": 1686,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1892
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 91,
                            "end": 113,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 451,
                            "end": 468,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 611,
                            "end": 632,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.765625
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
                    "score": 0.8911057584907371,
                    "section_title": "Architectures",
                    "char_start_offset": 11973,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 175,
                            "end": 312
                        },
                        {
                            "start": 313,
                            "end": 376
                        },
                        {
                            "start": 377,
                            "end": 563
                        },
                        {
                            "start": 564,
                            "end": 719
                        },
                        {
                            "start": 722,
                            "end": 735
                        },
                        {
                            "start": 736,
                            "end": 849
                        },
                        {
                            "start": 850,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1170
                        },
                        {
                            "start": 1171,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1290
                        },
                        {
                            "start": 1291,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1508
                        },
                        {
                            "start": 1509,
                            "end": 1607
                        },
                        {
                            "start": 1608,
                            "end": 1786
                        },
                        {
                            "start": 1789,
                            "end": 1822
                        },
                        {
                            "start": 1823,
                            "end": 1930
                        },
                        {
                            "start": 1931,
                            "end": 2052
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 451,
                            "end": 469,
                            "matchedPaperCorpusId": "3608234"
                        },
                        {
                            "start": 524,
                            "end": 544,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 649,
                            "end": 670,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59765625
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.",
                    "score": 0.7819578717336095,
                    "section_title": "Architectures",
                    "char_start_offset": 10211,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 384
                        },
                        {
                            "start": 387,
                            "end": 407
                        },
                        {
                            "start": 408,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1176
                        },
                        {
                            "start": 1177,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1567
                        },
                        {
                            "start": 1570,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1936
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 114,
                            "end": 135,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1321,
                            "end": 1342,
                            "matchedPaperCorpusId": "49313245"
                        },
                        {
                            "start": 1438,
                            "end": 1456,
                            "matchedPaperCorpusId": "237485423"
                        },
                        {
                            "start": 1476,
                            "end": 1500,
                            "matchedPaperCorpusId": "238582964"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52392578125
                },
                {
                    "corpus_id": "248118752",
                    "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                    "text": "When studying adaptation and the conversion from one architecture to another, we also considered converting to and from encoder-decoder models. Conversion across causal and non-causal decoder-only models is straightforward, simply by switching the attention mask; for encoder-decoder, parameters have to be either pruned or added for both the entire encoder, and for the cross-attention in the decoder. Results from one of our attempt to convert an encoder-decoder into a causal decoder are reported in Figure 10. While converting across causal/non-causal decoder provides an improvement over training from scratch, this is not the case here. Validation loss when adapting an encoder-decoder pretrained with MLM to a causal decoder-only using FLM. We adapted a pretrained (for 168B tokens) encoder-decoder model to decoder-only by feeding an empty input into the encoder and causally training with a FLM objective on the decoder. We stopped this adaptation once it was clear the performance would not match that of a causal FLM trained from scratch, in contrast with the other adaptations we studied.",
                    "score": 0.6343112013968137,
                    "section_title": "E.4 Adaptation from an encoder-decoder",
                    "char_start_offset": 46680,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 402
                        },
                        {
                            "start": 403,
                            "end": 513
                        },
                        {
                            "start": 514,
                            "end": 642
                        },
                        {
                            "start": 643,
                            "end": 747
                        },
                        {
                            "start": 748,
                            "end": 929
                        },
                        {
                            "start": 930,
                            "end": 1100
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.489013671875
                }
            ],
            "relevance_judgement": 0.765625,
            "relevance_judgment_input_expanded": "# Title: What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?\n# Venue: International Conference on Machine Learning\n# Authors: Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, Colin Raffel\n## Abstract\nLarge pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.\n## Architectures\nTransformer. Virtually all state-of-the-art LLMs are based on the Transformer architecture [Vaswani et al., 2017]. Due to its ubiquity, we only highlight a few relevant high-level characteristics. The main architectural unit of the Transformer is a Transformer block, which consists of (at minimum) multi-headed self attention [Cheng et al., 2016], layer normalization [Ba et al., 2016], a dense twolayer feedforward network, and residual connections [He et al., 2016]. A Transformer stack is a sequence of such blocks. In NLP applications, the Transformer ingests and outputs tokens. Since being introduced by Vaswani et al. [2017], various architectural variants of the Transformer have been proposed. A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider. \n\nEncoder-decoder. As originally proposed, the Transformer consisted of two stacks: an encoder and a decoder. The encoder is fed the sequence of input tokens and outputs a sequence of vectors of the same length as the input. Then, the decoder autoregressively predicts the target sequence, token by token, conditioned on the output of the encoder. To achieve this conditioning, the decoder includes cross-attention layers in each of its blocks, allowing the decoder to also attend to the output of the encoder. The self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right). \n\nWe hereafter refer to this architecture as the encoder-decoder (ED) . Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020].\n...\nNotable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.\n...\nSpecifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.\n\n## E.4 Adaptation from an encoder-decoder\nWhen studying adaptation and the conversion from one architecture to another, we also considered converting to and from encoder-decoder models. Conversion across causal and non-causal decoder-only models is straightforward, simply by switching the attention mask; for encoder-decoder, parameters have to be either pruned or added for both the entire encoder, and for the cross-attention in the decoder. Results from one of our attempt to convert an encoder-decoder into a causal decoder are reported in Figure 10. While converting across causal/non-causal decoder provides an improvement over training from scratch, this is not the case here. Validation loss when adapting an encoder-decoder pretrained with MLM to a causal decoder-only using FLM. We adapted a pretrained (for 168B tokens) encoder-decoder model to decoder-only by feeding an empty input into the encoder and causally training with a FLM objective on the decoder. We stopped this adaptation once it was clear the performance would not match that of a causal FLM trained from scratch, in contrast with the other adaptations we studied.",
            "reference_string": "[248118752 | Wang et al. | 2022 | Citations: 175]"
        },
        {
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 149,
            "citation_count": 72,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18013, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2287925430",
                    "name": "Zihao Yi"
                },
                {
                    "authorId": "2287922728",
                    "name": "Jiarui Ouyang"
                },
                {
                    "authorId": "2288039936",
                    "name": "Yuwen Liu"
                },
                {
                    "authorId": "2287923878",
                    "name": "Tianhao Liao"
                },
                {
                    "authorId": "2288033664",
                    "name": "Zhe Xu"
                },
                {
                    "authorId": "2288065597",
                    "name": "Ying Shen"
                }
            ],
            "abstract": "This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.",
            "corpus_id": 268041362,
            "sentences": [
                {
                    "corpus_id": "268041362",
                    "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
                    "text": "LLMs are a class of extensive artificial intelligence models characterized by their massive scale with billions of parameters [14]. Scaling up LLMs allows them to learn more intricate and accurate language representations, resulting in improved performance across diverse downstream Natural Language Processing (NLP) tasks, particularly excelling in Natural Language Generation (NLG) challenges [15,16]. The brief comparison of different structures of the LLMs mentioned can be seen in Table 1. Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. [18] The vanilla Transformer architecture [17], a sequence-to-sequence model, has emerged as a foundational framework for diverse LLMs, utilizing encoders and decoders with self-attention mechanisms as its core components, thanks to its exceptional parallelism and capacity. Based on the masking methods utilized by various attention mechanisms in the model, the current LLMs can be divided into three categories, i.e., Encoder-Decoder, Decoder-only, and Encoder-only. \n\nThe decoder-only category further includes distinctions such as causal decoders and prefix decoders, illustrated in Figure 1. \n\nIn the following subsection, we shall introduce different types of LLMs based on various Transformer architectures.",
                    "score": 0.6715250435075248,
                    "section_title": "GENERAL METHODS",
                    "char_start_offset": 4546,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 502
                        },
                        {
                            "start": 503,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 750
                        },
                        {
                            "start": 751,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1246
                        },
                        {
                            "start": 1247,
                            "end": 1440
                        },
                        {
                            "start": 1443,
                            "end": 1568
                        },
                        {
                            "start": 1571,
                            "end": 1686
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 402,
                            "matchedPaperCorpusId": "212747830"
                        },
                        {
                            "start": 972,
                            "end": 976,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7626953125
                }
            ],
            "relevance_judgement": 0.7626953125,
            "relevance_judgment_input_expanded": "# Title: A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems\n# Venue: arXiv.org\n# Authors: Zihao Yi, Jiarui Ouyang, Yuwen Liu, Tianhao Liao, Zhe Xu, Ying Shen\n## Abstract\nThis survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.\n## GENERAL METHODS\nLLMs are a class of extensive artificial intelligence models characterized by their massive scale with billions of parameters [14]. Scaling up LLMs allows them to learn more intricate and accurate language representations, resulting in improved performance across diverse downstream Natural Language Processing (NLP) tasks, particularly excelling in Natural Language Generation (NLG) challenges [15,16]. The brief comparison of different structures of the LLMs mentioned can be seen in Table 1. Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. [18] The vanilla Transformer architecture [17], a sequence-to-sequence model, has emerged as a foundational framework for diverse LLMs, utilizing encoders and decoders with self-attention mechanisms as its core components, thanks to its exceptional parallelism and capacity. Based on the masking methods utilized by various attention mechanisms in the model, the current LLMs can be divided into three categories, i.e., Encoder-Decoder, Decoder-only, and Encoder-only. \n\nThe decoder-only category further includes distinctions such as causal decoders and prefix decoders, illustrated in Figure 1. \n\nIn the following subsection, we shall introduce different types of LLMs based on various Transformer architectures.",
            "reference_string": "[268041362 | Yi et al. | 2024 | Citations: 72]"
        },
        {
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.00539, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2314473248",
                    "name": "Mingcong Lu"
                },
                {
                    "authorId": "2314649002",
                    "name": "Jiangcai Zhu"
                },
                {
                    "authorId": "2314113733",
                    "name": "Wang Hao"
                },
                {
                    "authorId": "2314323587",
                    "name": "Zheng Li"
                },
                {
                    "authorId": "2314311430",
                    "name": "Shusheng Zhang"
                },
                {
                    "authorId": "2314110211",
                    "name": "Kailai Shao"
                },
                {
                    "authorId": "2314192630",
                    "name": "Chao Chen"
                },
                {
                    "authorId": "2314343132",
                    "name": "Nan Li"
                },
                {
                    "authorId": "2324104105",
                    "name": "Feng Wang"
                },
                {
                    "authorId": "2324103820",
                    "name": "Xin Lu"
                }
            ],
            "abstract": "Multi-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.",
            "corpus_id": 271600495,
            "sentences": [
                {
                    "corpus_id": "271600495",
                    "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                    "text": "Recent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022] and GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation capability, LLMs have made remarkable achievements in different kinds of Natural Language Process (NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way to interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds increases, ensuring high quality and low latency of the generated answer by LLMs is a challenge. \n\nExisting language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al. [2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention. Our ISM utilizes alternate bidirectional and unidirectional attention on queries and answers in prefix sequences. All of them generate answer in auto-regressive.",
                    "score": 0.6548630300126572,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 157
                        },
                        {
                            "start": 158,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 438
                        },
                        {
                            "start": 439,
                            "end": 577
                        },
                        {
                            "start": 580,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1582
                        },
                        {
                            "start": 1583,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1818
                        },
                        {
                            "start": 1819,
                            "end": 1932
                        },
                        {
                            "start": 1933,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 718,
                            "end": 738,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 740,
                            "end": 759,
                            "matchedPaperCorpusId": "204960716"
                        },
                        {
                            "start": 822,
                            "end": 840,
                            "matchedPaperCorpusId": "147704286"
                        },
                        {
                            "start": 859,
                            "end": 878,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 906,
                            "end": 922,
                            "matchedPaperCorpusId": "247519241"
                        },
                        {
                            "start": 1183,
                            "end": 1202,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1247,
                            "end": 1263,
                            "matchedPaperCorpusId": "247519241"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.744140625
                }
            ],
            "relevance_judgement": 0.744140625,
            "relevance_judgment_input_expanded": "# Title: Intermittent Semi-working Mask: A New Masking Paradigm for LLMs\n# Venue: arXiv.org\n# Authors: Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu\n## Abstract\nMulti-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.\n## Introduction\nRecent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022] and GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation capability, LLMs have made remarkable achievements in different kinds of Natural Language Process (NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way to interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds increases, ensuring high quality and low latency of the generated answer by LLMs is a challenge. \n\nExisting language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al. [2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention. Our ISM utilizes alternate bidirectional and unidirectional attention on queries and answers in prefix sequences. All of them generate answer in auto-regressive.",
            "reference_string": "[271600495 | Lu et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Where does In-context Translation Happen in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.04510, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1666941674",
                    "name": "Suzanna Sia"
                },
                {
                    "authorId": "2290183827",
                    "name": "David Mueller"
                },
                {
                    "authorId": "2268399226",
                    "name": "Kevin Duh"
                }
            ],
            "abstract": "Self-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a\"task recognition\"point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that the most effective layers for MT fine-tuning are the layers critical to task recognition.",
            "corpus_id": 268264275,
            "sentences": [
                {
                    "corpus_id": "268264275",
                    "title": "Where does In-context Translation Happen in Large Language Models",
                    "text": "In-context learning differs from task-specific supervised learning in that, during test time, the desired task must be identified from the context first, then executed. At what stage in the feed-forward computation does a GPT-style model transition from an in-context learner to a translation model? To explore this question, we introduce layer-from context-masking which masks out all attention weights to the context (instructions or prompts) from a certain layer onwards (see Figure 1 for a graphical description). \n\nFor Causal Decoder-only Transformer Language Models, given each position i, the Attention weight \u03b1 ij over context positions j, j < i can be computed by a \n\n) is the dot product between a query vector and key vector q i \u2022 k j , where q i = W q x i , k j = W k x j for trained weight matrices W k and W q . 2 We apply the attention mask over the context so that the attention score is (q i \u2022k j )+m(j, u). Here u is the context that we wish to mask, and m(j, u) \n\nAll masks operate from the j-th layer (\u2113 j ) onwards, i.e. the task without seeing any instructions or examples in the context. \n\n2 Readers should note that there is a W k and Wq weight matrix for each layer and each attention head, but we omit the notation on this for readability. masking from \u2113 20 means causally masking out attention to all context positions from \u2113 20:n \u2113 , where n \u2113 is the total number of layers. To construct Fig 2, we increment \u2113 from 1 to n \u2113 and apply the set of masks {m(j, u)} \u2113:n \u2113 in each experiment and observe the performance of the model. \n\nUnder this causal masking treatment masking from layer \u2113, the model must rely on the representations of the target input sentence from layer \u2113 + 1 only to complete the task; if the target sentence representations do not already encode the target task (translation into a specific language) then the model will fail to generate translations. \n\nIn other words, the goal is to characterise where the model has \"located\" the task of translation.",
                    "score": 0.5153385919411557,
                    "section_title": "Layer-from Context Masking",
                    "char_start_offset": 8862,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 517
                        },
                        {
                            "start": 520,
                            "end": 674
                        },
                        {
                            "start": 677,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 980
                        },
                        {
                            "start": 983,
                            "end": 1110
                        },
                        {
                            "start": 1113,
                            "end": 1265
                        },
                        {
                            "start": 1266,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1555
                        },
                        {
                            "start": 1558,
                            "end": 1898
                        },
                        {
                            "start": 1901,
                            "end": 1999
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.71142578125
                }
            ],
            "relevance_judgement": 0.71142578125,
            "relevance_judgment_input_expanded": "# Title: Where does In-context Translation Happen in Large Language Models\n# Venue: arXiv.org\n# Authors: Suzanna Sia, David Mueller, Kevin Duh\n## Abstract\nSelf-supervised large language models have demonstrated the ability to perform Machine Translation (MT) via in-context learning, but little is known about where the model performs the task with respect to prompt instructions and demonstration examples. In this work, we attempt to characterize the region where large language models transition from in-context learners to translation models. Through a series of layer-wise context-masking experiments on \\textsc{GPTNeo2.7B}, \\textsc{Bloom3B}, \\textsc{Llama7b} and \\textsc{Llama7b-chat}, we demonstrate evidence of a\"task recognition\"point where the translation task is encoded into the input representations and attention to context is no longer necessary. We further observe correspondence between the low performance when masking out entire layers, and the task recognition layers. Taking advantage of this redundancy results in 45\\% computational savings when prompting with 5 examples, and task recognition achieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that the most effective layers for MT fine-tuning are the layers critical to task recognition.\n## Layer-from Context Masking\nIn-context learning differs from task-specific supervised learning in that, during test time, the desired task must be identified from the context first, then executed. At what stage in the feed-forward computation does a GPT-style model transition from an in-context learner to a translation model? To explore this question, we introduce layer-from context-masking which masks out all attention weights to the context (instructions or prompts) from a certain layer onwards (see Figure 1 for a graphical description). \n\nFor Causal Decoder-only Transformer Language Models, given each position i, the Attention weight \u03b1 ij over context positions j, j < i can be computed by a \n\n) is the dot product between a query vector and key vector q i \u2022 k j , where q i = W q x i , k j = W k x j for trained weight matrices W k and W q . 2 We apply the attention mask over the context so that the attention score is (q i \u2022k j )+m(j, u). Here u is the context that we wish to mask, and m(j, u) \n\nAll masks operate from the j-th layer (\u2113 j ) onwards, i.e. the task without seeing any instructions or examples in the context. \n\n2 Readers should note that there is a W k and Wq weight matrix for each layer and each attention head, but we omit the notation on this for readability. masking from \u2113 20 means causally masking out attention to all context positions from \u2113 20:n \u2113 , where n \u2113 is the total number of layers. To construct Fig 2, we increment \u2113 from 1 to n \u2113 and apply the set of masks {m(j, u)} \u2113:n \u2113 in each experiment and observe the performance of the model. \n\nUnder this causal masking treatment masking from layer \u2113, the model must rely on the representations of the target input sentence from layer \u2113 + 1 only to complete the task; if the target sentence representations do not already encode the target task (translation into a specific language) then the model will fail to generate translations. \n\nIn other words, the goal is to characterise where the model has \"located\" the task of translation.",
            "reference_string": "[268264275 | Sia et al. | 2024 | Citations: 0]"
        },
        {
            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.03206, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2319805585",
                    "name": "Mingze Gao"
                },
                {
                    "authorId": "2302790279",
                    "name": "Jingyu Liu"
                },
                {
                    "authorId": "2302785092",
                    "name": "Mingda Li"
                },
                {
                    "authorId": "2319964588",
                    "name": "Jiangtao Xie"
                },
                {
                    "authorId": "2258682951",
                    "name": "Qingbin Liu"
                },
                {
                    "authorId": "2304448412",
                    "name": "Bo Zhao"
                },
                {
                    "authorId": "2302990371",
                    "name": "Xi Chen"
                },
                {
                    "authorId": "2319814814",
                    "name": "Hui Xiong"
                }
            ],
            "abstract": "Multimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.",
            "corpus_id": 272423598,
            "sentences": [
                {
                    "corpus_id": "272423598",
                    "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
                    "text": "Another often overlooked key point is the design of attention masks within the transformer layers in large lan- guage models. In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text. Mathematically, the causal attention mask M \u2208 R T \u00d7T for a sequence of length T is defined as: \n\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as: \n\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, d k is the dimension of the key vectors, and M is the causal attention mask. However, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3. \n\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows: \n\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows: \n\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:",
                    "score": 0.608265607361175,
                    "section_title": "Frame-wise Block Causal Attention Mask",
                    "char_start_offset": 12069,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 429
                        },
                        {
                            "start": 430,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 711
                        },
                        {
                            "start": 714,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 897
                        },
                        {
                            "start": 900,
                            "end": 1052
                        },
                        {
                            "start": 1053,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1426
                        },
                        {
                            "start": 1427,
                            "end": 1651
                        },
                        {
                            "start": 1654,
                            "end": 1801
                        },
                        {
                            "start": 1802,
                            "end": 1842
                        },
                        {
                            "start": 1845,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 1983
                        },
                        {
                            "start": 1986,
                            "end": 2273
                        },
                        {
                            "start": 2274,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70947265625
                }
            ],
            "relevance_judgement": 0.70947265625,
            "relevance_judgment_input_expanded": "# Title: TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations\n# Venue: arXiv.org\n# Authors: Mingze Gao, Jingyu Liu, Mingda Li, Jiangtao Xie, Qingbin Liu, Bo Zhao, Xi Chen, Hui Xiong\n## Abstract\nMultimodal Large Language Models (MLLMs) have significantly improved performance across various image-language applications. Recently, there has been a growing interest in adapting image pre-trained MLLMs for video-related tasks. However, most efforts concentrate on enhancing the vision encoder and projector components, while the core part, Large Language Models (LLMs), remains comparatively under-explored. In this paper, we propose two strategies to enhance the model's capability in video understanding tasks by improving inter-layer attention computation in LLMs. Specifically, the first approach focuses on the enhancement of Rotary Position Embedding (RoPE) with Temporal-Aware Dual RoPE, which introduces temporal position information to strengthen the MLLM's temporal modeling capabilities while preserving the relative position relationships of both visual and text tokens. The second approach involves enhancing the Attention Mask with the Frame-wise Block Causal Attention Mask, a simple yet effective method that broadens visual token interactions within and across video frames while maintaining the causal inference mechanism. Based on these proposed methods, we adapt LLaVA for video understanding tasks, naming it Temporal-Considered LLaVA (TC-LLaVA). Our TC-LLaVA achieves new state-of-the-art performance across various video understanding benchmarks with only supervised fine-tuning (SFT) on video-related datasets.\n## Frame-wise Block Causal Attention Mask\nAnother often overlooked key point is the design of attention masks within the transformer layers in large lan- guage models. In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text. Mathematically, the causal attention mask M \u2208 R T \u00d7T for a sequence of length T is defined as: \n\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as: \n\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, d k is the dimension of the key vectors, and M is the causal attention mask. However, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3. \n\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows: \n\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows: \n\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:",
            "reference_string": "[272423598 | Gao et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 64,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02370, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46211375",
                    "name": "Tsz Kin Lam"
                },
                {
                    "authorId": "1736801422",
                    "name": "Marco Gaido"
                },
                {
                    "authorId": "2006601535",
                    "name": "Sara Papi"
                },
                {
                    "authorId": "2486762",
                    "name": "L. Bentivogli"
                },
                {
                    "authorId": "2322986291",
                    "name": "Barry Haddow"
                }
            ],
            "abstract": "Following the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention.",
            "corpus_id": 275336136,
            "sentences": [
                {
                    "corpus_id": "275336136",
                    "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
                    "text": "During training of encoder-decoder models, the target tokens in the decoder are causally masked to prevent them from looking at future information. The causal masking can be represented as a mask matrix M : \n\nthat is summed with the attention matrix before the softmax operator to make sure that each element i can only attend to itself and elements before it (i.e., j \u2264 i), obtaining \n\nIn standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,...,i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models.",
                    "score": 0.7725106142152471,
                    "section_title": "Audio Causal Masking",
                    "char_start_offset": 10649,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 147
                        },
                        {
                            "start": 148,
                            "end": 206
                        },
                        {
                            "start": 209,
                            "end": 384
                        },
                        {
                            "start": 387,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 726
                        },
                        {
                            "start": 727,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 965
                        },
                        {
                            "start": 968,
                            "end": 1015
                        },
                        {
                            "start": 1016,
                            "end": 1230
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 745,
                            "end": 762,
                            "matchedPaperCorpusId": "259501685"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68798828125
                },
                {
                    "corpus_id": "275336136",
                    "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
                    "text": "In previous sections, we presented each DFP configuration using its optimal causal masking strategy: 1) causal masking is not applied on decoder-only, whereas 2) it is applied on decoder-prepend. In the following, we provide an ablation study of causal masking, which is summarised in Table 3. The significance tests are computed between the pairs with and without causal masking. \n\nAs we can observe, decoder-only (both 18L and 32L) performs worse on all experimental settings when causal masking is applied. On CoVoST2, the performance degrades by at least 2 points, whereas the degradation can be up to 2 points on MuST-C. This indicates the importance of allowing the speech frames to attend each other in decoder-only models. Our finding is in accordance with the conclusion drawn by Gupta et al. (2024) for ASR, and we further extend it for ST. \n\nWhen causal masking is removed from decoderprepend (decoder-prepend TF), we observe a performance degradation of 1.2 WER and up to 0.9 BLEU points on MuST-C ASR and bilingual ST, respectively. What is even worse is the degradation of 8.18 BLEU points in multilingual ST. On the CoVoST2 dataset, however, removal of causal masking causes little improvement to both ASR and ST. In the case of Conformer, there are almost no performance changes on the CoVoST2 dataset when causal masking is removed, but a small degradation of 0.4 BLEU points (25.9 \u2192 25.5) on the MuST-C en-de direction when CTC compression is also applied. \n\nTherefore, our results lead to two interesting observations. Firstly, the behaviour of DFP models are quite different with causal masking, depending on whether a speech encoder is used or not. We hypothesise that the non-adversarial effect of causal masking on decoder-prepend is attributed to the self-attention within the speech encoder, which allows full attention within the speech frames.",
                    "score": 0.6690744113479616,
                    "section_title": "(Audio) Causality masking in decoder-only and decoder-prepend",
                    "char_start_offset": 25867,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 380
                        },
                        {
                            "start": 383,
                            "end": 509
                        },
                        {
                            "start": 510,
                            "end": 625
                        },
                        {
                            "start": 626,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 850
                        },
                        {
                            "start": 853,
                            "end": 1045
                        },
                        {
                            "start": 1046,
                            "end": 1123
                        },
                        {
                            "start": 1124,
                            "end": 1228
                        },
                        {
                            "start": 1229,
                            "end": 1474
                        },
                        {
                            "start": 1477,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1870
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.55810546875
                }
            ],
            "relevance_judgement": 0.68798828125,
            "relevance_judgment_input_expanded": "# Title: Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Tsz Kin Lam, Marco Gaido, Sara Papi, L. Bentivogli, Barry Haddow\n## Abstract\nFollowing the remarkable success of Large Language Models (LLMs) in NLP tasks, there is increasing interest in extending their capabilities to speech -- the most common form of communication. The most widespread approach to integrating speech into LLMs is dense feature prepending (DFP), which prepends the projected speech representations to the textual representations, allowing end-to-end training with a speech encoder. This raises questions about the need for a sophisticated speech encoder for DFP and how its performance compares with a standard encoder-decoder (i.e., cross-attention) architecture. We compare DFP and cross-attention under a variety of configurations, such as CTC compression, sequence-level knowledge distillation, on monolingual, bilingual, and multilingual models. To perform a controlled architectural comparison, we train all models from scratch rather than using large pretrained models and use comparable data and parameter settings, testing speech-to-text recognition (ASR) and translation (ST) on MuST-C v1.0 and CoVoST2 datasets. Despite the wide adoption of DFP, our results do not indicate a clear advantage of DFP over cross-attention.\n## Audio Causal Masking\nDuring training of encoder-decoder models, the target tokens in the decoder are causally masked to prevent them from looking at future information. The causal masking can be represented as a mask matrix M : \n\nthat is summed with the attention matrix before the softmax operator to make sure that each element i can only attend to itself and elements before it (i.e., j \u2264 i), obtaining \n\nIn standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,...,i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models.\n\n## (Audio) Causality masking in decoder-only and decoder-prepend\nIn previous sections, we presented each DFP configuration using its optimal causal masking strategy: 1) causal masking is not applied on decoder-only, whereas 2) it is applied on decoder-prepend. In the following, we provide an ablation study of causal masking, which is summarised in Table 3. The significance tests are computed between the pairs with and without causal masking. \n\nAs we can observe, decoder-only (both 18L and 32L) performs worse on all experimental settings when causal masking is applied. On CoVoST2, the performance degrades by at least 2 points, whereas the degradation can be up to 2 points on MuST-C. This indicates the importance of allowing the speech frames to attend each other in decoder-only models. Our finding is in accordance with the conclusion drawn by Gupta et al. (2024) for ASR, and we further extend it for ST. \n\nWhen causal masking is removed from decoderprepend (decoder-prepend TF), we observe a performance degradation of 1.2 WER and up to 0.9 BLEU points on MuST-C ASR and bilingual ST, respectively. What is even worse is the degradation of 8.18 BLEU points in multilingual ST. On the CoVoST2 dataset, however, removal of causal masking causes little improvement to both ASR and ST. In the case of Conformer, there are almost no performance changes on the CoVoST2 dataset when causal masking is removed, but a small degradation of 0.4 BLEU points (25.9 \u2192 25.5) on the MuST-C en-de direction when CTC compression is also applied. \n\nTherefore, our results lead to two interesting observations. Firstly, the behaviour of DFP models are quite different with causal masking, depending on whether a speech encoder is used or not. We hypothesise that the non-adversarial effect of causal masking on decoder-prepend is attributed to the self-attention within the speech encoder, which allows full attention within the speech frames.",
            "reference_string": "[275336136 | Lam et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 27,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.19572, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165228008",
                    "name": "Zhuocheng Gong"
                },
                {
                    "authorId": "2261393008",
                    "name": "Jiahao Liu"
                },
                {
                    "authorId": "2261393439",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2258759716",
                    "name": "Jingang Wang"
                },
                {
                    "authorId": "2259620212",
                    "name": "Xunliang Cai"
                },
                {
                    "authorId": "2253232138",
                    "name": "Dongyan Zhao"
                },
                {
                    "authorId": "2249533146",
                    "name": "Rui Yan"
                }
            ],
            "abstract": "In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model's understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to `observe' the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.",
            "corpus_id": 264805730,
            "sentences": [
                {
                    "corpus_id": "264805730",
                    "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
                    "text": "Most of the current decoder-only LLMs employ causal language modeling as the pre-training objective (Radford et al., 2018), which aims to predict future tokens based on the past. \n\narg min \n\nIn causal language modeling, the model only attends to tokens that occur before (the left context), resulting in a unidirectional attention scheme known as causal attention. This approach enables the model to process each token in the input sequence in order, without accessing any information from the future (the right context). \n\nCausal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j.",
                    "score": 0.5946194405634495,
                    "section_title": "Causal Language Modeling",
                    "char_start_offset": 4923,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 181,
                            "end": 188
                        },
                        {
                            "start": 191,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 521
                        },
                        {
                            "start": 524,
                            "end": 752
                        },
                        {
                            "start": 753,
                            "end": 862
                        },
                        {
                            "start": 865,
                            "end": 1005
                        },
                        {
                            "start": 1006,
                            "end": 1102
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6669921875
                }
            ],
            "relevance_judgement": 0.6669921875,
            "relevance_judgment_input_expanded": "# Title: Improving Input-label Mapping with Demonstration Replay for In-context Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Zhuocheng Gong, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Dongyan Zhao, Rui Yan\n## Abstract\nIn-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model's understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to `observe' the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.\n## Causal Language Modeling\nMost of the current decoder-only LLMs employ causal language modeling as the pre-training objective (Radford et al., 2018), which aims to predict future tokens based on the past. \n\narg min \n\nIn causal language modeling, the model only attends to tokens that occur before (the left context), resulting in a unidirectional attention scheme known as causal attention. This approach enables the model to process each token in the input sequence in order, without accessing any information from the future (the right context). \n\nCausal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j.",
            "reference_string": "[264805730 | Gong et al. | 2023 | Citations: 2]"
        },
        {
            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 12,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15477, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265383225",
                    "name": "Kai Yin"
                },
                {
                    "authorId": "2308073678",
                    "name": "Chengkai Liu"
                },
                {
                    "authorId": "2258714985",
                    "name": "Ali Mostafavi"
                },
                {
                    "authorId": "2308068627",
                    "name": "Xia Hu"
                }
            ],
            "abstract": "In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.",
            "corpus_id": 270702559,
            "sentences": [
                {
                    "corpus_id": "270702559",
                    "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                    "text": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                    "score": 0.7953885436312866,
                    "section_title": "Architecture of LLMs",
                    "char_start_offset": 7065,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 148,
                            "end": 503
                        },
                        {
                            "start": 503,
                            "end": 654
                        },
                        {
                            "start": 654,
                            "end": 749
                        },
                        {
                            "start": 751,
                            "end": 898
                        },
                        {
                            "start": 898,
                            "end": 1055
                        },
                        {
                            "start": 1055,
                            "end": 1243
                        },
                        {
                            "start": 1243,
                            "end": 1318
                        },
                        {
                            "start": 1318,
                            "end": 1430
                        },
                        {
                            "start": 1432,
                            "end": 1633
                        },
                        {
                            "start": 1633,
                            "end": 1756
                        },
                        {
                            "start": 1756,
                            "end": 1889
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1513,
                            "end": 1535,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6611328125
                }
            ],
            "relevance_judgement": 0.6611328125,
            "relevance_judgment_input_expanded": "# Title: CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics\n# Venue: arXiv.org\n# Authors: Kai Yin, Chengkai Liu, Ali Mostafavi, Xia Hu\n## Abstract\nIn the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.\n## Architecture of LLMs\nTypical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
            "reference_string": "[270702559 | Yin et al. | 2024 | Citations: 12]"
        },
        {
            "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.04779, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2281644597",
                    "name": "Qingyu Yin"
                },
                {
                    "authorId": "2283262102",
                    "name": "Xuzheng He"
                },
                {
                    "authorId": "2231289043",
                    "name": "Zhuang Xiang"
                },
                {
                    "authorId": "2283195201",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2238917409",
                    "name": "Jianhua Yao"
                },
                {
                    "authorId": "2283453441",
                    "name": "Xiaoyu Shen"
                },
                {
                    "authorId": "2254277130",
                    "name": "Qiang Zhang"
                }
            ],
            "abstract": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
            "corpus_id": 267523128,
            "sentences": [
                {
                    "corpus_id": "267523128",
                    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
                    "text": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
                    "score": 0.6077820253545078,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6611328125
                }
            ],
            "relevance_judgement": 0.6611328125,
            "relevance_judgment_input_expanded": "# Title: StableMask: Refining Causal Masking in Decoder-only Transformer\n# Venue: International Conference on Machine Learning\n# Authors: Qingyu Yin, Xuzheng He, Zhuang Xiang, Yu Zhao, Jianhua Yao, Xiaoyu Shen, Qiang Zhang\n## Abstract\nThe decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.\n",
            "reference_string": "[267523128 | Yin et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "venue": "Neurocomputing",
            "year": 2024,
            "reference_count": 203,
            "citation_count": 74,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.02038, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116426849",
                    "name": "Yi-Hsueh Liu"
                },
                {
                    "authorId": "2155082967",
                    "name": "Haoyang He"
                },
                {
                    "authorId": "2184719751",
                    "name": "Tianle Han"
                },
                {
                    "authorId": "2273584640",
                    "name": "Xu Zhang"
                },
                {
                    "authorId": "2210636248",
                    "name": "Mengyuan Liu"
                },
                {
                    "authorId": "2257433902",
                    "name": "Jiaming Tian"
                },
                {
                    "authorId": "2257095790",
                    "name": "Yutong Zhang"
                },
                {
                    "authorId": "2110238778",
                    "name": "Jiaqi Wang"
                },
                {
                    "authorId": "2277869261",
                    "name": "Xiaohui Gao"
                },
                {
                    "authorId": "2215167446",
                    "name": "Tianyang Zhong"
                },
                {
                    "authorId": "2221032216",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2211904452",
                    "name": "Shaochen Xu"
                },
                {
                    "authorId": "2263593041",
                    "name": "Zihao Wu"
                },
                {
                    "authorId": "2145977326",
                    "name": "Zheng Liu"
                },
                {
                    "authorId": "2257586495",
                    "name": "Xin Zhang"
                },
                {
                    "authorId": "2277750447",
                    "name": "Shu Zhang"
                },
                {
                    "authorId": "1742535",
                    "name": "Xintao Hu"
                },
                {
                    "authorId": "49104946",
                    "name": "Tuo Zhang"
                },
                {
                    "authorId": "2251076040",
                    "name": "Ning Qiang"
                },
                {
                    "authorId": "2254792886",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2257302793",
                    "name": "Bao Ge"
                }
            ],
            "abstract": "The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.",
            "corpus_id": 266755678,
            "sentences": [
                {
                    "corpus_id": "266755678",
                    "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                    "text": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
                    "score": 0.6683360501918196,
                    "section_title": "Decoder-only Architecture",
                    "char_start_offset": 35165,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 725
                        },
                        {
                            "start": 728,
                            "end": 912
                        },
                        {
                            "start": 913,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1114
                        },
                        {
                            "start": 1115,
                            "end": 1278
                        },
                        {
                            "start": 1279,
                            "end": 1377
                        },
                        {
                            "start": 1378,
                            "end": 1591
                        },
                        {
                            "start": 1594,
                            "end": 1743
                        },
                        {
                            "start": 1744,
                            "end": 1956
                        },
                        {
                            "start": 1957,
                            "end": 2102
                        },
                        {
                            "start": 2103,
                            "end": 2196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1951,
                            "end": 1955,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.654296875
                }
            ],
            "relevance_judgement": 0.654296875,
            "relevance_judgment_input_expanded": "# Title: Understanding LLMs: A Comprehensive Overview from Training to Inference\n# Venue: Neurocomputing\n# Authors: Yi-Hsueh Liu, Haoyang He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zheng Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge\n## Abstract\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.\n## Decoder-only Architecture\nLLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
            "reference_string": "[266755678 | Liu et al. | 2024 | Citations: 74]"
        },
        {
            "title": "LLM for SoC Security: A Paradigm Shift",
            "venue": "IEEE Access",
            "year": 2023,
            "reference_count": 242,
            "citation_count": 54,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.06046, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256992493",
                    "name": "Dipayan Saha"
                },
                {
                    "authorId": "2114625129",
                    "name": "Shams Tarek"
                },
                {
                    "authorId": "2256991081",
                    "name": "Katayoon Yahyaei"
                },
                {
                    "authorId": "2231854143",
                    "name": "Sujan Kumar Saha"
                },
                {
                    "authorId": "2257235852",
                    "name": "Jingbo Zhou"
                },
                {
                    "authorId": "145954982",
                    "name": "M. Tehranipoor"
                },
                {
                    "authorId": "1997019",
                    "name": "Farimah Farahmandi"
                }
            ],
            "abstract": "As the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to effectively verify modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, large language models (LLMs) are celebrated for their remarkable success in language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research explores leveraging the emergent capabilities of generative pre-trained transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, presents practical case studies, and demonstrates comprehensive experiments. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.",
            "corpus_id": 263829839,
            "sentences": [
                {
                    "corpus_id": "263829839",
                    "title": "LLM for SoC Security: A Paradigm Shift",
                    "text": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
                    "score": 0.6291326319983631,
                    "section_title": "A. Preliminaries",
                    "char_start_offset": 37836,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 465
                        },
                        {
                            "start": 466,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 853
                        },
                        {
                            "start": 856,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1187
                        },
                        {
                            "start": 1188,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1846
                        },
                        {
                            "start": 1847,
                            "end": 1942
                        },
                        {
                            "start": 1943,
                            "end": 1998
                        },
                        {
                            "start": 1999,
                            "end": 2140
                        },
                        {
                            "start": 2141,
                            "end": 2270
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 776,
                            "end": 781,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 791,
                            "end": 796,
                            "matchedPaperCorpusId": "252780443"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6494140625
                }
            ],
            "relevance_judgement": 0.6494140625,
            "relevance_judgment_input_expanded": "# Title: LLM for SoC Security: A Paradigm Shift\n# Venue: IEEE Access\n# Authors: Dipayan Saha, Shams Tarek, Katayoon Yahyaei, Sujan Kumar Saha, Jingbo Zhou, M. Tehranipoor, Farimah Farahmandi\n## Abstract\nAs the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to effectively verify modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, large language models (LLMs) are celebrated for their remarkable success in language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research explores leveraging the emergent capabilities of generative pre-trained transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, presents practical case studies, and demonstrates comprehensive experiments. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.\n## A. Preliminaries\nThe encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
            "reference_string": "[263829839 | Saha et al. | 2023 | Citations: 54]"
        },
        {
            "title": "Generative Prompt Tuning for Relation Classification",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2022,
            "reference_count": 58,
            "citation_count": 26,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.12435",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.12435, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "12212068",
                    "name": "Jiale Han"
                },
                {
                    "authorId": "46250837",
                    "name": "Shuai Zhao"
                },
                {
                    "authorId": "2153486826",
                    "name": "Bo Cheng"
                },
                {
                    "authorId": "2160451878",
                    "name": "Shengkun Ma"
                },
                {
                    "authorId": "143844110",
                    "name": "Wei Lu"
                }
            ],
            "abstract": "Using prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to verbalizations with fixed length, which has proven effective for tasks with simple label spaces. However, when applied to relation classification exhibiting complex label spaces, vanilla prompt tuning methods may struggle with label verbalizations with arbitrary lengths due to rigid prompt restrictions. Inspired by the text infilling task for pre-training generative models that can flexibly predict missing spans, we propose a novel generative prompt tuning method to reformulate relation classification as an infilling problem, which frees our approach from limitations of current prompt based approaches and thus fully exploits rich semantics of entity and relation types. In addition, we design entity-guided decoding and discriminative relation scoring to generate and align relations effectively and efficiently during inference. Extensive experiments under fully supervised settings and low-resource settings demonstrate the effectiveness of our approach.",
            "corpus_id": 252546669,
            "sentences": [
                {
                    "corpus_id": "252546669",
                    "title": "Generative Prompt Tuning for Relation Classification",
                    "text": "In this section, we detail the implementation based on RoBERTa. For the RoBERTa version, we concatenate the source and target as inputs together and use partial causal masking to distinguish the encoder-decoder representations, as shown in Figure 6. Specifically, we utilize one RoBERTa model as both the source and target embeddings by concatenating the source sequence T (x) and target y as inputs together. We provide a partial causal attention mask to distinguish the source / target representations. The attention mask -2-dimensional matrix denotes as M. Specifically, for the source tokens, the mask allows full source self-attention, but mask out all target tokens. For i = [0, 1, ..., |T (x)| \u2212 1], | \u2022 | represents the length of sequence, \n\nTo guarantee that the decoder is autoregressive, we enforce the target tokens to only attend to previous tokens and not attend to future tokens. For",
                    "score": 0.572077598599929,
                    "section_title": "RoBERTa Version",
                    "char_start_offset": 27423,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 63
                        },
                        {
                            "start": 64,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 409
                        },
                        {
                            "start": 410,
                            "end": 504
                        },
                        {
                            "start": 505,
                            "end": 672
                        },
                        {
                            "start": 673,
                            "end": 747
                        },
                        {
                            "start": 750,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 898
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.625
                }
            ],
            "relevance_judgement": 0.625,
            "relevance_judgment_input_expanded": "# Title: Generative Prompt Tuning for Relation Classification\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Jiale Han, Shuai Zhao, Bo Cheng, Shengkun Ma, Wei Lu\n## Abstract\nUsing prompts to explore the knowledge contained within pre-trained language models for downstream tasks has now become an active topic. Current prompt tuning methods mostly convert the downstream tasks to masked language modeling problems by adding cloze-style phrases and mapping all labels to verbalizations with fixed length, which has proven effective for tasks with simple label spaces. However, when applied to relation classification exhibiting complex label spaces, vanilla prompt tuning methods may struggle with label verbalizations with arbitrary lengths due to rigid prompt restrictions. Inspired by the text infilling task for pre-training generative models that can flexibly predict missing spans, we propose a novel generative prompt tuning method to reformulate relation classification as an infilling problem, which frees our approach from limitations of current prompt based approaches and thus fully exploits rich semantics of entity and relation types. In addition, we design entity-guided decoding and discriminative relation scoring to generate and align relations effectively and efficiently during inference. Extensive experiments under fully supervised settings and low-resource settings demonstrate the effectiveness of our approach.\n## RoBERTa Version\nIn this section, we detail the implementation based on RoBERTa. For the RoBERTa version, we concatenate the source and target as inputs together and use partial causal masking to distinguish the encoder-decoder representations, as shown in Figure 6. Specifically, we utilize one RoBERTa model as both the source and target embeddings by concatenating the source sequence T (x) and target y as inputs together. We provide a partial causal attention mask to distinguish the source / target representations. The attention mask -2-dimensional matrix denotes as M. Specifically, for the source tokens, the mask allows full source self-attention, but mask out all target tokens. For i = [0, 1, ..., |T (x)| \u2212 1], | \u2022 | represents the length of sequence, \n\nTo guarantee that the decoder is autoregressive, we enforce the target tokens to only attend to previous tokens and not attend to future tokens. For",
            "reference_string": "[252546669 | Han et al. | 2022 | Citations: 26]"
        },
        {
            "title": "Rethinking Causal Mask Attention for Vision-Language Inference",
            "venue": "",
            "year": 2025,
            "reference_count": 50,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203812724",
                    "name": "Xiaohuan Pei"
                },
                {
                    "authorId": "2265957484",
                    "name": "Tao Huang"
                },
                {
                    "authorId": "2288104814",
                    "name": "Yanxiang Ma"
                },
                {
                    "authorId": "2292018438",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "Causal attention has become a foundational mechanism in autoregressive vision-language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited from large language models (LLMs) where they are tailored for text-only decoding, and their adaptation to vision tokens is insufficiently addressed in the prefill stage. Strictly masking future positions for vision queries introduces overly rigid constraints, which hinder the model's ability to leverage future context that often contains essential semantic cues for accurate inference. In this work, we empirically investigate how different causal masking strategies affect vision-language inference and then propose a family of future-aware attentions tailored for this setting. We first empirically analyze the effect of previewing future tokens for vision queries and demonstrate that rigid masking undermines the model's capacity to capture useful contextual semantic representations. Based on these findings, we propose a lightweight attention family that aggregates future visual context into past representations via pooling, effectively preserving the autoregressive structure while enhancing cross-token dependencies. We evaluate a range of causal masks across diverse vision-language inference settings and show that selectively compressing future semantic context into past representations benefits the inference.",
            "corpus_id": 278905007,
            "sentences": [],
            "relevance_judgement": 0.6083984375,
            "relevance_judgment_input_expanded": "# Title: Rethinking Causal Mask Attention for Vision-Language Inference\n# Venue: \n# Authors: Xiaohuan Pei, Tao Huang, Yanxiang Ma, Chang Xu\n## Abstract\nCausal attention has become a foundational mechanism in autoregressive vision-language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited from large language models (LLMs) where they are tailored for text-only decoding, and their adaptation to vision tokens is insufficiently addressed in the prefill stage. Strictly masking future positions for vision queries introduces overly rigid constraints, which hinder the model's ability to leverage future context that often contains essential semantic cues for accurate inference. In this work, we empirically investigate how different causal masking strategies affect vision-language inference and then propose a family of future-aware attentions tailored for this setting. We first empirically analyze the effect of previewing future tokens for vision queries and demonstrate that rigid masking undermines the model's capacity to capture useful contextual semantic representations. Based on these findings, we propose a lightweight attention family that aggregates future visual context into past representations via pooling, effectively preserving the autoregressive structure while enhancing cross-token dependencies. We evaluate a range of causal masks across diverse vision-language inference settings and show that selectively compressing future semantic context into past representations benefits the inference.\n",
            "reference_string": "[278905007 | Pei et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
            "venue": "International Conference on Learning Representations",
            "year": 2022,
            "reference_count": 64,
            "citation_count": 16,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2203.12054",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.12054, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1419971650",
                    "name": "Tianyu Hua"
                },
                {
                    "authorId": "2476765",
                    "name": "Yonglong Tian"
                },
                {
                    "authorId": "1823941979",
                    "name": "Sucheng Ren"
                },
                {
                    "authorId": "2146231364",
                    "name": "Hang Zhao"
                },
                {
                    "authorId": "144398147",
                    "name": "L. Sigal"
                }
            ],
            "abstract": "Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effect various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, CIFAR100, ImageNet). While our pre-training strategy works with a vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder$'$s feature layers, which further improves the performance.",
            "corpus_id": 247618909,
            "sentences": [
                {
                    "corpus_id": "247618909",
                    "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
                    "text": "A standard transformer has an encoder-decoder structure (Vaswani et al., 2017). The encoder of a transformer maps a list of tokens X = (x 1 , ..., x n ) to a sequence of hidden representations Z = (z 1 , ..., z n ), also known as the memory. Given X and source sequence X src = (x 1 , ..., x n\u22121 ), during training, the decoder masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 , ..., x n ) autoregressively. Each layer of the transformer encoder has two sub-layers: multi-head self-attention and a fully connected feed-forward network; both have residual connections. The decoder layer has a third attention sub-layer, which performs multi-head attention from the hidden representation Z to the target representation X tgt . We leverage attention masking to achieve autoregressive segment prediction using this framework; we discuss details next. Figure 4: Attention-masking for Autoregressive Segment Prediction. For an image converted into a sequence of patches, we adopt a masked encoder-decoder transformer (Vaswani et al., 2017) for autoregressive segment prediction. In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask). \n\nAutoregressive Segment Encoder. Figure 4 shows our transformer encoder block and a decoder block. We leave out the fully connected layer and residual connections for simplicity and only show the attentions. In this visualization, there are six patches. These six patches are then grouped into three segments denoted by colors: green, blue, and red. The random segment serialization order is green \u2192 blue \u2192 red. One layer of transformer encoder is illustrated on the left in light green.",
                    "score": 0.5378582883473662,
                    "section_title": "MASKED TRANSFORMER FOR SEGMENT PREDICTION",
                    "char_start_offset": 21070,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 79
                        },
                        {
                            "start": 80,
                            "end": 241
                        },
                        {
                            "start": 242,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 776
                        },
                        {
                            "start": 777,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 965
                        },
                        {
                            "start": 966,
                            "end": 1124
                        },
                        {
                            "start": 1125,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1425
                        },
                        {
                            "start": 1428,
                            "end": 1525
                        },
                        {
                            "start": 1526,
                            "end": 1634
                        },
                        {
                            "start": 1635,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1776
                        },
                        {
                            "start": 1777,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 1914
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 56,
                            "end": 78,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 1063,
                            "end": 1085,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59228515625
                }
            ],
            "relevance_judgement": 0.59228515625,
            "relevance_judgment_input_expanded": "# Title: Self-supervision through Random Segments with Autoregressive Coding (RandSAC)\n# Venue: International Conference on Learning Representations\n# Authors: Tianyu Hua, Yonglong Tian, Sucheng Ren, Hang Zhao, L. Sigal\n## Abstract\nInspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effect various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, CIFAR100, ImageNet). While our pre-training strategy works with a vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder$'$s feature layers, which further improves the performance.\n## MASKED TRANSFORMER FOR SEGMENT PREDICTION\nA standard transformer has an encoder-decoder structure (Vaswani et al., 2017). The encoder of a transformer maps a list of tokens X = (x 1 , ..., x n ) to a sequence of hidden representations Z = (z 1 , ..., z n ), also known as the memory. Given X and source sequence X src = (x 1 , ..., x n\u22121 ), during training, the decoder masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 , ..., x n ) autoregressively. Each layer of the transformer encoder has two sub-layers: multi-head self-attention and a fully connected feed-forward network; both have residual connections. The decoder layer has a third attention sub-layer, which performs multi-head attention from the hidden representation Z to the target representation X tgt . We leverage attention masking to achieve autoregressive segment prediction using this framework; we discuss details next. Figure 4: Attention-masking for Autoregressive Segment Prediction. For an image converted into a sequence of patches, we adopt a masked encoder-decoder transformer (Vaswani et al., 2017) for autoregressive segment prediction. In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask). \n\nAutoregressive Segment Encoder. Figure 4 shows our transformer encoder block and a decoder block. We leave out the fully connected layer and residual connections for simplicity and only show the attentions. In this visualization, there are six patches. These six patches are then grouped into three segments denoted by colors: green, blue, and red. The random segment serialization order is green \u2192 blue \u2192 red. One layer of transformer encoder is illustrated on the left in light green.",
            "reference_string": "[247618909 | Hua et al. | 2022 | Citations: 16]"
        },
        {
            "title": "ENTP: Encoder-only Next Token Prediction",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 26,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.01600, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2323781863",
                    "name": "Ethan Ewer"
                },
                {
                    "authorId": "2253659910",
                    "name": "Daewon Chae"
                },
                {
                    "authorId": "2323820473",
                    "name": "Thomas Zeng"
                },
                {
                    "authorId": "2323851531",
                    "name": "Jinkyu Kim"
                },
                {
                    "authorId": "2323790154",
                    "name": "Kangwook Lee"
                }
            ],
            "abstract": "Next-token prediction is conventionally done using decoder-only Transformers with causal attention, as this approach allows for efficient reuse of keys and values. What if we were not compute-limited, should we still use decoder-only Transformers? In this work, we introduce Encoder-only Next Token Prediction (ENTP). We explore the differences between ENTP and decoder-only Transformers in expressive power and complexity, highlighting potential advantages of ENTP in settings with unbounded compute. We introduce the $\\operatorname{Count3}$ task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot. Finally, we empirically demonstrate the superior performance of ENTP across representative tasks where next-token prediction based Transformers can be evaluated, including addition, in-context learning, and language modeling.",
            "corpus_id": 273025546,
            "sentences": [
                {
                    "corpus_id": "273025546",
                    "title": "ENTP: Encoder-only Next Token Prediction",
                    "text": "In contrast, the causal decoder-only model (Brown et al., 2020;Chowdhery et al., 2023) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2020;Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants). \n\nWith the development of these models, recent studies have investigated the performance of each variant across various tasks. Notably, Wang et al. (2022) examined the zero-shot generalization performance of each model along with various objectives, and Ding et al. (2024) analyzed the performance of causal decoder-only and prefix decoder-only models in in-context learning. However, despite these diverse studies, there is a lack of research on encoder-only models that do not impose the constraint of causal attention for every next token prediction. Therefore, in this work, we analyze the characteristics of encoder-only next-token prediction (ENTP), comparing them with decoder-only models.",
                    "score": 0.8092091210425876,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 5409,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 254
                        },
                        {
                            "start": 255,
                            "end": 547
                        },
                        {
                            "start": 550,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1101
                        },
                        {
                            "start": 1102,
                            "end": 1244
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 43,
                            "end": 63,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 63,
                            "end": 86,
                            "matchedPaperCorpusId": "247951931"
                        },
                        {
                            "start": 285,
                            "end": 306,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 684,
                            "end": 702,
                            "matchedPaperCorpusId": "248118752"
                        },
                        {
                            "start": 802,
                            "end": 820,
                            "matchedPaperCorpusId": "260887420"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57177734375
                }
            ],
            "relevance_judgement": 0.57177734375,
            "relevance_judgment_input_expanded": "# Title: ENTP: Encoder-only Next Token Prediction\n# Venue: arXiv.org\n# Authors: Ethan Ewer, Daewon Chae, Thomas Zeng, Jinkyu Kim, Kangwook Lee\n## Abstract\nNext-token prediction is conventionally done using decoder-only Transformers with causal attention, as this approach allows for efficient reuse of keys and values. What if we were not compute-limited, should we still use decoder-only Transformers? In this work, we introduce Encoder-only Next Token Prediction (ENTP). We explore the differences between ENTP and decoder-only Transformers in expressive power and complexity, highlighting potential advantages of ENTP in settings with unbounded compute. We introduce the $\\operatorname{Count3}$ task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot. Finally, we empirically demonstrate the superior performance of ENTP across representative tasks where next-token prediction based Transformers can be evaluated, including addition, in-context learning, and language modeling.\n## RELATED WORK\nIn contrast, the causal decoder-only model (Brown et al., 2020;Chowdhery et al., 2023) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2020;Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants). \n\nWith the development of these models, recent studies have investigated the performance of each variant across various tasks. Notably, Wang et al. (2022) examined the zero-shot generalization performance of each model along with various objectives, and Ding et al. (2024) analyzed the performance of causal decoder-only and prefix decoder-only models in in-context learning. However, despite these diverse studies, there is a lack of research on encoder-only models that do not impose the constraint of causal attention for every next token prediction. Therefore, in this work, we analyze the characteristics of encoder-only next-token prediction (ENTP), comparing them with decoder-only models.",
            "reference_string": "[273025546 | Ewer et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 69,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.05063",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.05063, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "151488390",
                    "name": "Gerald Woo"
                },
                {
                    "authorId": "2257133300",
                    "name": "Chenghao Liu"
                },
                {
                    "authorId": "2257090206",
                    "name": "Akshat Kumar"
                },
                {
                    "authorId": "36187119",
                    "name": "Doyen Sahoo"
                }
            ],
            "abstract": "Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size. Accompanying these datasets and results is a suite of comprehensive benchmark results comparing classical and deep learning baselines to our pre-trained method - achieving a 27% reduction in error on the largest dataset. Code and datasets can be found https://github.com/SalesforceAIResearch/pretrain-time-series-cloudops.",
            "corpus_id": 263831258,
            "sentences": [
                {
                    "corpus_id": "263831258",
                    "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
                    "text": "While the masked encoder introduced in BERT (Devlin et al., 2019) was used in pre-training, masked reconstruction was not used in downstream tasks which mainly focused on obtaining a representation of the entire input. Thus, they focused on the bidirectional encoder architecture, and did not consider other attention masking schemes. \n\nCausal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. Dong et al. (2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example. Figure 10 illustrates the various attention masking schemes for the masked encoder architecture.",
                    "score": 0.5363789541679107,
                    "section_title": "D.3 ATTENTION MASKS",
                    "char_start_offset": 44444,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 334
                        },
                        {
                            "start": 337,
                            "end": 487
                        },
                        {
                            "start": 488,
                            "end": 616
                        },
                        {
                            "start": 617,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1134
                        },
                        {
                            "start": 1135,
                            "end": 1231
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 44,
                            "end": 65,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 488,
                            "end": 506,
                            "matchedPaperCorpusId": "147704286"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56689453125
                }
            ],
            "relevance_judgement": 0.56689453125,
            "relevance_judgment_input_expanded": "# Title: Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain\n# Venue: arXiv.org\n# Authors: Gerald Woo, Chenghao Liu, Akshat Kumar, Doyen Sahoo\n## Abstract\nTime series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size. Accompanying these datasets and results is a suite of comprehensive benchmark results comparing classical and deep learning baselines to our pre-trained method - achieving a 27% reduction in error on the largest dataset. Code and datasets can be found https://github.com/SalesforceAIResearch/pretrain-time-series-cloudops.\n## D.3 ATTENTION MASKS\nWhile the masked encoder introduced in BERT (Devlin et al., 2019) was used in pre-training, masked reconstruction was not used in downstream tasks which mainly focused on obtaining a representation of the entire input. Thus, they focused on the bidirectional encoder architecture, and did not consider other attention masking schemes. \n\nCausal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. Dong et al. (2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example. Figure 10 illustrates the various attention masking schemes for the masked encoder architecture.",
            "reference_string": "[263831258 | Woo et al. | 2023 | Citations: 14]"
        },
        {
            "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
            "venue": "AACL",
            "year": 2022,
            "reference_count": 69,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.11807",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.11807, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66122912",
                    "name": "Yingbo Gao"
                },
                {
                    "authorId": "1474566597",
                    "name": "Christian Herold"
                },
                {
                    "authorId": "48598969",
                    "name": "Zijian Yang"
                },
                {
                    "authorId": "145322333",
                    "name": "H. Ney"
                }
            ],
            "abstract": "Encoder-decoder architecture is widely adopted for sequence-to-sequence modeling tasks. For machine translation, despite the evolution from long short-term memory networks to Transformer networks, plus the introduction and development of attention mechanism, encoder-decoder is still the de facto neural network architecture for state-of-the-art models. While the motivation for decoding information from some hidden space is straightforward, the strict separation of the encoding and decoding steps into an encoder and a decoder in the model architecture is not necessarily a must. Compared to the task of autoregressive language modeling in the target language, machine translation simply has an additional source sentence as context. Given the fact that neural language models nowadays can already handle rather long contexts in the target language, it is natural to ask whether simply concatenating the source and target sentences and training a language model to do translation would work. In this work, we investigate the aforementioned concept for machine translation. Specifically, we experiment with bilingual translation, translation with additional target monolingual data, and multilingual translation. In all cases, this alternative approach performs on par with the baseline encoder-decoder Transformer, suggesting that an encoder-decoder architecture might be redundant for neural machine translation.",
            "corpus_id": 253080830,
            "sentences": [
                {
                    "corpus_id": "253080830",
                    "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
                    "text": "In the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations. That said, later we show through experiments, that for the task of MT, it is clearly better to use a full attention mask for C in TLM. \n\nThe matrix A in Fig. 1 is whitened throughout this work, because we do not allow the source positions attend to target positions. However, theoretically, when decoding position i, one could allow all source positions 1, 2, ..., J to attend to all previous target positions 1, 2, ..., i \u2212 1. This can be done by using a (J + I) \u00d7 (J + I) \u00d7 I attention 2 The exact format of the tags is not important so long as uniquely identifiable translation direction tags are used, be it source and target tags like <s>, </s>, <t>, </t>, or direction tags like <s2t>, or even only the target language tag <t>. While the later two reduce the total sequence length, the former is more versatile when data from multiple languages or multiple modalities is considered. mask tensor.",
                    "score": 0.5694799491169129,
                    "section_title": "On the Attention Mechanism",
                    "char_start_offset": 9099,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 316
                        },
                        {
                            "start": 317,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 685
                        },
                        {
                            "start": 686,
                            "end": 866
                        },
                        {
                            "start": 867,
                            "end": 1151
                        },
                        {
                            "start": 1152,
                            "end": 1286
                        },
                        {
                            "start": 1289,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1885
                        },
                        {
                            "start": 1886,
                            "end": 2040
                        },
                        {
                            "start": 2041,
                            "end": 2053
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 28,
                            "end": 50,
                            "matchedPaperCorpusId": "13756489"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.56640625
                }
            ],
            "relevance_judgement": 0.56640625,
            "relevance_judgment_input_expanded": "# Title: Is Encoder-Decoder Redundant for Neural Machine Translation?\n# Venue: AACL\n# Authors: Yingbo Gao, Christian Herold, Zijian Yang, H. Ney\n## Abstract\nEncoder-decoder architecture is widely adopted for sequence-to-sequence modeling tasks. For machine translation, despite the evolution from long short-term memory networks to Transformer networks, plus the introduction and development of attention mechanism, encoder-decoder is still the de facto neural network architecture for state-of-the-art models. While the motivation for decoding information from some hidden space is straightforward, the strict separation of the encoding and decoding steps into an encoder and a decoder in the model architecture is not necessarily a must. Compared to the task of autoregressive language modeling in the target language, machine translation simply has an additional source sentence as context. Given the fact that neural language models nowadays can already handle rather long contexts in the target language, it is natural to ask whether simply concatenating the source and target sentences and training a language model to do translation would work. In this work, we investigate the aforementioned concept for machine translation. Specifically, we experiment with bilingual translation, translation with additional target monolingual data, and multilingual translation. In all cases, this alternative approach performs on par with the baseline encoder-decoder Transformer, suggesting that an encoder-decoder architecture might be redundant for neural machine translation.\n## On the Attention Mechanism\nIn the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations. That said, later we show through experiments, that for the task of MT, it is clearly better to use a full attention mask for C in TLM. \n\nThe matrix A in Fig. 1 is whitened throughout this work, because we do not allow the source positions attend to target positions. However, theoretically, when decoding position i, one could allow all source positions 1, 2, ..., J to attend to all previous target positions 1, 2, ..., i \u2212 1. This can be done by using a (J + I) \u00d7 (J + I) \u00d7 I attention 2 The exact format of the tags is not important so long as uniquely identifiable translation direction tags are used, be it source and target tags like <s>, </s>, <t>, </t>, or direction tags like <s2t>, or even only the target language tag <t>. While the later two reduce the total sequence length, the former is more versatile when data from multiple languages or multiple modalities is considered. mask tensor.",
            "reference_string": "[253080830 | Gao et al. | 2022 | Citations: 4]"
        },
        {
            "title": "A Survey on Large Language Models for Code Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 245,
            "citation_count": 197,
            "influential_citation_count": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.00515, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294682530",
                    "name": "Juyong Jiang"
                },
                {
                    "authorId": "2304542351",
                    "name": "Fan Wang"
                },
                {
                    "authorId": "2305041631",
                    "name": "Jiasi Shen"
                },
                {
                    "authorId": "2304525068",
                    "name": "Sungju Kim"
                },
                {
                    "authorId": "2257349580",
                    "name": "Sunghun Kim"
                }
            ],
            "abstract": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.",
            "corpus_id": 270214176,
            "sentences": [
                {
                    "corpus_id": "270214176",
                    "title": "A Survey on Large Language Models for Code Generation",
                    "text": "where x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
                    "score": 0.5912633035627175,
                    "section_title": "Pre-training",
                    "char_start_offset": 42181,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 65
                        },
                        {
                            "start": 65,
                            "end": 128
                        },
                        {
                            "start": 128,
                            "end": 280
                        },
                        {
                            "start": 280,
                            "end": 497
                        },
                        {
                            "start": 497,
                            "end": 680
                        },
                        {
                            "start": 680,
                            "end": 765
                        },
                        {
                            "start": 765,
                            "end": 782
                        },
                        {
                            "start": 782,
                            "end": 905
                        },
                        {
                            "start": 907,
                            "end": 1016
                        },
                        {
                            "start": 1016,
                            "end": 1183
                        },
                        {
                            "start": 1183,
                            "end": 1309
                        },
                        {
                            "start": 1309,
                            "end": 1470
                        },
                        {
                            "start": 1470,
                            "end": 1493
                        },
                        {
                            "start": 1493,
                            "end": 1750
                        },
                        {
                            "start": 1750,
                            "end": 1902
                        },
                        {
                            "start": 1902,
                            "end": 1962
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1300,
                            "end": 1304,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1304,
                            "end": 1308,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1701,
                            "end": 1706,
                            "matchedPaperCorpusId": "237386541"
                        },
                        {
                            "start": 1744,
                            "end": 1749,
                            "matchedPaperCorpusId": "258685677"
                        },
                        {
                            "start": 1763,
                            "end": 1768,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1780,
                            "end": 1785,
                            "matchedPaperCorpusId": "237386541"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53857421875
                }
            ],
            "relevance_judgement": 0.53857421875,
            "relevance_judgment_input_expanded": "# Title: A Survey on Large Language Models for Code Generation\n# Venue: arXiv.org\n# Authors: Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim\n## Abstract\nLarge Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.\n## Pre-training\nwhere x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
            "reference_string": "[270214176 | Jiang et al. | 2024 | Citations: 197]"
        },
        {
            "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.00026, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2289612023",
                    "name": "Samuel J. K. Chin"
                },
                {
                    "authorId": "2289612073",
                    "name": "Matthias Winkenbach"
                },
                {
                    "authorId": "2289782615",
                    "name": "Akash Srivastava"
                }
            ],
            "abstract": "In this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a supervised manner on computationally inexpensive, sub-optimal MCVRP solutions obtained algorithmically. Through comprehensive computational experiments, we show that FM-MCVRP produces better MCVRP solutions than the training data and generalizes to larger sized problem instances not seen during training. Even when compared to near-optimal solutions from state-of-the-art heuristics, FM-MCVRP yields competitive results despite being trained on inferior data. For instance, for 400-customer problems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our results further demonstrate that unlike prior works in the literature, FM-MCVRP is a unified model, which performs consistently and reliably on a range of problem instance sizes and parameter values such as the vehicle capacity.",
            "corpus_id": 268201845,
            "sentences": [
                {
                    "corpus_id": "268201845",
                    "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
                    "text": "In defining our model architecture and the objective for optimizing the model, we build on the insights gathered by Raffel et al. (2020) and rely heavily on their definitions and notations. Specifically, Raffel et al. (2020) consider two distinct objectives for unsupervised pre-training. First, they pursue a denoising objective (see, Devlin et al. 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other. The model then attempts to predict the correct tokens that are masked, corrupted or left unedited at these positions. Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens. Through extensive experiments, Raffel et al. (2020) conclude that the combination of an encoder-decoder architecture with a denoising objective yields the highest performance on a set of benchmark NLP tasks (cf. Table 2 in Raffel et al. 2020). Given the generally superior performance exhibited by an encoder-decoder architecture in their analyses, we also adopt this architecture in our work. However, since a denoising objective is not suitable for our problem structure, we rely on an LM objective. We further elaborate on our architecture and the objective in the next paragraphs. \n\nEncoder-decoder architecture. Following the insights from Raffel et al. (2020), we adopt the encoderdecoder architecture. We first introduce two concepts at a high level that are necessary to understand the encoder-decoder architecture: the embedding layer and the output layer. In NLP, words are represented as a one-hot vector and ML models typically operate on feature vectors that contain continuous values.",
                    "score": 0.5909228974293755,
                    "section_title": "Model Architecture and Objective",
                    "char_start_offset": 32202,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 189
                        },
                        {
                            "start": 190,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 606
                        },
                        {
                            "start": 607,
                            "end": 745
                        },
                        {
                            "start": 748,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 924
                        },
                        {
                            "start": 925,
                            "end": 1007
                        },
                        {
                            "start": 1008,
                            "end": 1192
                        },
                        {
                            "start": 1193,
                            "end": 1404
                        },
                        {
                            "start": 1405,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1777
                        },
                        {
                            "start": 1780,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1901
                        },
                        {
                            "start": 1902,
                            "end": 2058
                        },
                        {
                            "start": 2059,
                            "end": 2191
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 116,
                            "end": 136,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 204,
                            "end": 224,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 336,
                            "end": 355,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 1224,
                            "end": 1244,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1416,
                            "end": 1435,
                            "matchedPaperCorpusId": "204838007"
                        },
                        {
                            "start": 1838,
                            "end": 1858,
                            "matchedPaperCorpusId": "204838007"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.51171875
                }
            ],
            "relevance_judgement": 0.51171875,
            "relevance_judgment_input_expanded": "# Title: Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem\n# Venue: arXiv.org\n# Authors: Samuel J. K. Chin, Matthias Winkenbach, Akash Srivastava\n## Abstract\nIn this paper, we present the Foundation Model for the Montreal Capacitated Vehicle Routing Problem (FM-MCVRP), a novel Deep Learning (DL) model that approximates high-quality solutions to a variant of the Capacitated Vehicle Routing Problem (CVRP) that characterizes many real-world applications. The so-called Montreal Capacitated Vehicle Routing Problem (MCVRP), first formally described by Bengio et al. (2021), is defined on a fixed and finite graph, which is analogous to a city. Each MCVRP instance is essentially the sub-graph connecting a randomly sampled subset of the nodes in the fixed graph, which represent a set of potential addresses in a real-world delivery problem on a given day. Our work exploits this problem structure to frame the MCVRP as an analogous Natural Language Processing (NLP) task. Specifically, we leverage a Transformer architecture embedded in a Large Language Model (LLM) framework to train our model in a supervised manner on computationally inexpensive, sub-optimal MCVRP solutions obtained algorithmically. Through comprehensive computational experiments, we show that FM-MCVRP produces better MCVRP solutions than the training data and generalizes to larger sized problem instances not seen during training. Even when compared to near-optimal solutions from state-of-the-art heuristics, FM-MCVRP yields competitive results despite being trained on inferior data. For instance, for 400-customer problems, FM-MCVRP solutions on average fall within 2% of the benchmark. Our results further demonstrate that unlike prior works in the literature, FM-MCVRP is a unified model, which performs consistently and reliably on a range of problem instance sizes and parameter values such as the vehicle capacity.\n## Model Architecture and Objective\nIn defining our model architecture and the objective for optimizing the model, we build on the insights gathered by Raffel et al. (2020) and rely heavily on their definitions and notations. Specifically, Raffel et al. (2020) consider two distinct objectives for unsupervised pre-training. First, they pursue a denoising objective (see, Devlin et al. 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other. The model then attempts to predict the correct tokens that are masked, corrupted or left unedited at these positions. Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens. Through extensive experiments, Raffel et al. (2020) conclude that the combination of an encoder-decoder architecture with a denoising objective yields the highest performance on a set of benchmark NLP tasks (cf. Table 2 in Raffel et al. 2020). Given the generally superior performance exhibited by an encoder-decoder architecture in their analyses, we also adopt this architecture in our work. However, since a denoising objective is not suitable for our problem structure, we rely on an LM objective. We further elaborate on our architecture and the objective in the next paragraphs. \n\nEncoder-decoder architecture. Following the insights from Raffel et al. (2020), we adopt the encoderdecoder architecture. We first introduce two concepts at a high level that are necessary to understand the encoder-decoder architecture: the embedding layer and the output layer. In NLP, words are represented as a one-hot vector and ML models typically operate on feature vectors that contain continuous values.",
            "reference_string": "[268201845 | Chin et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 50,
            "citation_count": 15,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.06744",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.06744, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2141320070",
                    "name": "Minsoo Kim"
                },
                {
                    "authorId": "2144376191",
                    "name": "Sihwa Lee"
                },
                {
                    "authorId": "2265920992",
                    "name": "Janghwan Lee"
                },
                {
                    "authorId": "2158125346",
                    "name": "S. Hong"
                },
                {
                    "authorId": "2180828053",
                    "name": "Duhyeuk Chang"
                },
                {
                    "authorId": "66936521",
                    "name": "Wonyong Sung"
                },
                {
                    "authorId": "2506452",
                    "name": "Jungwook Choi"
                }
            ],
            "abstract": "Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.",
            "corpus_id": 260886785,
            "sentences": [
                {
                    "corpus_id": "260886785",
                    "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                    "text": "In this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.",
                    "score": 0.599604176163786,
                    "section_title": "Quantization Challenges on GLMs",
                    "char_start_offset": 9244,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 193,
                            "end": 244
                        },
                        {
                            "start": 245,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 500
                        },
                        {
                            "start": 501,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 857
                        },
                        {
                            "start": 860,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1539
                        },
                        {
                            "start": 1540,
                            "end": 1650
                        },
                        {
                            "start": 1651,
                            "end": 1813
                        },
                        {
                            "start": 1814,
                            "end": 1997
                        },
                        {
                            "start": 2000,
                            "end": 2031
                        },
                        {
                            "start": 2032,
                            "end": 2301
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5078125
                }
            ],
            "relevance_judgement": 0.5078125,
            "relevance_judgment_input_expanded": "# Title: Token-Scaled Logit Distillation for Ternary Weight Generative Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Minsoo Kim, Sihwa Lee, Janghwan Lee, S. Hong, Duhyeuk Chang, Wonyong Sung, Jungwook Choi\n## Abstract\nGenerative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and no loss of accuracy in a reasoning task.\n## Quantization Challenges on GLMs\nIn this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.",
            "reference_string": "[260886785 | Kim et al. | 2023 | Citations: 15]"
        },
        {
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "venue": "IEEE Access",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2024.3420710",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.14283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2222734467",
                    "name": "Andrea Busto-Casti\u00f1eira"
                },
                {
                    "authorId": "2323809078",
                    "name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o"
                },
                {
                    "authorId": "1405165681",
                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                },
                {
                    "authorId": "2034282614",
                    "name": "Francisco de Arriba-P\u00e9rez"
                }
            ],
            "abstract": "In recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.",
            "corpus_id": 270832367,
            "sentences": [
                {
                    "corpus_id": "270832367",
                    "title": "Predictability and Causality in Spanish and English Natural Language Generation",
                    "text": "The contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
                    "score": 0.7614826609695546,
                    "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                    "char_start_offset": 5984,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 74
                        },
                        {
                            "start": 75,
                            "end": 190
                        },
                        {
                            "start": 191,
                            "end": 305
                        },
                        {
                            "start": 308,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 456
                        },
                        {
                            "start": 459,
                            "end": 507
                        },
                        {
                            "start": 510,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 738
                        },
                        {
                            "start": 739,
                            "end": 1012
                        },
                        {
                            "start": 1015,
                            "end": 1129
                        },
                        {
                            "start": 1132,
                            "end": 1211
                        },
                        {
                            "start": 1212,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1441
                        },
                        {
                            "start": 1442,
                            "end": 1514
                        },
                        {
                            "start": 1515,
                            "end": 1700
                        },
                        {
                            "start": 1701,
                            "end": 1932
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1118,
                            "end": 1122,
                            "matchedPaperCorpusId": "219463386"
                        },
                        {
                            "start": 1124,
                            "end": 1128,
                            "matchedPaperCorpusId": "209832341"
                        },
                        {
                            "start": 1138,
                            "end": 1142,
                            "matchedPaperCorpusId": "234785837"
                        },
                        {
                            "start": 1695,
                            "end": 1699,
                            "matchedPaperCorpusId": "265629619"
                        },
                        {
                            "start": 1794,
                            "end": 1798,
                            "matchedPaperCorpusId": "231924507"
                        },
                        {
                            "start": 1800,
                            "end": 1804,
                            "matchedPaperCorpusId": "231715684"
                        },
                        {
                            "start": 1806,
                            "end": 1810,
                            "matchedPaperCorpusId": "247126308"
                        },
                        {
                            "start": 1850,
                            "end": 1854,
                            "matchedPaperCorpusId": "218487230"
                        },
                        {
                            "start": 1882,
                            "end": 1886,
                            "matchedPaperCorpusId": "208224776"
                        },
                        {
                            "start": 1921,
                            "end": 1925,
                            "matchedPaperCorpusId": "210839508"
                        },
                        {
                            "start": 1927,
                            "end": 1931,
                            "matchedPaperCorpusId": "212657570"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.505859375
                }
            ],
            "relevance_judgement": 0.505859375,
            "relevance_judgment_input_expanded": "# Title: Predictability and Causality in Spanish and English Natural Language Generation\n# Venue: IEEE Access\n# Authors: Andrea Busto-Casti\u00f1eira, Francisco Javier Gonz\u00e1lez-Casta\u00f1o, Silvia Garc\u00eda-M\u00e9ndez, Francisco de Arriba-P\u00e9rez\n## Abstract\nIn recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.\n## A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS\nThe contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
            "reference_string": "[270832367 | Busto-Castineira et al. | 2024 | Citations: 1]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "Prefix language models are also decoder-only-based models but differ in the masking mechanism. Instead of a causal mask, a fully visible mask is used for the prefix part of the input sequence, and a causal mask is used for the target sequence. \n\nFor example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks.",
            "score": 0.9933250455227525,
            "section_title": "Prefix (Non-Causal) Language Model",
            "char_start_offset": 19013,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1798
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97265625
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture. We refer to this architecture as non-causal decoder-only (ND) . Sometimes called a prefix language model, this approach was introduced by [Liu et al., 2018] and was later explored as an architectural variant by [Raffel et al., 2020, Wu et al., 2021]. Despite single-task finetuning performance nearly on par with encoder-decoder models [Raffel et al., 2020], it has seen limited adoption in the literature. \n\nEncoder-only. As an aside, we note that another popular architectural variant is to only use a Transformer encoder layer stack. This model architecture underlies the ubiquitous BERT [Devlin et al., 2018] and its derivatives. However, this architecture is limited to producing the same number of tokens as it was fed as input, considerably limiting its applicability and making it only rarely used in the zero-shot setting [Tamborrino et al., 2020]. We therefore omit it from consideration. For full language modeling, all tokens in a sequence are used during training. For prefix language modeling, we randomly select a prefix size, and hence only half of the tokens are used on average to derive the loss. At inference time, the prefix would be over the input/conditioning information. Finally, for masked language modeling, we mask 15% of the tokens, in spans of 3 tokens on average. We use sentinel tokens to replace spans (not represented here), and the model outputs subsequently each sentinel followed by its prediction of the content masked by the sentinel. \n\nComparisons across architectures. Decoder-only models process a single sequence consisting of the concatenation of the input and target text. On the other hand, in an encoder-decoder, the encoder processes only the input and the decoder processes only the target.",
            "score": 0.8911057584907371,
            "section_title": "Architectures",
            "char_start_offset": 11973,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1822
                },
                {
                    "start": 1823,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 451,
                    "end": 469,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 524,
                    "end": 544,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 649,
                    "end": 670,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59765625
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "In contrast, the causal decoder-only model (Brown et al., 2020;Chowdhery et al., 2023) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2020;Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants). \n\nWith the development of these models, recent studies have investigated the performance of each variant across various tasks. Notably, Wang et al. (2022) examined the zero-shot generalization performance of each model along with various objectives, and Ding et al. (2024) analyzed the performance of causal decoder-only and prefix decoder-only models in in-context learning. However, despite these diverse studies, there is a lack of research on encoder-only models that do not impose the constraint of causal attention for every next token prediction. Therefore, in this work, we analyze the characteristics of encoder-only next-token prediction (ENTP), comparing them with decoder-only models.",
            "score": 0.8092091210425876,
            "section_title": "RELATED WORK",
            "char_start_offset": 5409,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 547
                },
                {
                    "start": 550,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1244
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 63,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 63,
                    "end": 86,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 285,
                    "end": 306,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 684,
                    "end": 702,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "260887420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57177734375
        },
        {
            "corpus_id": "261101210",
            "title": "Improving Translation Faithfulness of Large Language Models via Augmenting Instructions",
            "text": "Decoder-only architecture is designed for unified text generation tasks, including prefix decoder and causal decoder (Raffel et al., 2020). Most of the LLMs use the causal decoder architecture because of the wide observation of scaling law on the causal decoder. However, a more comprehensive investigation of other architectures' performance at a large scale is still lacking. (Zhao et al., 2023) A causal language model is composed of a stack of causal decoder layers. The function of the multihead attention mechanism is to combine the hidden representation of each position with contextual information. With a causal attention mask, text generation tasks in any format can be unified in training and decoding states. In detail, let m, n be the position indexes of two tokens, and q, k, v, o are, respectively, query, key, value, and output representation, and the length of input tokens is N . In a causal language model, when m > n, the attention score a m,n will be masked. \n\n(2)",
            "score": 0.803871583891449,
            "section_title": "Causal Language Model",
            "char_start_offset": 9036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 985
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 138,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "270702559",
            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
            "text": "Typical architectures for LLMs can be categorized as encoder-decoder, causal decoder, and prefix decoder (Wang and Hesslow, 2022;Zhao et al., 2023).Among them, the causal decoder architecture is the most frequently used by various LLMs, such as OPT (Susan Zhang et al., 2023), LLAMA (Touvron et al., 2023a), BLOOM (Scao et al., 2023) due to its superior zero-shot and fewshot generalization capacity (Wang and Hesslow, 2022) and the effectiveness of scaling law (Brown et al., 2020;Kaplan et al., 2020).The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.Representative LLMs based on prefix decoders include GLM-130B (Zeng et al., 2023) and U-PaLM (Tay et al., 2022).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
            "score": 0.7953885436312866,
            "section_title": "Architecture of LLMs",
            "char_start_offset": 7065,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 503
                },
                {
                    "start": 503,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 749
                },
                {
                    "start": 751,
                    "end": 898
                },
                {
                    "start": 898,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1243
                },
                {
                    "start": 1243,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1430
                },
                {
                    "start": 1432,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1756
                },
                {
                    "start": 1756,
                    "end": 1889
                }
            ],
            "ref_mentions": [
                {
                    "start": 1513,
                    "end": 1535,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "244119798",
            "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation",
            "text": "Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens.",
            "score": 0.7829994435109568,
            "section_title": "Decoder Self-attention Visibility",
            "char_start_offset": 14430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 607
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.467041015625
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020]. T5 in particular was recently used as the foundation for the T0 model [Sanh et al., 2021], which leveraged large-scale multitask finetuning to achieve strong zero-shot generalization, outperforming decoder-only models an order of magnitude larger. \n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left). On the one hand, this means that the representation for any conditioning text is inherently weaker; on the other hand, it yields a simpler architecture that is naturally suited to a standard autoregressive next-step-prediction pretraining objective. We refer to this architecture as causal decoder-only (CD) . Most notably, the CD architecture makes up the backbone of the GPT series of models [Radford et al., 2018, 2019, Brown et al., 2020] as well as many other recent record-breaking LLMs [Zeng et al., 2021, Kim et al., 2021, Smith et al., 2022, Thoppilan et al., 2022, Rae et al., 2021, Hoffmann et al., 2022, Chowdhery et al., 2022]. \n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e.",
            "score": 0.7819578717336095,
            "section_title": "Architectures",
            "char_start_offset": 10211,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1936
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 135,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1321,
                    "end": 1342,
                    "matchedPaperCorpusId": "49313245"
                },
                {
                    "start": 1438,
                    "end": 1456,
                    "matchedPaperCorpusId": "237485423"
                },
                {
                    "start": 1476,
                    "end": 1500,
                    "matchedPaperCorpusId": "238582964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52392578125
        },
        {
            "corpus_id": "275336136",
            "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
            "text": "During training of encoder-decoder models, the target tokens in the decoder are causally masked to prevent them from looking at future information. The causal masking can be represented as a mask matrix M : \n\nthat is summed with the attention matrix before the softmax operator to make sure that each element i can only attend to itself and elements before it (i.e., j \u2264 i), obtaining \n\nIn standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,...,i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models.",
            "score": 0.7725106142152471,
            "section_title": "Audio Causal Masking",
            "char_start_offset": 10649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 206
                },
                {
                    "start": 209,
                    "end": 384
                },
                {
                    "start": 387,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 726
                },
                {
                    "start": 727,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1230
                }
            ],
            "ref_mentions": [
                {
                    "start": 745,
                    "end": 762,
                    "matchedPaperCorpusId": "259501685"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68798828125
        },
        {
            "corpus_id": "270832367",
            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
            "text": "The contextual awareness of a transformer is controlled by self-attention. The base concept behind this attention mechanism is a mapping of a query (q) into pairs of keys (k) and values (v). By respectively denoting the queries', keys', and value sets' matrices as Q, K and V, we define self-attention as: \n\nTransformers, rather than a single attention function, project queries, keys, and values onto h separate heads. This is called multi-head attention: \n\n(2) By denoting each head attention function as: \n\nwhere W Q i , W K i , W V i and W O are parameter projection matrices for the queries, keys, values, and output respectively. This attention mechanism is present in all the layers of both the encoder and the decoder, if present. While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder. \n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], \n\n[22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31].",
            "score": 0.7614826609695546,
            "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
            "char_start_offset": 5984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1129
                },
                {
                    "start": 1132,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1932
                }
            ],
            "ref_mentions": [
                {
                    "start": 1118,
                    "end": 1122,
                    "matchedPaperCorpusId": "219463386"
                },
                {
                    "start": 1124,
                    "end": 1128,
                    "matchedPaperCorpusId": "209832341"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "234785837"
                },
                {
                    "start": 1695,
                    "end": 1699,
                    "matchedPaperCorpusId": "265629619"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "231924507"
                },
                {
                    "start": 1800,
                    "end": 1804,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 1806,
                    "end": 1810,
                    "matchedPaperCorpusId": "247126308"
                },
                {
                    "start": 1850,
                    "end": 1854,
                    "matchedPaperCorpusId": "218487230"
                },
                {
                    "start": 1882,
                    "end": 1886,
                    "matchedPaperCorpusId": "208224776"
                },
                {
                    "start": 1921,
                    "end": 1925,
                    "matchedPaperCorpusId": "210839508"
                },
                {
                    "start": 1927,
                    "end": 1931,
                    "matchedPaperCorpusId": "212657570"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.505859375
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "Model Architecture In representation learning with transformers (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020), two schemes of modeling are prevalent which differ in their attention masks for the contextualization of hidden vectors. For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2020). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks. \n\nLearning Algorithm The choice of encoder or decoder-based model architectures typically guides the selection of learning algorithms for language modeling. Encoder-based models may be trained with the task of masked language modeling in the form of denoising span corruptions (Devlin et al., 2019;Raffel et al., 2020).",
            "score": 0.7438494283572481,
            "section_title": "COMPONENTS: ARCHITECTURE, OBJECTIVE, SAMPLING, DATA",
            "char_start_offset": 9494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 64,
                    "end": 85,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 85,
                    "end": 104,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 104,
                    "end": 124,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 995,
                    "end": 1016,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "Pre-trained generative models often employ three types of attention mask patterns: bidirectional, causal, and prefix masking (Figures 3 and 4 in Raffel et al. (2020)). For our focus on non-generative tasks, we limited the explorations to causal attention and bidirectional attention. \n\nBidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence. \n\nGiven that Gemma models are pre-trained with causal attention, we investigated the impact of both bidirectional and causal attention masks during fine-tuning to maximize the performance of Gemma Encoder models.",
            "score": 0.7418955335720548,
            "section_title": "Attention Masking",
            "char_start_offset": 4030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1538
                },
                {
                    "start": 1541,
                    "end": 1751
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 165,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 347,
                    "end": 368,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92333984375
        },
        {
            "corpus_id": "254069435",
            "title": "BARTSmiles: Generative Masked Language Models for Molecular Representations",
            "text": "Wang et al. (2022b) argues that for zeroshot or k-shot prompting, causal language models with uni-directional attention is optimal, while bidirectionality (in both context and attention masks) is the primary driver of success in the fine-tuning setting. \n\nWithin the fine-tuning setting, there are different objectives conditioned on model type. For encoderbased models, the masked language modeling objective as initially proposed in Devlin et al. (2018) and further refined in Liu et al. (2019). The downside of encoder models is the inability to do generative fine-tuning, which led to the introduction of the denoising model for the encoder-decoder models Lewis et al. (2019). Decoder causal models are problematic because they are not bidirectional, although recently proposed objectives such as causal masking in Aghajanyan et al. (2022) are bidirectional in context but not in attention. \n\nFor BARTSmiles we select both the denoising objective and architecture from Lewis et al. (2019) while previous works have focused on encoder-only (Wang et al., 2019;Chithrananda et al., 2020).",
            "score": 0.7308166803572588,
            "section_title": "RELATED WORK",
            "char_start_offset": 5057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 894
                },
                {
                    "start": 897,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 1043,
                    "end": 1062,
                    "matchedPaperCorpusId": "202159174"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "246035820",
            "title": "CM3: A Causal Masked Multimodal Model of the Internet",
            "text": "For example, masked encoder-only models such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) excel in non-generative fine-tuning tasks. Masked encoder-decoder models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) excel in both discriminative and generative fine-tuning. Brown et al. (2020) on the other hand, showed that causal language models (de-facto, decoder only) are capable of non-trivial performance without the need of fine-tuning by simply prompting with appropriate string to control the generated outputs Radford et al. (2019); Brown et al. (2020); Artetxe et al. (2021). \n\nThere are pros and cons to both masked and causal language modeling in the context of prompting. Masking offers the critical ability to encode bi-directionality within the prompts at the cost of only decoding roughly 15% of the tokens of the input sequence during training (Devlin et al., 2018;Liu et al., 2019;Lewis et al., 2019). Conversely, decoder-only causal language models decode every token in the input sequence in the training but are typically limited to left-only contexts. Empirically, more work has also been done on scaling causal decoder-only rather than their counterparts. \n\nIn an effort to get most of the best of both worlds, we introduce a novel objective that combines the benefit of per-token generation with optional bi-directionality specifically tailored to prompting. For a document of size s we select n \u223c Clamp(Poisson(1), 1, 16) masks and for each of those masks we select span m \u223c (U nif orm(0, s), U nif orm(0, s)) which does not intersect with any other m. These values are chosen to, on average, select relatively few relatively long spans, which we expect will allow the model to learn to infill long spans.",
            "score": 0.7235716696037156,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1205
                },
                {
                    "start": 1208,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 567,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "277349741",
            "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
            "text": "Models like T5 (Raffel et al., 2020) and BART (Lewis et al., 2019) utilize both encoding and decoding mechanisms, enabling them to perform a wide range of tasks, including translation and summarization. The encoder applies stacked self-attention layers to encode the input sequence, and the decoder performs cross-attention on these representations and autoregressively generates the output. \n\n\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022). \n\nTo scale the capacity of LLMs efficiently, the Mixture of Experts (MoE) technique can be exploited to combine the above architectures, such as in Swich Transformer (Fedus et al., 2022) and GLaM (Du et al., 2022). MoE involves sparsely activating a subset of model parameters (the \"experts\") for each input, allowing the model to handle a vast number of parameters without incurring prohibitive computational costs. This is achieved by employing a trainable gating mechanism to route each input token to the most relevant subset of experts. \n\nApart from the mainstream Transformer architecture, there are also emerging architectures proposed to alleviate the inherent issues of Transformers (e.g., the quadratic complexity) such as State-Space Models (SSMs) (Gu et al., 2021), Mamba (Gu and Dao, 2023), and RWKV (Peng et al., 2023).",
            "score": 0.7102871368044792,
            "section_title": "Architecture",
            "char_start_offset": 34135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1061
                },
                {
                    "start": 1064,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 36,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 731,
                    "end": 756,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1228,
                    "end": 1248,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 1258,
                    "end": 1275,
                    "matchedPaperCorpusId": "245124124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3818359375
        },
        {
            "corpus_id": "231847072",
            "title": "Riiid! Answer Correctness Prediction Kaggle Challenge: 4th Place Solution Summary",
            "text": "Encoder and decoder layers are almost identical to those in the transformer original paper (Vaswani et al. 2017). \n\nThe differences reside in the multi-head attention (and selfattention) part. Fisrt of all, to prevent the current position from seeing future inputs, we must apply causal masks to the self-attention of encoder and decoder layers. This causal mask is computed from the timestamp sequence: a position at timestamp t 0 only attends to positions with timestamp t \u2265 t 0 (which means it can attend to itself and other interactions of the same container). Attention mask of the encoder output to the decoder is a little different because the decoder at a position cannot see user answer related inputs at that position. This means that the encoder output at a timestamp t 0 can only attend to decoder positions that are strictly in the future t > t 0 (i.e not including the current timestamp). \n\nThe other feature that gives the model some boost in performance is the Time-Weighted Multihead Attention, an improvement of the original Multihead Attention based on the observation that events in a distant past should attend less to the current position than recent events do. The idea is to decay the attention coefficients by a factor of T \u2212w ij where T ij = t j \u2212t i is the difference in timestamp of a position i and the position j it attends to and w is a non-negative trainable parameter. This is implemented by modifying the \"Scaled Dot-Product Attention\" of the encoders and decoders. In the original paper this \"Scaled Dot-Product Attention\" is: \n\nis in logarithmic scale with respect to the attention coefficients (the softmax of the dot prodcut), decaying the attention coefficients by a factor of T \u2212w ij is equivalent to substracting w log T ij from the dot product. In other words, the \"Scaled Dot-Product Attention\" can be rewritten as the following to take into account the time decay factor: \n\nwhere T is the timestamp difference matrix (T ij = t j \u2212 t i with t i , t j is the timestamp at the position i and j respectively).",
            "score": 0.7072459296070643,
            "section_title": "Attention And Self-Attention Mechanism",
            "char_start_offset": 6217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1561
                },
                {
                    "start": 1564,
                    "end": 1786
                },
                {
                    "start": 1787,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2049
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.168701171875
        },
        {
            "corpus_id": "276902726",
            "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation",
            "text": "It is important to note that: (i) when encoding X, we can modify its original causal attention mask M c to be fully-visible mask M f , similar to used in the encoderdecoder architecture, thus creating a variant of the CausalLM model known as the prefixLM model (Dong et al., 2019;Raffel et al., 2020); (ii) the fusion of source and target information is achieved by computing attention on concatenated representations of both, distinct from cross-attention, which we refer to in this paper as concat attention; (iii) the interaction between X and Y is Layer-Wise, rather than using only the top-layer representation of X as in the encoder-decoder architecture (TopOnly). \n\nThe overall comparison of these architectures is shown in Table 5. In practice, some modern deployment frameworks of LLMs (decoder-only models) explicitly separate encoding (prefilling) and decoding processes across distinct computational resources, making the architecture structurally resemble encoder-decoder models (Zhong et al., 2024;Patel et al., 2024). In this light, the so-called decoder-only model can be considered a variant of the encoder-decoder model, wherein the encoding function is implicitly integrated through shared parameters with the decoder. Conversely, one may view the encoder-decoder model as an extension of PrefixLM, with a more explicit division between encoding and decoding stages.",
            "score": 0.6925418169612179,
            "section_title": "Limitations",
            "char_start_offset": 34034,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1385
                }
            ],
            "ref_mentions": [
                {
                    "start": 261,
                    "end": 280,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 280,
                    "end": 300,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 992,
                    "end": 1012,
                    "matchedPaperCorpusId": "276409195"
                },
                {
                    "start": 1012,
                    "end": 1031,
                    "matchedPaperCorpusId": "265506047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28466796875
        },
        {
            "corpus_id": "276317556",
            "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs",
            "text": "While successful approaches are presented to mitigate this issue for encoder-only (Yang et al., 2022) and encoder-decoder (Yen et al., 2024;Cai et al., 2023) models, they leave decoder-only models, which account for most of the current LLMs, for more consideration. In contrast to transformer encoders that use bidirectional attention which is invariant by nature (Lee et al., 2019), transformer decoders use causal attention to learn causal relation signals, which is not invariant by nature (Haviv et al., 2022a). Therefore, positional bias for decoder-only models is known to stem from both positional encoding and causal attention mask (Yu et al., 2024;Wang et al., 2024) and is harder to mitigate.",
            "score": 0.6829735751376202,
            "section_title": "Introduction",
            "char_start_offset": 5414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 702
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 157,
                    "matchedPaperCorpusId": "259360659"
                },
                {
                    "start": 493,
                    "end": 514,
                    "matchedPaperCorpusId": "247839823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357421875
        },
        {
            "corpus_id": "270514433",
            "title": "EDC: Effective and Efficient Dialog Comprehension For Dialog State Tracking",
            "text": "Pre-trained language models built on multi-head attention mechanism have achieved state-of-theart performance on various natural language understanding and generation problems.They can be divided into three major categories: encoderonly models (Kenton and Toutanova, 2019;Liu et al., 2019), decoder-only auto-regressive models (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020) and encoder-decoder models (Vaswani et al., 2017;Lewis et al., 2020;Raffel et al., 2020).EDC is built on BART (Lewis et al., 2020), but can be extended to use any encoder-decoder language model as its backbone.It is also inspired by the causal attention mask construction of UniLM (Dong et al., 2019).",
            "score": 0.6819771590147912,
            "section_title": "Pre-trained Language Models",
            "char_start_offset": 6602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 482
                },
                {
                    "start": 482,
                    "end": 603
                },
                {
                    "start": 603,
                    "end": 694
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 373,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 442,
                    "end": 461,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 461,
                    "end": 481,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 503,
                    "end": 523,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 674,
                    "end": 693,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259765625
        },
        {
            "corpus_id": "256461326",
            "title": "Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation",
            "text": "In addition to the two sub-layers in the encoder layer, the decoder employ a third sub-layer, encoder-decoder attention (EDA), which performs multi-head attention over the outputs of the encoder and the current decoder layer. The masked multi-head attention (MMHA) has the same model structure as the multi-head attention in the encoder layer, except that it prevents positions from attending to subsequent positions. This masking ensures that the prediction for position i can depend only on the known outputs at positions before i, as the question is generated word by word. \n\nHere W Q de , W K de and W V de are the model parameters for the encoder-decoder attention.",
            "score": 0.6813653357741767,
            "section_title": "A.2 Question Decoder",
            "char_start_offset": 26865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 670
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.47412109375
        },
        {
            "corpus_id": "266164079",
            "title": "Absolute Position Embedding Learns Sinusoid-like Waves for Attention Based on Relative Position",
            "text": "This paper analyzed an encoder-only model (RoBERTa). In this section, we apply our methods to other transformer-based architectures: decoderonly and encoder-decoder. The target pre-trained models are GPT-2 (gpt2) for decoder-only and BART (facebook/bart-base) (Lewis et al., 2020) for encoder-decoder, both of which use absolute position embedding and are available on huggingface. To reduce the gap between architectures, when analyzing the decoders, we use the attention scores, which is the matrix before causal masking and softmax function are applied, instead of the attention weights. \n\nThe attention matrices of GPT-2 show two main patterns: one related to position and the other not. Figure 14a shows that the attention head of GPT-2 pays stronger attention to the closer tokens by concentrating attention on the backward tokens, which are masked in the subsequent processing. \n\nFigure 15 shows the result of clustering the attention scores of GPT-2 using k-means. The number of clusters was set to 2 and the inputs to k-means were t-offset traces (Eq.( 8)) from t = \u221230 to 30. \n\nThe heads that depend on position are found in the lower layers. Along \u00a74.4, we investigate the relationship between the frequency of the hidden state and the angle of the rotation matrix inherent in the parameters of attention head. Figure 16a differs from the case of RoBERTa (Figure 10) in that the peaks appear horizontally rather than holding a constant ratio between frequency and angle. Furthermore, according to Figure 13, the dominant components in position embedding of GPT-2 are those with frequencies below 10, i.e., with periods longer than 51.2 (= 512/10) tokens. These differences from RoBERTa imply that even when the attention is focused within a few tokens in a particular head, it cannot be attributed to the mechanism described in \u00a74.3. \n\nIn BART, the tendency in the positional dependence of attention differs between encoder and decoder.",
            "score": 0.6762881233161455,
            "section_title": "C Comparing Different Architectures",
            "char_start_offset": 27600,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 52
                },
                {
                    "start": 53,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 280,
                    "matchedPaperCorpusId": "204960716"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "268041362",
            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
            "text": "LLMs are a class of extensive artificial intelligence models characterized by their massive scale with billions of parameters [14]. Scaling up LLMs allows them to learn more intricate and accurate language representations, resulting in improved performance across diverse downstream Natural Language Processing (NLP) tasks, particularly excelling in Natural Language Generation (NLG) challenges [15,16]. The brief comparison of different structures of the LLMs mentioned can be seen in Table 1. Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. [18] The vanilla Transformer architecture [17], a sequence-to-sequence model, has emerged as a foundational framework for diverse LLMs, utilizing encoders and decoders with self-attention mechanisms as its core components, thanks to its exceptional parallelism and capacity. Based on the masking methods utilized by various attention mechanisms in the model, the current LLMs can be divided into three categories, i.e., Encoder-Decoder, Decoder-only, and Encoder-only. \n\nThe decoder-only category further includes distinctions such as causal decoders and prefix decoders, illustrated in Figure 1. \n\nIn the following subsection, we shall introduce different types of LLMs based on various Transformer architectures.",
            "score": 0.6715250435075248,
            "section_title": "GENERAL METHODS",
            "char_start_offset": 4546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 972,
                    "end": 976,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "268201845",
            "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
            "text": "In their seminal Transformer paper, Vaswani et al. (2017) propose an extension to this mechanism (see Appendix A), which we are referring to in this paper. We can further distinguish two variants of the attention mechanism: fully visible attention, where all tokens are able to attend to each other, and causal attention (see Raffel et al. 2020), where tokens in earlier parts of a sequence are unable to attend to tokens in later parts of the sequence. Finally, the Attention mechanism can be augmented with Multi-Head Attention (MHA), which applies the Attention mechanism with different learned weight matrices (Vaswani et al. 2017). \n\nThe key building blocks of a Transformer model are an encoder and a decoder and thus it is also commonly known as the encoder-decoder architecture. The encoder is made up of a series of encoder blocks, which themselves contain MHA and feed-forward layers. Essentially, it is a function that takes as input a matrix of features and transforms the matrix into embeddings. The decoder, in turn, consists of a series of decoder blocks, which themselves contain MHA, causal MHA and feed-forward layers. It is a function that takes the encoder embeddings and node features in the partial solution as input and outputs nodes in an autoregressive manner. Expressed in the context of vehicle routing, the encoder learns a representation of all nodes in the underlying graph of the MCVRP and the decoder seeks to capture the distribution of solutions in the graph. \n\nNLP model architectures. The field of NLP broadly consists of three main model architectures: the encoder-decoder as described above, the LM, and the Prefix LM. The LM architecture, which is commonly known as the decoder-only architecture, is first proposed in the Generative Pre-trained Transformer (GPT) model by Radford et al. (2019). In this model, causal attention is applied to the model such that a token at any given position can only see the previous tokens in the input and not future tokens. \n\nThe Prefix LM is essentially a modified version of the decoder-only architecture.",
            "score": 0.6711897229355122,
            "section_title": "Deep Learning Approaches",
            "char_start_offset": 17670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1833
                },
                {
                    "start": 1834,
                    "end": 1998
                },
                {
                    "start": 2001,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 345,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1811,
                    "end": 1832,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354248046875
        },
        {
            "corpus_id": "270064259",
            "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
            "text": "The causal attention mask in decoder-only LLMs is introduced for next-token prediction task (Vaswani et al., 2017). In principle, causal mask in decoder blocks prevents information leakage by allowing the decoder to attend only to previous positions during auto-regressive text generation. However, it is observed that unidirectional attention limits the model's representation power, as evidenced by the poor performance of GPT models compared to similarly sized BERT or T5 models on natural language understanding benchmarks (e.g., Wang et al., 2019). In recent, LLM2Vec (BehnamGhader et al., 2024) introduces additional training phase with a specially designed masked token prediction to warm-up the bidirectional attention. GRIT (Muennighoff et al., 2024) utilizes a hybrid objective with both bidirectional representation learning and causal generative training. In contrast, we simply remove the causal attention mask of decoder-only LLM during the contrastive learning and find it works compellingly well as demonstrated by our results. As a result, we go with simple solution.",
            "score": 0.6708468975505082,
            "section_title": "BIDIRECTIONAL ATTENTION",
            "char_start_offset": 9939,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1084
                }
            ],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 552,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478759765625
        },
        {
            "corpus_id": "271974388",
            "title": "Legilimens: Practical and Unified Content Moderation for Large Language Model Services",
            "text": "The input and output tokens are processed in the same fashion through the decoder. As representative language models of this architecture, the GPT series models [9,55,57] are developed based on the causal decoder architecture. So far, the causal decoders have been widely adopted as the architecture of LLMs by various existing LLMs, such as LLaMA [73,74], Dolly [17,18], and Falcon [61]. \n\nThe prefix decoder architecture (a.k.a., non-causal decoder) revises the masking mechanism of causal decoders, to enable performing bidirectional attention over the prefix tokens [23] and unidirectional attention only on generated tokens. In this way, like the encoder-decoder architecture, the prefix decoders can bidirectionally encode the prefix sequence and auto-regressively predict the output tokens one by one, where the same parameters are shared during encoding and decoding. Existing representative LLMs based on prefix decoders include GLM [24,89] and U-PaLM [72]. \n\nwhere \u2295 denotes concatenating the previous output tokens to the end of the input sequence until a special sentence ending token (usually denoted as [eos]) is generated. The first decoding method is greedy search, which predicts the most likely token at each step based on the previously generated tokens. The other decoding method is sampling, which randomly samples the next token based on the probability distribution to enhance the randomness and diversity during generation. \n\nFrom the inference process and the self-attention mechanism of LLMs we know that LLMs output the first token  1 of r leveraging the information of p, and output the last token (i.e., [eos]) with the information of both p and r, i.e., \n\nwhere H (\u2022) denotes the inference function of LLMs. We utilize this inference process as a feature extractor for the downstream content moderation task, which we elaborate on in \u00a74.",
            "score": 0.6707935419680469,
            "section_title": "Basic Component.",
            "char_start_offset": 11466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 388
                },
                {
                    "start": 391,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 570,
                    "end": 574,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 961,
                    "end": 965,
                    "matchedPaperCorpusId": "253018395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "258947629",
            "title": "How Powerful are Decoder-Only Transformer Neural Models?",
            "text": "To create a decoder-only model, the vanilla architecture is modified in two ways. First, the connection to the encoder is removed. Second, the cross-attention which allows the decoder to conditionally attend to the encoder output at each layer of the decoder is eliminated. These, along with the entire encoder, are surrounded by a dashed yellow line in 1 to visualize what is eliminated. As mentioned previously, this superficially suggests that encoder-only and decoder-only architectures are identical as seen in 2. \n\n2) Differentiating Encoder-only and Decoder-only Models: Decoder-only models have three necessary characteristics which are derived from their function in the vanilla transformer. The decoder must (1) provide a means of autoregressively predicting the next token based on the tokens generated so far given the encoder input as contextualization. In 2 this is shown as the recursive red connection mapping the output vector back into the last element of the input sequence of vectors. To be suited to this task, decoder-only models must (2) not see future values when evaluating a query on the input sequence of vectors. This is why decoder-only models are often referred to as causal language models (CLM). In 2, we refer to the decoder attention heads as causal attention heads rather than masked attention heads as they are called in [1]. The model must be (3) trained to predict the next token given the current input sequence of vectors. This training method coupled with recursion allows decoder-only models to autoregressively generate arbitrarily long (up to the max size of the input vector sequence) sequences. \n\nIf any of the above are violated, the model can't be reasonably considered a decoder-only model as it is no longer capable of auto-regressive next token prediction.",
            "score": 0.6698503402720029,
            "section_title": "1) Modifying the Vanilla Transformer to form a Decoderonly Model:",
            "char_start_offset": 3733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 518
                },
                {
                    "start": 521,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1807
                }
            ],
            "ref_mentions": [
                {
                    "start": 1357,
                    "end": 1360,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226318359375
        },
        {
            "corpus_id": "275336136",
            "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
            "text": "In previous sections, we presented each DFP configuration using its optimal causal masking strategy: 1) causal masking is not applied on decoder-only, whereas 2) it is applied on decoder-prepend. In the following, we provide an ablation study of causal masking, which is summarised in Table 3. The significance tests are computed between the pairs with and without causal masking. \n\nAs we can observe, decoder-only (both 18L and 32L) performs worse on all experimental settings when causal masking is applied. On CoVoST2, the performance degrades by at least 2 points, whereas the degradation can be up to 2 points on MuST-C. This indicates the importance of allowing the speech frames to attend each other in decoder-only models. Our finding is in accordance with the conclusion drawn by Gupta et al. (2024) for ASR, and we further extend it for ST. \n\nWhen causal masking is removed from decoderprepend (decoder-prepend TF), we observe a performance degradation of 1.2 WER and up to 0.9 BLEU points on MuST-C ASR and bilingual ST, respectively. What is even worse is the degradation of 8.18 BLEU points in multilingual ST. On the CoVoST2 dataset, however, removal of causal masking causes little improvement to both ASR and ST. In the case of Conformer, there are almost no performance changes on the CoVoST2 dataset when causal masking is removed, but a small degradation of 0.4 BLEU points (25.9 \u2192 25.5) on the MuST-C en-de direction when CTC compression is also applied. \n\nTherefore, our results lead to two interesting observations. Firstly, the behaviour of DFP models are quite different with causal masking, depending on whether a speech encoder is used or not. We hypothesise that the non-adversarial effect of causal masking on decoder-prepend is attributed to the self-attention within the speech encoder, which allows full attention within the speech frames.",
            "score": 0.6690744113479616,
            "section_title": "(Audio) Causality masking in decoder-only and decoder-prepend",
            "char_start_offset": 25867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1870
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55810546875
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "LLMs with a Decoder-only architecture utilize the decoder component of the traditional Transformer architecture. Unlike the Encoder-Decoder architecture, which incorporates both an encoder and a decoder, the Decoder-only architecture is solely focused on the decoding process. In this configuration, the model sequentially generates tokens, attending to preceding tokens in the sequence. This architecture has been applied to various language generation tasks, showcasing its effectiveness in various tasks such as text generation without the need for an explicit encoding phase. The Decoder-only architecture can be further classified into two categories: the Causal Decoder architecture and the Prefix Decoder architecture. \n\nThe Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder). The representative LLMs for the Causal Decoder architecture are the GPT series [18; 7; 8; 93; 19]. The GPT series of LLMs are currently known for their superior performance, with their foundational Causal Decoder architecture widely applied in other LLMs such as BLOOM [38], OPT [83], Gopher [84], and LLaMA [9]. \n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens. Representative LLMs utilizing the Prefix Decoder architecture include PaLM [36] and GLM [37].",
            "score": 0.6683360501918196,
            "section_title": "Decoder-only Architecture",
            "char_start_offset": 35165,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1591
                },
                {
                    "start": 1594,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2196
                }
            ],
            "ref_mentions": [
                {
                    "start": 1951,
                    "end": 1955,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.654296875
        },
        {
            "corpus_id": "262064663",
            "title": "End-to-End Speech Recognition Contextualization with Large Language Models",
            "text": "This indicates that the model does not necessarily get confused by similarly pronounced words with different meanings. Furthermore, when we take the rare words from the ground truth into the context, the WER improves to 10.50% (6% relative change) and Rare WER improves by 18% relative. This further proves the ability of the model to utilize contextual information when present in order to better recognize rare entities. Traditionally causal masking is used in all self-attention layers for decoder-only language models to prevent future information leakage. However for offline speech recognition we have full audio and text context observed at the time of decoding and only transcription tokens are necessary to be masked causally. In this section we experiment the impact of applying causal masking on all input tokens and contrast it with applying full mask on the text and audio context followed by causal masking on transcription tokens. While the audio representations are fully contextualized already, we hypothesize that textual tokens may benefit from full masking. We present our results in Table 3. The full-mask shows only marginally better WER then causal masking (improving from 11.22% \u2192 to 11.15%). This comes at a cost as efficient self-attention implementations are currently tailored towards causal masking (Flash-Attention v2) and using a custom masking slows down training by 10%.",
            "score": 0.6621446577233725,
            "section_title": "Context sensitivity",
            "char_start_offset": 12373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.465576171875
        },
        {
            "corpus_id": "235313355",
            "title": "Luna: Linear Unified Nested Attention",
            "text": "In the formulation of causal attention, P is expected to contain no information about X. Thus, we need to formulate P based on the usage mode of the causal attention. For the encoder-decoder mode in sequence-to-sequence modeling (e.g. for machine translation), we can use packed output from the Luna encoder as P . For the decoder-only mode (e.g. for language modeling), P might be formulated as a learnable parameter of each layer.",
            "score": 0.6592761041844157,
            "section_title": "Luna Causal Attention",
            "char_start_offset": 11132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 432
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0706787109375
        },
        {
            "corpus_id": "271600495",
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "text": "Recent developments in Large Language Models (LLMs) such as GPT-3.5 Ouyang et al. [2022] and GPT-4 Achiam et al. [2023] have attracted significant attention. Due to the powerful generation capability, LLMs have made remarkable achievements in different kinds of Natural Language Process (NLP) tasks through a unified generative paradigm. Specifically, the most natural and common way to interact with LLMs is through multi-turn dialogues. However, as the number of dialogue rounds increases, ensuring high quality and low latency of the generated answer by LLMs is a challenge. \n\nExisting language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], Raffel et al. [2020], Lewis et al. [2020], Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], Dong et al. [2019], and Decoder-Only Brown et al. [2020], Touvron et al. [2023a,b], Du et al. [2022]. Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders Brown et al. [2020], Touvron et al. [2023a] and prefix decoders Du et al. [2022]. The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention. Our ISM utilizes alternate bidirectional and unidirectional attention on queries and answers in prefix sequences. All of them generate answer in auto-regressive.",
            "score": 0.6548630300126572,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 577
                },
                {
                    "start": 580,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 718,
                    "end": 738,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 740,
                    "end": 759,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 822,
                    "end": 840,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 859,
                    "end": 878,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 906,
                    "end": 922,
                    "matchedPaperCorpusId": "247519241"
                },
                {
                    "start": 1183,
                    "end": 1202,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1247,
                    "end": 1263,
                    "matchedPaperCorpusId": "247519241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "Given that we can learn causal functions using either encoders and decoders, the natural question to ask is how the expressive power of each model is related, i.e. can the encoder express more causal functions than a decoder of the same model size? Or perhaps they express the exact same class of causal functions? Towards answering this question, one trivial observation is that there exists causal functions that encoders and decoders are equally well equipped to represent: Remark 1. For any L \u2265 1 and D \u2265 1, there exists a decoder D and encoder E that each has Llayers, embedding dimension D and context window N, and \n\nThe above result follows directly from the fact that one-layer decoders and encoders are equivalent (formal proof in Appendix A.3). Hence, any encoder and decoder that have the same weights in the first layer, and then act as the identity map in the subsequent layers will suffice. 2 Given Remark 1, one may further conjecture that an encoder has strictly larger expressive power than decoders, e.g. given an encoder and decoder model of the same fixed size, the family of causal (but not necessarily auto-regressive) models representable by the encoder model is strictly larger and contains the family of causal functions decoders can represent. This intuitively follows, as by \"relaxing\" the causal mask to full attention, the embedding of each token in an encoder can still attend to tokens up to and including itself (as in a decoder) while additionally gaining the ability to attend to tokens after it. \n\nHowever, this is not really the case -there exists causal functions expressible only by encoders, only by decoders, and by both. As intuition, consider some hypothetical task that heavily rely on the inductive bias created by the causal mask, we would expect it to be easy for a decoder to express. \n\nTo motivate this perspective, we give the following two theorems: \n\nTheorem 1. For any L \u2265 2 and D \u2265 1, there exists a position-free decoder D that has L-layers and embedding dimension D, such that for any encoder E, there exists some input sequence",
            "score": 0.6543450504967097,
            "section_title": "EXPRESSIVE POWER OF ENCODER-ONLY VS. DECODER-ONLY TRANSFORMERS",
            "char_start_offset": 9891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 1900
                },
                {
                    "start": 1903,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2084
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07147216796875
        },
        {
            "corpus_id": "269982953",
            "title": "Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers",
            "text": "Practical implementations introduce subtle modifications into the process described previously.The most relevant distinction is made between encoder-only models, that include BERT [15] and its variants, and decoder-only models such as the GPT models [38,39].\n\nEncoder-only models.Considering BERT as an example of an encoder-only model, the first token is used for the classification, i.e, r = 1.Usually, a special token [CLS] is prepended to the text at position 1, however this is not strictly necessary for the functioning of the model.\n\nDecoder-only models.In contrast, decoder-models like GPT-2 [39] add the classification head on top of the last token for classification, i.e., r = |t|.A key difference is that in GPT-2 and other decoder-only models, a causal mask is laid over the attention matrix, resulting in \u03b1 i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones.",
            "score": 0.6509823103285286,
            "section_title": "Encoder-Only and Decoder-Only models",
            "char_start_offset": 11540,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 95,
                    "end": 258
                },
                {
                    "start": 260,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 539
                },
                {
                    "start": 541,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 692
                },
                {
                    "start": 692,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 929
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 257,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 600,
                    "end": 604,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28564453125
        },
        {
            "corpus_id": "266755678",
            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
            "text": "The decoder module [32] of the Transformer model is also composed of multiple identical layers, each of which includes a multi-head attention mechanism and a feed-forward neural network. Unlike the encoder, the decoder also includes an additional encoder-decoder attention mechanism, used to compute attention on the input sequence during the decoding process. At each position, the decoder can only perform self-attention calculations with the positions before it to ensure that the generation of the sequence does not violate grammar rules. Masks play an important role in the decoder, ensuring that only information before the current time step is focused on when generating the output sequence, and not leaking information from future time steps. Specifically, the decoder's self-attention mechanism uses masks to prevent the model from accessing future information when generating predictions at each time step, maintaining the causality of the model. This ensures that the output generated by the model depends on the information at the current time step and before, without being influenced by future information.",
            "score": 0.6492925153589176,
            "section_title": "Decoder",
            "char_start_offset": 8899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1120
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355224609375
        },
        {
            "corpus_id": "256461326",
            "title": "Learning to Generate Question by Asking Question: A Primal-Dual Approach with Uncommon Word Generation",
            "text": "The decoder decodes the embeddings from the encoder to generate the question embedding. We adopt the similar decoder structure in Transformer (Vaswani et al., 2017), which is composed of a stack of identical layers. In addition to the two sub-layers in the encoder layer, the decoder employ a third sub-layer, encoder-decoder attention (EDA), which performs multi-head attention over the outputs of the encoder and the current decoder layer. The masked multi-head attention (MMHA) has the same model structure as the multi-head attention in the encoder layer, except that it prevents positions from attending to subsequent positions. This masking ensures that the prediction for position i can depend only on the known outputs at positions before i, as the question is generated word by word. The encoder-decoder attention has a similar structure as self-attention, the distinction is that the key and value are from the output of the encoder, whereas the query is from the decoder itself. We provide more technical details of both encoder and decoder in Appendix A.",
            "score": 0.6459579971435394,
            "section_title": "Question Decoder",
            "char_start_offset": 10897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1066
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 164,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.379638671875
        },
        {
            "corpus_id": "247011991",
            "title": "SOTitle: A Transformer-based Post Title Generation Approach for Stack Overflow",
            "text": "The structure of the decoder is similar to the encoder. The difference is that it uses a standard attention mechanism to focus on the encoder output after each self-attentive layer. The self-attention mechanism in the decoder also uses a type of autoregressive or causal self-attention that allows the model to only focus on the past outputs.",
            "score": 0.6402006567564886,
            "section_title": "C. Transformer-based Autoregressive Decoder",
            "char_start_offset": 16115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 342
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1248779296875
        },
        {
            "corpus_id": "257050658",
            "title": "Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios",
            "text": "We adopt the Transformer [22] as the backbone. To enable non-autoregressive interpretation, following [20], the decoder is modified in three aspects: input sequence, self-attention mask, and positional encoding. For input sequence modification, because previously generated tokens are unavailable under the non-autoregressive setting, we use a fertility predictor first to predict the length of the target explanation and produce decoder input with the tokens copied from the encoder input. \n\nFor the modification of the self-attention mask, because the decoder input is the copied sequence of encoder input, the self-attention module is allowed to attend all positions, rather than only left positions in the conventional Transformer decoder. Therefore, the self-attention mask is replaced with a non-causal mask in our non-autoregressive decoder. For positional encoding modification, different from the self-attention module, the positional attention module uses positional encoding as the query and key, and the hidden representations from the previous layer as the value.",
            "score": 0.6348164198951542,
            "section_title": "Encoder and Decoder",
            "char_start_offset": 3841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 490
                },
                {
                    "start": 493,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 29,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.320068359375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "When studying adaptation and the conversion from one architecture to another, we also considered converting to and from encoder-decoder models. Conversion across causal and non-causal decoder-only models is straightforward, simply by switching the attention mask; for encoder-decoder, parameters have to be either pruned or added for both the entire encoder, and for the cross-attention in the decoder. Results from one of our attempt to convert an encoder-decoder into a causal decoder are reported in Figure 10. While converting across causal/non-causal decoder provides an improvement over training from scratch, this is not the case here. Validation loss when adapting an encoder-decoder pretrained with MLM to a causal decoder-only using FLM. We adapted a pretrained (for 168B tokens) encoder-decoder model to decoder-only by feeding an empty input into the encoder and causally training with a FLM objective on the decoder. We stopped this adaptation once it was clear the performance would not match that of a causal FLM trained from scratch, in contrast with the other adaptations we studied.",
            "score": 0.6343112013968137,
            "section_title": "E.4 Adaptation from an encoder-decoder",
            "char_start_offset": 46680,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1100
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489013671875
        },
        {
            "corpus_id": "273662013",
            "title": "Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective",
            "text": "Therefore, bidirectional attention allows each token to attend to all other tokens, and there is no need for causal masks to block attention weights. a ik in Eq. ( 8) satisfies \n\nDuring training, masked tokens [m] are added at the end in the middle layer, and are the only ones in the sequence for which the loss needs to be computed as: \n\nwhere M is the index set of masked tokens. Note that our analysis is based on the next token prediction. Models like MAR etc.can predict multiple masked tokens in parallel, but masked tokens are not visible to each other, regardless of whether they have already been predicted. They can only attend to previous unmasked tokens. SAR [48] further removes this limitation by allowing unknown tokens, i.e., tokens waiting for prediction, to be dynamically divided into multiple sets and performing next set prediction, enabling unpredicted sets to observe predicted sets and the initially known tokens. They achieve this through a Fully Masked Transformer, which can be viewed as a hybrid approach combining causal and bidirectional Transformer. \n\nPrefix Transformer. As shown in Fig. 2 (c), some models adopt prefix attention [41], which also combines causal attention with bidirectional attention. In prefix Transformer, a prefix with the length of S is treated with bidirectional attention, while the remainder of the sequence is processed with causal attention. For tokens in the prefix, i.e., i \u2264 S, a ik in Eq. ( 8) satisfies a ik = 0 for k > S and S k=1 a ik = 1. For tokens outside the prefix, i.e., i > S, a ik in Eq. ( 8) satisfies a ik = 0 for k > i and K k=1 a ik = 1. During training, only tokens beyond the prefix are computed for loss:",
            "score": 0.6324263254352991,
            "section_title": "Autoregression Transformer",
            "char_start_offset": 18451,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 176
                },
                {
                    "start": 179,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1686
                }
            ],
            "ref_mentions": [
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "267028705"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164306640625
        },
        {
            "corpus_id": "263829839",
            "title": "LLM for SoC Security: A Paradigm Shift",
            "text": "The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 [137], and UL2 [138] are a few well-known encoder-decoder models to be named. \n\nIn Context of SoC Security: The encoder-decoder architecture, renowned for its ability in natural language understanding tasks, exhibits versatility in SoC security. Its twostage process of encoding the input data and then decoding it to produce an output makes it suitable for tasks that require both comprehension and generation. This model is particularly adept at vulnerability mitigation, where understanding the context (encoder) and generating a solution (decoder) are both crucial. However, while it is also a good fit for tasks like vulnerability insertion, security verification, and assessment, it might not always be the optimal choice when the task leans heavily toward either comprehension or generation b) Decoder-Only: Decoder-only LLMs have established impressive benchmarks in numerous NLP tasks, especially in the generation of free-form text. In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.",
            "score": 0.6291326319983631,
            "section_title": "A. Preliminaries",
            "char_start_offset": 37836,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2270
                }
            ],
            "ref_mentions": [
                {
                    "start": 776,
                    "end": 781,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 791,
                    "end": 796,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "For any L \u2265 2 and D \u2265 1, there exists a position-free decoder D that has L-layers and embedding dimension D, such that for any encoder E, there exists some input sequence \n\nTheorem 2. For any L \u2265 2 and D \u2265 1, there exists a position-free encoder E that has L-layers and embedding dimension D, such that for any decoder D with positional embeddings satisfying p 1 \u0338 = p 2 , there exists some input sequence (x 1 , x 2 , . . . \n\nThese theorems are existential in nature. Informally, Theorem 1 says that if we consider causal model defined over the entirety of R D as its vocabulary, we can find some decoder, for which any encoder will differ from it on some input sequence. Theorem 2 makes a similar (albeit weaker statement) in the other direction; namely the existence of a causal function computable by an encoder, but not by any decoder that uses \"non-trivial\" positional embeddings (e.g. embeddings for different positions are unique). Detailed proof of both theorems are deferred to Appendix A. \n\nOf course, the setting and assumptions of the above two statements are not necessarily very realistic. For one, they focus on general class of causal models rather than only auto-regressive ones. Furthermore, the results only pertain to exact realization and say nothing about approximation. The assumption of unbounded domain is also not realistic as in practice decoders are trained and used over a finite domain of tokens, each with some fixed embeddings. And specific to Theorem 2, no claim is made about decoders that do not use positional embeddings. But despite the limitations, these theorems give an indication that the expressive power of encoder and decoder model are different -despite the almost identical description modulo the attention mask. Changing the mask on the attention scores causes significant changes to the properties of the model. Thus, in the following sections we propose an auto-regressive tasks and run experiments comparing encoders and decoders that corroborates this view.",
            "score": 0.6249686942750137,
            "section_title": "EXPRESSIVE POWER OF ENCODER-ONLY VS. DECODER-ONLY TRANSFORMERS",
            "char_start_offset": 11805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 173,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1312255859375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We consider causal decoder (CD) , encoder-decoder (ED) , and non-causal decoder (ND) architectures. All models share the basic configuration outlined in Table 1. For fair comparison across architectures, we aim to approximately match pretraining compute budget; accordingly, our encoder-decoder models have twice as many layers as the decoder-only models. This results in encoder-decoder models with 11B parameters and decoder-only models with 4.8B parameters. We note that due to the cross-attention layers, encoder-decoder models are approximately \u223c 10% more computationally expensive to run than the decoder-only models we consider.",
            "score": 0.6234997765406711,
            "section_title": "Architecture",
            "char_start_offset": 21774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 635
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2113037109375
        },
        {
            "corpus_id": "277626724",
            "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
            "text": "Decoder-only pretraining often adopts causal language modeling on a single sequence. In contrast, encoder-decoder adaptation requires separate input and target sequences to be fed to the encoder and decoder separately. We explore two classical pretraining objectives for encoder-decoder modeling: prefix language modeling (PrefixLM) and UL2 (Tay et al., 2022;Wang et al., 2022). \n\nPrefixLM behaves similar to causal language modeling except for its prefix condition. To simplify the preprocessing, we split a sequence equally into two halves, the first half used as input and the second one as target. This also eases the adoption of knowledge distillation from decoder-only models. UL2 is more complicated. It is composed of several denoising tasks at different levels of complexity. We prepare UL2 data following Tay et al. (2022). We compare their performance in experiments.",
            "score": 0.6201348792706811,
            "section_title": "Pretraining Objective",
            "char_start_offset": 9025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 878
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 359,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 359,
                    "end": 377,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 815,
                    "end": 832,
                    "matchedPaperCorpusId": "252780443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.107421875
        },
        {
            "corpus_id": "264426063",
            "title": "Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages",
            "text": "The original transformer (Vaswani et al., 2017), designed for machine translation, had both an encoder and a decoder. In practice, both encoder-only models like BERT (Devlin et al., 2019) and decoderonly models like GPT (Brown et al., 2020) are common. Like much previous work on transformer expressivity (e.g. Hahn, 2020), we study an encoder-only setup, where the input is a string and the output is a binary classification; but our results could easily be adapted to a decoder-only setting where the input is a prefix and the output is the next symbol. \n\nThe transformers studied here use unique hard attention (or simply hard attention), in which an attention head focuses all attention on the position with the highest score, with ties broken to the left or right. Although this is different from the soft attention in actual transformers, theoretical studies unavoidably involve models of the real objects of study, and we are using unique-hard attention as a stepping-stone towards understanding real transformers. However, unique-hard attention may be more appropriate than it appears: \n\n\u2022 Real transformers are often observed to focus attention on a very small number of positions (Merrill et al., 2021). On Dyck languages, they have been found to learn effectively uniquehard attention in their second layer (Ebrahimi et al., 2020, Figure 1). \n\n\u2022 There exist soft-attention transformers that compute parity (Chiang and Cholak, 2022), but in practice, transformers cannot learn parity (Bhattamishra et al., 2020). Unique-hard attention transformers also cannot compute parity (Hahn, 2020), so they are in some sense more realistic. \n\n\u2022 Hard attention has occasionally been used in practice in previous research on interpretability (Kinley, 2020) and efficiency (Gupta et al., 2021;Xu et al., 2021). \n\nIn this paper, we use future masking, in which every position may only attend to positions to its left. This kind of masking is common in decoder-only models and has been studied in encoder-only models as well (Bhattamishra et al., 2020).",
            "score": 0.61890137121211,
            "section_title": "Transformer variants",
            "char_start_offset": 3771,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1807
                },
                {
                    "start": 1810,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 47,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 166,
                    "end": 187,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 220,
                    "end": 240,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 311,
                    "end": 322,
                    "matchedPaperCorpusId": "189928186"
                },
                {
                    "start": 1190,
                    "end": 1212,
                    "matchedPaperCorpusId": "237485630"
                },
                {
                    "start": 1318,
                    "end": 1340,
                    "matchedPaperCorpusId": "222271899"
                },
                {
                    "start": 1417,
                    "end": 1442,
                    "matchedPaperCorpusId": "247084324"
                },
                {
                    "start": 1494,
                    "end": 1521,
                    "matchedPaperCorpusId": "222225236"
                },
                {
                    "start": 1585,
                    "end": 1597,
                    "matchedPaperCorpusId": "189928186"
                },
                {
                    "start": 1770,
                    "end": 1790,
                    "matchedPaperCorpusId": "235422257"
                },
                {
                    "start": 1790,
                    "end": 1806,
                    "matchedPaperCorpusId": "237485437"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.310546875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "The complete set of results across all checkpoints obtained through this study is made available in Appendix E.2. We are first interested in the architecture and objective achieving the best zero-shot performance after unsupervised pretraining only. For this, we only consider the full/prefix language modeling objectives since masked language modeling does not yield a model appropriate for zero-shot prompted evaluation on its own. This is validated with early checkpoints in Appendix E.1. \n\nWe present our main full/prefix language modeling pretraining results in Table 3. On both our evaluation benchmarks, the causal decoder architecture systematically outperforms the other architectures when using language modeling pretraining alone. The non-causal decoder remains within a percent of the causal decoder performance, but the encoder-decoder performance lags far behind. Finally, we note that the performances on T0-Eval are close to the random baseline, while performance differences on EAI-Eval are significant enough to make comparison across experiments. \n\nFinding 1. Causal decoder-only models pretrained with a full language modeling objective achieve best zero-shot generalization when evaluated immediately after unsupervised pretraining , in line with current common practices for large language models.",
            "score": 0.6155303987625129,
            "section_title": "Evaluation",
            "char_start_offset": 28247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1065
                },
                {
                    "start": 1068,
                    "end": 1319
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138427734375
        },
        {
            "corpus_id": "253420279",
            "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",
            "text": "Although most modern language models are based on the Transformer architecture, there are significant deviations between architectural implementations. Notably, while the original Transformer is based on an encoder-decoder architecture, many popular models have opted for encoder-only (e.g. BERT, (Devlin et al., 2019)) or decoder-only (e.g. GPT, (Radford et al., 2018)) approaches. Currently, all state-of-the-art language models over 100 billion parameters are causal decoder-only models Rae et al., 2021;Chowdhery et al., 2022). This is in opposition to the findings of Raffel et al. (2020), in which encoderdecoder models significantly outperform decoder-only models for transfer learning.\n\nPrior to our work, the literature was lacking a systematic evaluation of the zero-shot generalization capabilities of different architectures and pretraining objectives. We explored this question in Wang et al. (2022a) where we evaluated encoder-decoder and decoder-only architectures and their interactions with causal, prefix, and masked language modeling pretraining objectives. Our results show that immediately after pretraining, causal decoderonly models performed best -validating the choice of state-of-the-art LLMs. Furthermore, they can be more efficiently adapted after pretraining to a non-causal architecture and objective-an approach which has been further explored and confirmed by .",
            "score": 0.6154269332044976,
            "section_title": "Architecture and Pretraining Objective",
            "char_start_offset": 30941,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 318,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 573,
                    "end": 593,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 894,
                    "end": 913,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.150634765625
        },
        {
            "corpus_id": "270521959",
            "title": "GenDistiller: Distilling Pre-trained Language Models based on an Autoregressive Generative Model",
            "text": "Language models are established to predict the embedding of the target token based on the context or the previous tokens of it.In this work, we use generative language model to predict the hidden layer embeddings of teacher model in the hope to involve the interaction of the intermediate layers and avoid utilizing the future information.Three generative architectures are considered: encoder-decoder, prefix decoder and causal decoder [24].The encoder-decoder architecture [25][26][27] consists of two stacks of Transformer blocks to construct encoder and decoder separately.The encoder encodes the input sequence into a common history and the decoder generates the target sequence based on the common history in an autoregressive way.The prefix decoder architecture [28,29] performs bidirectional attention over the prefix tokens and unidirectional attention on generated tokens.The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input through a unidirectional attention mask.In our work, we select the causal decoder architecture as the backbone of our generative distiller since it is more concise and is capable to predict the hidden layer outputs of the teacher model autoregressively.",
            "score": 0.6152203724076537,
            "section_title": "Generative Language Models",
            "char_start_offset": 5733,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 339
                },
                {
                    "start": 339,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 577
                },
                {
                    "start": 577,
                    "end": 737
                },
                {
                    "start": 737,
                    "end": 882
                },
                {
                    "start": 882,
                    "end": 1015
                },
                {
                    "start": 1015,
                    "end": 1228
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 441,
                    "matchedPaperCorpusId": "257900969"
                },
                {
                    "start": 479,
                    "end": 483,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 769,
                    "end": 773,
                    "matchedPaperCorpusId": "247951931"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.405517578125
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "In this paper, we base all models on a decoder-only Transformer pretrained with an autoregressive language modeling objective. This is a popular choice for large language models (Brown et al., 2020;Rae et al., 2021;Thoppilan et al., 2022), possibly because it lends itself to zero-shot application to many downstream tasks (Radford et al., 2019). Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., T5 Raffel et al. (2019)), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, Liu et al. (2018); Dong et al. (2019)). \n\nOur decision is motivated by the findings of Wang et al. (2022), which showed that decoderonly models combined with an autoregressive language modeling objective provide the best zeroshot generalization abilities immediately after pretraining. Although multitask finetuning (Sanh et al., 2021;Wei et al., 2021) will instead favor an encoder-decoder with span corruption for best zero-shot generalization, Wang et al. (2022) found a compromise between these two practices. Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning. Accordingly, we follow their recommendation, and train an autoregressive decoder-only model first which we will later consider adapting and finetuning.",
            "score": 0.6128873929267192,
            "section_title": "Architecture and Pretraining Objective",
            "char_start_offset": 2960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 198,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 556,
                    "end": 573,
                    "matchedPaperCorpusId": "3608234"
                },
                {
                    "start": 575,
                    "end": 593,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "272423598",
            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
            "text": "Another often overlooked key point is the design of attention masks within the transformer layers in large lan- guage models. In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text. Mathematically, the causal attention mask M \u2208 R T \u00d7T for a sequence of length T is defined as: \n\nThis ensures that each position i only attends to previous positions (including itself), thus implementing the causal attention mechanism. The final attention weights are computed as: \n\nwhere Q is the query vectors, K is the key vectors, V is the value vectors, d k is the dimension of the key vectors, and M is the causal attention mask. However, for multimodal information involving both visual and textual inputs, the visual modality is only used as a conditional input to the language model. During the unidirectional decoding process of the language model, this design weakens the bidirectional attention interactions obtained from the visual encoder, reducing them to unidirectional attention interactions. To explore the impact of different attention masks, we design three distinct attention masks to enhance and investigate better interactions within visual tokens and between visual and text tokens, as illustrated in Figure 3. \n\nFirstly, the Full Visual Mask modifies the causal attention mask to enable more extensive interactions among visual tokens across different frames. This mask can be represented as follows: \n\nSecondly is Frame-wise Block Mask, which limits the attention to adjacent visual tokens within the same frame. This is defined as follows: \n\nFinally, we proposed Frame-wise Block Causal Attention Mask (FwBC), which combines the characteristics of the previous causal and block visual attention masks by incorporating broader visual token interactions within the frame while maintaining causal inference mode across video frames. This can be presented as:",
            "score": 0.608265607361175,
            "section_title": "Frame-wise Block Causal Attention Mask",
            "char_start_offset": 12069,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1651
                },
                {
                    "start": 1654,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70947265625
        },
        {
            "corpus_id": "267523128",
            "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
            "text": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
            "score": 0.6077820253545078,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6611328125
        },
        {
            "corpus_id": "251554550",
            "title": "MILAN: Masked Image Pretraining on Language Assisted Representation",
            "text": "Since the decoder is discarded after pretraining, the encoder needs to learn rich semantic information in the latent representations of the unmasked patches from the reconstruction target. To achieve so, the functional roles of the encoder and decoder need to be clearly segregated. All representation learning for the unmasked patches is completed in the encoder, while the decoder is only for predicting the target features of the masked patches. However, in some previous works [3,61], the decoder is as simple as one linear layer, which may be insufficient to reconstruct the masked representations, and portions of the encoder may serve as the decoder. MAE [25] uses a deep decoder that not only updates the mask tokens but also enhances the features of the unmasked patches. Because our method reconstructs language guided latent representations instead of raw pixels, the encoder's outputs should only provide clues to the decoder to complete the missing patches' features without being updated in the decoder. Otherwise, the representation quality from the encoder becomes sub-optimal. \n\nIn MILAN, we propose a prompting decoder shown in Figure 1(b), where the representations of the unmasked patches from the encoder are frozen, serving as \"fixed prompts\". They are appended to the keys and values in each attention module of the prompting decoder's transformer block while the queries only contain the features of mask tokens. In specific, the multi-head attention (MHA) module in Figure 1(b) performs the following operations: \n\n(2) In ( 2), X and Z are the features of mask tokens and the encoder's outputs, respectively. \"Attn\" is short for the softmax attention operation. \"cat\" means concatenating along the sequence dimension. H is the number of heads. \n\no represent the query, key, value and output projection weight matrices in each head. d h is the embedding dimension in each head. As shown in Figure 1(c), our prompting decoder only computes the self-attention among the features of mask tokens and the cross-attention between the encoder's output and the features of mask tokens.",
            "score": 0.6072664368489414,
            "section_title": "Decoder design: prompting decoder",
            "char_start_offset": 10346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1768
                },
                {
                    "start": 1771,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093017578125
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017). Besides, multiple bidirectional encoders can be mounted to the causal language model, but the encoder-decoder architecture usually has only one encoder. \n\nNon-causal encoders as System 1, and causal language models as System 2. Cognition is usually categorized into two levels (Kahneman, 2011;Bengio, 2019): System 1 (i.e., intuitive, and unconscious) and System 2 (i.e., sequential, conscious, planning, and reasoning). In the proposed framework, the modules can be regarded as an implementation of these two levels, respectively. To be specific, non-causal encoders pretrained by masked data modeling, such as BERT (Devlin et al., 2019) and BEiT (Bao et al., 2022), are used as a perception layer to encode various input modalities. The encoding modules can be viewed as System 1. After we obtain the input representations, we  feed them to the causal language model, which has shown promising performance on commonsense reasoning (Chowdhery et al., 2022) and planning (Huang et al., 2022). The universal task layer is designed to play a role of System 2 in our method. \n\nNatural language interface between users and pretrained models. The universal task layer based on causal language modeling enables users to interact with pretrained non-causal encoders using natural language. First, language can be used as a programming language for the underlying pretrained or finetuned models, which is compiled by the universal interface. For example, we can write text-based instructions (Ouyang et al., 2022) and explanations (Wei et al., 2022) to repurpose and guide the model behaviors. Second, the universal interface enables the models to present the results using free texts, making predictions directly understandable and explainable. Third, the proposed framework natively supports multi-turn conversational interactions. In each turn, we can feed the encoded input to the interface layer and then generate response results in a semi-causal manner.",
            "score": 0.6069527079120582,
            "section_title": "body",
            "char_start_offset": 3968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 331
                },
                {
                    "start": 334,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 796,
                    "end": 817,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.073974609375
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "LLMs on Code Transformers capture dependency among sequence elements through attention mechanism (Bahdanau et al., 2014) and are highly scalable, as shown in natural language processing (Devlin et al., 2019;Lewis et al., 2020;Raffel et al., 2020). Several efforts explore these models for program synthesis (Chen et al., 2021a;Austin et al., 2021;Li et al., 2022;Fried et al., 2022;Nijkamp et al., 2022;Allal et al., 2023) and its effectiveness (Vaithilingam et al., 2022). \n\nAblation Studies Raffel et al. (2020) introduce the concept of non-causal decoder in the form of a Prefix-LM with favorable performance over causal decoders after fine-tuning on down-stream tasks. \n\nThe performance in few-shot generative tasks was not evaluated. Wang et al. (2022) conduct an extensive ablation study over architectures and objectives with the conclusion that decoder-only models with causal language modeling exhibit the strongest zero-shot generalization. Therefore, we limit our investigation to causal and non-causal decoders. Tay et al. (2022a) compare encoder-decoder, decoder-only, and Prefix-LM architectures and report the beneficial performance of encoder-decoder models, while zero-shot generation tasks are not evaluated. The authors later adopt Prefix-LM instead of encoder-decoder in (Tay et al., 2022b). \n\nData Mixtures LaMDA (Thoppilan et al., 2022) was trained on a mixture of various data sources including dialogues, code documents, Q&A data, tutorials, and, Wikipedia. However, the impact of this mixture and the specific sources are unclear. Xie et al. ( 2023) introduces a data selection method based on importance resampling which allows to mix datasets of various sizes, however, the evaluation only covers encoder-only models.",
            "score": 0.6064616520643077,
            "section_title": "RELATED WORK",
            "char_start_offset": 5422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1311
                },
                {
                    "start": 1314,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 186,
                    "end": 207,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 207,
                    "end": 226,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 226,
                    "end": 246,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 347,
                    "end": 363,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 445,
                    "end": 472,
                    "matchedPaperCorpusId": "247255943"
                },
                {
                    "start": 493,
                    "end": 513,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 739,
                    "end": 757,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10430908203125
        },
        {
            "corpus_id": "1179542",
            "title": "Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks",
            "text": "We emphasized the specific variants of those networks that are often used in the encoder-decoder model; a conditional language model based on RNNs (a conditional RNN-LM) and a pre-trained CNN for transfer learning. Then, we introduced the simple encoder-decoder model followed by the attention mechanism, which together form the central topic of this paper, the attention-based encoder-decoder model. \n\nWe presented four recent applications of the attention-based encoder-decoder models; machine translation (Sec. IV-A), image caption generation (Sec. IV-B), video description generation (Sec. IV-C) and speech recognition (Sec. IV-D). We gave a concise description of the attention-based model for each of these applications together with the model's performance on benchmark datasets. Furthermore, each description was accompanied with a figure visualizing the behaviour of the attention mechanism. \n\nIn the examples discussed above, the attention mechanism was primarily considered as a means to building a model that can describe the input multimedia content in natural language, meaning the ultimate goal of the attention mechanism was to aid the encoder-decoder model for multimedia content description. However, this should not be taken as the only possible application of the attention mechanism. Indeed, as recent work such as the pointer networks [65] suggests, future applications of attention mechanisms could run the range of AI-related tasks. \n\nBeside superior performance it delivers, an attention mechanism can be used to extract the underlying mapping between two entirely different modalities without explicit supervision of the mapping. From Figs. 2, 5, 7 and 8, it is clear that the attention-based models were able to infer -in an unsuperivsed way -alignments between different modalities (multimedia and its text description) that agree well with our intuition. This suggests that this type of attention-based model can be used solely to extract these underlying, often complex, mappings from a pair of modalities, where there is not much prior/domain knowledge. As an example, attention-based models can be used in neuroscience to temporally and spatially map between the neuronal activities and a sequence of stimuli [74].",
            "score": 0.6012357429816038,
            "section_title": "V. RELATED WORK: ATTENTION-BASED NEURAL NETWORKS",
            "char_start_offset": 41449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2084
                },
                {
                    "start": 2085,
                    "end": 2246
                }
            ],
            "ref_mentions": [
                {
                    "start": 2241,
                    "end": 2245,
                    "matchedPaperCorpusId": "12645140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09619140625
        },
        {
            "corpus_id": "250113742",
            "title": "Improving Deliberation by Text-Only and Semi-Supervised Training",
            "text": "Our model is illustrated in Fig. 1. Note that different from [9,10], the deliberation decoder is based on the non-causal encoder [2] instead of a causal encoder. The decoder attends to both the non-causal encoder output (e) and hypotheses (yr) from the non-causal path, i.e., decoded using non-causal encoder. The non-causal encoder often has a right-context for better recognition quality [2]. We use a conformer encoder [28] as the text encoder. A two-source attention LAS decoder is used as the deliberation decoder, similar to [9]. The decoder can be used for either re-decoding or rescoring. The deliberation model in this work does not stream compared to [29], as we focus on text-only and semi-supervised training.",
            "score": 0.6004603020014312,
            "section_title": "Model Overview",
            "char_start_offset": 4458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 721
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 64,
                    "matchedPaperCorpusId": "212747696"
                },
                {
                    "start": 64,
                    "end": 67,
                    "matchedPaperCorpusId": "231718654"
                },
                {
                    "start": 129,
                    "end": 132,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 390,
                    "end": 393,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 531,
                    "end": 534,
                    "matchedPaperCorpusId": "212747696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05340576171875
        },
        {
            "corpus_id": "264405877",
            "title": "GenDistiller: Distilling Pre-trained Language Models based on Generative Models",
            "text": "The language models can predict the embedding of the target token based on the context or the previous tokens of it. In this work, we use generative language model to predict the hidden layer outputs of teacher network in the hope to take the interaction of the hidden layers into account and avoid seeing the future information. Three language model architectures are considered: encoder-decoder, causal decoder, and prefix decoder [27]. The encoder-decoder architecture [3,28,29] consists of two stacks of Transformer blocks as the encoder and decoder, in which encoder is to encode the input sequence as a common history for the generated sequence and decoder generates the target sequence autoregressively. The causal decoder architecture [30][31][32][33] only attend to the past tokens of the input and itself through a unidirectional attention mask. The prefix decoder architecture [34,35] performs bidirectional attention over the prefix tokens and unidirectional attention only on generated tokens. In our work, we treat the original feature as the prefix tokens and the target hidden layers as the sequence to be generated one-by-one. Therefore, we select the prefix decoder architecture and modify it to some extend to build our distiller which can bidirectionally encode the input features and predict the output layers autoregressively, meanwhile, the required parameters are less than the encoder-decoder architecture.",
            "score": 0.5999255168086929,
            "section_title": "Generative Language Models",
            "char_start_offset": 6038,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1431
                }
            ],
            "ref_mentions": [
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "257900969"
                },
                {
                    "start": 472,
                    "end": 475,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 475,
                    "end": 478,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 478,
                    "end": 481,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 892,
                    "end": 895,
                    "matchedPaperCorpusId": "253018395"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333251953125
        },
        {
            "corpus_id": "260886785",
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "text": "In this section, we draw a comparison between the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs. \n\nCumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation. \n\nTo elucidate, consider the encoder self-attention depicted at the top of Fig. 1(a). During the computation of the weighted sum of value representations for tokens 1 and 3, all attention probabilities in the self-attention map impacted by quantization errors are incorporated. On the other hand, in the decoder's causal attention, as illustrated at the bottom of Fig. 1(a), the output value representation is computed using only the self-attention probabilities of the current token and those before it. For example, while generating the output value representation for token 1 (highlighted in a blue box), only two attention probabilities affected by quantization error are used. In contrast, for token 3 (indicated by the red box), probabilities from all preceding tokens are incorporated. This observation underscores that the inherent design of causal attention results in an accumulation of quantization errors, especially towards the latter tokens. Given this characteristic, it becomes imperative to devise a decoder QAT approach that mitigates this uneven distribution of quantization errors within the causal attention mechanism. \n\nNecessity of Ground Truth Loss. In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss.",
            "score": 0.599604176163786,
            "section_title": "Quantization Challenges on GLMs",
            "char_start_offset": 9244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 857
                },
                {
                    "start": 860,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1997
                },
                {
                    "start": 2000,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5078125
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "In Table 2, Prefix-LM generally achieves better results over causal decoder trained with the same objective on our SuperGLUE tasks, which partially answers our question that bi-directional attention may yield informative representations. However, our 1B parameter model could not compete with much smaller encoder-only pre-trained models such as CodeBERT on CodeXGLUE or RoBERTa-large on SuperGLUE, which leads us to conclude that the representations are not sufficiently informative to justify the substitution of smaller scale encoder-only pretrained models with one Prefix-LM. For few-shot XSum, we did not observe meaningful differences between the two models, regardless of the number of exemplars in the non-causal part.",
            "score": 0.5990679608429252,
            "section_title": "Results and Findings",
            "char_start_offset": 18882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 726
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07781982421875
        },
        {
            "corpus_id": "243865137",
            "title": "Exploring Non-Autoregressive Text Style Transfer",
            "text": "All the NAR models adopt the same Transformer based encoder-decoder architecture, and the BaseAR model only differs from the NAR models by the following three differences discussed in Section 3.1. \n\n\u2022 The NAR model discards the autoregressive mask in the self-attention layer. Since the NAR model removes the conditional dependency among the output tokens, the causal mask where the position t can only attend to positions 1 . . . t \u2212 1 is no longer needed. Following Gu et al. (2018), we set the masks to prevent a position from attending to itself. \n\n\u2022 The NAR model incorporates a positionalattention layer in the decoder, which has been shown to facilitate local reordering in decoding (Gu et al., 2018). The positional-attention layer, placed between the self-attention layer and the inter-attention layer, takes the position embeddings as queries and keys while the decoder states as values. \n\n\u2022 The NAR model uniformly maps the source words as the decoder input to enrich the information on the decoder side. Specifically, position t in the decoder input takes the word embedding of the source token in position i = round( Tx Ty \u2022 t), where T x and T y denote the lengths of source input and target output, respectively. \n\nBoth the encoder and the decoder use a Transformer structure with d model = d hidden = 128, n head = 4, n layer = 2. Following existing works (Lample et al., 2019), the target style is treated as a special start token in decoder. Both the style classifier for automatic evaluation and the pretrained p \u03c8 in the style compatibility loss follow the TextCNN (Kim, 2014) architecture but are independently trained. To backpropagate the gradients from p \u03c8 to \u03b8, we approximate y in Eq. 4 with the softmax distribution sequence from which y should be sampled.",
            "score": 0.5987844665663056,
            "section_title": "C.1 Model Architecture",
            "char_start_offset": 16259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 199,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 550
                },
                {
                    "start": 553,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1783
                }
            ],
            "ref_mentions": [
                {
                    "start": 468,
                    "end": 484,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 690,
                    "end": 707,
                    "matchedPaperCorpusId": "3480671"
                },
                {
                    "start": 1585,
                    "end": 1596,
                    "matchedPaperCorpusId": "9672033"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15576171875
        },
        {
            "corpus_id": "276423946",
            "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case",
            "text": "Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder. \n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems. \n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51]. \n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way. \n\nNon-causal nlg strategies perform much worse in English than their causal counterparts [49]. However, to our knowledge, no prior research has been conducted on non-causal nlg in languages other than English. This work aims to evaluate whether bidirectional transformers trained on the mlm task could be successfully exploited in Spanish nlg.",
            "score": 0.598596393842626,
            "section_title": "Causality in generative transformer language models",
            "char_start_offset": 6569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 864
                },
                {
                    "start": 867,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1548
                },
                {
                    "start": 1551,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "231645376"
                },
                {
                    "start": 572,
                    "end": 575,
                    "matchedPaperCorpusId": "234785837"
                },
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "231715684"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "247126308"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "218487230"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "208224776"
                },
                {
                    "start": 1138,
                    "end": 1142,
                    "matchedPaperCorpusId": "212657570"
                },
                {
                    "start": 1142,
                    "end": 1145,
                    "matchedPaperCorpusId": "267201220"
                },
                {
                    "start": 1231,
                    "end": 1235,
                    "matchedPaperCorpusId": "265629619"
                },
                {
                    "start": 1436,
                    "end": 1440,
                    "matchedPaperCorpusId": "211069634"
                },
                {
                    "start": 1638,
                    "end": 1642,
                    "matchedPaperCorpusId": "60441316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4482421875
        },
        {
            "corpus_id": "271600495",
            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
            "text": "In this section, we first give the mathematical description of causal LLM, prefix LLM, and our ISM. Then, we elaborate on the benefits of our ISM compared to causal and prefix LLM, respectively. In addition, we introduce how to implement our ISM on existing prefix and causal LLMs. Given a sequence of input vectors X = (x 1 , \u2022 \u2022 \u2022 , x n ), the output of standard Softmax Self-Attention (SSA) layer is \n\nwhere P corresponds to the output projection, V, K, Q corresponds to the value, key, and query transformation, respectively. \n\nSince the softmax attention of standard transformers is non-linear, its theoretical analysis becomes complicated even for a single layer. For this reason, theoretical approaches Von Oswald et al. [2023], Zhang et al. [2023] to analyze transformers have often dropped the softmax function from the attention, resorting to the Linear Self-Attention (LSA) layer, \n\n3.1.2 Causal LLM, Prefix LLM, and ISM for Multi-turn dialogue \n\nHere, we briefly introduce the attentions of causal LM, prefix LM, and ISM in their LSA version for multi-turn dialogue. Given a multi-turn dialogue sample including the prompt \n\nThe most classic form of attention is categorized as full (or bidirectional) attention, shown in equation 3.2, in which each input x j (j = 1, \u2022 \u2022 \u2022 , n) can attend to all positions. Full attention is typically used in the transformer encoder. \n\nFurthermore, another transformer decoder for in-context learning uses the auto-regressive attention \n\nwhich restricts each token x j to attend only to previous positions (and itself) from 1 to j. This restriction is due to the role of the decoder as a causal language model (causal LLM) which predicts the next token in the context of the previously generated ones. \n\nThe original transformer uses a full attention based encoder and an auto-regressive attention based decoder. However, NLP researchers have a preference for models that are either encoder-only (e.g. BERTDevlin et al.",
            "score": 0.5970226596917988,
            "section_title": "Methodology",
            "char_start_offset": 6850,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 402
                },
                {
                    "start": 405,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1966
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30517578125
        },
        {
            "corpus_id": "211096576",
            "title": "CBAG: Conditional biomedical abstract generation",
            "text": "The specific mechanism is defined as follows, with d k representing the dimensionality of each Q and K embedding: \n\nThe \"multi-headed\" aspect of the transformer indicates that the self-attention mechanism is applied multiple times per-layer, per-element of the sequence. These multiple heads are then recombined through a feed-forward layer: (4)   where \n\nThe transformer model presented by Vaswani et al. [36] use the attention attention mechanism in three different ways. Within the encoder stack, which processes the input sequence in their proposed sequence-tosequence model, the K, Q, and V embeddings all come from the same sequence of tokens. This is referred to all \"self attention.\" In the decoder stack, the part of the model that uses the encoder output to generate a new sequence, these embedding matrices are masked during the attention function such that the output embedding for position i can only depend on prior elements. This is called \"masked self attention\". Following this operation, each decoder embedding is attended with all of the encoder embeddings. Specifically, Q values are derived from the decoder, while K and V values depend on the encoder. We refer to this operation as \"Encoder-Decoder Attention.\" Note that BERT [38] uses only the encoder self-attention layers, while GPT-2 [29] uses the decoder's masked self-attention layers. The work presented here uses all three. \n\nThe multi-head components are combined with a feed-forward operation, denoted FF, that projects the concatenated embedding into a larger dimensionality, applies the ReLU activation function, and then reduces back to the set embedding rank: FF(X) = max(0, XW )W \n\nThen, combined with a learned layer-wise normalization, these components combine to form encoder and decoder blocks. Omitting the standard dropout between each operation, the encoder block is defined as: \n\nwhile the decoder block is defined as: \n\n3 Multi-Conditional Language Model",
            "score": 0.597012418560552,
            "section_title": "Background",
            "char_start_offset": 4517,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 1913
                },
                {
                    "start": 1916,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "60441316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304931640625
        },
        {
            "corpus_id": "260435365",
            "title": "Pre-Trained Language Models for Text Generation: A Survey",
            "text": "Then, GPT-2 [153] explored the transfer capacity of language models for zero-shot generation task, highlighting the significance of sufficient data. Furthermore, GPT-3 [14] showed that massive model parameters can significantly improve the downstream generation tasks, with a few examples or prompts. CTRL [90] is proposed as a conditional causal LM to generate text based on control codes that govern style, content, and task-specific behavior. Causal LMs are simple and straightforward for text generation, but they have several structural and algorithmic limitations: Causal LMs encode the tokens just from left to right, thus ignore the bidirectional information on the input side. Moreover, causal LMs are not specially designed for the sequence-to-sequence generation tasks, thus in practice they do not achieve high performance in tasks such as summarization and translation [153]. \n\n4.1.3 Prefix Language Models. Upon a single Transformer, prefix LMs adopt bidirectional encoding scheme in the input side and natural left-to-right generation pattern in the output side. By utilizing the mixture attention mask, the tokens in the input text  can attend to each other, while the tokens in the target text  can only attend to all input tokens and previous generated tokens. \n\nUniLM [36] was the first prefix LM. Compared to causal LMs, UniLM used prefix attention mask to solve conditional generation tasks, similar to the encoder-decoder architecture. UniLMv2 [5] and GLM [39] improved vanilla prefix masking strategy by introducing permuted language modeling in XLNet [198]. Although prefix LMs have several advantages, Raffel et al. [154] compared single-Transformer prefix LMs to Transformer-based encoder-decoder LMs and concluded that adding explicit encoder-decoder attention is more effective to capture conditional dependencies. \n\n4.1.4 Encoder-Decoder Language Models. Encoder-decoder LMs follow the standard Transformer architecture for text generation, consisting of stacks of both encoder and decoder layers.",
            "score": 0.5962211022012798,
            "section_title": "Standard Architecture",
            "char_start_offset": 34171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 888
                },
                {
                    "start": 891,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 17,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1466,
                    "end": 1469,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 1478,
                    "end": 1482,
                    "matchedPaperCorpusId": "232270067"
                },
                {
                    "start": 1575,
                    "end": 1580,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1641,
                    "end": 1646,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2493896484375
        },
        {
            "corpus_id": "204838007",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "text": "dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nat the timestep corresponding to the classification token is then used to make a prediction for classifying the input sequence. The self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output. An attention matrix for this masking pattern is shown in Figure 3, middle.\n\nThe decoder in an encoder-decoder Transformer is used to autoregressively produce an output sequence. That is, at each output timestep, a token is sampled from the model's predicted distribution and the sample is fed back into the model to produce a prediction for the next output timestep, and so on. As such, a Transformer decoder (without an encoder) can be used as a language model (LM), i.e. a model trained solely for next-step prediction Radford et al., 2018;Al-Rfou et al., 2019). This constitutes the second model structure we consider. A schematic of this architecture is shown in Figure 4, middle. In fact, early work on transfer learning for NLP used this architecture with a language modeling objective as a pre-training method (Radford et al., 2018).\n\nLanguage models are typically used for compression or sequence generation (Graves, 2013). However, they can also be used in the text-to-text framework simply by concate",
            "score": 0.5951206243280238,
            "section_title": "Model Structures",
            "char_start_offset": 41573,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1642,
                    "end": 1663,
                    "matchedPaperCorpusId": "52004855"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "264805730",
            "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
            "text": "Most of the current decoder-only LLMs employ causal language modeling as the pre-training objective (Radford et al., 2018), which aims to predict future tokens based on the past. \n\narg min \n\nIn causal language modeling, the model only attends to tokens that occur before (the left context), resulting in a unidirectional attention scheme known as causal attention. This approach enables the model to process each token in the input sequence in order, without accessing any information from the future (the right context). \n\nCausal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j.",
            "score": 0.5946194405634495,
            "section_title": "Causal Language Modeling",
            "char_start_offset": 4923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6669921875
        },
        {
            "corpus_id": "273835244",
            "title": "Optimizing AI Transformer Models for CO\u2082 Emission Prediction in Self-Driving Vehicles With Mobile/Multi-Access Edge Computing Support",
            "text": "The decoder layer in transformer architectures (right side of Fig. 2) is responsible for generating text sequences. It comprises multiple sub-layers, including two multi-headed attention layers, a pointwise feed-forward layer, residual connections, and layer normalization after each sub-layer. \n\nSimilar to the encoder layer, the decoder layer incorporates multi-headed attention, which allows the model to capture dependencies and relationships between different elements in the input sequence. However, the multi-headed attention in the decoder operates differently due to the autoregressive nature of decoding. To prevent the decoder from conditioning future tokens during attention computation, a look-ahead mask is applied. This mask ensures that each word in the sequence can only attend to itself and the preceding words, excluding future words. \n\nThe decoder layer also includes a second multi-headed attention layer, where the queries and keys are derived from the outputs of the encoder, and the values are obtained from the first multi-headed attention layer outputs. This mechanism enables the decoder to determine which encoder inputs are relevant for attention. \n\nAdditionally, a pointwise feed-forward layer is employed to further process the output of the second multi-headed attention layer. This layer introduces nonlinearity and enables the model to learn complex relationships and patterns. \n\nThe final stages of the decoder layer involve a linear classifier, which has a dimensionality equal to the number of classes or words in the vocabulary. This classifier is responsible for assigning probabilities to different classes or words. The output of the classifier is then passed through a softmax layer, producing probability scores. The highest probability corresponds to the predicted word. \n\nThe decoder layer can be stacked multiple times, with each layer receiving inputs from the encoder and preceding layers. By stacking the layers, the model can learn to extract and focus on different combinations of attention, enhancing its predictive capability.",
            "score": 0.5941837908326794,
            "section_title": "C. DECODER",
            "char_start_offset": 21831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 294
                },
                {
                    "start": 297,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1176
                },
                {
                    "start": 1179,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2079
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.110107421875
        },
        {
            "corpus_id": "270214176",
            "title": "A Survey on Large Language Models for Code Generation",
            "text": "where x < represents the sequence of preceding tokens { 1 , . . .,   \u22121 } before x  in the input,  denotes the model parameters.The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.Formally, the causal language modeling objective for training encoder-decoder LLMs is to minimize loss function as follows:\n\nwhere x \u2264 is the source sequence input and x < denotes the target sequence autoregressively generated so far.During the inference phase, pre-trained LLMs that have been trained on largescale code corpus can generate code in a zero-shot manner without the need for fine-tuning.This is achieved through the technique of prompt engineering, which guides the model to produce the desired output11 [31,186].Additionally, recent studies have explored the use of few-shot learning, also referred to as in-context learning, to enhance model performance further [131,178].Denoising Autoencoding.In addition to causal language modeling (CLM), the denoising autoencoding (DAE) task has been extensively applied in pre-training encoder-decoder architectures for code generation, such as PLBART [6], CodeT5 [234], and its enhanced successor, CodeT5+ [232].Following T5 [189] and CodeT5 [234], the DAE refers to initially perturbing the source sequence by introducing randomly masked spans of varying lengths.This corrupted sequence serves as the input for the encoder.",
            "score": 0.5912633035627175,
            "section_title": "Pre-training",
            "char_start_offset": 42181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 65,
                    "end": 128
                },
                {
                    "start": 128,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 680
                },
                {
                    "start": 680,
                    "end": 765
                },
                {
                    "start": 765,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 1016
                },
                {
                    "start": 1016,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1470
                },
                {
                    "start": 1470,
                    "end": 1493
                },
                {
                    "start": 1493,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 1300,
                    "end": 1304,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1701,
                    "end": 1706,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 1744,
                    "end": 1749,
                    "matchedPaperCorpusId": "258685677"
                },
                {
                    "start": 1763,
                    "end": 1768,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1780,
                    "end": 1785,
                    "matchedPaperCorpusId": "237386541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53857421875
        },
        {
            "corpus_id": "268201845",
            "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
            "text": "In defining our model architecture and the objective for optimizing the model, we build on the insights gathered by Raffel et al. (2020) and rely heavily on their definitions and notations. Specifically, Raffel et al. (2020) consider two distinct objectives for unsupervised pre-training. First, they pursue a denoising objective (see, Devlin et al. 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other. The model then attempts to predict the correct tokens that are masked, corrupted or left unedited at these positions. Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens. Through extensive experiments, Raffel et al. (2020) conclude that the combination of an encoder-decoder architecture with a denoising objective yields the highest performance on a set of benchmark NLP tasks (cf. Table 2 in Raffel et al. 2020). Given the generally superior performance exhibited by an encoder-decoder architecture in their analyses, we also adopt this architecture in our work. However, since a denoising objective is not suitable for our problem structure, we rely on an LM objective. We further elaborate on our architecture and the objective in the next paragraphs. \n\nEncoder-decoder architecture. Following the insights from Raffel et al. (2020), we adopt the encoderdecoder architecture. We first introduce two concepts at a high level that are necessary to understand the encoder-decoder architecture: the embedding layer and the output layer. In NLP, words are represented as a one-hot vector and ML models typically operate on feature vectors that contain continuous values.",
            "score": 0.5909228974293755,
            "section_title": "Model Architecture and Objective",
            "char_start_offset": 32202,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 204,
                    "end": 224,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 336,
                    "end": 355,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1224,
                    "end": 1244,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1416,
                    "end": 1435,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1838,
                    "end": 1858,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "3480671",
            "title": "Non-Autoregressive Neural Machine Translation",
            "text": "In order to translate non-autoregressively and parallelize the decoding process, we modify the decoder stack as follows. \n\nDecoder Inputs Before decoding starts, the NAT needs to know how long the target sentence will be in order to generate all words in parallel. More crucially, we cannot use time-shifted target outputs (during training) or previously predicted outputs (during inference) as the inputs to the first decoder layer. Omitting inputs to the first decoder layer entirely, or using only positional embeddings, resulted in very poor performance. Instead, we initialize the decoding process using copied source inputs from the encoder side. As the source and target sentences are often of different lengths, we propose two methods: \n\n\u2022 Copy source inputs uniformly: Each decoder input t is a copy of the Round(T t/T )-th encoder input. This is equivalent to \"scanning\" source inputs from left to right with a constant \"speed,\" and results in a decoding process that is deterministic given a (predicted) target length. \u2022 Copy source inputs using fertilities: A more powerful way, depicted in Fig. 2 and discussed in more detail below, is to copy each encoder input as a decoder input zero or more times, with the number of times each input is copied referred to as that input word's \"fertility.\" In this case the source inputs are scanned from left to right at a \"speed\" that varies inversely with the fertility of each input; the decoding process is now conditioned on the sequence of fertilities, while the resulting output length is determined by the sum of all fertility values. \n\nNon-causal self-attention Without the constraint of an autoregressive factorization of the output distribution, we no longer need to prevent earlier decoding steps from accessing information from later steps. Thus we can avoid the causal mask used in the self-attention module of the conventional Transformer's decoder. Instead, we mask out each query position only from attending to itself, which we found to improve decoder performance relative to unmasked self-attention.",
            "score": 0.590802360755218,
            "section_title": "DECODER STACK",
            "char_start_offset": 7700,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 123,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 743
                },
                {
                    "start": 746,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "214161830",
            "title": "RobuTrans: A Robust Transformer-Based Text-to-Speech Model",
            "text": "As discussed in Section 2.1, the encoder-decoder attention mechanism is a crucial factor for the instability. However, simply removing this attention will also discard the advantages it brings to the TTS model. The advantages can be considered as the following two aspects. On the one hand, the encoder-decoder attention provides a holistic view of input sequence for the decoder, while on the other hand, it composes frame-level context vectors according to decoder inputs (which are mel frames). These two advantages make great contribution to the decoding procedure, and we propose \"pseudo non-causal attention\" (PNCA) to replace the causal self-attention layers as shown in Figure 4, which not only inherits the two features above, but also makes the decoding procedure robust. \n\nLet T be the total length of mel spectrum to be decoded, x l i be the autoregressive output of step i and layer l, h i be the tiled encoder hidden state of step i. For the time step t, the PNCA of layer l takes x l\u22121 1 , x l\u22121 2 , ..., x l\u22121 t3 and [h t , h t+1 ...h T ] as input. Specifically, let Attention(Q, K) be the multi-head attention, Figure 4: Phoneme-level to frame-level conversion and pseudo non-causal attention (PNCA). The left part of PNCA is causal self-attention, which takes the encoder hidden states fused with padded mel spectrum frames by a linear projection as input, while the right part consumes only the encoder hidden states. \n\nThen y l t is added to x l\u22121 t and consumed by FFN and following residual connection to obtain x l t .",
            "score": 0.5901528797878043,
            "section_title": "Pseudo Non-causal Attention",
            "char_start_offset": 12308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 781
                },
                {
                    "start": 784,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1436
                },
                {
                    "start": 1439,
                    "end": 1541
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11376953125
        },
        {
            "corpus_id": "248239682",
            "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks",
            "text": "In section 3.1, we conclude that Transformer with the causal attention masks is naturally sensitive to word order. Since the position information is inevitable for BERT, we propose to enhance existing BERT model based on causal attention masks.\n\nIn this paper, we intend to add the causal attention masks into all or some hidden layers of BERT. In this way, the specific layers with such masks are sensitive to word order by design, which can enhance the position encodings ability of BERT. Such framework can further result in better language understanding performances, e.g., in pretrained language modeling, casual attention masks were added on all 12 layers of GPT (Radford and Narasimhan, 2018). However, comparing with BERT (Devlin et al., 2019), we observe that GPT lags behind BERT on almost all downstream tasks. 3 This is because self-attention mechanism with such masks only consider one-side information flow, it cannot process the input sentence comprehensively and has a high risk of language information loss. Therefore, we can conjecture that it is not effective to use the causal attention masks in all hidden layers. There is a strong need to maintain a balance between the gain of position encoding ability and the loss of language information.\n\nIn order to determine which layer(s) should add casual attention masks, we refer to the BERTology work (Jawahar et al., 2019) that conduct comprehensive experiments to analyze and interpret the information captured by each layer of BERT. The experimental results indicate that the lower layers of BERT capture rich language structure information. The position information is also a common structure information, so that we propose to add the causal attention masks into the lower layers (e.g., the first two layers 4 ) to improve the position encoding ability of BERT. We denote our model as DecBERT. There are two versions of our model, DecBERT-Same and DecBERT-Diff. All of them are 12-layer base size models.\n\n\u2022 DecBERT-Same: This model has a similar structure as BERT (see Figure 1(a)), but we use the causal attention masks to convert the first two Encoder layers into two Decoder layers with the same direction (from left to right). So the 12-layer model has 10 Encoder layers and 2 Decoder layers, which is shown in",
            "score": 0.5887015798295654,
            "section_title": "Our DecBERT Model",
            "char_start_offset": 8778,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 751,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1368,
                    "end": 1390,
                    "matchedPaperCorpusId": "195477534"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298095703125
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "As shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders. \n\nBackbone Network We use Transformer (Vaswani et al., 2017) to build the models. Given an input sequence, we first pack their vector representations together. Then we feed the vectors into a multilayer Transformer, which encodes the input to contextualized representations. In each Transformer block, there is a multi-head self-attention layer and a feed-forward network layer that are used to aggregate the hidden states of the previous layer. Moreover, attention masks are used to control the context access. We use a triangular matrix as the attention mask for the universal task layer, so that it processes the input from left to right. For the bidirectional encoder, we allow all the tokens to access each other. After obtaining the output vectors of the universal task layer, we use a softmax classifier to predict over the vocabulary. The weight matrix is shared with the input token embeddings. \n\nConnector As shown in Figure 2, there is a connector layer between the universal task layer and various bidirectional encoders. The connectors project vector representations of bidirectional encoders before feeding them into the general-purpose interface. Moreover, the connectors are used to match the output dimensions of foundation models with the universal task layer. We empirically find that both linear projection and feed-forward network work well in our experiments.",
            "score": 0.5886493460767706,
            "section_title": "Model Architecture",
            "char_start_offset": 7682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2071
                },
                {
                    "start": 2072,
                    "end": 2174
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 197,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 483,
                    "end": 504,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3994140625
        },
        {
            "corpus_id": "249940391",
            "title": "An Improved Transformer-Based Neural Machine Translation Strategy: Interacting-Head Attention",
            "text": "as a masked language model (MLM) [17] and connected to specific networks. e decoder blocks have a similar structure as encoder blocks. e only difference lies in an additional sublayer, which computes the attention scores between the representations of the source sequence given by the encoder and the current target token representation given by the multihead SAN of the decoder. is sublayer, known as the encoder-decoder attention layer, is followed by a multihead attention layer. In the decoder, two attention mechanisms, namely, multihead self-attention and encoder-decoder attention, are arranged to capture the hidden state of the target token in each block. Since the token is only visible to its leftward tokens, the self-attention scores form a lowerdimensional triangular matrix. In other words, the multihead self-attention layer aims to focus the current target token only on the leftward tokens and mask the future tokens in the target sequence. In addition, the decoder learns the leftward token representations to generate the token probability distribution in each time step. During training, the probability distribution of the target token is computed based on the ground-truth leftward target tokens or their representations. All the representations are given by the encoder as a context vector for generating the target sequence. During inference, the current token probability distribution is computed based on the previous target token distribution. All the token representations are given by the encoder. e decoder works in a teacher-forcing way during training, while in an auto-regressive way during inference. e difference between the two stages is that the last token feature comes from the last ground-truth token and the last generated token given by the trained model, respectively.\n\nBecause the attention mechanism is not order-aware, the transformer-based models add the positional information into the tokens, for example, absolute positional embedding.",
            "score": 0.5878900585498814,
            "section_title": "Transformer.",
            "char_start_offset": 7315,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.224609375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": ". First, we propose to pretrain a non-causal decoder model with an MLM objective and then further train the model as a causal decoder with a FLM objective (language modeling adaptation). This conversion is simple, as the parameters and overall architecture can be kept the same, and only the attention mask needs to be switched. We note that we also attempted this adaptation from the decoder portion of an encoder-decoder model, but it performed significantly worse than training from scratch, as discussed in Appendix E.4. \n\nValidations losses are plotted in Figure 6, on the left. Starting from an MLM-pretrained non-causal decoder model speeds up convergence significantly compared to training a causal-decoder model with an FLM objective from scratch. To achieve a loss comparable to the one achieved after 168B tokens of FLM pretraining, language modeling adaptation requires only 105B additional tokens (a 1.6\u00d7 speed-up). This makes it possible to obtain both a high-quality zero-shot model and a good generative model, for only 1.6\u00d7 the cost of training a single model. A causal decoder-only pretrained with FLM from scratch (Causal FLM) compared to a model being adapted with FLM following 168B pretraining tokens as a non-causal masked language model (LMadapted Non-causal MLM). The adaptation requires 63% of the tokens (1.6\u00d7 speedup) versus training from scratch. Right: Causal and non-causal decoder-only masked language models (Causal MLM, Non-causal MLM) trained from scratch compared to a model being adapted to a non-causal MLM following 168B pretraining tokens as a causal FLM (MLM-adapted Causal FLM). The adaptation requires 30% of the tokens (3.3\u00d7 speedup) versus training the non-causal MLM from scratch. \n\nNon-causal masked language modeling adaptation (NC-A). To investigate alternative avenues for adaptation, we now introduce non-causal masked language modeling adaptation: starting from a causal decoder model pretrained with FLM as the objective, we then continue training the model as a non-causal decoder using an MLM objective.",
            "score": 0.5869629340004167,
            "section_title": "Language modeling adaptation (LM-A)",
            "char_start_offset": 35459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1726
                },
                {
                    "start": 1729,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 2058
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3984375
        },
        {
            "corpus_id": "222290948",
            "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding",
            "text": "Each Transformer decoder layer has an additional sub-layer to perform attention on the encoder output. \n\n2. To ensure the decoder can only decode tokens depending on the tokens in the past, it uses an attention mask to mask the attention values of the subsequent tokens. \n\nTherefore, the Transformer decoder can decode tokens autoregressively like other conventional language models such as RNN.",
            "score": 0.5855117771294951,
            "section_title": "Transformer",
            "char_start_offset": 6693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 105,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.324462890625
        },
        {
            "corpus_id": "264172191",
            "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
            "text": "Language models are designed to assign probabilities for every possible sequence of generated text. This overarching goal can be achieved through two primary approaches: autoregressive and non-autoregressive language modeling. Autoregressive language models typically concentrate on natural language generation and employ a \"next-word prediction\" pretrain task (Radford et al., 2018(Radford et al., , 2019Brown et al., 2020a). In contrast, non-autoregressive models focus more on natural language understanding, frequently leveraging the masked language modeling objective as their foundational task (Devlin et al., 2019a). Classic models from the BERT family fall under the category of non-autoregressive models (Devlin et (Zoph et al., 2022), which are built upon the encoder-decoder structure, as well as BloombergGPT , Gopher (Rae et al., 2021) and Claude 2 (Models, C.), which are based on decoder architectures. The majority of LLMs are based on decoder-only structures, and a significant reason for this is the leading results achieved by OpenAI in the GPT series (from GPT-1 to , with the decoder-only family of models demonstrating impressive performance. Besides the decoder-only structure, there is another type of architecture known as the prefix-decoder architecture, which has found some degree of application in LLMs. In contrast to the \"next-word prediction\" function used in decoder-only LLMs, the prefix-decoder architecture employs bidirectional attention on prefix tokens, similar to an encoder, while maintaining consistency with the decoder-only LLMs for the prediction of subsequent tokens. Existing representative LLMs based on prefix decoders include GLM130B (Zeng et al., 2022) and U-PaLM (Tay et al., 2022b).",
            "score": 0.5844648657876148,
            "section_title": "Modeling",
            "char_start_offset": 5383,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "221879187",
            "title": "Generating Commonsense Explanation by Extracting Bridge Concepts from Reasoning Paths",
            "text": "We utilize a pre-trained Transformer decoder (Radford et al., 2019) as our generation model which shares the parameter with the Transformer encoder. Essentially, it takes the statement x and the concepts c as input and auto-regressively generates the explanation y: \n\nAs shown in Figure 2, the input to the Transformer decoder is the token embeddings of both the statement and the selected concepts concatenated along the sequence length dimension. \n\nTo model bi-directional attention on the input side while preserving the causal dependency of the generated sequence, we adopt a hybrid attention mask where each token on the input side could attend to all the tokens in the input sequence while the generated token at each time step only attends to the input sequence and the previously generated tokens.",
            "score": 0.5838399521619972,
            "section_title": "Explanation Generation",
            "char_start_offset": 16154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 265
                },
                {
                    "start": 268,
                    "end": 448
                },
                {
                    "start": 451,
                    "end": 805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17724609375
        },
        {
            "corpus_id": "221702858",
            "title": "Efficient Transformers: A Survey",
            "text": "It is important to note the differences in how the Transformer blocks are used. Transformers can primarily be used in three ways, namely: (1) encoder-only (e.g., for classification), (2) decoder-only (e.g., for language modeling), and (3) encoder-decoder (e.g., for machine translation). In encoder-decoder mode, there are usually multiple multi-headed self-attention modules, including a standard self-attention in both the encoder and the decoder, along with an encoder-decoder cross-attention that allows the decoder to utilize information from the encoder. This influences the design of the self-attention mechanism. In the encoder mode, there is no restriction or constraint that the self-attention mechanism has to be causal, i.e., dependent solely on the present and past tokens. In the encoder-decoder setting, self-attention used in the decoder (i.e. across decoding positions) must be causal since each auto-regressive decoding step can only depend on previous tokens, whereas the selfattention used in the encoder need not. Fulfilling this requirement can prove challenging for many efficient self-attention designs. \n\nThe mode of usage of a Transformer model generally depends on the target application. Given an input sequence, the sequence is typically passed through an encoder stack. At this stage, there might be too options. For multi-class classification, a linear layer with Softmax outputs typically projects the sequence representation down to the number of classes. In the case of BERT (Devlin et al., 2018), this is a [CLS] token that is appended to the start of the sequence as a prefix. Recent work has also explored the usage of Encoder-Decoder architectures for classification, such as T5 (Raffel et al., 2019). Decoder-only models are typically used for generation and are trained using a language modeling objective (of predicting the next token). Due to the nature of the loss, these models are often superior for open ended generation (Brown et al., 2020).",
            "score": 0.5832372952719187,
            "section_title": "Transformer Mode",
            "char_start_offset": 9721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1127
                },
                {
                    "start": 1130,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 1509,
                    "end": 1530,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1717,
                    "end": 1738,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.328125
        },
        {
            "corpus_id": "264408158",
            "title": "Design of a Modified Transformer Architecture Based on Relative Position Coding",
            "text": "In addition, when processing long input sequences, the fixed-length semantic vector struggled to retain critical feature information, leading to reduced accuracy. To overcome these issues, attention mechanisms were introduced [14]. \n\nThe concept of attention mechanisms was initially proposed by Bahdanau et al. for machine translation and later improved by Luong et al. [15,16]. Drawing inspiration from human selective attention, attention mechanisms mimic the human process of rapidly scanning and focusing on relevant information while disregarding irrelevant details. In the context of deep learning, attention mechanisms act as a resource allocation mechanism [17][18][19]. They dynamically redistribute the weight of information based on its importance, ensuring that critical information is given higher weight, while less important information is assigned lower weight. This feature extraction and sequential data analysis capability has found applications in various fields, including language modeling and image processing [20,21]. \n\nAttention mechanism in the decoding process [22], each output not only depends on the fixed-size semantic vector encoded by the encoder, but also depends on the hidden layer state of the previous output unit and the corresponding hidden layer state of the current output unit in the decoding process. Attention is introduced into the Seq2Seq model to solve the problem that the original RNN often loses part of the input sequence information, and the accuracy of the model is improved. In the specific translation task [23,24], the decoding phase is to translate one word by one word in the time series. When decoding one word, it will not have the same association with all the words in the source sequence. In the decoder phase, the selected reference contributes the most to the semantic vector of the current sequence word, rather than uniformly referring to all the semantic vectors. \n\nThe introduction of attention mechanisms into Seq2Seq models aimed to address limitations in retaining input sequence information and improve model accuracy. During the decoding phase, rather than uniformly considering all input semantic vectors, attention mechanisms enable the model to selectively focus on the most relevant reference for the current sequence word. Prior to this development, the most effective language models were based on Seq2Seq architecture with LSTM for modeling.",
            "score": 0.5830309463223107,
            "section_title": "Introduction",
            "char_start_offset": 2206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1933
                },
                {
                    "start": 1936,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2424
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "219472430"
                },
                {
                    "start": 375,
                    "end": 378,
                    "matchedPaperCorpusId": "1921173"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "247368810"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "250372733"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "249480572"
                },
                {
                    "start": 1034,
                    "end": 1038,
                    "matchedPaperCorpusId": "237596895"
                },
                {
                    "start": 1038,
                    "end": 1041,
                    "matchedPaperCorpusId": "231875037"
                },
                {
                    "start": 1089,
                    "end": 1093,
                    "matchedPaperCorpusId": "246984992"
                },
                {
                    "start": 1564,
                    "end": 1568,
                    "matchedPaperCorpusId": "213550456"
                },
                {
                    "start": 1568,
                    "end": 1571,
                    "matchedPaperCorpusId": "221305796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11260986328125
        },
        {
            "corpus_id": "276925996",
            "title": "End-to-End Vector Simplification for Building Contours via a Sequence Generation Model",
            "text": "The decoder generates a simplified sequence of coordinates \n\nwhere n \u2264 m, in an autoregressive manner. The structure is composed of six identical Transformer layers stacked together. However, unlike the encoder, the decoder must handle attention from the encoder's output using cross-attention. Each Transformer layer consists primarily of a masked multihead self-attention mechanism, an encoder-decoder cross-attention mechanism, and a feedforward network with a residual mechanism, as shown in Figure 4. \n\nThe decoder is autoregressive when generating a simplified output; therefore, a causal mask is required for multihead attention during the training process to prevent label leakage. The causal mask is shown in Figure 5, where S and E represent the start and end of the sequence, respectively. The core of the decoder is the cross-attention mechanism with the encoder output, which is a hidden state. Through cross-attention, the model focuses on different parts of the input sequence when generating each output coordinate of the simplified result, allowing it to model the overall structure of the building outline and the relationships between various vertices. \n\nThe output of the final layer of the encoder is transformed into key (K) and value (V) matrices. These matrices represent the encoded representation of the input building outline, with each key corresponding to a vertex or feature in the outline and the value containing contextual information about that vertex or feature. \n\nThe output of the current layer of the decoder is linearly transformed into a query (Q) matrix that represents the information required by the decoder at the current moment. In the autoregressive decoding process, because of the presence of a causal mask, the query at the current moment can only see the previously generated output vertices. However, because K and V are present in each attention calculation, the complete information of the input outline is used in the decoding process. \n\nThe position embedding and attention calculation still use Formula ( 6), and the feedforward network and residual mechanism are the same as those in the encoder. The output of the decoder is passed to the next step of autoregressive decoding to generate the next output vertex.",
            "score": 0.582474461563456,
            "section_title": "Decoder",
            "char_start_offset": 20036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 58
                },
                {
                    "start": 61,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 505
                },
                {
                    "start": 508,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.31396484375
        },
        {
            "corpus_id": "277740986",
            "title": "Fusing Global and Local: Transformer-CNN Synergy for Next-Gen Current Estimation",
            "text": "The Transformer decoder is responsible for generating the target sequence by leveraging masked self-attention and encoder-decoder cross-attention mechanisms. The mathematical formulation is outlined as follows. \n\n1) Input Definition: Let the decoder input be defined as \n\nwhich represents the target sequence embeddings, and let the encoder output be \n\nwhere T denotes the target sequence length, L represents the source sequence length, and B and D indicate the batch size and embedding dimension, respectively. \n\n2) Masked Multi-Head Self-Attention: [1] Causal Masking: To prevent information leakage from future time steps, a lower triangular mask is applied to the self-attention score matrix: \n\nAttention Calculation: The masked self-attention mechanism is computed as follows: \n\nwhere \n\nwith W QKV \u2208 R D\u00d7D being a learnable projection matrix. This mechanism ensures that each position in the target sequence can only attend to previous positions, preserving the autoregressive nature of decoding. \n\n3) Encoder-Decoder Cross-Attention: Cross-Modal Interaction: The encoder-decoder cross-attention mechanism enables the decoder to incorporate contextual information from the encoder output. Specifically, the intermediate decoder representation is used as the query, while the encoder output serves as the key-value pair: \n\nwhere \n\nwith Y \u2032 representing the processed decoder input and W Q \u2208 R D\u00d7D being a trainable projection matrix. This mechanism aligns the target sequence generation with the source sequence representations, facilitating effective sequence-tosequence modeling.",
            "score": 0.5822541597820846,
            "section_title": "E. Transformer Decoder Architecture",
            "char_start_offset": 19174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 512
                },
                {
                    "start": 515,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 782
                },
                {
                    "start": 785,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1586
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 555,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "13748556",
            "title": "Unsupervised Neural Machine Translation with Weight Sharing",
            "text": "Roles \n\nTable 1: Interpretation of the roles for the subnetworks in the proposed system. \n\nDirectional self-attention Compared to recurrent neural network, a disadvantage of the simple self-attention mechanism is that the temporal order information is lost. Although the Transformer applies the positional encoding to the sequence before processed by the self-attention, how to model temporal order information within an attention is still an open question. Following (Shen et al., 2017), we build the encoders in our model on the directional self-attention which utilizes the positional masks to encode temporal order information into attention output. More concretely, two positional masks, namely the forward mask M f and backward mask M b , are calculated as: \n\nWith the forward mask M f , the later token only makes attention connections to the early tokens in the sequence, and vice versa with the backward mask. Similar to (Zhou et al., 2016;Wang et al., 2017), we utilize a self-attention network to process the input sequence in forward direction. The output of this layer is taken by an upper self-attention network as input, processed in the reverse direction. \n\nWeight sharing Based on the shared-latent space assumption, we apply the weight sharing constraint to relate the two AEs. Specifically, we share the weights of the last few layers of the Enc s and Enc t , which are responsible for extracting high-level representations of the input sentences. Similarly, we also share the first few layers of the Dec s and Dec t , which are expected to decode high-level representations that are vital for reconstructing the input sentences. Compared to (Cheng et al., 2016;Saha et al., 2016) which use the fully shared encoder, we only share partial weights for the encoders and decoders. In the proposed model, the independent weights of the two encoders are expected to learn and encode the hidden features about the internal characteristics of each language, such as the terminology, style, and sentence structure. The shared weights are utilized to map the hidden features extracted by the independent weights to the shared-latent space. \n\nEmbedding reinforced encoder We use pretrained cross-lingual embeddings in the encoders that are kept fixed during training.",
            "score": 0.5821052435362972,
            "section_title": "Networks",
            "char_start_offset": 8886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 5
                },
                {
                    "start": 8,
                    "end": 88
                },
                {
                    "start": 91,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2149
                },
                {
                    "start": 2152,
                    "end": 2276
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "273346770",
            "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive Modeling",
            "text": "For example, if a 8-token sequence is split into 4 sets with 1, 2, 2, 3 tokens, the causal mask should be like that in Fig. 2 (b1). In short, SAR extends 'next-set prediction' in Eq. 2 to the training phase. \n\nThe model implementation-Fully Masked Transformer. The realization of SAR is not straightforward, though. Classical AR models, e.g., the decoder-only transformer fail in three aspects. i) When AR shifts the sequence to align the current set with the previous set, it will find the number of tokens may not be equal. ii) AR models can only model the output-seen relations with fixed and simple 'next token' forms of relative positional relationships, rendering them ineffective in complex scenarios involving arbitrary sets. iii) Given a token at a specific position, AR models output it based on its relative steps to the first token, leading to failure when outputting arbitrary sets. These drawbacks inspire the design philosophy: i) the model should have perception of absolute positions for outputting arbitrary token sets, and ii) the output tokens and the seen tokens should be placed into two containers, each with positional encoding, to facilitate their position-aware interaction. \n\nStarting from the AR model, we split the decoder-only transformer into two parts, an encoder and an decoder. The encoder takes in the image tokens and extract the semantic features. The decoder records the output position with position embeddings and models the interaction between output tokens and seen tokens from the encoder, at the cost of adding cross-attention in each decoder layer. Additionally, generalized causal masks are added into each attention process, following the spirit of 'the current token set to be predicted can only see the preceding sets'. In short, it can be regarded as a vanilla encoder-decoder transformer (Vaswani, 2017) with generalized causal masks in all attention processes, as shown in Fig. 4. Consequently, we refer to it as the Fully Masked Transformer (FMT). Due to the fully causal feature, FMT naturally supports causal techniques like KV cache acceleration. \n\nThe training procedure.",
            "score": 0.5820909832700409,
            "section_title": "SET AUTOREGRESSIVE MODELING",
            "char_start_offset": 12574,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1200
                },
                {
                    "start": 1203,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2102
                },
                {
                    "start": 2105,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 1839,
                    "end": 1854,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1444091796875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We now focus on the relatively new practice of multitask finetuning, where there has not yet been any systematic study of the influence of the architecture and training objective. Notably, the two main papers advocating this practice use completely different approaches: Sanh et al. [2021] finetunes an encoder-decoder model pretrained with span corruption, whereas Wei et al. [2021] finetunes a decoder-only pretrained with full language modeling. It is not immediately clear which approach is more natural: while decoder-only models trained with full language modeling are better at zero-shot generalization (as evidenced in Section 4.1), encoder-decoder models and masked language modeling pretraining have been shown to perform significantly better after finetuning [Raffel et al., 2020]. We therefore evaluate every architecture and objective combination after multitask finetuning. \n\nOur results are outlined in Figure 4. The encoder-decoder pretrained with span corruption offers the best performance after multitask finetuning. Specifically, on EAI-Eval, the best performance is achieved by the encoder-decoder with MLM, and the non-causal decoder with MLM comes in a close second. However, the difference is more significant on T0-Eval, where the encoder-decoder with MLM pretraining outperforms other models by a large margin. Finally, encoder-decoder pretrained with PLM and causal decoder with MLM achieve significantly worse performance than other models. These results are consistent across all levels of pretraining (see early checkpoints in Appendix D). \n\nFinding 2. Encoder-decoder models trained with masked language modeling achieve the best zero-shot performance after multitask finetuning . More broadly, approaches that perform well in the single-task finetuning setting perform well on multitask finetuning. \n\nTable 3: After full or prefix language modeling pretraining, the causal decoder (FLM) exhibits the best zero-shot generalization abilities, followed closely by the non-causal decoder (PLM).",
            "score": 0.5818248802825503,
            "section_title": "After multitask finetuning",
            "char_start_offset": 29597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 770,
                    "end": 791,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171630859375
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "In this paper, we systematically studied the effects of pretraining objective and architecture choices on the zero-shot generalization abilities of large language models. Specifically, we compared language modeling and masked language modeling objectives applied to causal/non-causal decoder-only and encoder-decoder architectures. We also evaluated zero-shot performance with and without multitask finetuning. Notably, we found that the best objective and architecture is the opposite in these two settings: a causal decoder-only pretrained with full language modeling performs best if evaluated immediately after pretraining, whereas when adding a multitask finetuning step, an encoder-decoder pretrained with masked language modeling performs best. We therefore evaluate the practice of adaptation, to convert models across architectures and objectives. We found a simple efficient compromise, where a causal decoder-only model pretrained with full language modeling underwent additional masked language model training as a non-causal decoder-only model, yielding significant speedup in convergence over starting from scratch. This enables practitioners to get both an excellent generative model and a model that delivers good performance after multitask finetuning. Our results provide significant new insights into the design of LLMs. In the future, we are interested in work investigating architectures and objectives that perform well regardless of whether multitask finetuning is performed. To facilitate future work, we release all models, code, and data used in our study.",
            "score": 0.5808875940254385,
            "section_title": "Conclusion",
            "char_start_offset": 40729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412353515625
        },
        {
            "corpus_id": "268157336",
            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
            "text": "An example of encoder-decoder architecture is the transformer model proposed in [24]. Its encoder and decoder blocks are stacked with multiple layers. As shown in Figure 3, the transformer encoder layer consists of a self-attention layer and a position-wise feed-forward layer. In addition to these two layers, decoder consists of a third cross-attention layer, which is responsible for attending to encoder output. \n\nEncoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks. T5 [25] uses encoder-decoder architecture. As stated in T5, using encoder-decoder structure helped to achieve good performance regarding classification as well as for generative tasks. \n\nAlthough encoder-decoder models end up having twice as many parameters as their decoder-only or encoder-only counterparts, they still have similar computational cost. Compared to PrefixLM models where the parameters are shared, here, the input and target are independently processed and use separate sets of parameters. Unlike decoder-only language models that are trained to generate the input, encoder-decoder models output target tokens. \n\nThe original transformer consisted of encoder-decoder blocks and was initially used for sequence-to-sequence tasks, such as NMT. However, it was discovered that, with the change in how the input is fed to the model, the single-stack (decoder or encoder) could also complete sequence-sequence model tasks. As a result, the subsequent models started containing either an encoder or decoder architecture. Below, we discuss these architectural variants of the original transformer model.",
            "score": 0.5803153321626553,
            "section_title": "Encoder-Decoder-Based Model",
            "char_start_offset": 15589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1169
                },
                {
                    "start": 1172,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1612
                },
                {
                    "start": 1615,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2098
                }
            ],
            "ref_mentions": [
                {
                    "start": 988,
                    "end": 992,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1787109375
        },
        {
            "corpus_id": "276902726",
            "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation",
            "text": "where l denotes the layer index, X l and Y l represent the representations of the source and target at l-th layer, respectively, with X 0 and Y 0 indicating the embeddings. Each layer of the encoder first performs self-attention (SAtt) with a fully-visible mask pattern M f , indicating that all tokens in the source can attend to each other, followed by a feed-forward neural network (FFN). The encoder generates its final representation X L in parallel by stacking L such layers. The decoder is similar to the encoder but differs in that: (i) due to its autoregressive nature, its self-attention uses a causal mask pattern M c , allowing attention only to historical tokens; (ii) it incorporates an additional cross-attention (CAtt) layer to integrate representations from the source, where the mask M f is a fully-visible mask pattern, enabling the target to attend to the entire source. \n\nContrastingly, the decoder-only architecture processes both the source and target sequences within a single module: \n\nIn practice, due to the nature of causal attention, the representation of X is independent of Y. This allows us to encode X independently to obtain representations at each layer, as shown in Equation ( 10). Subsequently, the representations of X l at each layer are concatenated with the corresponding target representations, enabling self-attention computations of the target across both source and target sequences, as shown in Figure 10(b). It is important to note that: (i) when encoding X, we can modify its original causal attention mask M c to be fully-visible mask M f , similar to used in the encoderdecoder architecture, thus creating a variant of the CausalLM model known as the prefixLM model (Dong et al., 2019;Raffel et al., 2020); (ii) the fusion of source and target information is achieved by computing attention on concatenated representations of both, distinct from cross-attention, which we refer to in this paper as concat attention; (iii) the interaction between X and Y is Layer-Wise, rather than using only the top-layer representation of X as in the encoder-decoder architecture (TopOnly).",
            "score": 0.5787116601744959,
            "section_title": "Limitations",
            "char_start_offset": 32579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 2125
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "202565479",
            "title": "Self-Attentional Models Application in Task-Oriented Dialogue Generation Systems",
            "text": "As discussed before, Transformer is the first model that entirely relies on the self-attention mechanism for both the encoder and the decoder. The Transformer uses the self-attention mechanism to learn a representation of a sentence by relating different positions of that sentence. Like many of the sequence modelling methods, Transformer follows the encoder-decoder architecture in which the input is given to the encoder and the results of the encoder is passed to the decoder to create the output sequence. The difference between Transformer (which is a self-attentional model) and other sequence models (such as recurrencebased and convolution-based) is that the encoder and decoder architecture is only based on the self-attention mechanism. The Transformer also uses multi-head attention which intends to give the model the ability to look at different representations of the different positions of both the input (encoder self-attention), output (decoder self-attention) and also between input and output (encoder-decoder attention) (Vaswani et al., 2017). It has been used in a variety of NLP tasks such as mathematical language understanding [110], language modeling (Dai et al., 2018), machine translation (Vaswani et al., 2017), question answering (Devlin et al., 2018), and text summarization (Liu et al., 2018).",
            "score": 0.57701614076964,
            "section_title": "Transformer",
            "char_start_offset": 9490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1325
                }
            ],
            "ref_mentions": [
                {
                    "start": 1041,
                    "end": 1063,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1217,
                    "end": 1239,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "For example, we can use task instructions (Ouyang et al., 2022) to repurpose the model, and use demonstrations of some examples to conduct few-shot learning. \n\nNon-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available. In addition, non-causal encoder pretrained by the masked language modeling objective achieves competitive performance on cross-lingual transfer (Conneau et al., 2020), which makes it effective to adapt models to the multilingual setting. \n\nSemi-causal language modeling as a meta-pretraining task. Semi-causal language modeling plays the role of linking together non-causal encoders and the causal language model. It is a meta task in the sense of universal interface pretraining of pretrained encoders. Specifically, non-causal encoders learn to represent various input data, and a causal language model serves as a universal task layer. Non-causal encoders dock with a causal language model, so that we can benefit from both modeling methods described as above. In comparison with previous encoder-decoder pretraining (such as prefix language modeling, and T5; Raffel et al. 2020), our task non-causally encodes random spans of the whole sequence, while generating the rest via causal language modeling. Moreover, in terms of architecture, we directly feed the outputs of bidirectional encoders into the causal decoder, rather than relying on cross attention (Vaswani et al., 2017).",
            "score": 0.5758016927020727,
            "section_title": "body",
            "char_start_offset": 2111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2035
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 388,
                    "end": 409,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 995,
                    "end": 1017,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1714,
                    "end": 1733,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135986328125
        },
        {
            "corpus_id": "270370926",
            "title": "The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More",
            "text": "The Encoder-Decoder architecture used to train the MLM-U objective is modeled with ideas from XLNet Yang et al. (2020) in mind in order to support different attention/masking strategies including permutation language modeling.The encoder has GPT-like blocks and works with RoPE as positional bias.The decoder also has GPT-like blocks, but it cross-attends over keys and values from the corresponding encoder layer, also via a RoPE bias.The decoder input contains the same learnable embedding for all tokens, such that only the positional bias defines the initial attention pattern.This idea comes from XLNet's positional attention stream.In left to right AR training mode, both encoder and decoder use a causal attention mask.In MLM-X modes, a fraction of inputs are masked before given to the model and neither decoder nor encoder attend over the masked tokens.All inference is performed in left-to-right AR fashion.",
            "score": 0.5742276505759673,
            "section_title": "G Architecture Details",
            "char_start_offset": 30716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 226,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 436
                },
                {
                    "start": 436,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 917
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44140625
        },
        {
            "corpus_id": "248266379",
            "title": "A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond",
            "text": "Model Architecture. As for model architecture, both AT and NAT models take the encoder and decoder framework for translation. The encoder and decoder can be different neural networks, such as RNN [9], CNN [11], and Transformer [13]. Due to the superior performance of the Transformer network, we focus on the Transformer model for discussion in this survey. The encoder is used to encode the source sentences, while the decoder is utilized for decoding the target sentence. Compared to AT and NAT models, they adopt the same encoder architecture, and the differences are reflected in the decoders to match the specific training objective. (1) Specifically, AT models need to prevent earlier decoding steps from peeking at information from later steps. Therefore, the constraint of an autoregressive factorization of the output distribution is required, and they adopt the strict causal mask by applying a lower triangular matrix in the self-attention module of the conventional Transformer decoder [13]. (2) However, for NAT models, including the iteration-based NAT models, this constraint is no longer necessary, so they adopt the unmasked self-attention over all target tokens [16]. \n\n(3) As for SAT models, they adopt a coarse-grained lower triangular matrix as the causal mask, which means that they allow k tokens to peep later information in the same group while keeping the constraint between different groups. Inference Schedule. When going to the inference stage, the differences are as follows. (1) The AT models predict the target tokens in a one-by-one manner, and the tokens predicted previously are fed back into the decoder to generate the next token. (2) While SAT models predict a group of target tokens at one time, the previously generated groups of tokens are fed into the decoder to generate the next group of tokens, which is the same as the AT models. (3) For iterationbased NAT models, it needs k iterations for inference. The translated results of the previous iteration will be fed into the decoder again for refinements. (4) As for fully NAT models, they generate all predicted target tokens at only one step, which greatly speeds up inference.",
            "score": 0.5742183656055424,
            "section_title": "Comparison",
            "char_start_offset": 12320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 227,
                    "end": 231,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2469482421875
        },
        {
            "corpus_id": "275336136",
            "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
            "text": "Throughout the paper, we refer to this DFP solution leveraging a speech encoder or SFM as decoder-prepend. \n\nThe effectiveness of decoder-prepend has recently been questioned on the basis of the analogy with classic S2T encoder-decoder models (Chen et al., 2024c;\u017belasko et al., 2024), where the integration of the encoder output into the decoder is performed through cross-attention modules (Ao et al., 2021;Radford et al., 2023;Barrault et al., 2023). In addition, the outstanding performance of decoder-only LLMs on NLP tasks traditionally handled by encoder-decoder models has motivated the exploration of decoder-only S2T models (Wu et al., 2023;Gupta et al., 2024), which can be regarded as DFP solutions that question the need for a speech encoder and directly prepend speech features to the text embeddings. In this context, Wu et al. (2023); Gupta et al. (2024) highlighted the crucial role of relaxing the causal 1 masking in the self-attention modules typical of autoregressive models for the speech features, allowing them to look at each other freely, rather than being restricted to only previous elements. Notably, Gupta et al. (2024) claimed that this approach enables decoder-only models to even surpass encoder-decoder ones on the ASR task. On the contrary, to the best of our knowledge, no investigation on the effect of the causality property has been carried out for decoder-prepend models.",
            "score": 0.5740496690406903,
            "section_title": "Introduction",
            "char_start_offset": 1483,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1411
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 430,
                    "matchedPaperCorpusId": "252923993"
                },
                {
                    "start": 634,
                    "end": 651,
                    "matchedPaperCorpusId": "259501685"
                },
                {
                    "start": 833,
                    "end": 849,
                    "matchedPaperCorpusId": "259501685"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "Traditionally, auto-regressive language modeling has relied on decoder-only Transformers (Vaswani et al., 2017) with causal attention, trained using the next-token prediction objective. Causal attention ensures that each token can only attend to previous tokens, preventing future tokens from influencing past outputs. This mechanism makes training and inference more efficient, as past keys and values do not need to be recomputed for each token. This efficiency enables the scaling of decoder-only Transformers, such as GPT-4 (Achiam et al., 2023) and Llama-3 (Dubey et al., 2024), up to billions of parameters using current hardware. \n\nHowever, causal attention also introduces artificial constraints. Given tokens x 1 , x 2 , ..., x n , the contextual embedding of x j (where j < n) can only attend to embeddings of earlier tokens, even when predicting x n+1 . While this constraint ensures a strict causal structure, it may not always be necessary or beneficial. In principle, there is no inherent reason that language models should be limited by this restriction. Encoder-only Transformers, which are typically used for tasks like classification, do not impose this causality constraint. Though traditionally not used for auto-regressive tasks, encoder-only architectures can be adapted for next-token prediction. When computing the output at the current time step, an encoder-only Transformer, or any sequence model, can be made causal by only providing inputs Decoders use a causal attention, ensuring that each token attends only to the preceding tokens. In contrast, encoders allow all tokens to attend to each other by performing attention computation from scratch for each token prediction. \n\nup to and including the current time step. Therefore, in this work, we investigate the idea of using encoder-only Transformers for next-token prediction. We summarize our findings below. \n\nFunctions expressible with Decoder-only and Encoder-only Transformers. We demonstrate that the sets of functions expressible by decoder-only and encoder-only Transformers are not comparable, which goes against intuition that the expressivity of encoders would subsume that of decoders.",
            "score": 0.5739546153104679,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 636
                },
                {
                    "start": 639,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 111,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274169921875
        },
        {
            "corpus_id": "268264275",
            "title": "Where does In-context Translation Happen in Large Language Models",
            "text": "The transformer consists of stacked blocks of self-attention, which itself consists of smaller units of self-attention heads that are concatenated before being fed through a fully connected layer. In autoregressive decoder-only transformers, training and inference adopts a causal mask, where current positions are only able to attend to previous timesteps, instead of being able to attend to the entire input sequence. Unlike encoder-decoder NMT models where source and target sentence have separate processing transformer blocks, decoder-only means that the same model weights are both used to \"encode\" the source sentence and \"decode\" the target sentence in a single continuous sequence.",
            "score": 0.5724417127750662,
            "section_title": "A.4. Autoregressive Decoder only Transformer",
            "char_start_offset": 30389,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 690
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25146484375
        },
        {
            "corpus_id": "270062996",
            "title": "Disentangling and Integrating Relational and Sensory Information in Transformer Architectures",
            "text": "The standard Transformer architecture is composed of repeated blocks of attention (information retrieval) followed by an MLP (local processing). Our proposed Dual Attention Transformer follows this same structure, but replaces multi-head self-attention with dual attention. At each layer, dual attention dynamically retrieves both sensory and relational information from the previous level of computation, which is then processed locally by an MLP. Algorithms 2 and 3 defines an encoder and decoder block with dual attention. Composing these blocks yields the Dual Attention Transformer architecture. \n\nAlgorithm 2: Dual Attention Encoder Block \n\nx \u2190 Norm(x + MLP(x)) \n\nOutput: x Algorithm 3: Dual Attention Decoder Block \n\nThe Dual Attention Transformer framework supports all architectural variants of the standard Transformer, making it applicable to a wide range of task paradigms. An encoder-decoder architecture with causal dual-head attention in the decoder can be applied to sequence-to-sequence tasks, as in the original Transformer paper [4]. An encoder-only architecture can be used for a BERT-style language embedding model [6] or a ViT-style vision model [10]. A decoder-only architecture with causal dual-head attention can be used for autoregressive language modeling.",
            "score": 0.5721688290939558,
            "section_title": "The Dual Attention Transformer Architecture",
            "char_start_offset": 17893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 600
                },
                {
                    "start": 603,
                    "end": 644
                },
                {
                    "start": 647,
                    "end": 667
                },
                {
                    "start": 670,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1283
                }
            ],
            "ref_mentions": [
                {
                    "start": 1048,
                    "end": 1051,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1136,
                    "end": 1139,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1168,
                    "end": 1172,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1461181640625
        },
        {
            "corpus_id": "252546669",
            "title": "Generative Prompt Tuning for Relation Classification",
            "text": "In this section, we detail the implementation based on RoBERTa. For the RoBERTa version, we concatenate the source and target as inputs together and use partial causal masking to distinguish the encoder-decoder representations, as shown in Figure 6. Specifically, we utilize one RoBERTa model as both the source and target embeddings by concatenating the source sequence T (x) and target y as inputs together. We provide a partial causal attention mask to distinguish the source / target representations. The attention mask -2-dimensional matrix denotes as M. Specifically, for the source tokens, the mask allows full source self-attention, but mask out all target tokens. For i = [0, 1, ..., |T (x)| \u2212 1], | \u2022 | represents the length of sequence, \n\nTo guarantee that the decoder is autoregressive, we enforce the target tokens to only attend to previous tokens and not attend to future tokens. For",
            "score": 0.572077598599929,
            "section_title": "RoBERTa Version",
            "char_start_offset": 27423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 898
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "269009682",
            "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
            "text": "in an unsupervised manner using only a set of unordered sentences. These unsupervised approaches typically create two different representations of the same sentence for contrastive learning. The methods vary in how they form these representations -perturbing the input sentence (Wu et al., 2020), or using different model instances (Carlsson et al., 2021). SimCSE (Gao et al., 2021), the approach used in this work, generates two representations of the same sentence by passing it through the model twice with different dropout masks. \n\nTurning decoder-only LLMs into text encoders While decoder-only LLMs have outperformed bidirectional encoders across a large variety of language understanding tasks (Brown et al., 2020;Touvron et al., 2023;Jiang et al., 2023a, inter alia), their impact on sentence representation learning remains limited. The most common approaches in literature use the final hidden state of the last token as the sentence embedding (Neelakantan et al., 2022;Ma et al., 2023;Wang et al., 2023). \n\nThere are few works that explore the limitations of using a causal attention mask when adapting decoder-only LLMs for text classification and sentence representation tasks. Li et al. (2023b) experiment with removing the causal mask of Llama-2 during supervised fine-tuning for text classification and NER tasks. Similarly, Duki\u0107 & \u0160najder (2024) enable bidirectional attention for a group of layers during supervised fine-tuning on NER and chunking. In the context of sentence representation learning, Li & Li (2024) explore enabling bidirectional attention in the last layer of a decoder-only model during supervised contrastive fine-tuning on STS tasks. \n\nConcurrent to our work, several works have focused on converting decoder-only-LLMs to text encoders in supervised and unsupervised manner. Jiang et al. (2023b) and Lei et al. (2024) prompt the language model to summarize the input text in one word, and take the last layer's hidden embedding for the last token as the text's representation.",
            "score": 0.5712746680676619,
            "section_title": "Unsupervised text encoders Another line of work has explored training text embedders",
            "char_start_offset": 22008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1016
                },
                {
                    "start": 1019,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 332,
                    "end": 355,
                    "matchedPaperCorpusId": "235613354"
                },
                {
                    "start": 364,
                    "end": 382,
                    "matchedPaperCorpusId": "233296292"
                },
                {
                    "start": 702,
                    "end": 722,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 981,
                    "end": 997,
                    "matchedPaperCorpusId": "16299141"
                },
                {
                    "start": 1521,
                    "end": 1535,
                    "matchedPaperCorpusId": "265066823"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.133544921875
        },
        {
            "corpus_id": "204838007",
            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
            "text": "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe first model structure we consider is an an encoder-decoder Transformer, which consists of two layer stacks: The encoder, which is fed an input sequence, and the decoder, which produces a new output sequence. A schematic of this architectural variant is shown in the left panel of Figure 4.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern and appends a special \"classification\" token to the input. BERT's output of the self-attention mechanism are denoted x and y respectively. A dark cell at row i and column j indicates that the self-attention mechanism is allowed to attend to input element j at output timestep i. A light cell indicates that the self-attention mechanism is not allowed to attend to the corresponding i and j combination. Left: A fully-visible mask allows the self-",
            "score": 0.5695582616857068,
            "section_title": "Model Structures",
            "char_start_offset": 39750,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "253080830",
            "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
            "text": "In the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations. That said, later we show through experiments, that for the task of MT, it is clearly better to use a full attention mask for C in TLM. \n\nThe matrix A in Fig. 1 is whitened throughout this work, because we do not allow the source positions attend to target positions. However, theoretically, when decoding position i, one could allow all source positions 1, 2, ..., J to attend to all previous target positions 1, 2, ..., i \u2212 1. This can be done by using a (J + I) \u00d7 (J + I) \u00d7 I attention 2 The exact format of the tags is not important so long as uniquely identifiable translation direction tags are used, be it source and target tags like <s>, </s>, <t>, </t>, or direction tags like <s2t>, or even only the target language tag <t>. While the later two reduce the total sequence length, the former is more versatile when data from multiple languages or multiple modalities is considered. mask tensor.",
            "score": 0.5694799491169129,
            "section_title": "On the Attention Mechanism",
            "char_start_offset": 9099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2053
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 50,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "274992300",
            "title": "Segment-Based Attention Masking for GPTs",
            "text": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding. \n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design. \n\nThe most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2020). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder. However, PrefixLM requires training from scratch and is limited to single-turn inputs, overlooking scenarios with multiple prefill phases, as often encountered in chat-based systems. \n\nIn contrast, our approach enables the easy enhancement of SOTA decoder-only models by unlocking the potential of bidirectional attention in non-generated segments through lightweight finetuning. Trained on massive corpora with causal masking, these models can be enhanced with limited hardware and just a few hours of fine-tuning, enabling them to effectively use bidirectional attention during the prefill phase.",
            "score": 0.5676424168617182,
            "section_title": "Related Work",
            "char_start_offset": 3399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1185
                },
                {
                    "start": 1188,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 616,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 950,
                    "end": 970,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1278,
                    "end": 1299,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55615234375
        },
        {
            "corpus_id": "273233776",
            "title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning",
            "text": "Unlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;Zhang et al., 2023a) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness. Therefore, we predict the masked regions autoregressively: Given a text query T -for instance, \"A person is walking in a circle\"-our primary objective is to rank motions from a comprehensive database based on their semantic alignment with the textual input. The ultimate aim is to retrieve the motion that exhibits the highest correspondence with the provided textual description, effectively bridging the gap between disparate modalities-text and motion. We hypothesize that a model exhibiting better alignment between these modalities will inherently possess improved cross-modal retrieval capabilities, and can serve as a more reasonable R-Precision evaluator.",
            "score": 0.5651876206356716,
            "section_title": "AUTOREGRESSIVE GENERATION",
            "char_start_offset": 18980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 238,
                    "matchedPaperCorpusId": "255942203"
                },
                {
                    "start": 980,
                    "end": 999,
                    "matchedPaperCorpusId": "232134936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "237491753",
            "title": "Exploring a Unified Sequence-To-Sequence Transformer for Medical Product Safety Monitoring in Social Media",
            "text": "and drop-out layers. The decoder attends to the output of the encoder using several attention layers. The self-attention layers, instead, employ masking to make the decoder only attend to the past tokens, in an auto-regressive manner:\n\nwhere p \u03b8 decoder (y i |Y 0:i\u22121 ) is the probability distribution of the next token y i . Finally, the output of the decoder passes through a SoftMax layer over the vocabulary. Raffel et al. (2019) proposed to add a prefix in front of the input sequence to inform the model about which task to perform (e.g. summarization, question answering, classification etc.; see Figure 1).The model was trained on the Colossal Clean Crawled Corpus (C4), a massive corpus (about 750 GB) of web-extracted and cleaned text.",
            "score": 0.5631474602826001,
            "section_title": "The T5 Model",
            "char_start_offset": 7847,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.160888671875
        },
        {
            "corpus_id": "269981935",
            "title": "Bitune: Bidirectional Instruction-Tuning",
            "text": "Our approach shares similarities with the concept of \"prefix language modeling\", which enables a decoder-only model to handle bidirectional context within a prefix (instruction) while maintaining causal generation for the output sequence.The prefix-LM architecture was introduced by Liu et al.\n\n(2018) and further explored and popularized by Raffel et al. (2020).In their work on T5, Raffel et al. (2020) pretrained the prefix-LM architecture alongside other architectures, such as encoder-decoder and decoder-only models, demonstrating that prefix-LM outperforms decoder-only models on both training objectives: denoising and language modeling.\n\nThe prefix-LM approach has been used in UniLM (Dong et al., 2019), which trains a single transformer on three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction.UniLM employs a shared Transformer network and utilizes specific self-attention masks to control the context that predictions are conditioned on, where the sequence-to-sequence task is equivalent to the prefix-LM approach.\n\nAdditionally, UL2 (Tay et al., 2023) introduces a pretraining objective called \"Mixture of Denoisers\", which combines various denoising strategies, including the prefix-LM approach.Lastly, XLNet Yang et al. ( 2019) also allows for non-causal word ordering by allowing random permutations to be used with a next-token prediction objective.\n\nAll these works focused on the model pretraining.As for the utilization of pretrained causal language models, Springer et al. (2024) show in their work that simply repeating the input to these models improves the quality of token embeddings for text-retrieval.This work addresses the limitation that token embeddings in autoregressive models cannot contain information from tokens appearing later in the input.By repeating the input twice, the early tokens are allowed to encode information Table 6: Qualitative results on Llama3-8B.We show a response for LoRA and Bitune.\n\n[GSM8K] Question:\n\nJanet's ducks lay 16 eggs per day.",
            "score": 0.5627690000494794,
            "section_title": "Related Work",
            "char_start_offset": 19262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 238,
                    "end": 293
                },
                {
                    "start": 295,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 645
                },
                {
                    "start": 647,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1080
                },
                {
                    "start": 1082,
                    "end": 1263
                },
                {
                    "start": 1263,
                    "end": 1420
                },
                {
                    "start": 1422,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1682
                },
                {
                    "start": 1682,
                    "end": 1832
                },
                {
                    "start": 1832,
                    "end": 1955
                },
                {
                    "start": 1955,
                    "end": 1994
                },
                {
                    "start": 1996,
                    "end": 2013
                },
                {
                    "start": 2015,
                    "end": 2049
                }
            ],
            "ref_mentions": [
                {
                    "start": 342,
                    "end": 362,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 384,
                    "end": 404,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.080078125
        },
        {
            "corpus_id": "273502294",
            "title": "All You Need is an Improving Column: Enhancing Column Generation for Parallel Machine Scheduling via Transformers",
            "text": "We employ the transformer architecture (Vaswani et al., 2017) depicted in Figure 3 as a baseline. A transformer consists of an encoder and a decoder, both of which utilize the multi-head attention mechanism, which plays a crucial role in capturing dependencies within an input sequence, essential for modeling interactions between jobs and the machine in our pricing subproblem. Attention Mechanism. The attention mechanism was introduced to improve the performance of neural networks in tasks involving long sequences, such as machine translation. In its most general form, the attention mechanism computes a weighted sum of values corresponding to elements in the input sequence. \n\nThe weights are determined by the similarity (often through dot-product or other scoring functions) between a set of queries and keys, where the queries search for the information that a model is looking for, and the keys represent various characteristics of the input elements. If keys, queries, and values are derived from the same input sequence, the mechanism is referred to as self-attention and allows tokens of the sequence to attend to each other. Mathematically, given an embedding matrix E \u2208 R n\u00d7d for an input sequence with  key, and value matrices, respectively. The attention score is then computed as: \n\nThe softmax operation normalizes the weights in each column of the attention matrix between 0 and 1 in such a way that they add up to 1. The final matrix obtained from the attention mechanism is in R n\u00d7dv and captures the relation between each pair of tokens in the input sequence. \n\nMasked Attention During training, the entire target sequence is available, and all sequence positions are generated (decoded) in parallel. Therefore, in the decoder part of the transformer, a masked version of self-attention is used to ensure that only tokens decoded in the earlier steps s 0 , s 1 , ..., s t\u22121 but not future tokens are considered when decoding the next token s t . For a target schedule of length n, the mask takes the form of an n \u00d7 n upper triangular matrix M given by: \n\nThe masked attention mechanism is then computed by: \n\nThis mask fills the attention score for the positions of future tokens with negative infinity.",
            "score": 0.5626333310215987,
            "section_title": "Transformer Network",
            "char_start_offset": 18542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1299
                },
                {
                    "start": 1302,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2076
                },
                {
                    "start": 2079,
                    "end": 2130
                },
                {
                    "start": 2133,
                    "end": 2227
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.260498046875
        },
        {
            "corpus_id": "273026224",
            "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
            "text": "The Transformer architecture Vaswani et al. (2017) has become a foundational model in a wide range of tasks across natural language processing (NLP), computer vision (CV), and multimodal applications. Central to its effectiveness is the attention mechanism, which enables the model to focus on relevant parts of the input data. In the vanilla attention mechanism, the attention weights are computed as a scaled dot-product between query and key vectors, as shown in Equation (1). To implement complex logic, the mask  can be added to the   term before applying the softmax function, controlling token visibility by setting certain elements to \u2212\u221e.  0 1 2 3 4 5 6 7 8 9                       0 1 2 3 4 5 6 7 8    Shared Question Mask is utilized in Reward Models (RM) and Direct Preference Optimization (DPO) models, allowing multiple answers to share a single question, thus eliminating redundant computations and speeding up training Ouyang et al. (2022). The Global + Sliding Window Mask combines global attention with sliding window attention, where global tokens attend to all tokens while others use a sliding window mask, effectively handling tasks requiring both global context and local details Zaheer et al. (2020). \n\nCausal BlockWise Mask, primarily used in in-context learning, divides sequences into blocks, where demonstrations only attend to nearby examples within small blocks, while the test example can attend to all demonstrations, allowing the study of model performance improvements in long-context tasks Bertsch et al. (2024). Prefix LM Causal Mask is tailored for language modeling tasks, allowing a prefix to attend to all tokens to generate coherent text based on the prefix Raffel et al. (2020). Prefix Document Mask extends this concept to multiple documents, where a prefix in each document attends to all tokens within that document but not across documents. \n\nQK-Sparse Mask optimizes self-attention by sparsifying query-key pairs, reducing computational load while maintaining performance, which is particularly beneficial for large-scale models Kitaev et al. (2020). Hash-Sparse Mask employs locality-sensitive hashing to partition sequences into smaller chunks, enabling efficient sparse attention for long sequences Kitaev et al. (2020).",
            "score": 0.5621861347403391,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1223
                },
                {
                    "start": 1226,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2269
                }
            ],
            "ref_mentions": [
                {
                    "start": 934,
                    "end": 954,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1698,
                    "end": 1718,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2420654296875
        },
        {
            "corpus_id": "252918454",
            "title": "ST-former for short-term passenger flow prediction during COVID-19 in urban rail transit system",
            "text": "Sequential Attention Block Sequential Attention Block consists of Causal Convolution ProbSparse Self-Attention with Mask (Masking CPSA) and conventional self-attention blocks, which can integrate the spatial-temporal dependencies learned by Encoder and Decoder. \n\nThe CPSA with mask aims to capture the decoder sequence, and it works the same as CPSA. The only difference is that the masking operation is adopted to prevent each position from attending to coming positions. The mask is designed as a  \u00d7  matrix with elements above the diagonal taking the value \u2212\u221e and others taking zero. Thus, the CPSA with mask can be modified as follows. \n\n  (, , ) = (  \u0305   \u221a  + ) (22) The conventional self-attention block is introduced to connect the output of the encoder with each decoder layer. In this block, the queries come from the previous Masking CPSA, and the memory keys and values come from the output of encoder. It can allow every position in the decoder sequence to attend overall positions in the encoder sequence.",
            "score": 0.5621418101852482,
            "section_title": "4.4.1",
            "char_start_offset": 39525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1019
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 672,
                    "matchedPaperCorpusId": "109275349"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.276611328125
        },
        {
            "corpus_id": "270870509",
            "title": "Visual Language Model-Based Cross-Modal Semantic Communication Systems",
            "text": "\u2022 FF sublayer: The FF layer comprises linear layers and activation functions, facilitating non-linear transformations of vectors for each patch to enhance the model's adaptability.The output of the FF layer in the first image encoder layer is\n\nwhere W b,f and b b,f are the weights and biases of the FF layer in the image encoder of the BLIP model, and GeLU denotes the activation function.Finally, the output of the image encoder with L encoder layers is\n\nwhere m f f,L means the output of the L-th encoder layer.\n\n2) Text decoder: The text decoder of the BLIP model adopts a BERT structure, capable of generating image-related textual content, such as descriptions, titles, and dialogues, based on features extracted from images.The text decoder is composed of multiple stacked decoder layers, each decoding layer comprising three sublayers: Causal Self-Attention (CSA), Cross Attention (CA), and FF sublayers.The specific workflow of the text decoder is as follows:\n\n\u2022 CSA sublayer: CSA is a type of self-attention mechanism that only allows the attention model to access the current and previous inputs, but not the future inputs [24].To ensure the causality of the textual generation process, the CSA sublayer utilizes a mask matrix to prevent the current token from accessing information from future tokens.Here, a token refers to the basic unit in the text, typically a word or a subword.The output of the CSA sublayer in the first text decoder layer is\n\nwhere CSA is the causal self-attention operator [24], D 0 is the initial token, which is typically set as \"[Decoder]\" by default.\u2022 CA sublayer: CA allows the vector of each token to interact with the feature vectors of visual information from the input image [25].The output of the CA sublayer in the first text decoder layer can be calculated as follows:\n\nwhere CA is the cross attention operator [25].\u2022 FF sublayer: The FF layer comprises linear layers and activation functions.",
            "score": 0.5617570014759284,
            "section_title": "A. BLIP-based CKB for semantic extraction",
            "char_start_offset": 22181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 242
                },
                {
                    "start": 244,
                    "end": 390
                },
                {
                    "start": 390,
                    "end": 455
                },
                {
                    "start": 457,
                    "end": 514
                },
                {
                    "start": 516,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 968
                },
                {
                    "start": 970,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1460
                },
                {
                    "start": 1462,
                    "end": 1591
                },
                {
                    "start": 1591,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1817
                },
                {
                    "start": 1819,
                    "end": 1865
                },
                {
                    "start": 1865,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 1134,
                    "end": 1138,
                    "matchedPaperCorpusId": "232135026"
                },
                {
                    "start": 1510,
                    "end": 1514,
                    "matchedPaperCorpusId": "232135026"
                },
                {
                    "start": 1721,
                    "end": 1725,
                    "matchedPaperCorpusId": "232404237"
                },
                {
                    "start": 1860,
                    "end": 1864,
                    "matchedPaperCorpusId": "232404237"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "47005349",
            "title": "Multilingual Neural Machine Translation with Task-Specific Attention",
            "text": "It would be possible to use a separate prefix token for the attention key (in a similar manner to that of source-specific attention), but there is no explicit set of attention parameters that should be used for the zero-shot directions. The prefix token could specify that the attention parameters associated with either the source or target language of the zero-shot direction should be used. \n\nAll three of our task-specific attention model variants still share most of their encoder and decoder parameters. The task-specific attention models require only a very small increase in the total number of parameters. For hidden state dimensionality d, we add one additional set of attention weights (d \u00d7 d parameters) and bias (d parameters) for each supported task. For the neural network topology used in our experiments (see Section 5, below), a target-specific attention model with support for four distinct target languages (i.e. tasks) requires only a 1.2% increase in the total number of model parameters, compared to the shared-attention version of the model.",
            "score": 0.5606636196336501,
            "section_title": "Task-Specific Attention Models",
            "char_start_offset": 11745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1065
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "256868804",
            "title": "Transformer models: an introduction and catalog",
            "text": "A generic encoder/decoder architecture (see Figure 1) is made up of two models. The encoder takes the input and encodes it into a fixed-length vector. The decoder takes that vector and decodes it into the output sequence. The encoder and decoder are jointly trained to minimize the conditional log-likelihood. Once trained the encoder/decoder can generate an output given an input sequence or can score a pair of input/output sequences. \n\nIn the case of the original Transformer architecture, both encoder and decoder had 6 identical layers. In each of those 6 layers the Encoder has two sub layers: a multi-head attention layer, and a simple feed forward network. Each sublayer has a residual connection and a layer normalization. The output size of the Encoder is 512. The Decoder adds a third sublayer, which is another multi-head attention layer over the output of the Encoder. Besides, the other multi-head layer in the decoder is masked to prevent attention to subsequent positions. It is clear from the description above that the only \"exotic\" elements of the model architecture are the multi-headed attention, but, as described above, that is where the whole power of the model lies! So, what is attention anyway? An attention function is a mapping between a query and a set of key-value pairs to an output. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Transformers use multi-headed attention, which is a parallel computation of a specific attention function called scaled dot-product attention. I will refer you again to the The Illustrated Transformer4 post for many more details on how the attention mechanism works, but will reproduce the diagram from the original paper in Figure 2 so you get the main idea There are several advantages of attention layers over recurrent and convolutional networks, the two most important being their lower computational complexity and their higher connectivity, especially useful for learning long-term dependencies in sequences.",
            "score": 0.5595070962772153,
            "section_title": "Encoder/Decoder architecture",
            "char_start_offset": 1310,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 2108
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08819580078125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "We find that the MLM-adapted model performs best by a significant margin and outperforms every other model we considered on EAI-Eval. Furthermore, the measured zero-shot generalization is in line with the MLM-pretrained non-causal decoder reported in Figure 4, though it still lags behind the MLM-pretrained encoder-decoder, despite the adapted models having seen 51 billion additional tokens. Finally, we note that performing non-causal multitask finetuning of the causal model produces no meaningful change in performance. \n\nFinding 3. Decoder-only models can be efficiently adapted from one architecture/objective prior to the other. Specifically, to obtain both a generative and a multitask model with the smallest total compute budget possible, we recommend starting with a causal decoder-only model, pretraining it with a full language modeling objective, and then using non-causal masked language modeling adaptation before taking it through multitask finetuning . Figure 7: Applying non-causal MLM adaptation to a causal decoder-only FLM before multitask finetuning improves zero-shot performance, even when controlling for additional LM pretraining for the same number of tokens. Zero-shot generalization on T0-Eval (left) and EAI-Eval (right), for the T5-LM and T0 baselines (grey), and for models from our study. Converting the model into a non-causal decoder for multitask finetuning only does not improve performance on T0-Eval. Results after adaptation are in line with non-causal decoder-only pretrained with MLM in Figure 4.",
            "score": 0.5595007422473148,
            "section_title": "Language modeling adaptation (LM-A)",
            "char_start_offset": 39174,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1540
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2191162109375
        },
        {
            "corpus_id": "258735144",
            "title": "Motif2Mol: Prediction of New Active Compounds Based on Sequence Motifs of Ligand Binding Sites in Proteins Using a Biochemical Language Model",
            "text": "The multi-head attention sub-layers corresponding to those in the encoder operate on encoder-generated hidden states as well as the output of the first decoder module. Thereby, the multi-head attention sub-layers can learn relationships between sequence encodings on the encoder side and structure encodings on the decoder side and pay attention (that is, assign importance) to particular sequence segments based on structural features (and vice versa). This architecture facilitates an effective use of the self-attention mechanism. By contrast, the masked attention sublayers representing the second type only operate on the output of the preceding attention sub-layer of the decoder modules. The masked attention sub-layers identify (and mask) transmitted information that should not be utilized to ensure that translated encodings are created in the correct sequential order. Hence, these layers are designed to prevent translation errors. SMILES tokens are sampled according to the probability distribution learned by the model. Output probabilities are derived in the softmax layer and the decoder generates a 512-dimensional embedding of the output sequence via positional encoding (corresponding ot the encoder).",
            "score": 0.5587990072282003,
            "section_title": "Model architecture",
            "char_start_offset": 8590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49609375
        },
        {
            "corpus_id": "274992300",
            "title": "Segment-Based Attention Masking for GPTs",
            "text": "Causal attention, as an autoregressive mechanism, restricts information flow to propagate only from earlier tokens to later ones. While this design is essential during the autoregressive decoding phase, it is unnecessarily restrictive during the prefill phase, in which the entire prompt is available at once. Specifically, causal masking prevents the model from leveraging information from later tokens in the prompt, introducing a constraint in the attention computation. \n\nFor instance, consider the following example from the commonsense reasoning task ARC-Challenge (Clark et al., 2018): Please choose the correct answer to the question: Giant sloths lived in the Americas and the Caribbean during the Ice Age... Most of these sloths disappeared... some of these sloths lived alongside humans. What is the most likely reason that these last giant sloths became extinct? Answer1: disease... Answer4: humans as predators...\" \n\nTo correctly answer this question, the model must infer a specific information from the prompt (that human lived next to giant sloths) and ignore other (the fact that sloths lived through the Ice Age is a distracting detail). In a standard autoregressive model with causal masking, the text is read unidirectionally. This means the model cannot contextualize the final question while reading the initial sentences. Its success relies solely on its ability to memorize the prompt, as it cannot revisit earlier parts of the text when processing subsequent information. In contrast, humans readers can revisit earlier sentences or questions to focus on relevant details and build a coherent understanding. \n\nThe unidirectional nature of GPTs with causal masking imposes a limitation on their capabilities to integrate context from the entire prompt. To address this limitation, we propose Masked Attention by Segment (MAS), which adapts the attention mechanism to process full prompts more effectively. Similar to encoder-decoder, MAS operates in two modes: i. Prefill Phase: MAS removes the strict causal masking within each input prompt, allowing tokens to attend to both earlier and later tokens in the same block.",
            "score": 0.5586576236393535,
            "section_title": "Method",
            "char_start_offset": 10843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2144
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79931640625
        },
        {
            "corpus_id": "243985875",
            "title": "A Survey of Visual Transformers",
            "text": "Fig. 4 shows the overall Transformer models with the encoder-decoder architecture. Specifically, it consists of N successive encoder blocks, each of which is composed of two sub-layers. 1) An MHSA layer aggregates the relationship within the encoder embeddings. 2) A position-wise FFN layer extracts feature representations. For the decoder, it also involves N consecutive blocks that follow a stack of the encoders. Compared with the encoder, each decoder block appends to a multi-head cross-attention layer to aggregate both decoder embeddings and encoder outputs, where Y corresponds to the former, and X is the latter as shown in Eq. (1). Moreover, all of the sub-layers in both encoder and decoder employ a residual connection [11] and a Layer Normalization [163] to enhance the scalability of the Transformer. In order to record the sequential information, each input embedding is attached with a positional encoding at the beginning of the encoder stack and the decoder stack. Finally, a softmax operation are used for predicting the next word. \n\nIn an auto-regressive language model, the Transformer is originated from the machine translation tasks. Given a sequence of words, the Transformer vectorizes the input sequence into the word embeddings, adds the positional encodings, and feeds the resulting sequence of the vectors into an encoder. During training, as illustrated in Fig. 4, Vaswani et al. design a masking operation according to the rule for the autoregressive task [1], where the current position only depends on the outputs of the previous positions. Based on this masking, the Transformer decoder is able to process the sequence of the input labels parallelly. During the inference time, the sequence of the previously-predicted words is processed by the same operation to predict the next word.",
            "score": 0.5584940981160471,
            "section_title": "D. Transformer Model",
            "char_start_offset": 9579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1820
                }
            ],
            "ref_mentions": [
                {
                    "start": 638,
                    "end": 641,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 732,
                    "end": 736,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1488,
                    "end": 1491,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26904296875
        },
        {
            "corpus_id": "257482688",
            "title": "An Overview on Language Models: Recent Developments and Outlook",
            "text": "The transformer architecture [82] can capture long-term dependencies and important sequence components by exploiting a self-attention mechanism. Unlike the recurrent structure of RNNs, a transformer is easy to parallelize in both training and inference. Its structure is shown in Fig. 6. It consists of an encoder and a decoder. Before being sent to the encoder, the input textual sequence is first converted to an embedding through an embedding layer plus positional embedding. Multi-head attention, which is an ensemble of multiple self-attention mechanisms, enables the transformer to capture more robust and diverse attention between tokens. The other parts in the transformer encoder include feed-forward layers, residual connections, and normalization layers. The difference between the transformer encoder and decoder is that the transformer decoder has an additional masked multi-head attention layer. The masking ensures the decoder can only access preceding tokens of the current one, which makes the decoder auto-regressive.\n\nBased on different purposes, transformers have encoder-only, decoder-only, and encoder-decoder three variants as shown in Table 1 and Fig. 7. Encoder-only models can access all positions given an input and utilize bi-directional contexts to predict words. They are suitable for tasks requiring understanding full sentences, such as text classification.\n\nTransformer decoder-only models can only use previous words to predict the current word (namely, auto-regressive models). They are good at text generation tasks such as story generation. Transformer encoder-decoder models can access all words in the encoding phase, and words before the current word in the decoding phase. They are suitable for sequence-to-sequence tasks such as translation and summarization.",
            "score": 0.5574007151708666,
            "section_title": "Transformers",
            "char_start_offset": 21464,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.470947265625
        },
        {
            "corpus_id": "274741420",
            "title": "A benchmark and a multi-stage pipeline for classifying underwater videos at scale",
            "text": "The attention layers follow a fully connected Feed-Forward network, which transforms the output of the attention layer through two linear transformations and an activation function, typically ReLU. It is also worth noting that each layer has a normalisation step, and its output is added to the layer's input before being passed on. \n\nThe decoder is structured similarly to the encoder, though the input consists of the sequence that the model is currently generating. Furthermore, the self-attention layer is masked, as it contains a masking mechanism to ensure that the prediction for a certain position only depends on the known outputs of positions before it (Vaswani et al. 2023). In language generation models, the masking step is crucial, as the words must be generated based on preceding words and not future ones. This step, however, is not always required in pure classification tasks, as the positional order of the input is not relevant. Following the masking layer, another attention layer receives the encoder output as input. By receiving the queries from the masked attention layer and the keys and values from the encoder, the decoder determines which parts of the input sequence hold the greater relevance (Vaswani et al. 2023). Given the purpose of the decoder, transformer models applied to image classification tasks do not need it. Instead, they rely entirely on the encoder, where the model is also adapted to handle two-dimensional image data (Liu et al. 2021a).",
            "score": 0.5565714837031719,
            "section_title": "Transformer",
            "char_start_offset": 12423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 332
                },
                {
                    "start": 335,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1486
                }
            ],
            "ref_mentions": [
                {
                    "start": 1467,
                    "end": 1485,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.346923828125
        },
        {
            "corpus_id": "257687801",
            "title": "CoBIT: A Contrastive Bi-directional Image-Text Generation Model",
            "text": "Overall, the two modes share the Transformer layers' parameters, and only differ in input processing and attention masks. Our assumption is that, compared with the design of plain image encoders as in previous work [57,49], the additional decoding mode can exploit the common knowledge learned in image encoding to generate image autoregressive features, which we hypothesize should boost the (text-to-)image generation capacity. \n\nText Unicoder. Similar to image unicoder mentioned above, the text unicoder also has both encoding and decoding modes, which reuse the Transformer parameters. In both modes, the same tokenizer and embedding layer are utilized to obtain token features, given that they share the same input formats. A causal attention mask is applied in decoding mode. During encoding of text, there are two options in previous work: bi-directional mask [11,34,58], or causal mask [4,32,54]. We empirically found that two masks make no difference in performance, and use causal masking as the default in the reported experiments. \n\nCross-modal Decoder The cross-modal decoder performs as a fusion-and-generation module, which structure-wise follows the cross-attention decoder [48,57]. When generating text, the input is the text autoregressive feature from the text unicoder in decoding mode; encoded image features will be treated as cross-attention information, i.e., key and value in cross-attention layers. When generating the image, symmetrically, the image token autoregressive feature from the image unicoder in decoding mode is input and crossattends to encoded text features. Also, different from text generation where plain causal (autoregressive) mask is used in cross-modal decoder, image generation employs a convshaped masked sparse attention [36,58,9], which can save memory and computation brought by long sequences of image tokens.",
            "score": 0.5563235290286879,
            "section_title": "Architecture",
            "char_start_offset": 10549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1863
                }
            ],
            "ref_mentions": [
                {
                    "start": 872,
                    "end": 875,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 895,
                    "end": 898,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 898,
                    "end": 901,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "232035663"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.437744140625
        },
        {
            "corpus_id": "221507666",
            "title": "Looking for change? Roll the Dice and demand Attention",
            "text": "The attention mechanism was first introduced by Bahdanau et al. (2014) for the task of neural machine translation 4 (hereafter NMT). This mechanism addressed the problem of translating very long sentences in encoder/decoder architectures. An encoder is a neural network that encodes a phrase to a fixedlength vector. Then the decoder operates on this output and produces a translated phrase (of variable length). It was observed that these types of architectures were not performing well when the input sentences were very long (Cho et al., 2014). The attention mechanism provided a solution to this problem: instead of using all the elements of the encoder vector on equal footing for the decoder, the attention provided a weighted view of them. That is, it emphasized the locations of encoder features that were more important than others for the translation, or stated another way, it emphasized some input words that were more important for the meaning of the phrase. However, in NMT, the location of the translated words is not in direct correspondence with the input phrase, because of the syntax changes. Therefore, Bahdanau et al. (2014) introduced a relative alignment vector, e i j , that was responsible for encoding the location dependences: in language, it is not only the meaning (value) of a word that is important but also its relative location in a particular syntax. Hence, the attention mechanism that was devised was comparing the emphasis of inputs at location i with respect to output words at locations j. Later, Vaswani et al. (2017) developed further this mechanism and introduced the scaled dot product self-attention mechanism as a fundamental constituent of their Transformer architecture. This allowed the dot product to be used as a similarity measure between feature layers, including feature vectors having large dimensionality. \n\nThe idea of using attention for vision tasks soon passed to the community. Hu et al. (2017) introduced channel based attention, in their squeeze and excitation architecture. Wang et al. (2017) used spatial attention to facilitate non-local relationships across sequences of images. Chen et al. (2016) combined both approaches by introducing joint spatial and channel wise attention in convolutional neural networks, demonstrating improved performance on image captioning datasets.",
            "score": 0.5543254283677898,
            "section_title": "On attention",
            "char_start_offset": 6607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1860
                },
                {
                    "start": 1863,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2343
                }
            ],
            "ref_mentions": [
                {
                    "start": 1536,
                    "end": 1557,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058349609375
        },
        {
            "corpus_id": "241033425",
            "title": "An Empirical Study of Training End-to-End Vision-and-Language Transformers",
            "text": "We then compare the encoder-only and encoder-decoder architecture. For the encoder-only model, we use the same co-attention model as in Section 4.2. For the encoderdecoder model, we set the number of layers to 3 for both the encoder and decoder, and each decoding layer has two separate cross-attention blocks that attend to the vision and text representations, respectively. According to [6], we adopt T5-style [36] language modeling objective as it works well for their model. Specifically, we mask 15% of input text tokens and replace contiguous text span with sentinel tokens, and the decoder is trained to reconstruct the masked tokens. For image-text matching, we feed the decoder with a special class token and it generates a binary output.\n\nResults. As shown in Table 6, the encoder-only model can outperform the encoder-decoder model on our two discriminative tasks, which is consistent with the findings in [6]. However, it should be noted that the encoder-decoder architecture is more flexible, as it can perform tasks such as image captioning which may not be that straightforward for an encoder-only model to be applied to.",
            "score": 0.5542191754729245,
            "section_title": "Encoder-Only vs. Encoder-Decoder",
            "char_start_offset": 24352,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 389,
                    "end": 392,
                    "matchedPaperCorpusId": "231802355"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 917,
                    "end": 920,
                    "matchedPaperCorpusId": "231802355"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.171875
        },
        {
            "corpus_id": "220525530",
            "title": "Explore and Explain: Self-supervised Navigation and Recounting",
            "text": "The structure of our decoder follows that of the Transformer [14], and thus relies on self-attentive and cross-attentive operations. \n\nGiven a partially decoded sequence of words W = {w 0 , w 1 , ..., w \u03c4 }, each represented as a one-hot vector, the decoder applies a self-attention operation in which W is used to build queries, keys and values. To ensure the causality of this sequence encoding process, we purposely mask the attention operator so that each word can only be conditioned to its left-hand sub-sequence, i.e. word w t is conditioned on {w t } t \u2264t only. Afterwards, a cross-attention operator is applied between W and R to condition words on regions, as follows: \n\nAs in the Transformer model, after a self-attention and a cross-attention stage, a position-wise feed-forward layer is applied, and each of these operators is encapsulated within a residual connection and a layer norm operation. Finally, our decoder stacks together multiple decoder layers, helping to refine the understanding of the textual input. \n\nOverall, the decoder takes as input word vectors, and the t-th element of its output sequence encodes the prediction of a word at time t + 1, conditioned on {w t } \u2264t . After taking a linear projection and a softmax operation, this encodes a probability over words in the dictionary. During training, the model is trained to predict the next token given previous ground-truth words; during decoding, we iteratively sample a predicted word from the output distribution and feed it back to the model to decode the next one, until the end of the sequence is reached. Following the usual practice in image captioning literature, the model is trained to predict an end-of-sequence token to signal the end of the caption.",
            "score": 0.5541707741989179,
            "section_title": "C. Captioning module",
            "char_start_offset": 17632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1747
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2137451171875
        },
        {
            "corpus_id": "271956878",
            "title": "Understanding Defects in Generated Codes by Language Models",
            "text": "The multi-head self-attention mechanism applies linear transformations to the inputs, creating multiple projections of the input that are later recombined into the final output. The decoder sequentially produces the output tokens in an auto-regressive manner, choosing each subsequent token based on both the input to the model and the partially formed output sequence. In this study, we used CodeT5+ [13]-an encoder-decoder model. \n\nDecoder-only models consist solely of a decoder, without an accompanying encoder. These models initiate with a baseline state and incrementally construct an output sequence, taking into account the tokens previously generated [17], [18]. The functionality of the decoders in these models mirrors that found in encoder-decoder architectures, with a focus on comprehending the target language and its subtleties [17]. In this study, we used CodeGen [14]-a decoder-only model.",
            "score": 0.5534913732878173,
            "section_title": "A. Language Models",
            "char_start_offset": 6113,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 431
                },
                {
                    "start": 434,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 907
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0836181640625
        },
        {
            "corpus_id": "267402678",
            "title": "The evolution, applications, and future prospects of large language models: An in-depth overview",
            "text": "As shown in Figure 3, similar to most seq2seq models, the Transformer architecture consists of an encoder and a decoder. Overall, LLM (Language Model) models can be categorized into three major types: Encoder-decoder Architecture, Causal Decoder Architecture, and Prefix Decoder Architecture [11]. The Encoder-decoder Architecture uses the most basic structure and was initially introduced by the Seq2Seq model to address sequence-to-sequence tasks, such as machine translation. It consists of an encoder and a decoder. The encoder is responsible for transforming the input sequence into a fixeddimensional semantic representation, while the decoder uses this semantic representation to generate the output sequence. Within the encoder-decoder structure, self-attention mechanisms are commonly employed for sequence modeling, enabling the model to handle variable-length input and output sequences. This architecture has proven to be highly effective in various sequence-to-sequence tasks, such as text translation, text summarization, and dialogue generation. Prominent examples of large language models following this architecture include ELMo, BERT, RoBERTa, among others [12]. \n\nCurrently, the most widely used architecture is the Causal Decoder, which is primarily employed for handling autoregressive generation tasks, where each element of the output sequence depends on previously generated elements. The Causal Decoder Architecture is an improvement over the Encoderdecoder structure, as it introduces an autoregressive mechanism in the decoder. This means that when generating the current element, the model only uses the elements generated before it. This ensures that the model does not have access to future information during the generation process, thereby preserving causality. The GPT series (e.g., GPT-3) is a typical example of models that use the Causal Decoder Architecture. These models generate text by sequentially producing words one by one, avoiding information leakage and enabling the generation of coherent and plausible text. \n\nCompared to the Encoder-decoder models, Decoder-only models offer several advantages due to their simpler structure, faster training and inference speed, suitability for pure generation tasks, and advantages in decoder self-supervision.",
            "score": 0.5526110307943141,
            "section_title": "Different structures for combining encoders and decoders.",
            "char_start_offset": 10124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2294
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289306640625
        },
        {
            "corpus_id": "237635166",
            "title": "Unsupervised Translation of German\u2013Lower Sorbian: Exploring Training and Novel Transfer Methods on a Low-Resource Language",
            "text": "We used the MASS (Song et al., 2019) model, which is a 12-layer encoder-decoder (6 layers each) Transformer model identical to the XLM (Lample and Conneau, 2019) architecture. The difference comes in the training, using the MASS sequence masking (MA) objective allows both the encoder and decoder to be trained in the language model pretraining phase. This can be contrasted with XLM, which only pretrains the encoder.",
            "score": 0.5507223276864242,
            "section_title": "Architecture",
            "char_start_offset": 6302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 418
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042083740234375
        },
        {
            "corpus_id": "6589519",
            "title": "Causal and anti-causal learning in pattern recognition for neuroimaging",
            "text": "Pattern recognition in neuroimaging aims to provide insights into the neural basis of cognitive processes. Two types of models are used in this endeavor: encoding-and decoding models. Encoding models predict a subject's brain state for a given experimental condition, while decoding models aim to reconstruct experimental conditions from neuroimaging data. This difference has important consequences for the interpretation of brain state features that are found to be relevant in each type of model. \n\nIt has been argued that only encoding models can provide a complete functional description of a region of interest [1]. Decoding models, on the other hand, may determine brain state features as relevant that are statistically independent of the experimental condition [2]. While in linear decoding models potential misinterpretations can be avoided by converting them into encoding models [3], this is a substantially more difficult problem for non-linear decoding models. As decoding models are becoming ever more popular in the analysis of neuroimaging data [4], the correct interpretation of such models is of considerable importance. \n\nIn this paper, we argue that the distinction between encoding-and decoding models is not sufficient to determine the meaning of relevant features in each type of model: Pattern recognition models need to be further distinguished with respect to whether they learn causal-or anti-causal relations [5]. In general, neuroimaging studies are based on the following causal structure: stimulus \u2192 brain activity \u2192 response. We note that more complex experimental paradigms, in which responses again act as stimuli [6], can also be modeled in this way by considering time-resolved variables, e. g. stimulus \n\n. Depending on whether experimental conditions are chosen to represent stimuli or responses, encoding-and decoding models then model causalor anti-causal relations. In the following, we argue that this has important consequences for the interpretation of relevant features in each type of model. Furthermore, we argue that interpretation of neuroimaging data de facto requires causal inference problems to be solved. \n\nThe remainder of this article is organized as follows. In section II we introduce the necessary notation and terminology to formulate our proposed distinction of pattern recognition models in section II-D.",
            "score": 0.5501392407324484,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 499
                },
                {
                    "start": 502,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1740
                },
                {
                    "start": 1743,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2159
                },
                {
                    "start": 2162,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 617,
                    "end": 620,
                    "matchedPaperCorpusId": "27072109"
                },
                {
                    "start": 770,
                    "end": 773,
                    "matchedPaperCorpusId": "15154531"
                },
                {
                    "start": 891,
                    "end": 894,
                    "matchedPaperCorpusId": "4512713"
                },
                {
                    "start": 1062,
                    "end": 1065,
                    "matchedPaperCorpusId": "9431428"
                },
                {
                    "start": 1438,
                    "end": 1441,
                    "matchedPaperCorpusId": "17675972"
                },
                {
                    "start": 1649,
                    "end": 1652,
                    "matchedPaperCorpusId": "4712461"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0296478271484375
        },
        {
            "corpus_id": "265511054",
            "title": "Recent Advances in Text Analysis",
            "text": "In a traditional encoder-decoder pair, the encoder maps the input sequence into a fixed-length vector, and the decoder has access to this vector only.The attention mechanism allows the encoder to pass all the hidden states (not just the final encoded vector) to the decoder, along with annotation vectors and attention weights to tell the decoder which part of information to \"pay attention to\".The attention mechanism was shown to be much more effective than RNNs in processing long documents.Vaswani et al. (2017) proposed a special architecture called transformer that uses self-attention within each of the encoder and decoder and cross-attention between them.The transformer has become the most popular architecture in NLP.For example, the encoder part of the transformer is the building block of models like BERT (see below), and the decoder part of the transformer is the building block of models like GPT (Radford et al. 2018) for text generation.",
            "score": 0.5498753517838612,
            "section_title": "Commonly used neural network architectures",
            "char_start_offset": 35204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 150,
                    "end": 395
                },
                {
                    "start": 395,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 955
                }
            ],
            "ref_mentions": [
                {
                    "start": 494,
                    "end": 515,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17236328125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.",
            "score": 0.549491233197658,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2315673828125
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "Context and Hypothesis Prefix-LM is appealing due to the bi-directional attention, which allows for the model to contextualize hidden states with both past and future tokens. Controlled by only the existence of an attention mask, the ease of switching the role as an encoder or decoder allows for trivial unification, if the resulting model is competitive. \n\nBased on the recent success with Prefix-LM (Tay et al., 2022b), we hypothesize that the trained Prefix-LM yields a competitive encoder representation that can be used for a range of discriminative tasks, while retaining generative capability as the decoder. \n\nTo test this hypothesis, we train Prefix-LM with a mixture of causal language modeling and span corruption, given that denoising yields strong representations (Raffel et al., 2020;Tay et al., 2022a). Aiming for wider coverage of tasks, we train the model on both programming and natural language and evaluate each model on the defect detection task from CodeXGLUE (Lu et al., 2021) and 4 tasks from SuperGLUE (Wang et al., 2019), respectively. We attach a classification head on top of the hidden state at a certain time step (first or last token) to adapt to each task. In addition to the finetuning-based discriminative tasks, we examine if the in-context learning ability is enhanced by bi-directional attention. We follow UL2 and evaluate the models trained on natural language  1: Pass@1 on HumanEval. For HumanEval-Infill, only end-of-mask truncation is used. \n\non XSum as a few-shot generation task. We examine the change in performance by increasing the number of examples fed into the model as context.",
            "score": 0.5491709338111463,
            "section_title": "REPRESENTATION",
            "char_start_offset": 17227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1484
                },
                {
                    "start": 1487,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 778,
                    "end": 799,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1028,
                    "end": 1047,
                    "matchedPaperCorpusId": "143424870"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17578125
        },
        {
            "corpus_id": "44172616",
            "title": "Dense Information Flow for Neural Machine Translation",
            "text": "Prior works show a trend of designing more expressive attention mechanisms (as discussed in Section 2). However, most of them only use the last encoder layer. In order to pass more abundant information from the encoder side to the decoder side, the attention block needs to be more expressive. Following the recent development of designing attention architectures, we propose DenseAtt as the dense attention block, which serves for the dense connection between the encoder and the decoder side. More specifically, two options are proposed accordingly. For each decoding step in the corresponding decoder layer, the two options both calculate attention using multiple encoder layers. The first option is more compressed, while the second option is more expressive and flexible. We name them as DenseAtt-1 and DenseAtt-2 respectively. Figure 3 shows the architecture of (a) multi-step attention (Gehring et al., 2017), (b) DenseAtt-1, and (c) DenseAtt-2 in order. In general, a popular multiplicative attention module can be written as: \n\nwhere Q, K, V represent query, key, value respectively. We will use this function F in the following descriptions. \n\nDenseAtt-1 In the decoding phase, we use a layer-wise attention mechanism, such that each decoder layer absorbs different attention information to adjust its output. Instead of treating the last hidden layer as the encoder's output, we treat the concatenation of all hidden layers from encoder side as the output. The decoder layer multiplies with the encoder output to obtain the attention weights, which is then multiplied by a linear combination of the encoder output and the sentence embedding. The attention output of each layer a l can be formally written as: \n\nwhere F(\u2022, \u2022, \u2022) is the multiplicative attention function, [\u2022] is a concatenation operation that combines all features, and L(\u2022) is a linear transformation function that maps each variable to a fixed dimension in order to calculate the attention value. \n\nNotice that we explicitly write the L(h 0 ) term in (9) to keep consistent with the multi-step attention mechanism, as pictorially shown in Figure 3(a).",
            "score": 0.5487766973024276,
            "section_title": "Dense attention",
            "char_start_offset": 9769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1974
                },
                {
                    "start": 1977,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 893,
                    "end": 915,
                    "matchedPaperCorpusId": "3648736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08587646484375
        },
        {
            "corpus_id": "263605499",
            "title": "Label Supervised LLaMA Finetuning",
            "text": "The causal masks CM , as shown in Equation 2, \n\nin decoder blocks prevent information leaking, as the decoder is only allowed to attend to earlier positions in text generation. Bidirectional dependency extraction of the self-attention layer is reduced to single-direction, leading to critical information loss at the token level. Our empirical studies show that using token representations learned with causal masks significantly underperforms in token classification tasks. To address such an issue, we remove the causal masks from LlamaForTokenClassification5 and extract the latent representations H \u2032 for token classification, as illustrated in Figure 2, \n\nThe essential bidirectional information is expected to be replenished in token representations during finetuning as all the tokens can attend to each other. \n\nWe believe the essential global dependency is also helpful in the sequence classification, and hence, remove the causal masks in LlamaForSequenceClassification. With bidirectional self-attention resumed, we have more choices in pooling. We have tested three pooling methods, i.e., max, average, and last, and the experiments show that max-over-time pooling yields better performance than average pooling and last-token pooling in classification tasks without causal masks. \n\n3 Experiment",
            "score": 0.5485794201522908,
            "section_title": "Label-supervised unmasked LLaMA",
            "char_start_offset": 8049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 48,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 817
                },
                {
                    "start": 820,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1307
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306884765625
        },
        {
            "corpus_id": "3949880",
            "title": "Deep Choice Model Using Pointer Networks for Airline Itinerary Prediction",
            "text": "independence assumptions.\n\nBy connecting the encoder and decoder with an attention module [2], the decoder can consult the entire sequence of the encoder's states, instead of only the final one. This allows the decoder to focus on different regions of the source sequence during the decoding process, which improves results significantly .\n\nIn this new model, c is no longer constant and equal to the last encoder state. Therefore, each conditional probability is now defined as:\n\nThe new ci vector is computes as follows:\n\nwhere the weights \u03b1 i j are defined as:\n\nwhere a is modeled as a feed-forward neural network (jointly trained with the rest of the system) and the softmax function is used to normalize vector u i j . This normalized vector is referred to as the attention mask (or alignment vector) over the inputs. The process is summarized in Figure 1. Although it has been shown that the additional information available to the decoder significantly improves the results of seq2seq, this does not solve the fact that the output dictionary depends on the length of the input sequence.\n\nPtr-Net achieves this by adapting the attention mechanism to create pointers to elements in the input sequence. The following modification to the attention model was proposed:\n\nwhere j \u2208 (1, ..., n), I = I1, ..., Im a sequence of m indices (each between 1 and n) and v, W1, W2 are learnable parameters. Softmax normalizes vector u to be an output distribution over the dictionary of inputs. It should be noted that unlike the standard attention mechanism, the Ptr-Net model does not use the encoder states to propagate extra information to the decoder, but instead uses u j i as pointers to the input sequence elements.",
            "score": 0.5484268568651429,
            "section_title": "Pointer Network",
            "char_start_offset": 13652,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "11212020"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.166259765625
        },
        {
            "corpus_id": "214802239",
            "title": "CG-BERT: Conditional Text Generation with BERT for Generalized Few-shot Intent Detection",
            "text": "The decoder aims to reconstruct the input utterance x using the latent variable z and the intent y. As shown in Figure 2, we add a residual connection from the input representation H 0 to the input of the decoder H 6 . The input of the decoder, \n\n, is constructed by concatenating the latent variable z and the input representation H 0 which represents the input tokens. The input representation for the [CLS] token, h 0 1 , is ignored since its position is taken by z. \n\nThe input for the decoder H 6 is fed into the last six transformer blocks of BERT-base, as shown in Figure 2. To reconstruct the input utterance, the decoder is trained to generate the utterance in a left-to-right manner, since we can only use the left context to predict the next token. The original BERT model is trained on a large corpus to predict a randomly masked token given its contexts in the sentence. We try to leverage the conditional prediction ability and modify it for text generation in a left-to-right manner. \n\nInspired by the Unified Language model (Dong et al., 2019), which utilizes specific self-attention masks to control what context the prediction conditions on, we adopt the attention mask which helps the transformer blocks fit into the conditional text generation task. Instead of applying the whole bidirectional attention to the input, a mask matrix M \u2208 R T \u00d7T is added to determine whether a pair of tokens can be attended to each other. We update the calculation for the attentions in transformer blocks from Equation 3 to: \n\nwhere \n\nThe proposed attention mask matrix for the transformer blocks in the decoder is shown in the upper right corner of Figure 2. To keep the transformers in the encoder having the same structure, we apply the bidirectional self-attention mask for the encoder as shown in the lower right corner of Figure 2. The bidirectional self-attention mask allows all the tokens in S 1 and S 2 to attend to all the tokens.",
            "score": 0.5479961174573836,
            "section_title": "The Decoder",
            "char_start_offset": 9930,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 469
                },
                {
                    "start": 472,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1527
                },
                {
                    "start": 1530,
                    "end": 1535
                },
                {
                    "start": 1538,
                    "end": 1944
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22265625
        },
        {
            "corpus_id": "52100894",
            "title": "Semi-Autoregressive Neural Machine Translation",
            "text": "The decoder is similar with the encoder and is also composed by N blocks. In addition to the two layers in each encoder block, the decoder inserts a third layer, which performs multi-head attention over the output of the encoder. \n\nIt is worth noting that, different from the encoder, the self-attention layer in the decoder must be masked with a causal mask, which is a lower triangular matrix, to ensure that the prediction for position i can depend only on the known outputs at positions less than i during training.",
            "score": 0.5477206385536729,
            "section_title": "The Decoder",
            "char_start_offset": 7412,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 519
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.126708984375
        },
        {
            "corpus_id": "260886785",
            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
            "text": "In fine-tuning processes, encoder-only models, often used in Natural Language Understanding (NLU), and decoder-only models, common in Natural Language Generation (NLG) with causal language modeling approach, exhibit different mechanisms for receiving ground truth loss. Fig. 1(b) illustrates the differences. In NLU tasks, encoder models leverage the representation of a unique special token, transformed into a logit vector, for calculating cross-entropy loss, with the number of classes usually being a few [12]. Conversely, decoder models in NLG tasks predict the subsequent token for each input token, transforming each token representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k [25]. Hence, while encoder models gain ground truth loss from a single special token's logit, decoder models derive it from the output logits of all input tokens, which could provide more detailed token-level prediction information. Motivation. The inherent nature of masked self-attention, where each token representation builds upon the representation of the previous tokens, presents previously unseen challenges when applying quantization to decoder models. For a clearer understanding of the decoder model, we conducted a comparative analysis with the encoder model to examine the impact of quantization error on the model. In Fig. 2 (a), the quantization error of the encoder self-attention map exhibits a widespread presence of errors due to the absence of masking in self-attention, and the per-token quantization errors along the layers also show irregular patterns depending on the token index. However, in Fig. 2 (b), the heat map of the decoder model reveals an increasing brightness of quantization errors as we move toward the later tokens. When examining the token index, the phenomenon of quantization errors accumulating toward the later tokens becomes even more pronounced. This previously unconsidered phenomenon of token quantization error accumulation in the decoder model is a crucial feature to consider in GLM QAT. Reflecting on this feature, we analyze the effectiveness of prior KD methods for language modeling and explore suitable KD approaches for the decoder model. \n\nComparison of KD Methods for Decoder QAT.",
            "score": 0.5471233602980102,
            "section_title": "Quantization Challenges on GLMs",
            "char_start_offset": 11276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2233
                },
                {
                    "start": 2236,
                    "end": 2277
                }
            ],
            "ref_mentions": [
                {
                    "start": 509,
                    "end": 513,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39306640625
        },
        {
            "corpus_id": "238744157",
            "title": "Automated Essay Scoring Using Transformer Models",
            "text": "For the application of a transformer model, it is fundamental to understand the three basic architecture types of a transformer: encoder models, decoder models, and encoder-decoder models. Table 2 provides a brief overview of the three types, the tasks they are predominantly used for, and exemplary models that have been implemented and trained based on the respective type. \n\nIn general, transformer models are neural networks based on the so-called attention mechanism and were originally introduced in the context of language translation. The attention mechanism was presented in 2014 by Bahdanau et al. [35]. They showed that instead of encoding a text from the source language into a vector representation and then decoding the vector representation into the text of the target language, the attention mechanism allows to avoid this bottleneck of a vector representation to be avoided between the encoder and decoder by allowing the model to directly search for relevant tokens in the source text when predicting the next token for the target text. 4  In 2017, Vaswani et al. [3] then showed that \"Attention is All You Need\". While the encoder and decoder models for translation tasks before were mainly based on RNNs, the authors demonstrated that not only can the described vector representation be replaced by an attention mechanism but the encoder and decoder models themselves can be implemented based on the attention mechanism alone. They implemented a self-attention mechanism, in which different attention layers tell the model to focus on informative words and neglect irrelevant words. They showed that this way the model achieves new performance records on several For a detailed introduction to the attention mechanism, we recommend Andrew Ng's lecture on Attention Model Intuition: https://www.youtube.com/watch?v=SysgYptB198 translation benchmarks while having a fraction of the training cost compared to the best models previously used. 5  A major advantage of the transformer architecture is that it allows parallel processing of the input data and is not affected by the vanishing gradient problem. This makes it possible to train with larger and larger datasets, resulting in better and better language models [36].",
            "score": 0.5469398274589987,
            "section_title": "Methodological Background on Transformer Models",
            "char_start_offset": 14514,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2240
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1614990234375
        },
        {
            "corpus_id": "224705393",
            "title": "Infusing Sequential Information into Conditional Masked Translation Model with Self-Review Mechanism",
            "text": "Figure 2 illustrates the overall architecture of our proposed model, which is composed of three modules, an ENCODER, a DECODER and an ARDECODER. Notably, ARDECODER is obtained by solely adding a left-to-right mask to DECODER, where their weights are tied. Rather than a pure CMTM, we also propose a Self-Review Mechanism to ask the ARDECODER to review the predicted target from DECODER in a left-to-right manner. In this section, we will detail each module and the Self-Review Mechanism. \n\nEncoder Our ENCODER is identical to the standard Transformer (Vaswani et al., 2017). Built upon self attention, it encodes a source input x into a series of contextual representations H enc = {h 1 enc , h 2 enc , ..., h |x| enc } by: \n\nDecoder The non-autoregessive property of our model mainly lies in our DECODER. Different from ENCODER, the DECODER has two sets of attention heads as shown in Figure 2: the inner heads are attending over the target words, and the inter heads are over the hidden outputs of ENCODER. It is worth noting that we use a bidirectional mask (denoted as M bi ) as shown in the middle of Figure 2. Such mask allows the DECODER to use both left and right contexts to predict each target word, ensuring that the prediction for t-th position can depend not only on the information before t-th but also right after t-th. \n\nOur DECODER is optimized using the objective in Equation 2. Given a source input x and part of observed target words y obs , the DECODER is required to predict those words of y mask . Firstly, we obtain a series of DECODER hidden outputs H dec = {h 1 dec , h 2 dec , ..., h |y| dec }, by feeding DECODER with the observed target y obs and the ENCODER outputs H enc . Mathematically, we parameterize H dec as: \n\nThen, we apply a linear projection on the hidden outputs H dec , and obtain the probabilities of target words using softmax. Notably, we only focus on the probabilities of masked words during training.",
            "score": 0.5457953879888776,
            "section_title": "Model Architecture",
            "char_start_offset": 4675,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 487
                },
                {
                    "start": 490,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1334
                },
                {
                    "start": 1337,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1745
                },
                {
                    "start": 1748,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1949
                }
            ],
            "ref_mentions": [
                {
                    "start": 551,
                    "end": 573,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "231925325",
            "title": "Thank you for Attention: A survey on Attention-based Artificial Neural Networks for Automatic Speech Recognition",
            "text": "The dimensionalities of query, key and value vectors are d k , d k and d v , respectively. In practice, attention is computed on a set of query, key and value together by stacking these vectors in a matrix form. Mathematically, it is given by (7). \n\nwhere Q, K, V are matrices which represent Query, Key and Value, respectively. Positional information is added to the input sequence to generate the input embedding upon which the attention will be performed. Instead of directly applying attention on input embeddings, they are linearly projected to d k and d v dimensional vectors using learned projections given by ( 8) \n\nwhere \n\nare trainable parameters. d model is the dimension of input embeddings. X is the input embedding for the encoder section and the output embedding for the masked multi-head block for the decoder section. For the second multi-head block of the decoder section, X is the encoder output for k and v projection. However, for q projection, X is the output from the masked multi-head section. \n\nIn Transformer network [20], the attention mechanism have been used in three different ways. They are as follows. \n\n1) Encoder self-attention: In the encoder section, attention mechanism is applied over the input sequences to find the similarity of each token of a sequence with rest of the tokens. 2) Decoder masked self-attention: Similar to the encoder self-attention, output (target) sequence tokens attend each other in this stage. However, instead of accessing the entire output sequence at a time, the decoder can only access the tokens preceding the token which decoder attempts to predict. This is done by masking current and all the future tokens of a particular decoder time step. This approach prevents the training phase to be biased. 3) Encoder-decoder attention: This occurs at the decoder section after decoder masked self-attention stage. With reference to (7), at this stage, Q is the linear projection of the vector coming from decoder's masked self-attention block. Whereas, K and V are obtained by linearly projecting the vector resulting from encoder self-attention block.",
            "score": 0.5456509918088097,
            "section_title": "B. Transformer-based encoder-decoder architeture",
            "char_start_offset": 14384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 247
                },
                {
                    "start": 250,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1133
                },
                {
                    "start": 1136,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1711
                },
                {
                    "start": 1712,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26318359375
        },
        {
            "corpus_id": "259360659",
            "title": "Scaling In-Context Demonstrations with Structured Attention",
            "text": "It is more natural to use bidirectional attention for encoding demonstrations because there is no canonical order of demonstrations, but the causal mask in decoder-only models induces an order. Also, as mentioned before, the relative positional encoding in T5 enables the permutation invariance in SAICL. Yet, we believe that it is easy to adapt our methods to a decoder-only model with modifications on the positional encoding and the attention mask.",
            "score": 0.5449493362049427,
            "section_title": "SAICL",
            "char_start_offset": 16072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.46337890625
        },
        {
            "corpus_id": "249152323",
            "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
            "text": "The seq2seq attention mask as in Fig. 3 is applied such that the text token only depends on the preceding tokens and all image tokens, and image tokens can attend to each other. This is different from a unidirectional attention mask, where not every image token can rely on all other image tokens. \n\nInstead of well initializing the image encoder, we randomly initialize the text decoder. This design choice is highly motivated from the experiment studies of Wang et al. (2020), in which the random initialization shows similar performance, compared with the BERT initialization. This could be because the BERT initialization cannot understand the image signal, which is critical for VL tasks. Without dependency of the initialization, we can easily explore different design choices. The concurrent work of Flamingo (Alayrac et al., 2022) employs a similar architecture of image encoder + text decoder, but their decoder is pre-trained and frozen to preserve the generalization capability of the large language model. In our GIT, all parameters are updated to better fit the VL tasks. \n\nFigure 3: seq2seq attention mask is applied to the transformer. If (i, j) is 1, the i-th output can depend on the j-th input; otherwise, not. \n\nAn alternative architecture is the cross-attention-based decoder to incorporate the image signals instead of concatenation with self-attention. Empirically as shown in supplementary material (Appendix G.2), with large-scale pre-training, we find the self-attention-based decoder achieves better performance overall, while in small-scale setting, the cross-attentionbased approach wins. A plausible explanation is that with sufficient training, the decoder parameters can well process both the image and the text, and the image tokens can be better updated with the self-attention for text generation. With cross-attention, the image tokens cannot attend to each other.",
            "score": 0.544483010640501,
            "section_title": "Network Architecture",
            "char_start_offset": 6709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1899
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1689453125
        },
        {
            "corpus_id": "252383196",
            "title": "Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design",
            "text": "Based on their network structure, attention-based NNs can be classified into three categories: i) encoder-decoder, ii) encoder-only, and iii) decoder-only networks. The encoder-decoder NNs are mainly designed for sequenceto-sequence tasks, such as machine translation [2]. One of the most widely used encoder-decoder network is the Transformer, which is constructed by a stack of encoder and decoder blocks. Figure 2 illustrates the structure, where N l , D hid and R fft represent input length, hidden size and FFN expand ratio respectively. Each encoder starts with a multi-head attention module, followed by a feed-forward network (FFN) consisting of two linear (fully-connected) layers. Finally, residual addition [22] and layer normalization (LN) [23] are used after FFN. Within each multi-head attention, the inputs are first mapped to query (Q), key (K) and value (V) matrices through three different linear layers. The query matrix is then multiplied with K T , followed by a softmax operation to get the score matrix (S). The generated S is multiplied with V and the resultant matrix will flow into another linear layer, which generates the final output matrix of the multi-head attention. Similar to the encoder, the decoder employs two multi-head attention modules and one FFN, where the difference is that the inputs of the query and key matrices in the second attention module come from the last encoder. \n\nBased on the original encoder-decoder structure of Transformer, different variants have been proposed. The encoderonly networks, such as BERT [3] and XLM [24], are autoencoding models that have been widely applied to NLP tasks, such as sequence classification [25]. The Vision Transformer (ViT) [5] also lies in this category. An extra linear projection layer is introduced at the beginning, while its encoder layers correspond to the encoder part of the original Transformer. Finally, the decoder-only networks represent the autoregressive models designed for NLP tasks, such as language modeling [26].",
            "score": 0.5444734348952396,
            "section_title": "A. Attention-Based Neural Networks",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2023
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 271,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 718,
                    "end": 722,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1562,
                    "end": 1565,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1574,
                    "end": 1578,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 1680,
                    "end": 1684,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1715,
                    "end": 1718,
                    "matchedPaperCorpusId": "225039882"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08416748046875
        },
        {
            "corpus_id": "268065524",
            "title": "BPDec: Unveiling the Potential of Masked Language Modeling Decoder in BERT pretraining",
            "text": "DeBERTa represents a significant milestone in the ongoing evolution of language models, introducing the disentangled attention mechanism represents a significant deviation from vanilla multi-head attention [8]. However, compared to a BERT model with the same number of layers and similar amount of parameters, DeBERTa incurs a higher computational cost for both training and serving. Furthermore, for developers considering transitioning from BERT to DeBERTa, there is an additional development cost need to be considered, which may not always be feasible or cost-effective for every application or organization. However, DeBERTa's introduction of a pretraining-only module called the Enhanced Mask Decoder, which is essentially two additional layers discarded during fine-tuning, intrigued us more than previous architectural modifications. We were drawn to explore whether this decoder module could be generalized and optimized for use with the vanilla BERT model, and whether it could unlock encoder's ability to match or even surpass the performance of more computationally expensive models. This line of inquiry forms the basis of our research. \n\nIn this paper, we propose BPDec (BERT Pretraining Decoder), a novel architecture that harnesses the potential of the MLM (Masked Language Modeling). We introduce a novel architecture BPDec. Its encoder is identical to the vanilla BERT, but it includes a redesigned MLM decoder called BPDec. The main modifications are: \n\n-Adding multiple transformer layers after the encoder to function as a decoder. \n\n-We proposed a new attention mechanism called Gradual Unmasking Attention (GUA) for the pretraining decoder. Conventionally, the attention mechanism for BERT is applied with a mask that prevents attending to masked positions. Instead, in Gradual Unmasking Attention, we remove this restric-tion and allows the model to attend to the embeddings at masked positions randomly and gradually. -Integration of a degree of randomness before output, where the final output is a randomly combined result of the encoder output and the decoder output. \n\nTo assess the performance of our proposed method, we conducted a series of rigorous evaluations and ablation studies across a variety of NLP tasks.",
            "score": 0.5441525799320316,
            "section_title": "Introduction",
            "char_start_offset": 2066,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1149
                },
                {
                    "start": 1152,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1552
                },
                {
                    "start": 1555,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2245
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.396484375
        },
        {
            "corpus_id": "251631358",
            "title": "Hardware-friendly compression and hardware acceleration for transformer: A survey",
            "text": "Each sublayer is also connected to an Add & Norm block, which is similar to Encoder. The Masked Multi-head attention layer masks the incoming output sequence and masks the words after the currently processed word to ensure that the prediction of the current position is only related to the word in its previous position. The Multi-head attention layer is different from the Encoder layer. The Multi-head attention of the Decoder plays a role in interacting with the output of the Encoder. It accepts the Encoder's key matrix and value matrix output and combines it with the Decoder's query matrix to achieve interaction. \n\nMulti-head Attention: As shown in Figure 3, it is obvious that the Multi-head Attention mechanism is a combination of a single Self-Attention Mechanism, and its function is that the model can obtain information from multiple spaces, thereby capturing more feature information. The expressions are shown in Eqs (2) and (3). Therefore, we mainly introduce Self-Attention Mechanism. From Eq (1), it can be seen that the Self-Attention function is not complicated. The input is only three matrices of Q, K, and V, and both Self-Attention Mechanism and the entire Transformer are a matrix operation. QK T represents the inner product operation to obtain the similarity score, and then the corresponding weight can be obtained through the softmax function, and dk is a scale factor to prevent the gradient vanishing due to too small gradient when backpropagation is performed during training.",
            "score": 0.5440582827384486,
            "section_title": "Brief preliminaries of transformer",
            "char_start_offset": 8011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1509
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "227231213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2027587890625
        },
        {
            "corpus_id": "271161636",
            "title": "Music Proofreading with RefinPaint: Where and How to Modify Compositions given Context",
            "text": "The inpainting model aims to predict, or fill in, missing parts of a MIDI sequence based on a given mask.We adopt an encoder-decoder architecture for sequence-to-sequence tasks, inspired by the PIA study for music generation [15].This model involves an encoder converting input data into a latent representation and a decoder predicting the final output.\n\nWith an anti-causal mask, self-attention within the encoder prevents future data access, while with a causal mask, self-attention within the decoder limits access only to previous data.With an identity mask, cross-attention enforces positional alignment between the encoder and decoder outputs, which is helpful for aligned sequence tasks.\n\nThe attention mechanisms are defined as follows, where M type is the mask type (anti-causal, causal, or identity):\n\n(1) This structure enhances the capability of the model to handle bidirectional input-output relationships, essential for inpainting, where future context influences the generation process.Furthermore, we add an extra binary embedding to the encoder input with information about the mask M s -the tokens to regenerate-for the inpainting model.",
            "score": 0.5438465488980785,
            "section_title": "Inpainting model (I)",
            "char_start_offset": 4578,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 354
                },
                {
                    "start": 356,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 695
                },
                {
                    "start": 697,
                    "end": 811
                },
                {
                    "start": 813,
                    "end": 1002
                },
                {
                    "start": 1002,
                    "end": 1156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326416015625
        },
        {
            "corpus_id": "235421816",
            "title": "Pre-Trained Models: Past, Present and Future",
            "text": "We give an example in Figure 6, where the self-attention accurately captures the referential relationships between \"Jack\" and \"he\", generating the highest attention score.\n\n(2) Masked self-attention is used in the decoder, whose attention matrix satisfies A ij = 0, i > j.This attention is beneficial to autoregressive language modeling.In the decoding phase, the selfattention is similar to the encoding, except that it only decodes one representation from left to right at one time.Since each step of the decoding phase only consults the previously decoded results, we thus require to add the masking function into the self-attention.\n\n(3) Cross-attention is also used in the decoder, which uses the output of the previous decoder block as Q as well as the output of the encoder as K and V.Such a procedure is essentially an aggregation of the information of the whole input sequence, and it will be applied to all the words to generate in the decoding phase.Taking advantage of the input context is of great significance to some seq2seq tasks such as machine translation and text summarization.\n\nFor more details of Transformer, please refer to its original paper (Vaswani et al., 2017) and the survey paper (Lin et al., 2021).Due to the prominent nature, Transformer gradually becomes a standard neural structure for natural language understanding and generation.Moreover, it also serves as the backbone neural structure for the subsequently derived PTMs.Next, we will introduce two landmarks that completely open the door towards the era of large-scale self-supervised PTMs, GPT and BERT.In general, GPT is good at natural language generation, while BERT focuses more on natural language understanding.BERT GPT",
            "score": 0.5427348904851318,
            "section_title": "Transformer",
            "char_start_offset": 26556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 173,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 337
                },
                {
                    "start": 337,
                    "end": 484
                },
                {
                    "start": 484,
                    "end": 636
                },
                {
                    "start": 638,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1097
                },
                {
                    "start": 1099,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1367
                },
                {
                    "start": 1367,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1593
                },
                {
                    "start": 1593,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1715
                }
            ],
            "ref_mentions": [
                {
                    "start": 1167,
                    "end": 1189,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.201171875
        },
        {
            "corpus_id": "269032853",
            "title": "Adapting LLaMA Decoder to Vision Transformer",
            "text": "Encoder and decoder.We briefly summarize the encoder and decoder in Transformer [52].Both of them basically consist of attention module and a MLP module, each followed by a residual connection.The key difference between them is the mask scheme in their self-attention.Encoders use bi-directional self-attention, and decoders employ causal self-attention and cross-attention.However, the latter is typically omitted in decoder-only LLMs [49,50], we thus focus on comparing causal and bi-directional self-attention as follows, in terms of the mask setting.Denote X \u2208 R N \u00d7d , O \u2208 R N \u00d7d as the input and output sequences, where N is the number of tokens and d is the embedding dimension.W q , W k , W v \u2208 R d\u00d7d denotes the linear mapping of query, key and value, respectively.Generally, self-attention module can be formulated as (set the head number and batch size as 1 for simplicity):\n\nwhere i, j \u2208 [1, N ], A \u2208 R N \u00d7N , M \u2208 R N \u00d7N denote the attention map and mask.P \u2208 R N \u00d7N , Q \u2208 R N \u00d7N are masks in the encoder and decoder, respectively.For a causal self-attention, we have M = Q.Such design allows subsequent tokens only attend to the preceding ones, but not vice versa.For a bi-directional self-attention, we have M = P, ensuring mutual visibility for each token.\n\nRecent LLMs-related image models.Recent image models [2,21,14] are trained with an autoregressive objective, targeting at solving visual tasks.Pang et al. [38] add a text pre-trained frozen LLM block to a ViT encoder to facilitate the performance.Our work, on the other hand, is motivated to explore in-depth how the decoder design in LLMs can be adapted to image models using simple supervised learning to achieve an architectural alignment.A concurrent work VisionLLaMA [8] proposes vision models for recognition and generation tasks based on the LLaMA components.",
            "score": 0.5425400606576376,
            "section_title": "Preliminaries and Motivation",
            "char_start_offset": 4649,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 20,
                    "end": 85
                },
                {
                    "start": 85,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 374
                },
                {
                    "start": 374,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 685
                },
                {
                    "start": 685,
                    "end": 774
                },
                {
                    "start": 774,
                    "end": 885
                },
                {
                    "start": 887,
                    "end": 967
                },
                {
                    "start": 967,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1270
                },
                {
                    "start": 1272,
                    "end": 1305
                },
                {
                    "start": 1305,
                    "end": 1415
                },
                {
                    "start": 1415,
                    "end": 1519
                },
                {
                    "start": 1519,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1838
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.215576171875
        },
        {
            "corpus_id": "219708219",
            "title": "On the Computational Power of Transformers and Its Implications in Sequence Modeling",
            "text": "The encoder, as in the vanilla case, does very little other than pass information along. The vectors in (K e , V e ) are obtained by the trivial attention mechanism followed by simple linear transformations: \n\nAs before, the proof is by induction on t. \n\nThe first one-layer decoder, the decoder-decoder attention block is trivial: p \n\nt = y t . In the decoderencoder attention block, we give equal attention to all the t + 1 values, which along with O enc (\u2022), leads to z \n\nwhere essentially \u03b4 t = \u03c3(\u03c9 t \u2212 \u03c9 t\u22121 ), except with a change for the last coordinate due to the special status of the last symbol $ in the processing of RNN. \n\nIn the second layer, the decoder-decoder attention block is again trivial with p \n\nt . We remark that in this construction the scoring function is the standard multiplicative attention. Now p \n\nt , k e j = \u03b4 t , s j = \u03b4 t,j , which is positive if and only if s j = s t , as mentioned earlier. Thus attention weights in Att(p \n\nt , k e 1 , . . . , p \n\nt , k e t ) = 1 \u03bbt (I(s 0 = s t ), I(s 1 = s t ), . . . , I(s t = s t )), where \u03bb t is a normalization constant and I(\u2022) is the indicator. Refer to Lemma E.3 for more details. \n\nAt this point, O dec (\u2022) has at its disposal the hidden state h t (coming from z \n\n(1) \n\nt and the residual connection) and the input symbol x t (coming via the attention mechanism and the residual connection). Hence O dec (\u2022) can act just like the FFN underlying the RNN to compute h t+1 and thus y t+1 , proving the induction hypothesis. The complete construction can be found in Sec. E in the appendix. \n\nIn practice, [34] found that for NMT, Transformers with only positional masking achieve comparable performance as compared to the ones with positional encodings. Similar evidence was found by [30].",
            "score": 0.5423678960271867,
            "section_title": "Turing-Completeness Results",
            "char_start_offset": 16775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 252
                },
                {
                    "start": 255,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 633
                },
                {
                    "start": 636,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1770
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05877685546875
        },
        {
            "corpus_id": "273507100",
            "title": "From Attention to Activation: Unravelling the Enigmas of Large Language Models",
            "text": "Language Models. Current language models are based on Transformer models (Vaswani et al., 2017). While there are Transformer-based LLMs that used the original encoder-decoder architecture such as T5 (Raffel et al., 2020), researchers developed models such as BERT (Devlin et  and RoBERTa (Liu et al., 2019), which are encoder-only. However, most current LLMs such as the GPT series (Radford et al., 2018;2019;Brown et al., 2020) and Llama series (Touvron et al., 2023a;b;Dubey et al., 2024) use a decoder-only architecture. In our work, we focus on this variant using GPT2 and Llama2 architectures. \n\nAttention Dominance. Bondarenko et al. (2023) identify the dominance of bland tokens in the attention maps of the BERT encoder-only Transformer, and suggest complex clipping schemes, additional hyperparameters, and a gating mechanism to to mitigate this issue. Other researchers found the same issue in long-range attention (Xiao et al., 2024) and found a workaround using \"attention sinks\" and discontinuous attention masking. In vision Transformers, Darcet et al. (2024) made the same observation and proposed a solution using \"registers\". In contrast to these works, we first find the root cause of this issue, the softmax mechanism in attention, and then reformulate it to prevent the first token dominance ever happening. \n\nOutlier Activations. Previous works have shown that in certain Transformer models which use post-normalisation the norm of the weights of the learnt model must increase (Arora et al., 2019;Soudry et al., 2018). However the same reasoning does not apply for most recent decoder-only Transformers which use pre-normalisation (Xiong et al., 2020) (i.e. normalisation before the residual connection). A blog-post by Elhage et al. (2023) discusses the presence of outlier activations in the hidden states of Transformer models and rules out numerical precision as the cause.",
            "score": 0.5419903389382661,
            "section_title": "RELATED WORK",
            "char_start_offset": 31096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 598
                },
                {
                    "start": 601,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1899
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 95,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 199,
                    "end": 220,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 409,
                    "end": 428,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 622,
                    "end": 646,
                    "matchedPaperCorpusId": "259224568"
                },
                {
                    "start": 925,
                    "end": 944,
                    "matchedPaperCorpusId": "263310483"
                },
                {
                    "start": 1053,
                    "end": 1073,
                    "matchedPaperCorpusId": "263134283"
                },
                {
                    "start": 1499,
                    "end": 1519,
                    "matchedPaperCorpusId": "54464191"
                },
                {
                    "start": 1519,
                    "end": 1539,
                    "matchedPaperCorpusId": "233307138"
                },
                {
                    "start": 1653,
                    "end": 1673,
                    "matchedPaperCorpusId": "211082816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.345458984375
        },
        {
            "corpus_id": "259287245",
            "title": "Leveraging Cross-Utterance Context For ASR Decoding",
            "text": "While typically a transformer [15] may consist of a bidirectional encoder, followed by a causal decoder, for language modelling we use the decoder-only variant transformer. This consists of alternating multi-headed self-attention with a causal mask, and feed-forward modules. \n\nGiven a word sequence w = (w1, .., wT ) causal language models are trained to estimate the conditional probability of P (wt|w<t). Word sequence probabilities can then be obtained by an expansion resulting in P (w), when working with loglikelihoods this equates to: T t=1 log P (wt|w<t). For the purpose of decoding these likelihoods can be treated as scores and combined with the AM through a log-linear interpolation.",
            "score": 0.5416981026352627,
            "section_title": "Transformer Language Modelling",
            "char_start_offset": 3407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 275
                },
                {
                    "start": 278,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 696
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1551513671875
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Adaptation extends pretraining with a different objective and/or architecture. In contrast with finetuning, no new downstream data is used, only additional pretraining data. Language modeling adaptation (LM-A) takes a model pretrained with MLM and extend its training with PLM or FLM. It has been used to convert encoder-decoder models pretrained with MLM, such as T5, into better generative models. Notably, it is used as a first step before prompt tuning [Lester et al., 2021] and also to prepare the model before multitask finetuning in T0 [Sanh et al., 2021]. When we perform language modeling adaptation on a non-causal decoder-only model, we convert it into a causal decoder-only by simply switching the attention mask. Furthermore, we propose to study the opposite adaptation: starting from a causal decoder pretrained with FLM, we cast the model into a non-causal decoder (again by switching the attention mask) and we extend pretraining with MLM. We call this approach non-causal MLM adaptation (NC-A) ; to our knowledge, this is an entirely novel practice.",
            "score": 0.5413742189874535,
            "section_title": "Model adaptation",
            "char_start_offset": 16725,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.356201171875
        },
        {
            "corpus_id": "272827489",
            "title": "Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task",
            "text": "As a consequence, it is more tedious to apply complex self-reasoning mechanisms, such as chain-of-thought (Wei et al., 2022), or to interface it with external tools (Schick et al., 2024), because the outputs of such method (the reasoning process) should, preferably, be treated as inputs of the model. For the same reasons, it is much more computationally expensive to rely on an encoder-decoder for conversational purposes, making this architecture less efficient for modern workflows such as iterative translation. Indeed, at each round (the user's query and the system's answer) should be appended to the input side, and reprocessed by the encoder for the next round. Decoder-only models support it by design, without needing to recompute the representation of the ever-growing inputs. While we do not explore these directions in this work, we do leverage the flexibility of the decoder architecture to include input-or-output parameters. As we are tackling the multilingual and multidomain machine translation task, the model needs input tokens to represent the language direction and the domain. We propose to train the model to predict the source language and the domain so that, during inference, they can be seamlessly predicted or provided by arXiv:2409.15051v1 [cs.CL] 23 Sep 2024 the user. \n\nGenerally speaking, decoder-only models simply expect the input to be the whole discussion and process it in a single forward step. Causal masking enable efficient caching of already computed keys and values so inference is much cheaper. The main downside is that the quality of the input representation might be inferior, as input tokens can attend only on past tokens. But it should not be a major issue, as generated tokens attend to the whole past sequence, they do have access to the same quantity of information as with an encoder-decoder model. In addition, previous work propose to update the attention mask so that input tokens can attend to all input tokens while generated tokens can attend only on past tokens (Tay et al., 2022;Raffel et al., 2020).",
            "score": 0.5409266145856424,
            "section_title": "Introduction",
            "char_start_offset": 1480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1300
                },
                {
                    "start": 1303,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 124,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 165,
                    "end": 186,
                    "matchedPaperCorpusId": "256697342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355224609375
        },
        {
            "corpus_id": "276150945",
            "title": "Comprehensive Study on Zero-Shot Text Classification Using Category Mapping",
            "text": "DeBERTa [25] (Decoding-enhanced BERT with Disentangled Attention) introduces the following modifications to the BERT model: \n\n1) Disentangled Attention Mechanism: DeBERTa modifies the input layer of the BERT model by employing a disentangled attention mechanism. While BERT represents word vectors as the sum of word embeddings and positional embeddings, DeBERTa separates the positional encoding and content encoding, utilizing two distinct vectors to encode the position and content of each word. Attention scores between words are then computed using a decoupled matrix. This approach enhances the attention distribution by taking into account the relative positions of words, thereby better capturing contextual semantic information. 2) Enhanced Mask Decoder at the Output Layer: De-BERTa replaces BERT's softmax layer with an enhanced mask decoder to predict the masked tokens. \n\nDuring fine-tuning, the output of BERT is typically connected to a task-specific decoder; however, during training, the decoder is not integrated, and results are output through softmax. Therefore, during pre-training, the authors employ a two-layer Transformer decoder alongside softmax as the enhanced mask decoder. Additionally, absolute positional embeddings of words are incorporated before the softmax layer of the decoder to alleviate mismatches between pre-training and finetuning. \n\nThrough these two modifications, DeBERTa achieves faster training efficiency. Compared to RoBERTa-large, De-BERTa demonstrates superior performance on downstream tasks while utilizing only half of the training data.",
            "score": 0.5408541466186051,
            "section_title": "5) DeBERTa",
            "char_start_offset": 21967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 126,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 882
                },
                {
                    "start": 885,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1374
                },
                {
                    "start": 1377,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1883544921875
        },
        {
            "corpus_id": "273323719",
            "title": "JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles",
            "text": "Most LLMs are built upon the decoder-only transformer architecture [60,50], and their recent success is largely due to their ability to scale to tens or even hundreds of billions of parameters [5,52]. This extreme scale allows them to be highly versatile, with the capability to generalize effectively to novel domains through zero-shot and few-shot learning. However, this versatility comes with significant trade-offs, including high operational costs and substantial computational demands. \n\nIn contrast, encoder-only models like BERT [15] and DeBERTa [23] are optimized for understanding and analyzing text, making them particularly efficient for targeted tasks such as text classification, named entity recognition, and sentiment analysis. These models typically have fewer parameters, which leads to faster inference and lower computational overhead. While encoder-only models excel in specific, narrow tasks and offer greater adaptability through efficient fine-tuning [76,63,26], they are less versatile compared to decoder-only models. However, for domain-specific applications, encoder-only models often provide a more sustainable and cost-effective solution, balancing performance with resource efficiency. \n\nThe difference in attention mechanisms is reflected in the training objectives of these models. Encoder-only models typically use Masked Language Modeling (MLM), where the objective is to predict masked tokens given their context: \n\nDecoder-only models, on the other hand, are trained to predict the next token in a sequence: \n\nEncoder models benefit from bidirectional attention, allowing them to consider the entire context of an input simultaneously, which is particularly useful for classifying completed texts. This bidirectional understanding, combined with their MLM training objective, often makes encoders preferable for many classification tasks. However, decoder models have gained prominence due to their sample efficiency, using 100% of tokens for training compared to the typical 15% masked tokens in encoder training. This efficiency, along with their versatility in reformulating tasks as text generation problems, has led to the recent trend of scaling decoder models to much larger sizes.",
            "score": 0.5406843996949775,
            "section_title": "Transformer Architectures: Encoders vs Decoders",
            "char_start_offset": 8969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1217
                },
                {
                    "start": 1220,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1990966796875
        },
        {
            "corpus_id": "258804048",
            "title": "Novel Transformer-Based Fusion Models for Aero-Engine Remaining Useful Life Estimation",
            "text": "As shown in Figure 1, the output of the encoder are passed to an MHA sublayer of the decoder, which is known as the encoder-decoder attention mechanism. It should be noted that each decoder layer utilizes the output of the encoder's final layer. In this mechanism, the K and V matrices are acquired from the encoder's output, which encapsulate the underlying information of the inputted feature sequence. And Q is acquired from the output of the masked MHA sublayer in the decoder, which contains the information of the RUL labels. In the encoder, the MHA sublayers only aim to learn the relationships between features at different temporal positions. To enhance RUL estimation accuracy, it is essential to take into account the relationships between the feature input sequence and the corresponding RUL labels. As seen in Figure 1, the RUL labeled data is fed into the masked MHA module, which then endeavors to capture the relationships within the data. \n\nWhen training, the RUL label are included in the training dataset, and the decoder decodes for every time step in parallel. However, in the computation of SDPA, the decoder inevitably learns information from future RUL label data at each time step, which is unrealistic. To address this issue, a masking operation is added to the decoder's MHA module, which masks the information from future RUL labels. In the masking process, a matrix that has values 1 in the diagonal and lower triangle, and 0 in the upper triangle, is added into QK T to exclude the future information during computation. During testing or evaluation, the estimated RUL from the proposed model is fed into the decoder as the RUL label data, in an auto-regressive manner. \n\nThe original proposal for the Transformer model was in the field of natural language processing, in which both features and labels were represented as word vectors with identical dimensions. The identical dimensions expedite the straightforward computation in MHA sublayers. But in RUL estimation scenario, the features are composed of multiple dimensions while the RUL labels have only one dimension. This dimensionality difference hinders the functioning of the encoder-decoder attention mechanism.",
            "score": 0.5400386198822297,
            "section_title": "2) DECODER",
            "char_start_offset": 15866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 955
                },
                {
                    "start": 958,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1699
                },
                {
                    "start": 1702,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2202
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278564453125
        },
        {
            "corpus_id": "261530579",
            "title": "Evaluating Transformer\u2019s Ability to Learn Mildly Context-Sensitive Languages",
            "text": "The in-distribution tests were solved almost perfectly by all three model setups, including a Transformer decoder setup where we remove PE and  rely only on indirect positional information from the causal mask. For OOD tests, we evaluate models on strings where both n, m are OOD, as well as strings in which only one of n, m is OOD. We find that removing PE and relying only on causal mask helped with the Transformer's extrapolation, which is consistent with findings in Bhattamishra et al. (2020a) on other languages. However, such ability is still worse than that of LSTMs (Table 3). \n\nFigure 5 shows that the Transformer's attention formed a checkerboard pattern for recognizing crossing, as the result of each symbol in the query attends to every occurrence of itself and its dependent in the key but not to the non-dependents. As for nesting, the pattern is very similar except that the dependency relations are different. Models trained with or without sinusoidal PE end up learning very similar patterns, except that without PE, the attention from one query symbol to every occurrence of a key symbol is uniformly distributed, resulting in a stack of color bands on the attention map of the visualized head. \n\nThe attention maps suggest that in the optimal case, each symbol in query identifies to which other symbol in key it is pairwise dependent, and then in the visible portion without look-ahead mask, distributes its attention to every occurrence of itself and the dependent, and gives zero attention to the other pair. We measure as an example how accu-  rately the visualized head in the crossing model without PE has implemented this optimum, and we find that across all in-distribution test set datapoints, keys that expect 100% of the attention weights from each symbol in query have received on average 93.0% of the attention weights.",
            "score": 0.5394460076073478,
            "section_title": "Crossing Dependency",
            "char_start_offset": 17048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1216
                },
                {
                    "start": 1219,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 473,
                    "end": 500,
                    "matchedPaperCorpusId": "222225236"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.114990234375
        },
        {
            "corpus_id": "257899280",
            "title": "A CTC Alignment-Based Non-Autoregressive Transformer for End-to-End Automatic Speech Recognition",
            "text": "The attention mechanism is important for a transformer. The most basic computation in the attention mechanism is scaled dot-product self-attention using a sequence as input. Conventional self-attention utilizes information across the whole sequence. But each output of the self-attention mechanism is not necessarily dependent on all of the input sequence.\n\nFor example, a neural language model uses an upper triangular matrix (mask matrix) to gather information from only past tokens in a sequence. In Fig. 1, we show three mask matrices for different purposes in the four major modules. Since we do not consider a streaming ASR model at this moment, non-causal mask (NCM) is used in the encoder. The NCM is a matrix where the paddings are zeros to prevent the padded tokens or padded frames from attention computation. In TAEE, a trigger mask (TM) is used to extract accurate token-level acoustic embeddings. TM marks out the triggered frames, such that the positions of used frames are marked as ones, while other positions are marked as zeros. Examples of TM can be found in Section III-A3. MAD contains both self-attention and cross-attention layers, which are similar to an AT decoder. In such cases, either a causal mask (CM) or NCM can be used for the self-attention layer, and either a NCM or TM can be used for the cross-attention layer. The different choices of the mask matrices in MAD are explored experimentally in Section V-A. Eventually, the self-attention computation can be augmented with a mask matrix as follows:\n\nand M \u2208 R n q \u00d7n k are the query, key, value and mask matrices, respectively.\n\n2) Encoder: The encoder extracts high-level acoustic representations H from speech features X. A linear layer with a CTC loss function is added after the encoder as shown in Fig. 1. The role of CTC is to obtain an alignment over the CTC output space to offer auxiliary information for the token acoustic embedding extractor (TAEE). During training, Viterbi-alignment is used. During inference, various methods for sampling from the CTC output space are explored experimentally.\n\n3) Token Acoustic Embedding Extractor: The token acoustic em",
            "score": 0.5393848598673454,
            "section_title": "1) Mask in Attention:",
            "char_start_offset": 12755,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "252893308",
            "title": "Natural Language Generation Using Machine Learning Techniques",
            "text": "Encoder-decoder architecture is a process where an input is encoded into a cryptic representation and then decoded into an output. The process involves the \"attention\" which places emphasis on a specific part of the input most relevant to the prediction step. The results are highly accurate machine translations, language modelings, and speech to text transcriptions. Despite having previously mentioned that LSTM could avoid the long-term dependency issue, encoderdecoder architecture actually has stronger back propagation skills and can access a bigger window of text to reference from. \n\nGiven an input sentence, encoder-decoder architecture will first pass the input through a word embedding and gain a vector representation of the words. The vectors then pass through the encoder and transform into an abstract representation holding learned information. In the encoder layer, there exists a multi-headed attention module that computes attention weights for each word. The attention model relates words with other words. How closely each word is associated with another is determined in a score matrix; the higher the score, the higher the relation. Through mathematical transformations, the higher scores are heightened and the lower scored are reduced, helping the system become more confident on which words to attend to. Before the encoded representation is sent to the decoder, is it possible for it to pass through the encoder n times, each time learning a new attention representation. \n\nThe decoder has similar layers as the encoder and generates an output sequence given information from the encoder and a previous network, much like RNN -and simultaneously LSTM-models. It is autoregressive, taking past information to predict future information and stops when it generates a token. As similar to the encoder, the input from a previous network pass through an embedding layer and a multi-headed attention module. However, this multiheaded attention module has a more specific function attention between given words with words following behind it. It does so by masking, which only allows words to attend to itself and past words. The information from the encoder passes through a separate multi-headed attention module and is compared with the decoder's inputs to further determine relevant attention on which encoder inputs. Through further processing the information reaches the linear classifier which generates a predicted output.",
            "score": 0.5380480885442913,
            "section_title": "Encoder-Decoder Architecture",
            "char_start_offset": 8179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 590
                },
                {
                    "start": 593,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1499
                },
                {
                    "start": 1502,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2342
                },
                {
                    "start": 2343,
                    "end": 2451
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1988525390625
        },
        {
            "corpus_id": "247618909",
            "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
            "text": "A standard transformer has an encoder-decoder structure (Vaswani et al., 2017). The encoder of a transformer maps a list of tokens X = (x 1 , ..., x n ) to a sequence of hidden representations Z = (z 1 , ..., z n ), also known as the memory. Given X and source sequence X src = (x 1 , ..., x n\u22121 ), during training, the decoder masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 , ..., x n ) autoregressively. Each layer of the transformer encoder has two sub-layers: multi-head self-attention and a fully connected feed-forward network; both have residual connections. The decoder layer has a third attention sub-layer, which performs multi-head attention from the hidden representation Z to the target representation X tgt . We leverage attention masking to achieve autoregressive segment prediction using this framework; we discuss details next. Figure 4: Attention-masking for Autoregressive Segment Prediction. For an image converted into a sequence of patches, we adopt a masked encoder-decoder transformer (Vaswani et al., 2017) for autoregressive segment prediction. In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask). \n\nAutoregressive Segment Encoder. Figure 4 shows our transformer encoder block and a decoder block. We leave out the fully connected layer and residual connections for simplicity and only show the attentions. In this visualization, there are six patches. These six patches are then grouped into three segments denoted by colors: green, blue, and red. The random segment serialization order is green \u2192 blue \u2192 red. One layer of transformer encoder is illustrated on the left in light green.",
            "score": 0.5378582883473662,
            "section_title": "MASKED TRANSFORMER FOR SEGMENT PREDICTION",
            "char_start_offset": 21070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 78,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1063,
                    "end": 1085,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "235390434",
            "title": "U2++: Unified Two-pass Bidirectional End-to-end Model for Speech Recognition",
            "text": "The training loss is the combined CTC and AED loss as listed in the equation1, and the AED loss consists of an L2R AED loss and an R2L AED loss as listed in the equation 2. Both of the two attention decoders are trained in the teacher forcing mode. In both equation 2 and equation 1, x are the acoustic features, y are the corresponding annotations, LCTC (x, y) and LAED (x, y) are the CTC and AED loss respectively. \u03bb is a hyperparameter that balances the importance of the CTC and AED loss. \u03b1 is another hyperparameter that balances the R2L AED loss and the L2R AED loss. In [14], the L2R and R2L decoders share the same weights. \n\nIn the decoding process, these two decoders run separately and the sequence with the best score is chosen as the final result. \n\nAlthough This strategy could give more candidates, it is hard to combine the two direction scores due to the autoregressive mode. \n\nOur models have mainly two differences from [14] . First, we used different weights for the two decoders. Because the reversed version language should not be considered as the reasonable original language. Second, in the U2++ framework, the attention decoders are used for re-scoring instead of autoregressive decoding, so the two decoder scores of the candidates could be combined. \n\nThe implementations are described in the following. The mask in Figure 2(a) is applied to the self-attention in the L2R decoder to make each word only see its left words. Similarly, The mask in Figure 2(b) is applied in the R2L decoder to make each word only see its right words. Another implementation is to reverse the input sentence and use Figure 2(a) mask. The two implementations are only different in the position encoding information and give almost the same performance in experiments. So in this paper, we only report the result of the first implementation.",
            "score": 0.5368268362942007,
            "section_title": "Combined Loss",
            "char_start_offset": 5479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 892
                },
                {
                    "start": 895,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1847
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.081787109375
        },
        {
            "corpus_id": "258461229",
            "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
            "text": "In this Section, we present the empirical results and conclusions of the attempt toward unification and ablation study for which the findings are distilled into the following five lessons. \n\n3.1 LESSON 1: PREFIX-LM'S BENEFIT IS QUESTIONABLE \n\nAs discussed, Prefix-LM behaves as an encoder with bi-directional attention, and as an autoregressive decoder with a causal mask. However, it is unclear if this unification of the architecture leads to competitive performance on both ends. We evaluate this question from three angles: data, representation, and objective. In the following, we refer to the first half of the sequence that is covered by bi-directional attention as non-causal part, and the rest of the sequence as causal part.",
            "score": 0.5367944642850471,
            "section_title": "RESULTS: LESSONS AND RECIPE",
            "char_start_offset": 14299,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 191,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 734
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0631103515625
        },
        {
            "corpus_id": "263831258",
            "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
            "text": "While the masked encoder introduced in BERT (Devlin et al., 2019) was used in pre-training, masked reconstruction was not used in downstream tasks which mainly focused on obtaining a representation of the entire input. Thus, they focused on the bidirectional encoder architecture, and did not consider other attention masking schemes. \n\nCausal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. Dong et al. (2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example. Figure 10 illustrates the various attention masking schemes for the masked encoder architecture.",
            "score": 0.5363789541679107,
            "section_title": "D.3 ATTENTION MASKS",
            "char_start_offset": 44444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1231
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 65,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 488,
                    "end": 506,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56689453125
        },
        {
            "corpus_id": "234680504",
            "title": "Do Context-Aware Translation Models Pay the Right Attention?",
            "text": "We now investigate how models trained with attention regularization handle context differently compared to the baseline model. How does attention regularization influence alignment with human rationales? We revisit the similarity metrics from \u00a74.2 to measure align-ment with SCAT. In Table 5, the dot product alignment over attention in the decoder increases with attention regularization, suggesting that attention regularization guides different parts of the model to pay attention to useful context. Interestingly, although only the encoder self-attention was explicitly regularized for attnreg-pre, the model seems to also have learned better alignment for attention in the decoder. Moreover, attnreg-pre generally has better alignment than attnreg-rand, suggesting that models respond more to attention regularization once it has been trained to perform translation.\n\nWhich attention is the most useful? For each of attnreg-rand and attnreg-pre, we perform attention regularization on either the encoder self-attention, decoder cross-attention or decoder self-attention only. In Table 7, encoder self-attention seems to contribute the most to both translation performance and contrastive evaluation. Although attnreg-rand models achieve higher BLEU and COMET scores, attnreg-pre obtain higher scores on metrics targeted to pronoun translation. Attention regularization seems to have limited effect on WSD performance, the scores vary little between attention types.\n\nHow much do models rely on supporting context? We compare model performance on contrastive evaluation on SCAT when it is given full context, and when we mask either the supporting context, random context words with p = 0.1, the source context, the target context, or all of the context. In Table 8, we find that baseline varies little when the supporting context is masked, which again suggests that context-aware baselines do not use the relevant context, although they do observe a drop in contrastive performance when the source and all context are masked. Models with attention regularization, especially attnreg-pre observe a large drop in contrastive performance when supporting context is masked, which indicates that they learned to rely more on supporting context. Furthermore, for attnreg-pre, the score after masking supporting context is significantly lower than when masking",
            "score": 0.5363726107945662,
            "section_title": "Analysis",
            "char_start_offset": 22028,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "265985194",
            "title": "Too Much Information: Keeping Training Simple for BabyLMs",
            "text": "We opted to use encoder-only models. We initially experimented with encoder-decoder models, but found that the evaluation metrics for this shared task being non-generative gave encoder-only models an advantage, as it allows for full attention, rather than only causal attention. In terms of specific model selection, we opted for RoBERTa-base (Liu et al., 2019) in order to directly compare with the provided baseline. We also experimented with (and ultimately submitted) DeBERTa-large (He et al., 2021) as it is a larger model and considered state-of-the-art for encoder-only models.",
            "score": 0.5362509576119571,
            "section_title": "Model Choice",
            "char_start_offset": 4521,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 584
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047698974609375
        },
        {
            "corpus_id": "262168933",
            "title": "Machine Translation of Electrical Terminology Constraints",
            "text": "The Transformer uses an encoder-decoder structure, as shown in Figure 1. The encoder and decoder in the model are both N = 6 layers; each layer has the same structure, but the parameters are not shared. \n\nEach layer of the encoder part contains two sub-layers, followed by the self-attention sub-layer and the feedforward network sub-layer. The attention mechanism in the Transformer is formalized as: \n\nwhere \n\nEach sub-layer has residual connection and layer normalization operation: \n\nwhere Sublayer(x) represents the self-attention sublayer or the feedforward network sublayer. \n\nThe output dimensions of all sub-layers in the network are equal. Each layer of the decoder part contains three sub-layers. A cross-attention sub-layer is added between the self-attention sub-layer and the feedforward network sub-layer. The sub-layer query is the output from the self-attention sub-layer. The keys and values are the output of the encoder so that each position in the decoder can perform attention calculations on all positions in the encoder. The self-attention sub-layer of the decoder is different from the encoder. It adds a mask mechanism so that each position in the decoder can only calculate the left and the current position to ensure that only the current position and the left side have been generated when predicting the next word. The Transformer uses a parameter-free functional position encoding to preserve the position of words in the sequence to identify the order relationship in the language: \n\nwhere p is the position and i is the dimension subscript. This functional position encoding can also enable the model to generalize to a sequence length not seen in the training process [32]. \n\nwhere () represents the self-attention sublayer or the feedforward network sublayer. \n\nThe output dimensions of all sub-layers in the network are equal. Each layer of the decoder part contains three sub-layers. A cross-attention sub-layer is added between the self-attention sub-layer and the feedforward network sub-layer. The sub-layer query is the output from the self-attention sub-layer.",
            "score": 0.5351728720406899,
            "section_title": "Traditional Transformer",
            "char_start_offset": 18139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 202
                },
                {
                    "start": 205,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 401
                },
                {
                    "start": 404,
                    "end": 409
                },
                {
                    "start": 412,
                    "end": 485
                },
                {
                    "start": 488,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1707
                },
                {
                    "start": 1710,
                    "end": 1794
                },
                {
                    "start": 1797,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2102
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "8063399",
            "title": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model",
            "text": "Neural machine translation has shown promising results lately. Most NMT methods follow the encoder-decoder framework proposed by , which typically consists of two RNNs: the encoder RNN reads the source sentence and transform it into vector representation; the decoder RNN takes the vector representation and generates the target sentence word by word. The decoder will stop once a special symbol denoting the end of the sentence is generated. This encoder-decoder framework can be used on general sequence-to-sequence tasks (Sutskever et al., \u2020 Work done while Shi was an intern at Microsoft Research. 2014), like question answering and text summarization. After some modification, for example replacing the RNN encoder with a CNN, the model can also be applied to tasks like image captioning . In the following discussion, we focus on the task of machine translation.\n\nIn the original encoder-decoder model, although the encoder RNN generates a set of hidden states, one at each position of the source sentence, the decoder only takes the last one. This design in effect compresses the variable-length source sentence into a fixed-length context vector, with the information of each source word implicitly stored in the context vector. Thus the decoder cannot easily make full use of the whole sequence of encoder hidden states. To make it more flexible and generalize the fixed-length representation to a variablelength one, it was proposed to use attention mechanism for machine translation .\n\nAttention mechanism was first proposed to allow models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014).\n\nIn , attention mechanism was applied to machine translation to learn an alignment between source words and target words. Fig. 1 shows a sample alignment given by attention mechanism.\n\nWith the ability of learning alignments between different modalities from attention mechanism, attention-based encoder-decoder model is more powerful than just encoder-decoder and has been used for many tasks like question answering (Hermann et al., 2015), speech recognition (Bahdanau Each row is a distribution of how the corresponding target word (English) is aligned to source word. Darker color denotes higher weight",
            "score": 0.534451530310135,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1679,
                    "end": 1698,
                    "matchedPaperCorpusId": "17195923"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1351318359375
        },
        {
            "corpus_id": "270371018",
            "title": "Symmetric Dot-Product Attention for Efficient Training of BERT Language Models",
            "text": "In this work, we revisited the traditional scaled dot-product used in the Transformer self-attention mechanism.We challenged the use of two dis-tinct operators to compute the dot-product between queries and keys, in favor of single shared operator and a weighted dot-product with pairwise factors.By doing so, we enforced a symmetric structure to the compatibility operator of the attention mechanism, reducing the number of parameters used in the Transformer layer by a third.As a result, when applied to BERT models, our pairwise compatibility operator reduces the overall number of parameters of the model by 6%, reduces the number of pre-training steps required by half and improves accuracy on the GLUE benchmark, making Transformer-based encoders more efficient, faster to train and lowering their resource requirements.We believe our work can be applied to other Transformer architectures like decoder and encoder-decoder models, as well as to other NLP tasks like machine translation and language modeling.And, more generally, to the concept of attention as a whole, where it would bring improvement in other fields such as computer vision.\n\nFor future work, we plan to evaluate the pairwise dot-product attention mechanism on larger models reaching into the billion parameters, and to evaluate our attention mechanism on other benchmarks, like SuperGLUE (Wang et al., 2019a) and SQuAD2.0(Rajpurkar et al., 2018).We plan on implementing the pairwise compatibility operator for the crossattention mechanism, and evaluating it on decoder and encoder-decoder tasks like language modeling and machine translation.Finally, we want to evaluate our pairwise dot-product attention not only on natural language processing tasks, but also on tasks from other fields, computer vision, time series forecasting and reinforcement learning.",
            "score": 0.5342063394762055,
            "section_title": "Conclusions",
            "char_start_offset": 20815,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 111,
                    "end": 297
                },
                {
                    "start": 297,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 1014
                },
                {
                    "start": 1014,
                    "end": 1148
                },
                {
                    "start": 1150,
                    "end": 1396
                },
                {
                    "start": 1396,
                    "end": 1421
                },
                {
                    "start": 1421,
                    "end": 1617
                },
                {
                    "start": 1617,
                    "end": 1833
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093505859375
        },
        {
            "corpus_id": "272208875",
            "title": "Advancements in Deep Learning-Based Image Captioning",
            "text": "The decoder part usually uses variants of Recurrent Neural Networks (RNN) such as Long Short-Term Memory Networks (LSTMs) or Gated Recurrent Units (GRUs) to deal with tasks that do not involve long sequences of indeterminate data, such as generating image captioning. Unlike the traditional encoder-decoder structure, the Encoder-Decoder with Attention model introduces an attention mechanism. As the decoder generates each word or phrase, the attention mechanism allows the model to dynamically focus on different regions of the image encoder, thus enabling the model to better capture the correspondence between the image and the text, and improve the accuracy and fluency of the generated descriptions. This attention mechanism helps the model to pay more attention to the image regions that are relevant to the currently generated word, thus making the generated text more descriptive. The topic of Jingqiang Chen, Hai Zhuge's research is to add captions to news images, while News Image Captioning contains more detailed information than general image captioning, such as entity names and events, they generate captions for news images through a multimodal attention encoder-decoder model. Their research methodology consists of four main parts: the text encoder encodes the text through an RNN model, and the image encoder completes the encoding of the image by obtaining the vector representation of the image through Oxford VGGNet. The decoder decodes the words through the RNI model. In order to balance the text encoding and image encoding, they developed a multimodal attention mechanism based on the traditional attention mechanism. In their experimental data analysis, it can be seen that NNattSim, which combines text and image encoding, outperforms NNattImg, which only pays attention to the image input, and NNattTxt, which focuses on the text summarization problem. Umemura, K et al. They also used the attention mechanism to connect the encoder to the decoder, where the encoder is used to convert the image into a vector and the decoder is used to generate the text, which can be used to generate an accurate description of the image based on the output of the encoder.",
            "score": 0.5340357699517264,
            "section_title": "Method based on encoder decoder structure",
            "char_start_offset": 10064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2188
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12347412109375
        },
        {
            "corpus_id": "263685196",
            "title": "An End-to-End Air Writing Recognition Method Based on Transformer",
            "text": "In this study, we used an original transformer decoder [35]. The standard Transformer decoder also has a set of similar layers, with a structure similar to the layers in the encoder, except that the decoder inserts ''encoder-decoder attention'' between the multi-head self-attention and the feedforward network to allocate different attention to the encoder's output. In the encoder-decoder attention module, three input matrices are accepted: query (Q), key (K), and value (V) [35]. \n\nThe key (K) and value (V) are derived from the encoder output, whereas the query is derived from the decoder input. These matrices are different representations embedded in the input through the dense or linear layers. Attention scores are evaluated by the dot product of the encoder-decoder hidden states (in the encoder-decoder attention mechanism), as shown in equation1 [35]. \n\nIn addition, the decoder uses attention masks in selfattention to prevent access to more information during training than during inference. Given that the decoder's output is shifted right by one position from the input, the attention mask ensures that the output at position ''i'' can only attend to the input up to position ''i'' in the output. We stacked three encoder layers sequentially, followed by three decoder layers. The earliest version of the transformer model used six encoder and six decoder layers. Owing to the limited training set in this study, too many layers may cause overfitting. Therefore, the depth of the deep learning model is reduced [36]. Finally, during the testing process, we use a linear layer with softmax activation to output the character sequence. For the training process, we need the softmax matrix to calculate the CTC loss. Figure 8 shows the specific structure of the Transformer encoder and decoder parts in this study.",
            "score": 0.5338911505022912,
            "section_title": "3) TRANSFORMER ENCODER AND DECODER",
            "char_start_offset": 26608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1829
                }
            ],
            "ref_mentions": [
                {
                    "start": 1529,
                    "end": 1533,
                    "matchedPaperCorpusId": "226281941"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26416015625
        },
        {
            "corpus_id": "240420122",
            "title": "Explaining Documents' Relevance to Search Queries",
            "text": "Documents on top of a search engine result page often contain query tokens with high frequencies [39]. In addition, our encoder consists of masked-document query attention layers, which highlight the query related part of the document. Therefore, it is likely that the decoder generates query tokens as the explanation of a document's relevance. However, this is not desired; the model should generate explanations that provide more information other than the query itself, otherwise the generated explanation is useless. Therefore, we generate explanations by extending the Transformers Decoder architecture [71] using a query masking mechanism to reduce the probability of generating query tokens by the decoder. To achieve this, we use a masked multi-headed attention for the encoder-decoder attention layer in which the query tokens in both query and document are masked. This makes every decoder position to attend over only non-query tokens in the input sequence. \n\nNote that query tokens are not replaced with a special mask token, because these tokens are central in building the representation of documents. Instead, the representations of query tokens are masked during the decoding process to allow the model to describe the query-related information that the document provides.",
            "score": 0.5330946288878516,
            "section_title": "Query-Masked Decoding",
            "char_start_offset": 27790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1289
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 613,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.377197265625
        },
        {
            "corpus_id": "259108359",
            "title": "Improving Long Context Document-Level Machine Translation",
            "text": "This method was proposed by Zhang et al. (2020) and is called Long-Short Term (LST) attention. The authors find that their approach outperforms the baseline concatenation approach but they only use a maximum of 3 sentences as context. Nevertheless we deem this approach promising, since it also focuses the attention onto the current sentence. The input to the system is augmented in the same way as described in Section 3.1. Given some queries Zhang et al. (2020) formulate their restricted version of the attention as1 \n\nwith d being the hidden dimension of the model and M \u2208 R I\u00d7J being the masking matrix. This masking matrix is defined as \n\nwhere s(\u2022) \u2208 1, .., N is a function that returns the sentence index that a certain position belongs to. This means we are restricting the attention to be calculated only within the current sentence. For selfattention in the encoder and the decoder, Zhang et al. (2020) calculate both the restricted and the non-restricted variant and then combine the output context-vectors via concatenation and a linear transformation. The cross-attention between encoder and decoder remains unchanged in this approach and the memory consumption remains O(L 2 ).",
            "score": 0.5329102686552137,
            "section_title": "LST-attention",
            "char_start_offset": 11247,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 520
                },
                {
                    "start": 523,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 643
                },
                {
                    "start": 646,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 47,
                    "matchedPaperCorpusId": "221818887"
                },
                {
                    "start": 445,
                    "end": 464,
                    "matchedPaperCorpusId": "221818887"
                },
                {
                    "start": 895,
                    "end": 914,
                    "matchedPaperCorpusId": "221818887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1859130859375
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "The Transformer architecture, proposed by Vaswani et al. in 2017(Vaswani et al., 2017), has emerged as the leading choice for developing large language models (LLMs) due to its exceptional parallelizability and capacity (Zhao et al., 2023b). This scalability allows language models to be expanded to include hundreds or even thousands of billions of parameters, enabling them to capture more complex language patterns and improve performance on various tasks. In general, large language models can be categorized into three main architecture types: encoder-decoder structures, causal-decoder, and prefix decoder (Zhao et al., 2023b), each with its own characteristics and applications. \n\nEncoder-decoder Architecture: The vanilla Transformer proposed in (Vaswani et al., 2017) is based on encoder-decoder architecture, which comprises separate encoder and decoder components. The encoder processes the input sequence and captures its latent representation, which is then used by the decoder to generate the output sequence autoregressively. This architecture is well-suited for tasks involving sequence-to-sequence mapping, such as machine translation, text summarization, and dialogue generation. Encoder-decoder pretrained model. Encoder-decoder pretrained models, such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2020), have demonstrated excellent performance across various downstream tasks. However, with the development of LLM there are only a few large language models based on the encoder-decoder architecture, such as Flan-T5 (Chung et al., 2022) and CodeT5+ (Wang et al., 2023f). \n\nCausal Decoder Architecture: The causal decoder architecture is commonly implemented as a stack of decoder layers. It utilizes a diagonal mask matrix, allowing each token to only have access to information from preceding tokens. This constraint ensures a unidirectional and autoregressive generation process.",
            "score": 0.5329018645922929,
            "section_title": "Model Architecture",
            "char_start_offset": 10774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 1308,
                    "end": 1329,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1663818359375
        },
        {
            "corpus_id": "273453350",
            "title": "FLCMC: Federated Learning Approach for Chinese Medicinal Text Classification",
            "text": "Self-attention [37,[42][43][44] simulates the visual process in which humans automatically ignore non-critical information when observing things. It was originally applied to machine translation problems by Bahdanau et al. [42]. This study described attention as performing a weighted average on the encoder hidden layer to calculate decoding. The contribution of each encoder to the weighted average is determined by the similarity between the encoder's state and the decoder's previous hidden state. Among them, the architecture diagram of the encoder-decoder presented by Yu et al. [45] is shown in Figure 2. \n\napplied to machine translation problems by Bahdanau et al. [42]. This study described attention as performing a weighted average on the encoder hidden layer to calculat decoding. The contribution of each encoder to the weighted average is determined by th similarity between the encoder's state and the decoder's previous hidden state. Among them, the architecture diagram of the encoder-decoder presented by Yu et al. [45] i shown in Figure 2.  The encoder-decoder architecture is also called the sequence-to-sequence problem. It is often used to deal with the problem of encoding and decoding a natural language sentence sequence to obtain a new sequence, such as multi-label text classification problems. In Figure 2, X represents the source sequence, Y represents the target sequence, and C represents the semantic encoding process, which integrates the sentence information input into the encoder, and Y is only related to C. \n\nThe role of the self-attention mechanism is to calculate the weights between each word and all other words, applying different levels of attention to different encoder word vectors and giving greater weight to the parts crucial for deciding the decoder word vectors. Less important information is given lesser weight, effectively solving the problem of longdistance dependencies in sentences. This mechanism has been widely applied to enhance RNN and CNN models. Below, the specific theory is introduced: \n\nThe self-attention mechanism treats the source text sequence processed by the encoder as a sequence of key-value pairs composed of a key vector K and a value vector V.",
            "score": 0.5328230738457513,
            "section_title": "Global Aggregation",
            "char_start_offset": 34742,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2051
                },
                {
                    "start": 2054,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 15,
                    "end": 19,
                    "matchedPaperCorpusId": "56176898"
                },
                {
                    "start": 23,
                    "end": 27,
                    "matchedPaperCorpusId": "245796964"
                },
                {
                    "start": 27,
                    "end": 31,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "52984984",
            "title": "An Analysis of Attention Mechanisms: The Case of Word Sense Disambiguation in Neural Machine Translation",
            "text": "Attention mechanisms were initially proposed to learn the alignment between source and target tokens by Bahdanau et al. (2015) and Luong et al. (2015), in order to improve the performance of NMT. However, attention mechanisms are different from the traditional word alignment in SMT which learns the hard alignment between source and target tokens. Attention mechanisms learn to extract features from all the source tokens when generating a target token. They assign weights to all the hidden states of source tokens. The more related hidden states are assigned larger weights. Then attention mechanisms feed a context vector c t , which is extracted from the encoder, into the decoder for target-side predictions. \n\nWe use h to represent the hidden state set {h 1 , h 2 , \u2022 \u2022 \u2022 , h n } in the encoder, where n is the number of source-side tokens. Then c t is computed by Equation 1: \n\nwhere \u03b1 t is the attention vector at time step t. \u03b1 t is  a normalized distribution of a score computed by the hidden state set h and the decoder state s t\u22121 , as described by Equation 2: \n\nThere are different score() functions to compute the attention vector a t , including multi-layer perceptron (MLP), dot product, multi-head attention, etc. In this paper, the vanilla attention mechanism employs MLP. The advanced attention mechanism applies multi-head attention with scaled dot product, which is the same as the attention mechanism in Transformer (Vaswani et al., 2017). Figure 1 illustrates different attention mechanisms. In vanilla attention mechanisms (Bahdanau et al., 2015;Luong et al., 2015), the context vector c t is only fed into the first layer of the decoder networks. Then the single-or multilayer decoder networks compute from bottom to top to predict target tokens. The vanilla attention mechanisms can only extract the source-side features once, which may be insufficient. Therefore, Gehring et al. (2017) and Vaswani et al. (2017) feed a context vector into each decoder layer. The higher layer could take the result of the previous layer into account when computing the new attention.",
            "score": 0.5327506506836369,
            "section_title": "Attention Mechanisms",
            "char_start_offset": 5895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 1073
                },
                {
                    "start": 1076,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 104,
                    "end": 126,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 131,
                    "end": 150,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1439,
                    "end": 1461,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1548,
                    "end": 1571,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 1571,
                    "end": 1590,
                    "matchedPaperCorpusId": "1998416"
                },
                {
                    "start": 1892,
                    "end": 1913,
                    "matchedPaperCorpusId": "3648736"
                },
                {
                    "start": 1918,
                    "end": 1939,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12054443359375
        },
        {
            "corpus_id": "271903271",
            "title": "FASST: Fast LLM-based Simultaneous Speech Translation",
            "text": "As shown in Figure 2, our model is composed of a speech encoder, an adapter and a LLM decoder. \n\nBlockwise-Causal Speech Encoder (BCSE) extracts contextualized acoustic features from the raw waveform incrementally. It consists of several casual convolutional layers as the audio feature extractor and a blockwise-causal Transformer Encoder as the contextual encoder. \n\nOur causal convolutional layers are built upon non-causal ones. Denote H in \u2208 R l\u00d7d as the input vectors to non-causal convolution Conv(\u2022) with kernel size w. We add additional zero padding Pad \u2208 R (w/2\u22121)\u00d7d to its left so that each output vector only depends on input vectors to its left, and remove the last w/2 \u2212 1 states to keep its output length the same as before, \n\n(2) \n\nBesides, we apply blockwise-causal masking to Transformer Encoder. Define attention mask M of speech encoder as follows \n\nwhere b is the block size, i.e., the number of hidden states of the speech encoder corresponding to one segment, and j Q , j K are row indices of query matrix Q and key matrix K. The attention output of speech encoder during training can then be written as \n\nwhere V is the value matrix. \n\nAdapter receives speech encoder outputs and converts them to the LLM embedding space. It consists of two causal convolutional layers to reduce the length of speech encoder outputs by four and one linear layer to project features into the LLM embedding space. We call the adapter outputs as speech embeddings, \n\nLLM receives speech embeddings and embeddings of previously generated tokens to decode autoregressively according to a wait-k-stride-n policy \u03c0. Wait-k-stride-n policy waits for k speech segments at the beginning and then alternate between generating n words and reading new segment. Figure 3 shows an example of wait-1-stride-2.",
            "score": 0.5325525076371629,
            "section_title": "Model Architecture",
            "char_start_offset": 3810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 97,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1800
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.194091796875
        },
        {
            "corpus_id": "249017975",
            "title": "On the Role of Bidirectionality in Language Model Pre-Training",
            "text": "Our study focuses on the role of bidirectionality on language model pre-training, and does not explore other factors that might affect model performance. \n\nIn particular, we mask individual tokens without considering longer spans, and do not explore the impact of the masking rate. In addition, we do not consider sequence-to-sequence models in our study, which combine bidirectional attention in the encoder and unidirectional attention in the decoder. Finally, we train all variants for the same number of tokens, making them comparable in terms of training cost, but resulting in models using a bidirectional attention prefix or a masking objective seeing less tokens of supervision. We start with the original sequence in the input, and predict the next token in the output; 2) We choose n mask tokens at random, replace them with the special <mask> token in the input, and predict the masked token (rather than the next token) in the output; 3) We move the masked tokens and their corresponding positional embeddings to the end; 4) We only predict the last n predict tokens, using bidirectional attention for the first n bidir tokens and unidirectional attention for the rest (final objective).",
            "score": 0.5325228689539218,
            "section_title": "Limitations",
            "char_start_offset": 22919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 156,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 1199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.379638671875
        },
        {
            "corpus_id": "265329916",
            "title": "Large Language Models and Information Retrieval",
            "text": "The research investigates fundamental architectural designs integral to Large Language Models (LLMs): the encoder-decoder architecture and the decoder-only architecture. These structural frameworks have played a pivotal role in advancing language understanding and generation, laying the foundation for innovative applications across diverse domains. \n\nThe encoder-decoder architecture, a widely embraced structure in Natural Language Processing (NLP) tasks, comprises two essential components: an encoder and a decoder. The encoder processes the input sequence, generating a fixed-length context vector. Conversely, the decoder receives this context vector and produces the output sequence. Typically, recurrent layers such as LSTM or GRUs construct the encoder, processing the input sequentially and capturing contextual information. Attention mechanisms further enhance this architecture by enabling the decoder to selectively focus on different parts of the input during decoding. \n\nIn contrast, the decoder-only architecture revolves around a solitary transformer-based decoder, devoid of an explicit encoder. This architectural model generates the output sequence based on a learned positional representation of the input sequence. Initially introduced in encoder-decoder models, transformer-based architectures have been adapted for decoder-only models. They incorporate selfattention mechanisms, allowing the model to weigh the significance of each input token when generating the output token. The transformer decoder processes tokens in parallel, enhancing computational efficiency and significantly accelerating training and inference processes. These architectural paradigms have significantly propelled the field of language modeling. Encoder-decoder architectures have notably augmented language understanding by efficiently capturing contextual relationships in both input and output sequences, crucial for translation and summarization tasks. On the other hand, the decoder-only architecture, particularly with transformer models, has immensely elevated generation capabilities. The self-attention mechanism's ability to consider all input tokens simultaneously results in coherent and contextually appropriate text generation. Understanding these architectural designs is fundamental in the development of sophisticated LLMs adept at both understanding and generating human-like text. Their flexibility and effectiveness continue to drive innovation, paving the way for a multitude of applications in the realm of natural language processing.",
            "score": 0.5320641736793791,
            "section_title": "LLM Architecture",
            "char_start_offset": 13306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 350
                },
                {
                    "start": 353,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 984
                },
                {
                    "start": 987,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2243
                },
                {
                    "start": 2244,
                    "end": 2401
                },
                {
                    "start": 2402,
                    "end": 2559
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314453125
        },
        {
            "corpus_id": "221712854",
            "title": "Multitime Resolution Hierarchical Attention-Based Recurrent Highway Networks for Taxi Demand Prediction",
            "text": "e decoder contains another RHN used to capture the time-dependent information of the historical demand sequence data of the region to be predicted. An attention mechanism is introduced to solve the problem of encoding longer input sequences. \n\nAn attention model was originally used for machine translation [29] and has been widely used in natural language processing, statistical learning, speech, and the computer fields. A hierarchical attention mechanism, which performs better than the traditional attention mechanism, was developed based on the original attention model. For example, when processing document classification, the hierarchical attention mechanism can simultaneously build sentence-and word-level attention models, while the traditional attention mechanism can only construct a single level of attention model. e decoder of HRHN introduces a hierarchical attention mechanism, which can mine the information stored in different layers to capture temporal dynamics at different levels, which will have a better impact on predicting future target series compared to the traditional attention mechanism [9]. e alignment model e [k]  t,i is calculated as follows: \n\nwhere \n\nt\u22121 \u2208 R p represents the output of the last layer of RHN in the decoder at time t-1, and v k \u2208 R l , T k \u2208 R e\u00d7p , and v k \u2208 R l\u00d7l are all trainable parameters. By computing the subcontext vector d [k]  t as a weighted sum of all the encoder's hidden states in the k-th layer, the soft alignment for layer k is obtained as \n\nen, the context vector that we feed to the decoder is calculated as d t \ufffd d [1]  t , d [2]  t , . . . \n\nwhere K r is the number of RHN layers. \n\nFrom the output of the encoder to the input of the decoder,  D p t is a time-dependent variable representing the interaction between D p i,t and d t : \n\nwhere  W \u2208 R 1\u00d71 and  V \u2208 R 1\u00d7K r e are the weight matrices and  b \u2208 R 1 is the bias term. \n\nRHN in the decoder is similar to that in the encoder, with the following related equations:",
            "score": 0.5317648411322881,
            "section_title": "Decoder.",
            "char_start_offset": 14585,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 241
                },
                {
                    "start": 244,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1178
                },
                {
                    "start": 1181,
                    "end": 1186
                },
                {
                    "start": 1189,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1615
                },
                {
                    "start": 1618,
                    "end": 1656
                },
                {
                    "start": 1659,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 1119,
                    "end": 1122,
                    "matchedPaperCorpusId": "46937148"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040924072265625
        },
        {
            "corpus_id": "258987259",
            "title": "The Impact of Positional Encoding on Length Generalization in Transformers",
            "text": "As a result of this rotation, the attention dot product will only depend on the relative distance between tokens, effectively making it a relative positional encoding (Su et al., 2021). \n\nALiBi, used in BLOOM (Scao et al., 2022a), is similar to T5's Relative Bias but instead subtracts a scalar bias from the attention score. This bias grows linearly with the distance between the query and key tokens. This, in effect, creates a preference toward recent tokens (recency bias). \n\nNote that encoder-only Transformers, such as BERT, become bag-of-words models in the absence of positional encoding. However, decoder-only Transformers with causal attention mask are not permutation invariant and can model sequences even without explicit position information (Tsai et al., 2019). But it is unclear if these models encode position information implicitly or generalize to unseen lengths. We demystify this in Section 5.",
            "score": 0.5315006971994343,
            "section_title": "Background: Positional Encoding in Transformers",
            "char_start_offset": 6539,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 914
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 775,
                    "matchedPaperCorpusId": "201698358"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4580078125
        },
        {
            "corpus_id": "271874360",
            "title": "Mamba Retriever: Utilizing Mamba for Effective and Efficient Dense Retrieval",
            "text": "In Section 3.1, we let Mamba be the base model  of retriever. As comparison, the base model  is changed from Mamba to other frequently-used base models, including the Transformer encoderonly models and decoder-only models. In this section, we analyze the differences between using Mamba and these models as . \n\nMamba and Decoder-only vs Encoder-only. Mamba and Transformer decoder-only models share similarities that distinguish them from Transformer encoder-only models. In terms of data, Mamba and decoder-only models are pre-trained on more data than most encoder-only models. In particular, Mamba and Pythia [2] are pre-trained on the same data. In terms of architecture, Mamba and decoder-only models have causal characteristics, which is not as suitable as encoder-only models with bi-directional attention for comprehension tasks like retrieval. Mamba can be reconstructed to be bi-directional, but this would lead to a decrease in efficiency. \n\nMamba vs Decoder-only. Intuitively, Transformer decoderonly model can capture long-term dependencies by self-attention mechanism, while Mamba may be limited by the maximum amount of information that can be compressed in latent states. \n\nHowever, some works [1,3,5,16] analyze that Mamba has some mechanism similar to or even surpassing Transformer: Mamba has implicit attention mechanism with good expressiveness; if each SSM is regarded as one head in multi-head self-attention mechanism, then Mamba has more heads than the Transformer; the softmax in self-attention can cause problems, such as over-smoothing, whereas Mamba does not use softmax and thus may better capture subtle differences between different tokens. \n\nIn addition, Mamba has an additional explicit process of summarizing previous information using the latent states. When calculating a token at position , decoder-only model uses the attention mechanism to access keys and values of all previous tokens.",
            "score": 0.5313108464980731,
            "section_title": "Base Model Comparison",
            "char_start_offset": 8036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1926
                }
            ],
            "ref_mentions": [
                {
                    "start": 612,
                    "end": 615,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15771484375
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "To investigate the impact of attention masking strategies on Gemma Encoder performance, we evaluated both causal and bidirectional masking using the GLUE benchmark. Table 6 provides a comparison of the GLUE average scores for Gemma-2 2B and 9B models under both masking conditions. For the bidirectional masking results, we report the optimal performance achieved across different pooling strategies, consistent with the methodology in Table 5. \n\nA key observation from our experiments is the superior performance of bidirectional masking compared to causal masking. This pattern is consistently observed across the majority of tasks within the GLUE benchmark, detailed in Section A.2 for different model sizes. This finding is particularly noteworthy given that the Gemma decoder's pretraining process utilizes solely causal masking. It suggests that employing bidirectional masking, at finetuning time, can still enhance performance on encoder-related tasks, even with a causally pretrained decoder and limited finetuning data. Furthermore, our results demonstrate that when causal masking is employed, last-token pooling exhibits significantly better performance than alternative pooling methods. This observation aligns with the inherent nature of causal masking, where the last token possesses a comprehensive contextual understanding due to its attention over the entire preceding sequence. This is analogous to the pretraining objective of decoderbased language models, where the embedding of the last token serves as the basis for predicting the subsequent token.",
            "score": 0.5310019991518748,
            "section_title": "Attention Masking",
            "char_start_offset": 19050,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1571
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80224609375
        },
        {
            "corpus_id": "277510528",
            "title": "Why do LLMs attend to the first token?",
            "text": "In this work, we focus on decoder-only Transformer models (Radford et al., 2018), that apply a causal mask to the attention mechanism. These are by far the most common types of Transformer models that are used in modern LLMs to date (Gemma Team et al., 2024;Dubey et al., 2024). We follow the notation of Barbero et al. (2024), but we importantly also consider a model with H \u2265 1 attention heads: \n\nwhere \u03c8 (\u2113) is a non-linearity, k takes queries q, keys k, and positional encodings p ij to produce an activation, W (\u2113,h) \u2208 R d\u00d7d and W (\u2113) \u2208 R Hd\u00d7d are learnable matrices, and represents a direct sum (concatenation). To simplify notation, we ignore the layer normalisations without loss of generality. The sum ranging over j such that j \u2264 i is due to the causal mask. If we represent the attention coefficients via a matrix A (\u2113,h) such that \n\n, this condition is equivalent to enforcing that A (\u2113,h) is lower triangular. An LLM consists of L such blocks, with L often called the depth. New tokens are generated autoregressively by considering the final layer representation v (L) n of the last token and mapping it to a distribution over the token vocabulary. A new token is sampled from this distribution, and the process repeats. \n\nAttention Sinks. The term attention sink was first used by Xiao et al. (2024) to indicate tokens that, although likely to have limited semantic meaning, attract a large portion of the attention within an attention head. They showed that it is important to keep such tokens when computing sliding-window attention to retain performance. Recent work by Gu et al. (2025) empirically ablated a number of components in the pre-training setup to study under which conditions attention sinks occur. While attention sinks are a broader term, in our work we focus on attention sinks forming exactly at the first token, as this is the most common pattern by far. To measure the presence of the sink we follow the metric proposed by Gu et al. (2025)",
            "score": 0.530196958050794,
            "section_title": "Background",
            "char_start_offset": 4432,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 326,
                    "matchedPaperCorpusId": "270285801"
                },
                {
                    "start": 1587,
                    "end": 1603,
                    "matchedPaperCorpusId": "273346029"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1981201171875
        },
        {
            "corpus_id": "261064777",
            "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks",
            "text": "This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., , 2019;;Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture. While GPT (Radford et al., 2018) and GPT-2 (Radford et al., 2018) did not exhibit the same level of performance as GPT-3 (Brown et al., 2020), with the increase in model size and the amount of data used for pretraining, GPT-3 (Brown et al., 2020) showcased a remarkable few-shot capability that earlier models did not possess. Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b). \n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a).",
            "score": 0.5279813684724924,
            "section_title": "Model Architecture",
            "char_start_offset": 12603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1853
                }
            ],
            "ref_mentions": [
                {
                    "start": 154,
                    "end": 179,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 179,
                    "end": 198,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 403,
                    "end": 423,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 508,
                    "end": 528,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11260986328125
        },
        {
            "corpus_id": "221702979",
            "title": "Current Limitations of Language Models: What You Need is Retrieval",
            "text": "We consider some of the successful recent language model approaches to improve the performancecomputes trade-off of language model (Kaplan et al., 2020) and classify them for the ease of our argument. Each category tends to be orthogonal to each other, so that they can often be used in combination for further gain. Fig. 1 summarizes our classification. There are four major categories: non-causal models, extension of batch length with efficient attention, memory and retrieval. We note that batch length in this paper is defined to be the length of a minibatch across timestep dimension. We avoid calling it context length to be more specific. In this paper, the default modality of our consideration is text unless specified otherwise. Also, memory and retriever are defined to possess a certain property addressed later in this section, so their usage may differ from the usual one. Efficient attention also refers to non-attention alternatives with better complexity, such as variants of convolution, for the convenience of our argument. \n\nNon-causal models in this paper are defined to be the model that predicts a token using future information. The example includes various BERT variants, including BART and T5 (Devlin et al., 2018;Raffel et al., 2019;Yang et al., 2019;Liu et al., 2019;Beltagy et al., 2020;Zaheer et al., 2020;Lewis et al., 2019;Hendrycks et al., 2020). On the other hand, causal models include the original Transformer encoder-decoder (Vaswani et al., 2017) and GPT-2/3. Though we usually consider the decoder-only model as the latter for a causal model, it should be clear from the context whether a causal model refers to the former, the latter or both.",
            "score": 0.5273144443959947,
            "section_title": "Classification of Recent Language Model Approaches",
            "char_start_offset": 53,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1683
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04046630859375
        },
        {
            "corpus_id": "235829695",
            "title": "The Piano Inpainting Application",
            "text": "The encoder E and the decoder D can be implemented by any autoregressive model. \n\nIn this work, we use Linear Transformers [16] as they provide a way to overcome the O(T 2 ) complexity of computing the self-attention, which allows us to consider longer sequences, and we rely on a standard encoder-decoder architecture with transformers as proposed in [25]. In order to enforce the particular dependency structure displayed in Eq. ( 1), we propose to use anti-causal masks on the encoder part, and causal masks on the decoder part. Thanks to the alignment between the sequence of constraints c and the input sequence x, cross-attention can be simplified with a much simpler operation since at location t, the decoder only needs attend to E(c \u2265t ). Such operation is equivalent to masking out all off-diagonal terms of the cross-attention matrix in the standard setting. Compared with [8], replacing RNNs with Linear Transformers allows to compute predictions for all timesteps in parallel.",
            "score": 0.5270416058728437,
            "section_title": "Efficient masking scheme",
            "char_start_offset": 11526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 82,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 989
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 127,
                    "matchedPaperCorpusId": "220250819"
                },
                {
                    "start": 884,
                    "end": 887,
                    "matchedPaperCorpusId": "53873453"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1558837890625
        },
        {
            "corpus_id": "272550747",
            "title": "Integrating the Expected Future in Load Forecasts with Contextually Enhanced Transformer Models",
            "text": "In our experiments, we adopt non-causal (bi-directional) attention, as introduced by Devlin et al. [26] in the BERT model. This choice is motivated by the non-causal nature of our time-series regression task, where the entire context -both past and future -is accessible at any point during the forecasting period. By leveraging bi-directional attention, we effectively utilize all available data, enabling more comprehensive integration of contextual information to enhance forecasting accuracy. This approach contrasts with causal attention mechanisms, which prevent unavailable future tokens from influencing the prediction of current tokens. Causal attention ensures that the model only uses information available at the time of prediction. However, in our context, where future contextual information is available and beneficial, non-causal attention can provide an advantage. This is typically achieved through a masking technique: \n\nHere, O ij is set to \u2212\u221e if j > i (for future tokens) and 0 otherwise, effectively ignoring positions j that are greater than i. By modifying the mask O ij = 0 for all i and j, the model can leverage the full bidirectional future context. This adjustment enhances the model's ability to integrate information across the entire input sequence, enabling it to utilize both past and future data effectively. The Spacetimeformer, which emerged as the best performing model in our tests, leverages the permutation invariance property of self-attention. This allows it to flatten the multivariate time series, extending the attention across all N i \u00d7 w tokens in the encoder and N i \u00d7 h tokens in the decoder, respectively.",
            "score": 0.5269286352828786,
            "section_title": "Non-Causal Attention:",
            "char_start_offset": 52158,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1656
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.340087890625
        },
        {
            "corpus_id": "269032853",
            "title": "Adapting LLaMA Decoder to Vision Transformer",
            "text": "Attention collapse issue.As a common practice for Transformer decoders, the key component for causal self-attention is the causal mask, i.e., a lower triangular mask matrix, illustrated in Eq. 1 and Figure 2(a).With such, each token can get the attention score of all its previous ones.We add the causal mask to our network via a non-autoregressive way.The reason is that visual perception tasks, unlike text generation, require only inference once.As a result, we observe that the training loss fails to converge in both ViT-T/16 and ViT-B/16 regimes (line 1 in Table 1).We posit that such issue stems from the influence of the lower triangular matrix, which prevents the class token from \"seeing\" other image tokens.As illustrated in Figure 2(a), when the class token is positioned at the start of the patch embedding, its attention score for all other image tokens gets zero due to a causal mask.We term this occurrence as the attention collapse issue, which leads to a loss of connection between the class token and other image patches, thereby hindering the optimization of the network.Post-sequence class token (PS [cls]).The attention collapse issue stems from the inappropriate placement of the token.To this end, we suggest a PS [cls] strategy, by placing it at the end of the token sequence, without changing the causal mask, as shown in Figure 2(b) and Figure 1.Such modification ensures that the class token can achieve global information about all image tokens, while maintaining a causal self-attention property.As a result, we observe that the attention collapse issue is eliminated and the training process starts to stabilize, leading the network performance to 71.9% for ViT-T/16 and 80.6% for ViT-B/16 regime, respectively (line 2 in Table 1).\n\nTo test our hypothesis about the reason of the attention collapse issue, we also explore a mask setting in Figure 2(c).In this setting, we do not change the position of the class token.",
            "score": 0.5266896911008414,
            "section_title": "Causal Self-Attention Leads to Attention Collapse",
            "char_start_offset": 9673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 25,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 353
                },
                {
                    "start": 353,
                    "end": 449
                },
                {
                    "start": 449,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 899
                },
                {
                    "start": 899,
                    "end": 1091
                },
                {
                    "start": 1091,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1209
                },
                {
                    "start": 1209,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1762
                },
                {
                    "start": 1764,
                    "end": 1883
                },
                {
                    "start": 1883,
                    "end": 1949
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1279296875
        },
        {
            "corpus_id": "252968120",
            "title": "On effects of Knowledge Distillation on Transfer Learning",
            "text": "This section focuses on the origin and motivation for developing an attention mechanism in neural networks. Attention mechanism refers to the act of attending to just a relevant region of input instead of the whole input image or sentences. In contrast to encoding a variable-length sequence into a fixed-length vector [97], Bahdanau et. al. [98] introduced the attention mechanism with encoderdecoder architecture. In encoder-decoder architecture, a neural network called an encoder is used to encode the training input to a representation. After that, another network called a decoder is used to decode the representation to get the output. In attention-based networks, the decoder can essentially attend/focus on a specific region of the input/source sequence directly. This allows the model to learn the corresponding mapping of relevant words in different sentences for natural language processing.\n\nDiving further into attention, Vaswani et. al. [32] proposed to remove the whole sequence/recurrence bit from the model. Removing recurrence would enable models to be trained in parallel on massive datasets in significantly less time. They fundamentally only used a type of attention to process the whole sequence in parallel with the help of positional encoding. This model was called Transformers [32] which would employ self-attention, which allows the model to learn representations by looking at the input sequence itself. This Transformer model [32] actually revolutionized the NLP research as we know it today.\n\nWith the help of self-attention in transformers, Transformers encoders [32] learn unidirectional representations of words with the help of context. BERT [9] learns representation looking at both forward and backward section inspired by Cloze procedure [99]. To learn the word-word interaction, BERT masks a word and uses context around it to predict the masked word. The intuition is that the model will be able to predict masked words once it learns about the syntax and semantics of the words. It learns more general sentence-level representation with the help of proxy tasks like Next Sentence Prediction (NSP). The representations that it learns were so rich that when applied to multiple downstream tasks ranging from machine translation, Question Answering, etc. BERT works in the form of transfer learning, i.e., pre-training with massive datasets and",
            "score": 0.5265642227267316,
            "section_title": "Attention, Transformers, and BERT",
            "char_start_offset": 64201,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 323,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 342,
                    "end": 346,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 952,
                    "end": 956,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1304,
                    "end": 1308,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1456,
                    "end": 1460,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1595,
                    "end": 1599,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1677,
                    "end": 1680,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1776,
                    "end": 1780,
                    "matchedPaperCorpusId": "206666846"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2210693359375
        },
        {
            "corpus_id": "272593363",
            "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM",
            "text": "where   is a special token representing the -th self-supervised task: it is a reconstruction task when 0 <  \u2264  and a generation task when  <  \u2264 . For example, this could be: \"reconstruct_title: title: Yellowstone tourist injured ...\" Cascaded Attention Mask. Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking. \n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty. \n\nPost-pretraining. As depicted in Figure 3, we replicate each item content  times to create  training samples. Each sample is tailored to a specific reconstruction or generation task   . \n\nWe will employ low-rank adaptation (LoRA) [8], a parameterefficient fine-tuning approach, to train the language model. Additionally, we will freeze the pretrained word embeddings while tuning the dense token embeddings and task embeddings. \n\nTo perform the filling operation on the placeholder block, we will conduct dual forward propagation: first to capture the output from the token block to fill the placeholder, and then to tune the language model with the next token prediction task: \u00e2,+1 = argmax  \u2208W  ( | ,1 ,  ,2 , . . . ,  , ), \n\noptimized using cross-entropy loss, where W denotes the token vocabulary and  , denotes the -th token of the attribute a  .",
            "score": 0.5263825258916119,
            "section_title": "Dense Tokenizer",
            "char_start_offset": 12504,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1590
                },
                {
                    "start": 1593,
                    "end": 1880
                },
                {
                    "start": 1881,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 2014
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "Transformer architectures have revolutionized NLP tasks, such as sequence modeling, by effectively capturing 2 https://paperswithcode.com/ long-range dependencies and modeling relationships between words. The advantages of the transformer architecture include enhanced parallelization, faster training, and the ability to model long-range dependencies efficiently. The attention mechanism allows the model to focus on relevant parts of the input sequence, contributing to its success in handling variable-length sequences without sacrificing performance. Recognizing the shift from encoder-decoder to decoder-only architectures, understanding pre-training strategies, and the advantages of transformer models provide a more nuanced perspective on their capabilities in Generative AI and various NLP tasks. Here, we will distinguish between the original encoder-decoder architecture and the decoder-only architecture and the pre-training strategies of transformer models. \n\nEncoder-Decoder Architecture. The encoder-decoder architecture serves as a fundamental structure in Transformer models, employed for sequence-to-sequence tasks such as machine translation, where an input sequence (source language) is transformed into an output sequence [18], [110]. In an encoder-decoder architecture, the model consists of two main components featuring multiple layers of self-attention and feedforward layers: an encoder and a decoder network. The encoder network processes the input sequence, capturing relevant information and creating a contextualized representation that encompasses semantic and syntactic details of the input. Subsequently, the decoder network, in turn, utilizes this contextualized representation from the encoder to generate the output sequence step by step. At each step, the decoder attends to various parts of the encoder's output, facilitating the alignment of source and target language information. Both the encoder and decoder components typically employ the self-attention mechanism [18]. This mechanism enables the model to weigh the importance of different positions in the input sequence during the generation of the output sequence, thereby allowing for the capture of long-range dependencies.",
            "score": 0.5262935206959196,
            "section_title": "D. Architecture of Transformer Models",
            "char_start_offset": 51023,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1249,
                    "end": 1254,
                    "matchedPaperCorpusId": "259501685"
                },
                {
                    "start": 2007,
                    "end": 2011,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239013671875
        },
        {
            "corpus_id": "257038220",
            "title": "Learning Language Representations with Logical Inductive Bias",
            "text": "The CausalLM has a lower triangular mask for the binary atoms so that the information pattern is always unidirectional. On the other hand, the PrefixLM will have a bi-directional mask for the input segment (the top-left part in Figure 4(b)) and a unidirectional mask for the output (target) segment (the bottom-right part in Figure 4(b)). Accordingly, the binary atoms of the PrefixLM version will share a similar pattern as its mask in Figure 4(b), which models the relatons for the prefix and the output segments separately. With such simple modifications, our decoder-only FOLNet model will have the same input-output interface as the decoder-only transformers; it can be pretrained to predict the next tokens using a linear classifier over the unary atoms u L (x t ). After the training, it can generate tokens in an auto-regressive manner. \n\nEncoder-Decoder For the encoder-decoder variant, we use two separate stacks of FOLNet for the encoder and decoder, where each of them has the same overall architecture as in Figure 1 with its own set of atoms (the green and blue blocks in Figure 4(c)). In particular, the encoder will be identical to the encoder-only version that we have thoroughly discussed earlier in the main paper. Meanwhile, the decoder part will be similar to the decoder-only variant with a few additional modifications. First, the decoder needs to maintain a slightly different version of binary atoms (Figure 4(c)). Specifically, besides the relations between the output tokens, the decoder also has to model the (unidirectional) relations from the input tokens to the output tokens (i.e., the bottom-left part of u h (x, y) in Figure 4(c)). These relations are crucial in deducing the output tokens from the input ones, which plays a \n\nTable 5: The binary kernels and premises to be masked for decoder-only versions of FOLNet. In the encoder-decoder variant, similar part of the kernels and premises would be masked in its decoder.",
            "score": 0.5259382665682433,
            "section_title": "A.5 EXTENDING FOLNET TO TEXT-TO-TEXT VERSIONS",
            "char_start_offset": 38134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1758
                },
                {
                    "start": 1761,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1956
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1673583984375
        },
        {
            "corpus_id": "220055529",
            "title": "FastSpec: Scalable Generation and Detection of Spectre Gadgets Using Neural Embeddings",
            "text": "Although recurrent models with attention mechanisms learn the representations of long sequences, attention-only models, namely Transformer architectures [63], are shown to be highly effective in terms of computational complexity and performance on long-range dependencies. Similar to seq2seq architecture, Transformer architecture consists of encoder-decoder model. The main difference of Transformer is that recurrent models are not used in encoder or decoder units. Instead, the encoder unit is composed of L hidden layers where each layer has a multi-head self-attention mechanism with A attention heads and a fully connected feed-forward network. The input embedding vectors are fed into the multi-head attention and the output of the encoder stack is formed by feed-forward network which takes the output of the attention sub-layer. The decoder unit also has L hidden layers, and it has the same sublayers with encoder. In addition to one multi-head attention unit and one feed-forward network, the decoder unit has an extra multi-head attention layer that processes the encoder stack output. In order to process the information in the sequence order, positional embeddings are used with token embeddings where both embedding vectors have a size of H. \n\nKeeping the same Transformer architecture, Devlin et al. [9] introduced a new language representation model called BERT (Bidirectional Encoder Representations from Transformers) which surpasses the state-of-the-art scores on language representation learning. BERT is designed to pre-train the token representation vectors of deep bidirectional Transformers. For the detailed description of the architecture, we refer the readers to [9,63]. The heavy part of the training is handled by processing unlabeled data in an unsupervised manner. The unsupervised phase is called pre-training which consists of masked language model training and next sentence prediction procedures. The supervised phase is referred to as fine-tuning where the model representations are further trained with labeled data for a text classification task. Both phases are further explained in detail for Spectre gadget detection model in Section 5. \n\n3 Related Work",
            "score": 0.5248386203279946,
            "section_title": "Transformer and BERT",
            "char_start_offset": 12589,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2178
                },
                {
                    "start": 2181,
                    "end": 2195
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 157,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1316,
                    "end": 1319,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1691,
                    "end": 1694,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1694,
                    "end": 1697,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "276771845",
            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
            "text": "When processing sequences of varying lengths in batched inputs, padding is essential to create uniform input tensors. However, the choice between left padding (prepending padding tokens) and right padding (appending padding tokens) may impacts model behavior, especially when adapting encoder model from decoder-only models. \n\nIn decoder-only transformer models, such as those used for language generation (e.g., GPT (OpenAI, 2023), Gemma (Gemma Team, 2024a,b) models), left padding is typically used to align the inputs during training or inference, due to the auto-regressive training objective and efficient positional embedding. Decoder-only models are trained in an auto-regressive manner, where the task is to predict the next token based on all previous tokens. Decoder-only transformer models use positional embeddings to encode the order of tokens. Left-padding ensures that the relative positions of the actual tokens remain consistent regardless of the sequence length. \n\nHowever, right padding is acceptable for encoder models because of the way these models process input sequences. Unlike decoder-only models, encoders like those in BERT (Devlin et al., 2019), T5 (Raffel et al., 2020) or other bidirectional transformer models handle the entire input sequence at once, and their attention mechanisms allow tokens to attend to any other token in the input through bidirectional attention. In encoder models, positional embeddings are applied to the entire sequence, including padding tokens. Since the padding tokens are ignored during attention, their positional embeddings don't interfere with the actual computation. Whether the padding is on the right or left does not affect the functionality. \n\nThe choice of padding strategy has implications for the pooling layer, especially in conjunction with causal attention and First-K/Last-K token pooling.",
            "score": 0.5240544179538517,
            "section_title": "Padding Strategies",
            "char_start_offset": 10417,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1178,
                    "end": 1199,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282958984375
        },
        {
            "corpus_id": "277940014",
            "title": "DETAM: Defending LLMs Against Jailbreak Attacks via Targeted Attention Modification",
            "text": "Problem formulation. Attention mechanism. In decoder-only models, the attention calculation is based on the similarity between the query (Q), key (K), and value (V ) matrices. For the i-th attention head at the j-th layer, the attention weights are computed as follows: \n\nwhere Q, K, and V are the result of multiplying the input X with the learnable projection matrices W q , W k , and W v , respectively. The input X can be the output of the previous layer or the initial embedding layer for the first layer. M is the auto-regressive mask matrix, which ensures that the model does not access future tokens, thereby preserving the causal structure of the generation process. The computed attention weights A i,j are then used to weight the value matrix V i,j , yielding the final attention output:",
            "score": 0.5232218486588004,
            "section_title": "Preliminary",
            "char_start_offset": 7241,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 21,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 798
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1885986328125
        },
        {
            "corpus_id": "256901258",
            "title": "Write and Paint: Generative Vision-Language Models are Unified Modal Learners",
            "text": "Given the superior performance of auto-regressive language models (LM) (Brown et al., 2020;Chowdhery et al., 2022;Rae et al., 2021) on zero-shot and few-shot transfer abilities, we decided to adopt a decoder optimized by language modeling loss to retain the generalization capabilities, and an encoder to represent the prefix input. Unlike using a causal mask in the decoder, the encoder employs fully-visible attention for the prefix input. This architecture resembles prefix language modeling, which shows effectiveness in a wide range of language tasks (Dong et al., 2019;Raffel et al., 2020) and enables zero-shot generalization abilities. Contrary to the previous multi-stage approaches (Wang et al., 2021a;Singh et al., 2021), our model is trained from scratch in an end-to-end manner thanks to the model's simplicity. In this section, we introduce the proposed prefix multi-modal modeling framework and the DAVINCI model. The overall architecture of DAVINCI is depicted in Figure 1. \n\nWe first explain our model architecture in detail in \u00a73.1 and then introduce pre-training objectives and procedures in \u00a73.2.",
            "score": 0.5232087757412589,
            "section_title": "DAVINCI",
            "char_start_offset": 9340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1116
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 91,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 556,
                    "end": 575,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 575,
                    "end": 595,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0784912109375
        },
        {
            "corpus_id": "220041944",
            "title": "Differentiable Window for Dynamic Local Attention",
            "text": "To verify our modeling decisions, we performed an ablation study in the WMT'14 En-De translation task. In particular, we evaluated (i) the impact of applying our differentiable window attentions in all layers vs. only in certain lower layers of the Transformer network, (ii) which window attention methods (additive or multiplicative) are suitable particularly for the encoder/decoder selfattention and cross-attention, and (iii) the impact of segment-based masking in different attention modules. (iv) training efficiency and performance of our best model with the similar models. Plus, to further interpret our window-based attention, we also provide the local window visualization. Full vs. Partial. Table 2 shows BLEU scores for the Transformer models that employ our windowbased attentions in all 6 layers (Full) vs. only in the first 3 layers (Partial), as well as the methods used in different attention modules (encoder/decoder self-attention, cross-attention). We can see that almost all the models with window-based methods in the first 3 layers outperform those that use them in all 6 layers. This gives the setup significant advantages as it performs not only better in BLEU but also requires less parameters. \n\nThe results also show that multiplicative window (MW) attention is preferred in decoder selfattention, while additive window (AW) is more suitable for encoder self-attention and for crossattention. This suggests that the global context, which is maintained in AW, is more useful when it is entirely available like in encoder selfattention and cross attention. In contrast, incomplete and partially-generated context in decoder self-attention may induce more noise than information, where MW attention renders better performance than AW. \n\nToken-vs. Segment-based. Table 3 compares the results for using token-based vs. segment-based masking methods in different attention modules of the network. Note that it is preferred for decoder self-attention to adopt token-based masking since the decoder cannot point to unfinished segments in autoregressive generation, if it had used segmentbased masking.",
            "score": 0.5226183266916655,
            "section_title": "Ablation Study",
            "char_start_offset": 22873,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1760
                },
                {
                    "start": 1763,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1787
                },
                {
                    "start": 1788,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2122
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43212890625
        },
        {
            "corpus_id": "270703043",
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "text": "The emergence of pre-trained language models represented a paradigm shift, driving research toward designing more efficient architectures and refining pre-training strategies. These pre-trained models have been commonly adapted or specialized to downstream tasks via fine-tuning, which involves transferring knowledge by further training a model on new data. There are significant advantages demonstrated by these models in language understanding and model performance in various tasks [9,13]. \n\nELMo is one of the earliest attempts at pre-trained language models [15]. This model was developed to capture context-aware word representations by pre-training a bidirectional Long Short-Term Memory (biLSTM) network and fine-tuning it for subsequent downstream tasks. Later the Transformer architecture was introduced, revolutionizing the NLP field by offering highly parallelizable structures and self-attention mechanisms. The Transformer [6] follows the autoencoder archetype, from which three families of models arose: (1) BERT-family or encoder-only models, (2) GPT-family or decoder-only models, and (3) text-to-text or encoder-decoder models. In Fig. 1, the graphical representations of these  Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively. families are shown.",
            "score": 0.5224071970542904,
            "section_title": "Pre-trained language models",
            "char_start_offset": 5957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2097
                },
                {
                    "start": 2098,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 564,
                    "end": 568,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1898193359375
        },
        {
            "corpus_id": "257019949",
            "title": "JEIT: Joint End-to-End Model and Internal Language Model Training for Speech Recognition",
            "text": "We train HAT and MHAT with 2-pass cascaded encoders and separate decoders as in [38,39]. They share the same front-end and encoder architecture. Specifically, 128-dim log Mel filterbanks are extracted from speech signal and are subsampled to form a 512-dim feature every 30 ms. Each speech feature is appended with a 16dim domain ID [40]. The causal encoder is a 7-layer conformer with causal convolution and left-context attention. The non-causal encoder is a 10-layer conformer with right-context attention that processes 900 ms of speech into the future. Each conformer layer uses a 512-dim 8-head self-attention and a convolution kernel of size 15. \n\nThe causal and non-causal decoders of HAT or MHAT decode using the outputs of the causal and non-causal encoders, respectively. The label decoders of HAT and MHAT are 2-layer LSTMs with 2048 hidden units in each layer. In HAT, the label decoder output passes through a 640-dim feedforward joint network before projected to 4096 output units representing word pieces [41]. In MHAT, the label decoder output is directly projected to the output layer of the same size. ILMs of HAT and MHAT have 30.7M and 30M parameters, respectively. The blank decoder of MHAT is a 320-dim V 2 embedding decoder [42] with a look-up table shared between the last 2 tokens and has 1.5M parameters. Overall, HAT and MHAT have in total 205M and 210M model parameters, respectively. We report only the 2nd pass WER in this paper. We train baselines with only audio-transcript pairs and show their WERs in Table 1. \n\nMoreover, we train a 12-layer conformer LM with 384-dim selfattention and 3072-dim feedforward layer [6]. The external LM has left attention context of 31 and has in total 70M parameters.",
            "score": 0.5223203229322125,
            "section_title": "Modeling",
            "char_start_offset": 9470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1413
                },
                {
                    "start": 1414,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1734
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 84,
                    "matchedPaperCorpusId": "248157514"
                },
                {
                    "start": 84,
                    "end": 87,
                    "matchedPaperCorpusId": "225094578"
                },
                {
                    "start": 333,
                    "end": 337,
                    "matchedPaperCorpusId": "204900916"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "22320655"
                },
                {
                    "start": 1248,
                    "end": 1252,
                    "matchedPaperCorpusId": "237532186"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09136962890625
        },
        {
            "corpus_id": "274192172",
            "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
            "text": "The Decoder-only Transformer model [15], also known as the autoregressive Transformer, is a variation of the original encoder-decoder transformer model proposed by Vaswani et. al. [16]. This model omits the encoder stack, utilizing only the decoder stack to learn the causal attention structure in Z Q , and to perform autoregressive sequence generation, once trained. The model's self-attention mechanism enables the model to predict the next token in a sequence by attending to all previous tokens. It also allows for the use of crossattention, but does not specify how the cross-attending context is obtained (as opposed to an encoder-decoder architecture where the cross-attention context is coming from the encoder). \n\nAs illustrated in Fig. 2, during the training of the transformer, the masked indexed output of the VQ-VAE quantizer Z Q is compared with its estimate \u1e90Q at the output of the transformer using a cross-entropy loss. At the inference, we start from a random token, and continue generating subsequent tokens of a fake \u1e90Q auto-regressively. We convert \u1e90Q (2) into its non-indexed version \u1e91q and feed it into the VQ-VAE decoder to generate the spectrogram of the discrete latent deep fake produced by the transformer.",
            "score": 0.5222356610977994,
            "section_title": "B. Decoder-Only Transformer",
            "char_start_offset": 9264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 721
                },
                {
                    "start": 724,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1235
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2362060546875
        },
        {
            "corpus_id": "91183924",
            "title": "A Simple and Robust Convolutional-Attention Network for Irregular Text Recognition",
            "text": "where a i = Atten(q i , K, V). \n\nThe above scaled dot-product attention can be applied multiple times (multi-head) with different linear projections to Q, K and V, followed by a concatenation and projection operation: \n\nwhere \n\nThe parameters are \n\nWe set the number of attention heads H to 8 for our proposed model (see Section 4.3 for the ablation study on the selection of H). Masked Self-Attention This attention layer is used to model the dependencies between different decoding positions, where the queries, keys and values are the same, i.e., the right-shifted outputs. In this case, M = M = the length of decoded sequence. A mask is applied to prevent each position from attending to positions after that position. Two-Dimensional Attention In this layer, the queries come from the masked self-attention layer, and the keys and values are the 2D output features of the CNN encoder. In this case, M = 4 \u00d7 13 and M is the length of decoded sequence. It is the only connection between the encoder and decoder, and allows each decoding position attend to the 2D positions of the encoder outputs. Point-wise Feed-Forward Layer A simple feed-forward network is applied at each position of the outputs of two-dimensional attention layer, which contains two linear transformations of dimension d and a ReLU non-linearity in between. The parameters of this layer are shared across all positions. Prediction and Loss Function A linear transformation followed by a softmax function is used to transform the decoder output into prediction probabilities over character classes. Here we use 94 symbol classes, including digits, case-sensitive letters and 32 punctuation characters. The parameters are also shared over all decoding positions. The standard cross-entropy function is adopted to compute the loss of the predicted probabilities w.r.t. the ground-truth, at each decoding position.",
            "score": 0.5221499129050291,
            "section_title": "Decoder",
            "char_start_offset": 14372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 33,
                    "end": 217
                },
                {
                    "start": 220,
                    "end": 225
                },
                {
                    "start": 228,
                    "end": 246
                },
                {
                    "start": 249,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1885
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "214727638",
            "title": "X-Linear Attention Networks for Image Captioning",
            "text": "Image captioning is the task of automatically producing a natural-language sentence to describe the visual content of an image. The essential practice of neural captioning models follows encoder-decoder paradigm [24,33], which is derived from neural machine translation [30]. In between, Convolutional Neural Network (CNN) is utilized to encode an input image and Recurrent Neural Network (RNN) is adopted as sentence decoder to generate the output sentence, one word at each time step. Despite involving two Comparison between conventional attention mechanism and our X-Linear attention block for image captioning. (a) Conventional attention mechanism linearly fuses query (Q) and key (K) via element-wise sum to compute spatial attention weight for each value (V), which characterizes the 1 st order interaction between query and key. (b) X-Linear attention block fully capitalizes on bilinear pooling to capture the 2 nd order feature interaction in between, and measures both spatial and channel-wise attention distributions. The two attention weights are adopted to accumulate the enhanced values of bilinear pooling on query and value. different major modalities (visual content and textual sentence) in image captioning, such paradigm of approaches seldom explores the multi-modal interactions particularly at the early stage. In other words, vision and language are treated independently. That prompts the recent stateof-the-art methods [2,35] to adopt visual attention mechanisms which trigger the interaction between visual content and natural sentence. Concretely, these visual attention mechanisms boost performance by learning to identify selective spatial regions in an image conditioning on current hidden state of language decoder, and in turn accumulating encoded region features with attention weights to guide decoding process. Figure 1(a) illustrates the most conventional attention measure which estimates attention weights via linearly fusing the given query (hidden state of sentence decoder) and key (encoded image features) from different modalities. The attention is then applied to the value (encoded image features) to derive a weighted sum. Nevertheless, we argue that the design of conventional attention inherently exploits only the 1 st order feature interaction and is still lacking in efficacy. That severely limits the capacity of complex multi-modal reasoning in image captioning.",
            "score": 0.5215649869648716,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2416
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "3527896"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "1169492"
                },
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1445,
                    "end": 1448,
                    "matchedPaperCorpusId": "3753452"
                },
                {
                    "start": 1448,
                    "end": 1451,
                    "matchedPaperCorpusId": "1055111"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036224365234375
        },
        {
            "corpus_id": "248239682",
            "title": "DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks",
            "text": "Transformer is a neural network model proposed by Vaswani et al. (2017), which relies on the multi-head self-attention (MHA) mechanism.\n\nInput Layer. Due to the order-invariance of MHA , a token embedding is added with a position embedding as the input of Transformer Encoder or Decoder:\n\nwhere x i is a token at the i th position. T E is a token embedding matrix and P E is a position embedding matrix. In the paper of Vaswani et al. (2017), they use a fixed sinusoidal P E:\n\nwhere j is the dimension and d m is the model size.\n\nIn the later work, Devlin et al. (2019) choose to use a learnable P E matrix.\n\nMulti-head Self-attention (MHA). MHA takes a sequence of vectors h = [h 1 , h 2 , ..., h n ] as input. Then they are transformed into three different vectors, query (Q), key (K) and value (V), by three linear transformations and passed to the multi-head self-attention (MHA). The computation process of a single head is:\n\n(3) where d k is the dimension of a single head. MHA repeats the same process for h heads. The outputs of all heads are concatenated together and passed through a linear projection W O again:\n\nTransformer Encoder and Decoder. An Encoder layer consists of multi-head attention following with a feed-forward network (FFN). The outputs of MHA and FFN are passed through a Lay-erNorm (Ba et al., 2016) with residual connections (He et al., 2016). Then we stack multi-layer to form a Transformer Encoder. The difference between Decoder and Encoder is that Decoder uses the causal attention masks to mask the attention values of the subsequent tokens so that Decoder can only decode tokens relying on the tokens in the past. 1",
            "score": 0.5210045876383305,
            "section_title": "Background: Transformer",
            "char_start_offset": 3766,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 50,
                    "end": 71,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 420,
                    "end": 441,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 549,
                    "end": 569,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1355,
                    "end": 1372,
                    "matchedPaperCorpusId": "206594692"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26123046875
        },
        {
            "corpus_id": "201269007",
            "title": "An Overview of End-to-End Automatic Speech Recognition",
            "text": "The decoder is usually an RNN network that receives context information computed by attention and then decodes it to get the final transcription. Structurally, the decoder has two working modes in the existing work. One is to use previous hidden state s u\u22121 to perform attention on encoding result sequence F, get the context information c u firstly, and then calculate the decoder's current hidden state s u according to c u [47,58,61]. The other is to calculate the decoder's current hidden state s u firstly, and then perform the attention on encoding result sequence to calculate the context information c u . The latter method has been used more widely. [48,49,[51][52][53]56,59]. \n\nA lot of work has tried to improve attention-based ASR performance from the perspective of decoder. \n\nRef. [58] found that, in general, the decoder had only a one-layer network, and was not specifically designed for language features. So they designed the following decoder: \n\nCompared to the traditional decoder structure, this work added the network layer shown by the Equation (36). This layer only acts on the historical output sequence, so it implicitly acts as a language model, which also allows the Decoder to have a longer memory. \n\nRef. [55] designed a multi-head-decoder technology for decoding. In the multi-header, different attention mechanisms are used on the encoding result sequence for context extraction and decoding. Results of these multiple attention decoding processes are aggregated to obtain the final decoding output. \n\nThe continuity, monotonicity, etc. of the attention mechanism naturally do not exist in CTC because CTC itself requires continuity and monotonicity of soft alignment between input and output. Ref. [54] attempted to combine CTC and attention in the training and inferring process to solve these problems for attention. It combined CTC and attention by sharing the encoder between them and extending the training loss function to L = \u03bb log p ctc (L|X) + (1 \u2212 \u03bb) log p att (L|X), which taking into account the loss of CTC and attention. For decoding, the probability of CTC and attention were also jointly considered.",
            "score": 0.5207744953141284,
            "section_title": "Works on Decoder",
            "char_start_offset": 55754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 426,
                    "end": 430,
                    "matchedPaperCorpusId": "453615"
                },
                {
                    "start": 430,
                    "end": 433,
                    "matchedPaperCorpusId": "1054182"
                },
                {
                    "start": 433,
                    "end": 436,
                    "matchedPaperCorpusId": "41712217"
                },
                {
                    "start": 659,
                    "end": 663,
                    "matchedPaperCorpusId": "1921173"
                },
                {
                    "start": 663,
                    "end": 666,
                    "matchedPaperCorpusId": "11760007"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "18165915"
                },
                {
                    "start": 670,
                    "end": 674,
                    "matchedPaperCorpusId": "28854814"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "9655643"
                },
                {
                    "start": 678,
                    "end": 681,
                    "matchedPaperCorpusId": "1506905"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "206742954"
                },
                {
                    "start": 795,
                    "end": 799,
                    "matchedPaperCorpusId": "1054182"
                },
                {
                    "start": 1068,
                    "end": 1072,
                    "matchedPaperCorpusId": "28742309"
                },
                {
                    "start": 1731,
                    "end": 1735,
                    "matchedPaperCorpusId": "24376954"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089599609375
        },
        {
            "corpus_id": "231986066",
            "title": "Position Information in Transformers: An Overview",
            "text": "There are different set-ups for using a Transformer model. One common possibility is to have an encoder only. For example, BERT (Devlin et al. 2019) uses a Transformer model T(X) as encoder to perform masked language modeling. In contrast, a traditional sequence-to-sequence approach can be materialized by adding a decoder. The decoder works almost identically to the encoder with two exceptions: (1) The upper triangle of the attention matrix A is usually masked in order to block information flow from future positions during the decoding process. (2) The output of the encoder is integrated through a cross-attention layer inserted before the feed-forward layer. See Vaswani et al. (2017) for more details. The differences between an encoder and encoder-decoder architecture are mostly irrelevant for the injection of position information and many architectures rely just on encoder layers. Thus for the sake of simplicity we will talk about Transformer encoder blocks in general for the majority of the article. See \u00a74.4 for position encodings that are tailored for encoder-decoder architectures.",
            "score": 0.5207279102229967,
            "section_title": "Encoder-Decoder",
            "char_start_offset": 6676,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 147,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 671,
                    "end": 692,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2147216796875
        },
        {
            "corpus_id": "276961508",
            "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
            "text": "Causal Sequence Modeling. State-of-the-art large language models [1,8] adopt a decoder-only Transformers [42] for causal modeling of language sequences and autoregressive generation, a method commonly known as the GPT [1] style approach. In the vision domain, images can be quantized into discrete tokens [41] and flattened from 2D to 1D, enabling generation via the next-token prediction paradigm, as seen in models like VQGAN [7], Llam-aGen [35] etc. [19,20,43,43,50]. These methods have demonstrated impressive generative performance. However, this token-by-token image generation approach is inefficient, especially when dealing with high-resolution images. Additionally, since the generation can only proceed in a specific token order, it encounters difficulties in zeroshot inference that require non-causal dependencies, such as inpainting and outpainting. \n\nMasked Sequence Modeling. Unlike causal sequence modeling, another mainstream approach to sequence modeling is the encoder-only [29,42,48,51] architecture with bidirectional attention, where multiple tokens in a sequence are randomly masked and then predicted. This method is widely used in BERT [16] for language modeling. In the vision domain, MaskGIT etc. [2,3,9,10,21,45,49,51] adopts the same approach for image modeling and generation. By leveraging bidirectional attention, this masked image generation method eliminates the constraints of causal dependencies, enabling multi-token generation in a single step with a randomized order, leading to significantly faster inference. However, due to the absence of KV cache in bidirectional attention, inference efficiency remains limited. The position of the next predicted token is integrated as a query within the attention mechanism. The corresponding probabilistic model, formalized in Eq. ( 8).",
            "score": 0.5202950104396873,
            "section_title": "Related Work",
            "char_start_offset": 5186,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 25
                },
                {
                    "start": 26,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "20282961"
                },
                {
                    "start": 428,
                    "end": 431,
                    "matchedPaperCorpusId": "229297973"
                },
                {
                    "start": 453,
                    "end": 457,
                    "matchedPaperCorpusId": "247244535"
                },
                {
                    "start": 994,
                    "end": 998,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1225,
                    "end": 1228,
                    "matchedPaperCorpusId": "246680316"
                },
                {
                    "start": 1228,
                    "end": 1230,
                    "matchedPaperCorpusId": "255372955"
                },
                {
                    "start": 1232,
                    "end": 1235,
                    "matchedPaperCorpusId": "257767316"
                },
                {
                    "start": 1235,
                    "end": 1238,
                    "matchedPaperCorpusId": "253553243"
                },
                {
                    "start": 1241,
                    "end": 1244,
                    "matchedPaperCorpusId": "254563906"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1651611328125
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "Transformer. Virtually all state-of-the-art LLMs are based on the Transformer architecture [Vaswani et al., 2017]. Due to its ubiquity, we only highlight a few relevant high-level characteristics. The main architectural unit of the Transformer is a Transformer block, which consists of (at minimum) multi-headed self attention [Cheng et al., 2016], layer normalization [Ba et al., 2016], a dense twolayer feedforward network, and residual connections [He et al., 2016]. A Transformer stack is a sequence of such blocks. In NLP applications, the Transformer ingests and outputs tokens. Since being introduced by Vaswani et al. [2017], various architectural variants of the Transformer have been proposed. A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider. \n\nEncoder-decoder. As originally proposed, the Transformer consisted of two stacks: an encoder and a decoder. The encoder is fed the sequence of input tokens and outputs a sequence of vectors of the same length as the input. Then, the decoder autoregressively predicts the target sequence, token by token, conditioned on the output of the encoder. To achieve this conditioning, the decoder includes cross-attention layers in each of its blocks, allowing the decoder to also attend to the output of the encoder. The self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right). \n\nWe hereafter refer to this architecture as the encoder-decoder (ED) . Notable pretrained language models using an encoder-decoder architecture include BART [Lewis et al., 2019] and T5 [Raffel et al., 2020].",
            "score": 0.5196015578736635,
            "section_title": "Architectures",
            "char_start_offset": 8455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1683
                },
                {
                    "start": 1686,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1892
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 113,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 451,
                    "end": 468,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 611,
                    "end": 632,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "258461112",
            "title": "ContraCLM: Contrastive Learning For Causal Language Model",
            "text": "Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations. \n\nTaking the encoder-only models in Table 7a for illustration, on average, BERT-Base (Devlin et al., 2019) and Roberta-Base (Liu et al., 2019)   between CodeGen and the BERT models trained on programming languages, i.e., CodeBERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2021), decreases or even diminishes when evaluated on the code search tasks, the performance gap is still significant as both the model size and pretraining data in CodeGen are much larger than those used by the encoder-only models in Table 7b. Similar trends were observed in the performance gap between the decoder-only and encoderdecoder models on both natural language (Lewis et al., 2020;Raffel et al., 2020) and programming language (Ahmad et al., 2021;Wang et al., 2021). The large performance gap severely limits the decoder-only models used in many discriminative tasks. To this end, contrastive learning shows the promise to largely bridge the gap. As seen in Table 7a, on STS, CONTRACLM reduces the relative performance gap from 67.24% (absolute 21.12%) to 16.17% (absolute 7.33%) regarding BERT-Base, and from 84.62% (absolute 26.64%) to 28.24% (absolute 12.8%). Similarly, Table 7b shows that CONTRACLM outperforms encoder-decoder models and performs comparably to the encoder-only model, GraphCodeBERT. Gao et al. (2021) showed that the dropout-based augmentation is an effective strategy for unsupervised contrastive learning, and the follow-up works (Chuang et al., 2022;Wu et al., 2022) endorse the effectiveness.",
            "score": 0.5189923564610295,
            "section_title": "D.1 Bridge the Gap on Discriminative Tasks",
            "char_start_offset": 38682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1785
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 504,
                    "end": 523,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 542,
                    "end": 560,
                    "matchedPaperCorpusId": "221761146"
                },
                {
                    "start": 928,
                    "end": 948,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 994,
                    "end": 1014,
                    "matchedPaperCorpusId": "232185260"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16943359375
        },
        {
            "corpus_id": "269981935",
            "title": "Bitune: Bidirectional Instruction-Tuning",
            "text": "But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2020) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020;Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling.As there is simply more unlabeled data available for pretraining, training a decoder-only architecture on unlabeled data, and then finetuning it for tasks with instruction-tuning, is the best modus operandi of today (Wang et al., 2022).However, with this switch to decoder-only architectures, we lost bidirectional attention in the process.As we know this can improve feature representations for instructions, we set out to re-introduce bidirectional attention, such that it can be integrated into pretrained decoder-only LLMs.\n\nOur new method Bitune adds bidirectional attention to decoder-only architectures and combines it with causal attention to generate two sets of instruction features, using two different sets of weights.These features are then integrated, utilizing learnable mixing coefficients, and later used as the KV-cache for response generation.Notably, the autoregressive response generation process remains unaffected by the bidirectional attention and continues to be causal.",
            "score": 0.5176177796974206,
            "section_title": "Introduction",
            "char_start_offset": 1637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1520
                },
                {
                    "start": 1522,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1855
                },
                {
                    "start": 1855,
                    "end": 1988
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 269,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 293,
                    "end": 314,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 419,
                    "end": 439,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1209,
                    "end": 1228,
                    "matchedPaperCorpusId": "248118752"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162109375
        },
        {
            "corpus_id": "234482683",
            "title": "Paying Attention to Astronomical Transients: Photometric Classification with the Time-Series Transformer",
            "text": "Seminal work by Vaswani et al. (2017) introduced an architecture dubbed the transformer, which is constructed entirely around selfattention. They showed that state-of-the-art performance in neural machine translation can be achieved without the need for any CNN or RNN components; as they put simply \"attention is all you need\". Such was the impact of this work that there has since been an explosion of transformer variants as researchers strive to develop more efficient implementations and new applications (Tay et al. 2020). It is the original architecture by Vaswani et al. (2017) that inspired the architecture proposed in this article, and as such the remainder of this section focuses on describing the inner workings of this model. \n\nAs can be seen in Figure 2, the transformer consists of two sections: an encoder and a decoder. Within each encoder and decoder there exists a transformer-block, which contains the multi-head attention mechanism. In the context of neural machine translation, one could think of this set up as the encoder encoding a sentence in English, transforming the input into a certain representation, and the decoder taking this representation and performing the translation to French. To ensure the model only attends to words it has seen up to a certain point when decoding, an additional causal mask is applied to the input sentence. As an example, this may be the equivalent of only providing inputs   0 . . .   2 of an input sequence of say  = 5 but requiring the decoder to output predictions up to   =5 . We focus our discussion on the transformer block without this causal mask since it is this block that is most relevant when we come to classification tasks later in this chapter. Notwithstanding, there is scope for further study to investigate the usefulness of applying a causal mask to the input sequence for early light curve classification. This would present an architecture that does not require full phase light curve information for predictions. By applying a causal mask, one can build a classifier that can ingest partial light curves and still provide predictions. Then by varying the amount of masking (i.e. increasing or decreasing the amount of the light curve that is visible to the network) we can investigate the feasibility of early light curve classification.",
            "score": 0.517535568368663,
            "section_title": "The Rise of the Transformer",
            "char_start_offset": 13468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2323
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1551513671875
        },
        {
            "corpus_id": "247292621",
            "title": "Unfreeze with Care: Space-Efficient Fine-Tuning of Semantic Parsing Models",
            "text": "In the prefix tuning model used to obtain the main results, we prepend prefixes to all attention layers in both the encoder and the decoder. Here we explore two variants of this technique: 1) inserting these vectors in different locations; and 2) prepending prefixes only in the top 2 decoder layers. Different vector locations: Because BART's decoder is autoregressive and proceeds from left to right, prefixes are the only reasonable choice for where to add task-specific hidden vectors. However, encoder layers are bidirectional and symmetric, and therefore these vectors don't need to be prepended to the left of the attention layers. Accordingly, we investigate the effect of different location choices by: \u2022 appending these vectors to the right of the encoder layers as suffixes ( \u2020 Suffix); and by \u2022 splitting the prefix vectors into a prefix sequence and a suffix sequence ( \u2020 Prefix&Suffix). \n\nIn the \u2020 Prefix&Suffix model, the prefix vectors are split evenly with rounding up on the prefix side. For example, a \u2020 Prefix&Suffix 5 model includes a prefix of length 3 and a suffix of length 2. Table 4 shows that \u2020 Suffix models produce almost identical performance as the \u2020 Prefix models on average, while the \u2020 Prefix&Suffix model can slightly improve model performance by an average of 0.80 and 0.11 absolute points for prefix lengths of 30 and 5, respectively. This demonstrates that prefix tuning is not sensitive to prefix location. \n\nPrefix tuning only the top 2 decoder layers: Inspired by the finetuning of the top 2 decoder layers, we study Prefix top-2-layers , a prefix-tuning variant that only prepends prefixes to the top two decoder layers. An additional motivation behind this experiment is that even though prefix tuning can dramatically reduce the number of task-specific parameters, it cannot reduce training latency, since the gradients still need to back-propagate to the very first layer in the encoder. On the other hand, with prefixes in only the top 2 decoder layers, the backward propagation can be terminated immediately after the lowest prefix has been processed, which should make the training process significantly faster.",
            "score": 0.5175141560370243,
            "section_title": "Prefix Variants",
            "char_start_offset": 21527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1445
                },
                {
                    "start": 1448,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2159
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050811767578125
        },
        {
            "corpus_id": "269804069",
            "title": "On the Adaptation of Unlimiformer for Decoder-Only Transformers",
            "text": "In contrast to encoder-decoder models, decoderonly transformers use causal (unidirectional) attention.This difference means that a token has seen enough contextual information if a certain number of tokens are behind it.As a result, instead of only storing the hidden states of the middle half tokens, we can keep all the non-overlapping ones.This will allow us to be slightly more efficient when processing long documents.Note that only the first instance of overlapping tokens is added to the index, as illustrated by orange tokens in Figure 1.",
            "score": 0.5170091116153607,
            "section_title": "Chunks Encoding",
            "char_start_offset": 6884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 102,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 343
                },
                {
                    "start": 343,
                    "end": 423
                },
                {
                    "start": 423,
                    "end": 546
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "271903802",
            "title": "WPN: An Unlearning Method Based on N-pair Contrastive Learning in Language Models",
            "text": "In the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence. Empirically, we can take the text representation of the last token vS as the embedding of the entire text sequence. Alternatively, we can use the mean pooling method, averaging the vector representations of all tokens as the sequence embedding. The methods of last-token pooling and mean pooling can be respectively expressed as h lP = vS and hmP = 1 S S i=1 vi, where S is the sequence length, v = (v1, v2, ..., vS) is the hidden state vector of the last hidden layer of LMs after the input text data x, with a dimension of (SeqLen, dim). \n\nHowever, last token pooling discards the preceding tokens, resulting in a loss of semantic information. Mean pooling simply aggregates the vector representations of all tokens, a method nearly identical to how models with encoders obtain text representations. Therefore, to obtain a richer text vector representation from decoder-only LMs, we employ a position-weighted mean pooling method [23]. Specifically, the vector representation of x, hwP , is: \n\nwhere the definition of wi is as follows: \n\nThe position-weighted mean pooling method can give higher weights to subsequent tokens, which is consistent with the causal attention masking mechanism. We will show the comparative results of these three pooling methods in the Further Analysis section.",
            "score": 0.5167436872793989,
            "section_title": "Position-weighted Mean Pooling",
            "char_start_offset": 10862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1286
                },
                {
                    "start": 1289,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1542
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "275038234",
            "title": "A review of deep learning-based approaches to sign language processing",
            "text": "In recent years, LLMs have revolutionized deep learning and AI. With vast training datasets and extensive parameters, LLMs combine rich knowledge with advanced reasoning capabilities, enabling their application to an expanding range of tasks. LLMs can be broadly categorized into two types: BERT-style and GPT-style architectures [97]. BERT-style models include encoder-decoder and encoder-only architectures, while GPT-style models are decoder only. Although decoder-only models can be further divided into causal and prefix decoders [98], this paper treats them as a single category. A key distinction between these two types lies in attention mechanisms: BERT-style models can attend to all input tokens, whereas GPT-style models attend only to preceding  [71] 2023 mBART [99] Decoder-only LLM \n\nSign2GPT [18] 2024 XGLM [100] SignLLM [73] 2024 LLaMA [101] tokens, generating the next token in an autoregressive manner.",
            "score": 0.5166506436595019,
            "section_title": "LLM for sign language processing",
            "char_start_offset": 21639,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 796
                },
                {
                    "start": 799,
                    "end": 921
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 759,
                    "end": 763,
                    "matchedPaperCorpusId": "260202788"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1231689453125
        },
        {
            "corpus_id": "277626915",
            "title": "GOLLuM: Gaussian Process Optimized LLMs - Reframing LLM Finetuning through Bayesian Optimization",
            "text": "LLMs process textual inputs by converting them into dense vector representations through a sequence of tokenization , embedding and attention-based transformations. Tokenization involves the process of splitting the input text into subword units (tokens) using a model-specific vocabulary (e.g., SentencePiece 84 , Byte-Pair Encoding 85 ). The tokens are mapped to continuous vectors via learned embedding layers and passed through multiple self-attention layers that capture contextual relationships between tokens. \n\nLLMs can follow different architectural designs: encoder-only (e.g., BERT 2 ), decoder-only (e.g., Qwen 52 ), and encoder-decoder (e.g., T5 49 ). Encoder-based models process the full input bidirectionally and are suited for classification and regression . Decoder-only models generate text autoregressively with causal masking . Encoder-decoder models combine both components and are often used for sequence-to-sequence tasks . The architecture choices impact the structure and pooling strategies used to extract unified representations from the variable-length token sequences. \n\nPooling refers to the process of aggregating a sequence of token-level representations produced by a language model into a single fixed-dimensional embedding. Encoder-based models often use the hidden state corresponding to the special [CLS] token or apply mean-pooling across token embeddings. Decoder-only models typically use the final hidden state of the last non-padding token. For encoder-decoder models, pooling is applied over the encoder-side hidden states.",
            "score": 0.516211031376028,
            "section_title": "B.5 Large Language Models",
            "char_start_offset": 39674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 516
                },
                {
                    "start": 519,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1259
                },
                {
                    "start": 1260,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1567
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 336,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 659,
                    "end": 661,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2454833984375
        },
        {
            "corpus_id": "253080620",
            "title": "LittleBird: Efficient Faster & Longer Transformer for Question Answering",
            "text": "First, despite the efficiency of LittleBird and its excellent performance in question answering, it is still unknown whether LittleBird works well for other NLP tasks. Further research is needed on other tasks. Second, in the encoder-decoder architecture model that requires cross-attention, since position information is not injected into each token at all, it may be difficult for the decoder layer to find appropriate tokens of the encoder to attend. Lastly, causal masking cannot be applied to the pack and unpack attention due to its characteristics, which means that LittleBird cannot be used to the decoder-only autoregressive language model.",
            "score": 0.5159493146374999,
            "section_title": "Limitations",
            "char_start_offset": 25938,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1593017578125
        },
        {
            "corpus_id": "252917569",
            "title": "RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder",
            "text": "One limitation about the decoding process is that the training signals, i.e., the cross-entropy loss, can only be derived from the masked tokens. Besides, every masked token is always reconstructed based on the same context, i.e., H Xdec . We argue that the pre-training effect can be further enhanced providing that 1) more training signals can be derived from the input sentence, and 2) the reconstruction task can be performed based on diversified contexts. To this end, we propose the enhanced decoding inspired by two-stream self-attention (Yang et al., 2019) and position-specific attention mask (Dong et al., 2019). Particularly, we generate two input streams: H 1 (query) and H 2 (context), for the decoding operation (Figure 2. C): \n\nwhere h X is the sentence embedding, e x i is the token embedding (no token is masked in this place), and p i is the position embedding. We introduce the position-specific attention mask M \u2208 R L\u00d7L , where the self-attention is performed as: \n\n(5) \n\nThe output A, together with H 1 (because of the residual connection) are used to reconstruct the original input (other operations, like layer-norm and FFN, are omitted from our discussion). Finally, the following objective will be optimized: \n\nKnowing that the decoder only consists of one single transformer layer, each token x i is reconstructed based on the context which are visible to the i-th row of matrix M. In this place, the following rules are applied to generate the position specific attention mask matrix M: \n\nThe sampled tokens, s(X =i ), and the 1st position (except for the 1st row) will be visible when reconstructing x i . The diagonal elements, i.e., x i for the i-th row, will always be excluded, which means they will always be masked; as a result, each token cannot attend to itself during the reconstruction. We summarize the pre-training workflow with the enhanced decoding as Algorithm 1. Note that the original masked language modeling task in BERT is kept in encoder.",
            "score": 0.5159380396008673,
            "section_title": "Enhanced Decoding",
            "char_start_offset": 10995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 989
                },
                {
                    "start": 992,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 1987
                }
            ],
            "ref_mentions": [
                {
                    "start": 545,
                    "end": 564,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 602,
                    "end": 621,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.364501953125
        },
        {
            "corpus_id": "52168831",
            "title": "Attention-based Audio-Visual Fusion for Robust Automatic Speech Recognition",
            "text": "This subsection describes the key architectural contribution. Our premise is that conventional dual-attention mechanisms, such as the one in [19], overburden the decoder in Seq2seq architectures. In the uni-modal case, a typical decoder has to perform both language modelling and acoustic decoding. Adding another attention mechanism that attends to a second modality requires the decoder to also learn correlations among the input modalities. We aim to make the modelling of the audio-visual correlation more explicit, while completely separating it from the decoder. Thus, we move this task to the encoder side. Our strategy is to decouple one modality from the decoder and to introduce a supplementary attention mechanism to the top layer of the coupled modality that attends to the encoding memory of the decoupled modality. The decoder only receives the final state and memory of the coupled encoder's top layer like a standard uni-modal attention decoder. A diagram of the fusion strategy is shown in Figure 1.\n\nThe queries from Eq. (1, 2) come from the states of the top audio encoder layer, while the values represent the visual encoder memory. The acoustic encoder's top layer can no longer be considered to hold acoustic-only representations. They are fused audio-visual representations based on corresponding high level features from the two modalities matched via attention. This layer could also be viewed separately as a higher level encoder (red layer in Figure 1) operating on acoustic and visual hidden representations.\n\nThe following intuition motivates our choice. The top layers of stacked RNNs encode higher order features, which are easier to correlate than lower levels. They provide speech related abstractions from visual and acoustic features. In addition, any time one feature stream is corrupted by noise its encodings may be automatically corrected by correlated encodings of the other stream.\n\nThe audio and visual modalities can have two-way interactions. However, by design only the acoustic modality learns from the visual. This is because in clean speech, the acoustic modality is dominant and sufficient for recognition, while the visual one presents intrinsic ambiguities: the same mouth shape can explain multiple sounds. The design assumes that acoustic encodings can be partially corrected",
            "score": 0.5157460195923153,
            "section_title": "Audio-Visual fusion strategy",
            "char_start_offset": 8072,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 145,
                    "matchedPaperCorpusId": "1662180"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0765380859375
        },
        {
            "corpus_id": "227254536",
            "title": "Data-Informed Global Sparseness in Attention Mechanisms for Deep Neural Networks",
            "text": "We prune self-attention mechanisms in encoder-decoder models trained with softmax on the IWSLT14 translation tasks for p \u2208 {20, 40, 50, 60, 80} using either random or out-of-distribution attention masks, and we compare to the baseline results above. For the out-of-distribution experiments, we generate attention masks on IWSLT14 en-de. Tables 8 and  9 show the results. We note that data-informed masks outperform random pruning by a large Table 9: Results from the IWSLT14 en-de translation tasks. For simple attention patterns, such as those in self-attention mechanism for translation tasks, AP is robust under distributional shifts. \n\nmargin, especially as the percentage p increases. However, using attention masks gathered for the other translation direction does not meaningfully influence our results. We speculate that this is because we only look at self-attention patterns, which in the case of translation are very sharp and exhibit a constant context window.",
            "score": 0.5153965905831688,
            "section_title": "Machine Translation",
            "char_start_offset": 19720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 972
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0843505859375
        },
        {
            "corpus_id": "268264275",
            "title": "Where does In-context Translation Happen in Large Language Models",
            "text": "In-context learning differs from task-specific supervised learning in that, during test time, the desired task must be identified from the context first, then executed. At what stage in the feed-forward computation does a GPT-style model transition from an in-context learner to a translation model? To explore this question, we introduce layer-from context-masking which masks out all attention weights to the context (instructions or prompts) from a certain layer onwards (see Figure 1 for a graphical description). \n\nFor Causal Decoder-only Transformer Language Models, given each position i, the Attention weight \u03b1 ij over context positions j, j < i can be computed by a \n\n) is the dot product between a query vector and key vector q i \u2022 k j , where q i = W q x i , k j = W k x j for trained weight matrices W k and W q . 2 We apply the attention mask over the context so that the attention score is (q i \u2022k j )+m(j, u). Here u is the context that we wish to mask, and m(j, u) \n\nAll masks operate from the j-th layer (\u2113 j ) onwards, i.e. the task without seeing any instructions or examples in the context. \n\n2 Readers should note that there is a W k and Wq weight matrix for each layer and each attention head, but we omit the notation on this for readability. masking from \u2113 20 means causally masking out attention to all context positions from \u2113 20:n \u2113 , where n \u2113 is the total number of layers. To construct Fig 2, we increment \u2113 from 1 to n \u2113 and apply the set of masks {m(j, u)} \u2113:n \u2113 in each experiment and observe the performance of the model. \n\nUnder this causal masking treatment masking from layer \u2113, the model must rely on the representations of the target input sentence from layer \u2113 + 1 only to complete the task; if the target sentence representations do not already encode the target task (translation into a specific language) then the model will fail to generate translations. \n\nIn other words, the goal is to characterise where the model has \"located\" the task of translation.",
            "score": 0.5153385919411557,
            "section_title": "Layer-from Context Masking",
            "char_start_offset": 8862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 1999
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71142578125
        },
        {
            "corpus_id": "250311056",
            "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
            "text": "A causal model computes the output of t-th frame for l-th layer only depending on previous results from l-1-th layer. To save computation costs, it is necessary to cache the historical outputs Y l\u22121 t\u2212h , Y l\u22121 t\u2212h+1 , ..., Y l\u22121 t\u22121 when streaming decoding, and the output is \n\nFigure 1: Attention mask used in attention mechanism. Q and K denote the query and key matrix. Green means there is a dependency, while white means there is no dependency. \n\nwhere h is the number of history states, f is a mapping function, and other calculation (i.e., projection, feed-forward) is omitted. Non-causal models usually introduce information of future frames when computing the output of t-th frame, so the output is changed to \n\nwhere c is the number of future states participating in the computation. Since the causal model lacks future information Y l\u22121 t+1 , ..., Y l\u22121 t+c at every l-th layer, it performs much worse than the non-causal model. \n\nTo improve the causal model, we propose an encoder states revision strategy to add future information. The model outputs causally at first. Once the input frame reaches the index of the revision interval, the model revises encoder states based on previous information of revision steps and outputs a new hypothesis. The model then outputs causally again until the next index of the revision interval and keeps looping. History states before the index of revision step are only involved in the computation and will not be revised. This will add future information for frames before revision interval without too many forward computation costs. \n\nIn practical implementation, to ensure the accuracy of realtime outputs, we revise each interval before the first revision step utilizing all the inputs less than the step size. When the index of the first revision step reaches, all previous states are uniformly changed. Then the information on revision step size is used to revise the states every revision interval. \n\nSpecifically, we mainly focus on the Transformer encoder in this work. We plot an attention mask in Figure 1 for a more intuitive presentation of the revision strategy applied in the attention mechanism. The attention mask limits the range of query-key (Q-K) dot products for each frame in the attention mechanism.",
            "score": 0.5151670364688924,
            "section_title": "Encoder states revision strategy for causal model",
            "char_start_offset": 3552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 450
                },
                {
                    "start": 453,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 719
                },
                {
                    "start": 722,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 940
                },
                {
                    "start": 943,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1585
                },
                {
                    "start": 1588,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2273
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.322509765625
        },
        {
            "corpus_id": "277857043",
            "title": "Integrating Structural and Semantic Signals in Text-Attributed Graphs with BiGTex",
            "text": "Continuing from the architectural foundations of large language models, it is important to distinguish between two major classes of transformer-based designs: encoder-only and decoder-only models. Each follows a unique training paradigm and serves different purposes in natural language understanding or generation tasks. \n\nEncoder-only models, such as BERT [18], are designed to generate holistic representations of input sequences. These models take an entire sequence as input and process it bidirectionally-each token has access to the full left and right context during training. This characteristic allows the model to deeply capture semantic dependencies across the input. \n\nThe standard training objective for such models is Masked Language Modelling (MLM), where a random subset of tokens is replaced with a special [MASK] token. The model is then trained to reconstruct the masked tokens using the surrounding unmasked tokens. This approach encourages the network to develop contextualized embeddings grounded in full-sequence comprehension. The loss function for MLM is typically expressed as: \n\nwhere \\M denotes the set of masked positions, x \\M is the unmasked sequence, and \u03b8 represents model parameters. \n\nIn contrast, decoder-only models, such as GPT [19], operate unidirectionally. They are trained in an autoregressive fashion, where each token is generated based only on preceding tokens. The model has no access to future inputs during training or inference, enforcing a strict left-to-right dependency. This makes them particularly well-suited for generative tasks such as open-ended text generation or dialogue modelling. \n\nThe pretraining objective for decoder-only models is Causal Language Modelling (CLM), where the model learns to predict the next token in a sequence, given all previous ones. The associated loss is defined as: \n\nHere, each token x t is conditioned solely on the sequence x <t , making the model capable of generating coherent text step by step. \n\nThe distinction between encoder-only and decoder-only models is foundational: the former are optimized for understanding input context and are widely used in classification and embedding tasks, while the latter are tailored for sequential prediction and text generation.",
            "score": 0.515150662280603,
            "section_title": "Large language Models (LLMs) and Attention Mechanism",
            "char_start_offset": 11813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1855
                },
                {
                    "start": 1858,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 1267,
                    "end": 1271,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1512451171875
        },
        {
            "corpus_id": "257505035",
            "title": "The Life Cycle of Knowledge in Big Language Models: A Survey",
            "text": "Currently, pre-trained language models usually acquire various knowledge from pure text through self-supervised learning on a large-scale text corpus. In this section, we will first introduce several widely used learning objectives (Qiu et al., 2020), and then discuss the learning mechanisms behind them. \n\nCausal Language modeling aims to autoregressively predict the next token in the input sequence, which is the most popular pre-training tasks (Radford et al., 2019b;Brown et al., 2020;Ouyang et al., 2022;Scao et al., 2022) and has demonstrated excellent effectiveness in capturing context dependency and text generation paradigms. One limitation of causal language modeling is unidirectional, which can only capture contextual information from left to right. \n\nMasked Language Modeling aims to mask some tokens in the input randomly, and then predict the masked token conditioned on the rest of sequence (Devlin et al., 2019;Liu et al., 2019c). Unlike causal language modeling, which can only obtain information in a unidirectional manner, masked language modeling can capture contextual information from both left-to-right and right-to-left directions. \n\nSeq2seq Masked Language Modeling uses an encoder-decoder architecture for pre-training, which first feeds the encoder with masked sequence, and the decoder is supposed to predict the masked tokens autoregressively (Raffel et al., 2020;Song et al., 2019). \n\nDenoising Autoencoder first corrupts the input sequence with randomly mask symbols, then feed the input into a bidirectional encoder, and the likelihood of the whole original input is calculated with an auto-regressive decoder (Lewis et al., 2020a).",
            "score": 0.515001671359602,
            "section_title": "Learning from Text Data",
            "char_start_offset": 12176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1160
                },
                {
                    "start": 1163,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1669
                }
            ],
            "ref_mentions": [
                {
                    "start": 232,
                    "end": 250,
                    "matchedPaperCorpusId": "212747830"
                },
                {
                    "start": 449,
                    "end": 472,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 472,
                    "end": 491,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 491,
                    "end": 511,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 911,
                    "end": 932,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 932,
                    "end": 950,
                    "matchedPaperCorpusId": "198953378"
                },
                {
                    "start": 1377,
                    "end": 1398,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1398,
                    "end": 1416,
                    "matchedPaperCorpusId": "146808476"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07489013671875
        },
        {
            "corpus_id": "262088801",
            "title": "Attentive Multi-Layer Perceptron for Non-autoregressive Generation",
            "text": "Given a source sequence X 1:m , conditional sequence generation targets to predict a target sequence Y 1:n by modeling the conditional probability p(Y |X). \n\nAutoregressive generation decomposes the probability p(Y |X) as: \n\nwhich is implemented as a typical encoder-decoder architecture shown in Fig. 1a. Although such decomposition is proved effective, it suffers from two main drawbacks: efficiency and exposure bias. On the one hand, the autoregressive decoding process, where each token depends on the previous predicted ones, prevents the model from fast inference in usage. On the other hand, teacher-forcing exposes ground truth tokens in network inputs during the training process, where the exposed tokens are unable to observe in inference. Such exposure creates an inconsistency between the training and inference, and harms the prediction quality. \n\nRecently, non-autoregressive generation, depicted as Fig. 1b, shows its capability of sequence modeling in terms of both efficiency and efficacy, which decomposes the conditional probability p(Y |X) via a Na\u00efve Bayes assumption: \n\nThe NAR decomposition enables parallel decoding for each token, and speeds up the inference process substantially. Although NAR generation is much faster than AR generation, its speed is still limited by the O n 2 + nm + m 2 time complexity of the multi-head softmax attention module. This is especially problematic in modeling long sequences. \n\nAttention Types in AR & NAR Models Although autoregressive and nonautoregressive models differ from each other in sequence generation paradigms, their underlying attention mechanisms in their architectures are also different. \n\nThe token-by-token generation of AR models requires a causal decoder that forces tokens to attend to only previous features. A typical causal decoder utilizes causal softmax attentions both in self-attention and cross-attention. The attention causality entails that during the computation, it is important to ensure that the query token does not attend to the context on its right side, just as the shaded blocks in Fig. 1a. In contrast, the NAR model, which allows for parallel generation of the output sequence and global contextualization using attention, employs a noncausal decoder in Fig. 1b.",
            "score": 0.5149145187279704,
            "section_title": "Background: Autoregressive and Non-Autoregressive Generation",
            "char_start_offset": 3179,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 1091
                },
                {
                    "start": 1094,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1665
                },
                {
                    "start": 1668,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2092
                },
                {
                    "start": 2093,
                    "end": 2266
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2939453125
        },
        {
            "corpus_id": "224818566",
            "title": "Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model",
            "text": "Recently, pre-trained language model (Peters et al., 2018;Sun et al., 2019;Zhang et al., 2019;Dong et al., 2019) are widely applied to various NLP tasks including dialogue generation. Some approaches (Wolf et al., 2019;Lin et al., 2019) utilize a decoder-only transformer initialized with GPT (Generative Pre-Training) parameters (Radford et al., 2018). This decoder-only architecture encodes dialogue history using only left-to-right attention (i.e. attention is allowed only to previous positions), which does not allow exploit the full context in a sentence. Using bi-directional attention could enable the encoder to leverage richer context information in dialogue history. Zheng et al. (2019) utilizes an encoder-decoder transformer architecture (Vaswani et al., 2017) with a clear separation between encoder and decoder. This makes it more difficult for the fine-tuning process to update the encoder's parameters, as has been shown in some previous work on abstractive summarization (Liu et al., 2018). In contrast, we use BERT (Devlin et al., 2018) for both encoder and decoder. Similar approaches have been used in non-conditioned dialogue (e.g. Bao et al. (2020)). To cope with the difference between encoder and decoder, we use different input representations and self-attention masks, which will be described in detail.",
            "score": 0.5147642305835878,
            "section_title": "Pre-training Based Transformer",
            "char_start_offset": 8020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1330
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 58,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 94,
                    "end": 112,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 751,
                    "end": 773,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12286376953125
        },
        {
            "corpus_id": "249974508",
            "title": "Controllable protein design with language models",
            "text": "Transformers are a current revolution in NLP. Their success derives from the evolution of a series of concepts built on top of each other, with the attention mechanism possibly being the most notable of these advances. The attention mechanism originated as a solution to traditional sequence-to-sequence (seq2seq) models, which are widely used for tasks that process sequences from one domain to another, such as machine translation or text summarization. In seq2seq models, the input is stepwise-processed in a module termed encoder to produce a context vector passed to a decoder, which is responsible for generating an output (Fig. 3a). Traditionally, encoder and decoder architectures have usually been RNNs or LSTMs (Fig. 2b), and the context vector corresponded to the final output of the last encoder step (word) (Fig. 2b). Due to this inherently sequential nature, RNNs presented the major drawback of degrading performance and increasing training times with sequence length 15 . The introduction of the attention mechanism provided a solution to this problem by allowing the decoder to analyse the whole input and focus on specific parts of it, a notion similar to attention in the human mind. A simplified example in English-French translation would be focusing on the input word 'home' , when outputting the word 'maison' (Fig. 3a).\n\nAlthough attention mechanisms had been ubiquitously applied in many types of neural networks, they became particularly prominent in 2017, when researchers at Google published their seminal work ' Attention is all you need' , which introduced an architecture that not only applied attention between the modules but also throughout them 39 . This new design permitted the encoder to focus on specific parts of the input sequence, producing a much better performance in many tasks 39 . The model was termed the Transformer and gave its name to all similar architectures that followed in subsequent years (Fig. 2a).\n\nThe transformer's encoder and decoder modules contain a stack of six submodules or layers (N) that process inputs from the previous layer in a parallel fashion, enabling much faster training times (Fig. 3b). The encoder submodules contain two layers: a self-attention layer, which applies the attention mechanism to the input sequence itself,",
            "score": 0.5142706585379718,
            "section_title": "attention mechanism and transformers",
            "char_start_offset": 16963,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 983,
                    "end": 985,
                    "matchedPaperCorpusId": "3397190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10504150390625
        },
        {
            "corpus_id": "277128409",
            "title": "A review on the applications of Transformer-based language models for nucleotide sequence analysis",
            "text": "The functioning of the Decoder has many parts in common with the Encoder, yet there are some major differences. In a nutshell, the Decoder processes the output component of the input-output pairs of the training data; encodes such information and produces its own embedding using a multi-head attention scheme similar to the Encoder, but: \n\n1. once the output has been encoded, it is combined with the  and  matrices coming from the Encoder. This is the step where Transformers learn the relation between the input of the training (e.g., the question) and the output (e.g., the answer to the question); considering the question-answer example, the Decoder performs cross-attention on the Encoder output (which represents the question) while processing the answer; 2. at the end of the process a linear layer has a number of output neurons equal to the size of the vocabulary; such network uses a softmax function to produce likelihood for each term in the vocabulary. Then, the term with the highest likelihood is the output of the Decoder i.e. the predicted next word; 3. The Decoder produces the output one word at a time; in the training phase, knowing the correct word, the error is computed and used to drive the backpropagation step and the correction of the weights to reduce the error. \n\nThe steps as mentioned above can be visualised in Fig. 1(c). To fully appreciate the subtleties of the Encoder-Decoder mechanism, one should be aware that Encoder and Decoder are trained simultaneously, both in pretraining and in finetuning. Both strategies depend on the model type. For example, the weights obtained in pretraining for Encoder-only model like BERT, are based on randomly masked words; on the other hand, for Decoder-only model like GPT, causal masking is used, where only the future tokens are masked, and the model predicts the next token given past tokens. In finetuning, the pretrained weights are refined when input-output training pairs are presented to the system.",
            "score": 0.5142545605186677,
            "section_title": "Encoding and decoding",
            "char_start_offset": 16803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 1984
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.333251953125
        },
        {
            "corpus_id": "244715077",
            "title": "Question Answering for Complex Electronic Health Records Database using Unified Encoder-Decoder Architecture",
            "text": "We initialize the encoder-as-decoder model with BERT-base (Devlin et al., 2019) and train our model with the two training objectives: 1) Masked Language Modeling (MLM) is used for reconstructing the input masks applied to the NLQ part; 2) Sequenceto-Sequence (Seq2Seq) is used to train our model to act as a decoder. The difference between MLM and Seq2Seq is the structure of the attention masks. As shown in Fig. 2, the NLQ tokens can freely attend to one another but not to the query tokens. The query tokens, on the other hand, can only attend to the previous query tokens but freely attend to all NLQ tokens. By jointly optimizing with two aforementioned training objectives, the model should recover for masked NLQ tokens while also predicting the corresponding query tokens.",
            "score": 0.5141809825453079,
            "section_title": "Model Training",
            "char_start_offset": 10646,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 780
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 79,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.431640625
        },
        {
            "corpus_id": "245769906",
            "title": "Compact Bidirectional Transformer for Image Captioning",
            "text": "To make use of order information, position encodings (Vaswani et al. 2017) are added to word embedding features. It is worth emphasizing that the L2R and R2L flows in the decoder run in parallel and the bidirectional interactive only optionally happens in the mask multi-head bidirectional interactive attention sublayer. Specifically, the decoder consists of L identical layers and each layer has three sublayers: a masked multi-head bidirectional interactive attention sublayer, a multi-head cross-attention sublayer and a position-wise feed-forward sublayer. Residual connection and layer normalization are also applied after each sublayer. The multi-head cross-attention is similar to the multihead self-attention mentioned above except that the key and value are now contextual region features and the query is the output of its previous sublayer. The masked multi-head bidirectional interactive attention sublayer can be seen as an extension of the masked multi-head attention sublayer in the original Transformer decoder (Vaswani et al. 2017). The main difference between them lies in the scaled dot-product attention module. An intuitional illustration is shown in the right of Figure 2. Formally, the scaled dot-product attention for each head i in the original masked multi-head attention  module is extended to: \n\nis the conventional one to capture past context and \u2212 \u2192 H f uture i is the extended part to capture future context by using query in the L2R direction and key/value in R2L direction. \n\n\u2212 \u2192 H i is the bidirectional-aware state of L2R direction by non-linearly fusing past and future context and AF denotes activation function, such as Relu or Tanh. It is worth noting that the self-attention in the decoder usually is equipped with a lower triangular matrix mask for preventing positions from attending to subsequent positions and we omit it here for brevity. And the bidirectional-aware state of R2L direction \u2190 \u2212 H i can be symmetrically computed as: \n\nIn particular, the masked multi-head bidirectional interactive attention sublayer degrades to the original masked multi-head attention sublayer when \u03bb = 0 .",
            "score": 0.514131273809686,
            "section_title": "CBTIC Model",
            "char_start_offset": 11900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 53,
                    "end": 73,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1028,
                    "end": 1049,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09466552734375
        },
        {
            "corpus_id": "273025546",
            "title": "ENTP: Encoder-only Next Token Prediction",
            "text": "Because an encoder has to compute O(n 2 ) attention scores from scratch for each token, it takes O(n 2 ) time to generate each token. Thus, it takes O(n 3 ) time to generate the entire sequence. We could generate tokens similarly using a decoder. However, due to causality, we can cache the previous keys and values (Kwon et al., 2023), allowing token generation in O(n) time and sequence generation in O(n 2 ) time. Given the gap in time complexity between encoders and decoders over an entire sequence (O(n 3 ) vs. O(n 2 )), this suggests that any causal function that can be expressed by an encoder requiring \u03c9(n 2 ) time cannot be efficiently expressed by a decoder. \n\nSpace Complexity Comparison. Both encoder-only and decoder-decoder use O(n) space complexity to generate an entire sequence. Although the standard implementation of attention uses O(n 2 ) space, attention can be implemented using only O(n) space. For details of algorithmic implementations of attention using O(n) memory, refer to Algorithm 3 in the appendix. Thus, we need a more detailed approach to find a difference between the space complexity of the two models. \n\nTowards this end, we classify memory used for the computation over the current token as either precomputed or additional. Precomputed memory stores values from computation over past tokens. \n\nValues stored in precomputed memory persist, and are used for computation over current and future tokens, e.g. the keys and values of previous tokens for a decoder. Additional memory stores values that depend on the current token, e.g. keys and values of the current token. \n\nWhen generating the nth token, a decoder uses O(n) precomputed memory to store keys and values of previous tokens and O(1) additional memory to compute results over the current token. An encoder computes everything from scratch for each token, so it uses O(n) additional memory and no precomputed memory. Under this view, there is a space complexity gap between encoder and decoder.",
            "score": 0.5139865897919066,
            "section_title": "Time Complexity Comparison.",
            "char_start_offset": 14494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 670
                },
                {
                    "start": 673,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 316,
                    "end": 335,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1568603515625
        },
        {
            "corpus_id": "248118752",
            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
            "text": "An important step in building LLMs is pretraining, where the model is trained on a large, unlabeled dataset via self-supervision. The choice of pretraining objective can have significant impact on the downstream usability of the LLM, and we therefore include objective choice as a factor in our empirical study. Figure 3 outlines the input and target tokens for the pretraining objectives considered. \n\nLanguage modeling. Since the advent of GPT-2 [Radford et al., 2019], large decoder-only models have generally been pretrained with an autoregressive language modeling objective [Brown et al., 2020, Wu et al., 2021, Rae et al., 2021]. Given previous tokens, the model is tasked with predicting the following one. We refer to this as full language modeling (FLM) . This objective is particularly efficient during pretraining: all tokens in a sequence can generate a loss signal in parallel. At inference time, the model is iteratively asked to predict the next token. \n\nPrefix language modeling. For encoder-decoder and non-causal decoder-only models to perform language modeling, one can define a prefix where the attention mask is allowed to be non-causal. Similar to standard language modeling, the model is tasked to predict each token outside the prefix given all previous tokens. We hereafter refer to this objective as prefix language modeling (PLM) . Loss on the prefix is ignored as tokens in the prefix can attend to their targets. For inference, the prefix is naturally the input text; during pretraining, it is usually chosen at random for each sample. \n\nMasked language modeling. Encoder-only models, such as BERT [Devlin et al., 2018], have typically been pretrained with a masked language modeling objective. Tokens or spans of tokens in the input text are replaced with a special mask token and the model is trained to predict the missing tokens. Raffel et al. [2020] introduced a version of this objective adapted to text-to-text models in the form of span corruption: sentinel tokens are used to flag masked spans of short random lengths, and, after processing the masked input, the model outputs the sentinels followed by their respective predicted content.",
            "score": 0.5138201757462337,
            "section_title": "Pretraining objectives",
            "char_start_offset": 14465,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 968
                },
                {
                    "start": 971,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1565
                },
                {
                    "start": 1568,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 448,
                    "end": 470,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "270391178",
            "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
            "text": "In autoregressive acoustic modeling, issues such as word skipping, repetition, and attention collapse often occur due to inaccuracies in the attention alignments learned within the encoder-decoder framework.To alleviate this problem, considering some properties of the alignments between text and waveform sequence, applying monotonic mechanism to enhance attention has demonstrated to be particularly effective to strictly preserve monotonicity and locality [He et al., 2019].Considering the alignments between text and speech are depending on their positions, numerous TTS models have incorporated location-based attention mechanisms to exploit the positional information for more accurate alignment [Sotelo et al., 2017, Vasquez and Lewis, 2019, Battenberg et al., 2020].\n\nFor monotonic attention, it leverages the prior that the alignments between text and speech are monotonic [He et al., 2019, Raffel et al., 2017].In each decoding step, the attention alignment position moves forward at most one step, so there is no situation where any input unit is skipped or repeated.While such methods have enhanced stability by introducing linear alignment, the constraints of hard attention significantly hinders the encapsulation of global contextual information, where the model only focuses on one token at a time.Therefore, models in [Tachibana et al., 2018, Chiu andRaffel, 2017] restrict the attention on the source sequence into a sliding window, replacing hard attention.Furthermore, Chen et al. [Chen et al., 2020] introduce an innovative approach to penalize off-diagonal attention weights by constructing a band mask that promotes the concentration of attention weights within a diagonal band, thus enhancing alignment fidelity while maintaining contextual awareness.Although these methods have effectively improved the robustness of autoregressive TTS systems, they are all based on the encoder-decoder architecture and are not compatible with the current popular decoder-only architecture.",
            "score": 0.513416963005435,
            "section_title": "Monotonic Alignment",
            "char_start_offset": 8070,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 774
                },
                {
                    "start": 776,
                    "end": 921
                },
                {
                    "start": 921,
                    "end": 1078
                },
                {
                    "start": 1078,
                    "end": 1314
                },
                {
                    "start": 1314,
                    "end": 1476
                },
                {
                    "start": 1476,
                    "end": 1775
                },
                {
                    "start": 1775,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 747,
                    "end": 773,
                    "matchedPaperCorpusId": "204837879"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0751953125
        },
        {
            "corpus_id": "214802239",
            "title": "CG-BERT: Conditional Text Generation with BERT for Generalized Few-shot Intent Detection",
            "text": "The proposed attention mask matrix for the transformer blocks in the decoder is shown in the upper right corner of Figure 2. To keep the transformers in the encoder having the same structure, we apply the bidirectional self-attention mask for the encoder as shown in the lower right corner of Figure 2. The bidirectional self-attention mask allows all the tokens in S 1 and S 2 to attend to all the tokens. \n\nIn the attention mask for the decoder, the tokens in the first intent sentence S 1 can only attend to all the tokens in S 1 , while the tokens in the second sentence S 2 can attend to both S 1 and all the left tokens in S 2 . With this attention matrix, we are able to control the decoder to generate the output from left to right conditioned on the given intent. \n\nA special token here is the first token which holds the latent variable z, it is only allowed to attend to itself. This is due to the vanishing latent variable problem (Zhao et al., 2017) when adapting VAE/CVAE to natural language generators. Previous works (Zhao et al., 2017) reported the problem that the LSTM decoder tends to ignore the latent variable. We met the same issue when applying the variational autoencoder with transformers. To alleviate this problem, we only allow the latent variable z to attend to itself to avoid it been overwhelmed by the information of other tokens. \n\nThe output of the 12-th transformer block in the decoder is represented as \n\nwhere h 12 1 is the embeddings for the latent variable z. To further increase the impact of z and alleviate the vanishing latent variable problem, we concatenate the embedings of z with all the tokens: \n\n. Two fully-connected layers with a layer normalization (Ba et al., 2016) are applied to get the final representation: \n\nwhere (Hendrycks and Gimpel, 2016) activation function and g is for layer normalization. The embeddings in H f at position at position t is used to predict the next token at position t + 1:",
            "score": 0.5133975394302429,
            "section_title": "The Decoder",
            "char_start_offset": 11468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 1957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.175537109375
        },
        {
            "corpus_id": "257037143",
            "title": "A Survey of Malware Detection Using Deep Learning",
            "text": "Attention mechanism [70] has achieved high performance in sequential learning applications such as machine translation [71], image recognition [72], text summarization [73], and text classification [74]. Attention mechanism was designed to improve the performance of the encoder-decoder machine translation approach [75]. The encoder and decoder are usually many stacked RNN layers such as LSTM as shown in Fig. 12. The encoder converts the text into a fixedlength vector while the decoder generates the translation text from this vector. The sequence { 1 ,  2 , ...,   } can either be a representation of text or image as shown in Fig. 13. In case of sequences, Recurrent Neural Networks (RNNs) can take two sequences with the same or arbitrary lengths. In Fig. 14, the encoder creates a compressed representation called context vector of the input, while the decoder gets the context vector to generate the output sequence. In this approach, the network is incapable of remembering dependencies in long sentences. This is because the context vector needs to handle potentially long sentences, and a shoot overall representation does not have the especially to store many potential dependencies. \n\nAttention in encoder-decoder: Bahdanau et al. [76] proposed an encoder-decoder attention mechanism framework for machine translation. A single fixed context vector is created by an RNN by encoding the input sequence. Rather than using just the fixed vector, we can also use each state of the encoder along with the current decoder state to generate a dynamic context vector. There are two benefits; the first benefit is encoding information contained in a sequence of vectors not just in one single context vector. The second benefit is to choose a subset of these vectors adaptively while decoding the translation. An attention mechanism is another Lego block that can be used in any deep learning model. Vaswani et al. [77] showed that an attention mechanism is apparently the only Lego block one needs. It improved the performance of a language translation model by dynamically choosing important parts of the input sequence that matter at a certain point in the output sequence. We can 10 entirely replace traditional Recurrent Neural Network (RRN) blocks by an attention mechanism block.",
            "score": 0.5132016812150711,
            "section_title": "Sequence to Sequence Neural Models",
            "char_start_offset": 28739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2291
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "244580517"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "233198441"
                },
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "238773712"
                },
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "233562906"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "236460116"
                },
                {
                    "start": 1920,
                    "end": 1924,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07879638671875
        },
        {
            "corpus_id": "235590121",
            "title": "AlertNet: Deep convolutional-recurrent neural network model for driving alertness detection",
            "text": "The encoded sequence of every epoch is further used to get the target sequence using attention network, which is a decoder part of the network. The decoder is also built using LSTMs. In the standard decoder, for every sequence of inputs, the decoder generates the new representation of the input sequence along with a target input element. The last input coming to the decoder is the last effect to update for the hidden state of encoder. Thus, the model has to be biased according to the last element. So, the use of attention mechanism in the model can address such a problem. The attention network learns different portions of the output sequence of encoder for each decoding step along with considering the entire encoder Without the attention mechanism, the decoder operation relies on the hidden vector of the decoder's Bi-LSTM. The sequence to sequence model including attention mechanism is more effective as it includes both encoder's representation and decoder with hidden vector calling the context or attention vector, represented as (ct). Attention weights are computed as a function of f (.), before computing attention vector (ct). The context or attention vector (ct) is probabilities (\u03b1i), relating to the significance of each hidden state, multiplied by a hidden state (  ). \n\nwhere \u03b1i is the significant of part i of the hidden state. The f(.), is a combination of the encoder's hidden state (  ), and decoder's hidden state (\u210e \u22121 ), with the \u210e layer followed by. Later, f(.), is given to the softmax module to calculate \u03b1i for n parts. Then the computation of   , is performed by the attention module, which is a weighted sum of all (  ) and \u221d  vectors. Hence, while decoding, the model can only consider the important regions of the input vector sequence.",
            "score": 0.513161636575568,
            "section_title": "The attention network",
            "char_start_offset": 13603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1292
                },
                {
                    "start": 1295,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1555
                },
                {
                    "start": 1556,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1776
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.083740234375
        },
        {
            "corpus_id": "268537007",
            "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
            "text": "Improving the efficiency of transformer decoding hinges on minimizing memory access and reducing redundant computations via two primary ideas, increasing operational intensity and the reuse of key-value tensors. While most current research focuses on decoder-only models, our work emphasizes encoder-decoder models, as they offer superior performance in specialized domains. \n\nAttention optimizations. Increasing operational intensity can be achieved through hardwarefriendly attention functions or model architectures, which increase the number of operational operations per memory access. FlashAttention [5] and FlashInfer [42] use kernel-level optimization to fuse the attention mechanism into a single kernel function. Multi-query attention [30] and grouped-query attention [1] modify transformer architectures to employ a single (or fewer) key-value attention head for multiple query heads to reduce the memory access. \n\nKey-value tensor caching. Another method to enhance decoding efficiency is the reuse of key-value tensors of shared common prefixes. This technique reduces redundant computations for subsequent requests with the same shared prefix. Methods like paged attention [17] and radix attention [49] mitigate the redundant storage of overlapping key-value cache. While paged attention and radix attention address the memory fragmentation issue and enable memory reuse for shared prefix, they remain less than optimally efficient because their implementation lacks compute-level memory optimization. Consequently, the key-value cache of the shared prefixes still needs to be loaded multiple times during computation. \n\nAlthough most aforementioned methods focus on decoder-only models, our method is compatible with these kernel-level efficiency techniques, e.g., FlashAttention2 and paged attention3 , in principle leading to further efficiency gains when used in concert. \n\nHydragen [15] is the most closely related concurrent research to our work. Both our method and Hydragen aim to increase operational intensity by sharing key-value tensors for shared prefixes. However, Hydragen mainly focuses on decoder-only models and emphasizes speed-up metrics in question answering. In contrast, we focus on smaller encoder-decoder models, which can outperform larger decoder-only models in specialized in-domain tasks.",
            "score": 0.5131235423070986,
            "section_title": "Related Work",
            "char_start_offset": 22884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1889
                },
                {
                    "start": 1892,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2083
                },
                {
                    "start": 2084,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 606,
                    "end": 609,
                    "matchedPaperCorpusId": "249151871"
                },
                {
                    "start": 1187,
                    "end": 1191,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2000732421875
        },
        {
            "corpus_id": "258556908",
            "title": "Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens",
            "text": "To reduce the complexity of implementations, the method is proposed for the encoder module of the Transformer that assumes full access to the entire sequence. The proposed compression might be extended to approximate the computation in the decoder, but it requires more implementation efforts, so we leave it for the future work. We briefly describe two possible options to do so. (1) We can use the input tokens of the decoder as VIP-tokens to compress the representations of context sequence generated by the encoder before Cross Attention computation to reduce the cost of Cross Attention. \n\n(2) Auto-regressive decoding operates using Causal Attention at each step. This Causal Attention operation requires memory and computation that is linear in the length of the prefix. We can keep the same Causal Attention VIP-token (the representation of the token currently being generated) and apply our method to compress the representations of the previously generated tokens. This reduces the linear complexity of the Causal Attention operation to sublinear. This is useful for reducing the cost of inference. For training, we can break the sequence into two segments: prefix segment and decoding segment. Then, we can use the proposed compression in prefix segment and vanilla computation in decoding segment. To prevent look ahead to the future tokens, we might only use the first token in the decoding segment as VIP-token.",
            "score": 0.5128639845290817,
            "section_title": "Potential Application to Decoders",
            "char_start_offset": 46628,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1108
                },
                {
                    "start": 1109,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1425
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0982666015625
        },
        {
            "corpus_id": "250311056",
            "title": "Improving Streaming End-to-End ASR on Transformer-based Causal Models with Encoder States Revision Strategies",
            "text": "However, this work only involves the decoder part without any modification to the encoder, and the mask-predict operation must be conducted after the entire block has been calculated. In [23], authors propose to use dynamic latency and revise the state of encoder and decoder so that both streaming and non-streaming models can apply incremental decoding [24]. This method revises the states that have not been completed for RNN-T models and can not change the previous information of the encoder either. In practical applications, to avoid distracting the user's attention, it is more desirable to ensure a fast response and previous stable outputs simultaneously [25]. The above methods will significantly change the previous outputs when the encoder states are inaccurate, which can not be used to improve causal models. \n\nIn this paper, we explore a new method to improve causal models, called encoder states revision strategy. Specifically, we first let the model calculate causally until the revision interval to revise the previous states and correct the decoding path. By applying revision, later outputs will be more accurate, and there is no need to change the hypothesis after all final outputs. Furthermore, we design a CTC spike position alignment decoding algorithm to reduce the computation costs, which is applicable to any revision methods that change the decoding path. Our experiments are based on the wav2vec2.0 [26] fine-tuning models using CTC and conducted on Librispeech benchmark [27]. Our best method can achieve 3.7/9.2 WERs which brings tremendous improvement for causal models and can be compared with chunk-based and knowledge distillation methods (23% and 6% relative reduction).",
            "score": 0.512564454068999,
            "section_title": "Introduction",
            "char_start_offset": 1770,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1710
                }
            ],
            "ref_mentions": [
                {
                    "start": 1432,
                    "end": 1436,
                    "matchedPaperCorpusId": "219966759"
                },
                {
                    "start": 1505,
                    "end": 1509,
                    "matchedPaperCorpusId": "2191379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034759521484375
        },
        {
            "corpus_id": "268723594",
            "title": "RankMamba: Benchmarking Mamba's Document Ranking Performance in the Era of Transformers",
            "text": "We benchmark the performance of different models on three sizes: models with \u2248 110M parameters (e.g.bert-base-uncased); models with \u2248 330M parameters (e.g.bert-large-uncased) and models with > 700M parameters (e.g.T5-large and state-spaces/mamba-790m).\n\nFor encoder-only models, we choose BERT [Dev+19] and RoBERTa [Liu+19]; for decoder-only models, we include Pythia [Bid+23], OPT [Zha+22] and Mamba [GD23], and we opt for T5 [Raf+20] as the representative encoder-decoder models.The compared models are from different model families, varying in terms of pretraining objectives (e.g.causal language modeling, masked language modeling, denoising, next sentence prediction), information direction (bidirectional vs unidirectional), model structure (Attention vs Mamba) and Positional Encoding (no positional encoding, learned positional encoding, relative positional encoding and rotary positional encoding [Su+24]).An important note is that we do not include instruction-finetuned language models such as Flan-T5 models [Chu+22] for a fair comparison to other language model families.For a review of these models, refer to Table 1.",
            "score": 0.5125388924048746,
            "section_title": "Backbone Language Models",
            "char_start_offset": 10474,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 100,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 214
                },
                {
                    "start": 214,
                    "end": 252
                },
                {
                    "start": 254,
                    "end": 481
                },
                {
                    "start": 481,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 915
                },
                {
                    "start": 915,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1131
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 376,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 427,
                    "end": 435,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 906,
                    "end": 913,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021087646484375
        },
        {
            "corpus_id": "237940640",
            "title": "Time Series Analysis and Modeling to Forecast: a Survey",
            "text": "In the self-attention mechanism of a decoder layer, each position in the decoder only attends to positions in the decoder up to and including this position. Values corresponding to subsequent forbidden positions are masked out in the input of the softmax function.\n\nTogether with look-ahead masking, one-position shifting between the decoder input and target output (decoder output) ensures that the prediction for a given position only depends on the known outputs at positions strictly less than this position. Thus can be preserved the autoregressive property.\n\nGiven a time series containing N data points x t\u2212N +1 , ..., x t\u22121 , x t , H-step ahead prediction is formulated as a supervised machine learning task: the input of the model is x t\u2212N +1 , ..., x t\u2212H , and the output is x t\u2212H+1 , x t\u2212H+2 , ..., x t . During model training, one-position shifting between the decoder input and the target output (decoder output) prevents learning the decoder to merely copy its input, and contribute to ensure that the prediction for a given position only depends on the known outputs at positions strictly less than this position. This shifting mechanism is combined with look-ahead masking that restrains attention to datapoints in the past: in the self-attention mechanism of a decoder layer, each position in the decoder only attends to positions in the decoder up to and including this position. Values corresponding to subsequent forbidden positions are masked out. In the training phase, the first neural network in an encoder layer reads the input sequence one time step at a time.\n\nTransformer models have been applied in such various domains as influenza-like illness forecasting (Wu et al., 2020) and prediction of vehicle trajectories (Park et al., 2018). To guess the future trajectory of surrounding vehicles in real time, the latter work relies on the following architecture: lstm-based encoders analyze the patterns underlying the past trajectory data, whereas lstm-based decoders predict the future trajectories.\n\nThe point-wise dot-product self-attention in canonical transformers is agnostic of local surrounding context. This may entail confusion regarding whether an observed point is an anomaly, a",
            "score": 0.5123324529131983,
            "section_title": "Forecasting",
            "char_start_offset": 144939,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1744,
                    "end": 1763,
                    "matchedPaperCorpusId": "3653452"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.304931640625
        },
        {
            "corpus_id": "258049081",
            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
            "text": "We compare the PALM structure against the following models. \n\nED is the traditional Encoder-Decoder framework built on Transformer (Vaswani et al., 2017) implemented with fairseq (Ott et al., 2019). \n\nLM is the traditional Language Model built with a Transformer decoder. It concatenates the source sequence and the target sequence to train an LM. When testing, it predicts the target sequence after inputting the source sequence into the LM. \n\nLM-SPE adopts the Separate Positional Encoding (Conneau and Lample, 2019;He et al., 2018) in an LM as the positional encoding. The first position for the target sequence is always set to 1. \n\nLM-LE uses a Language Embedding (Conneau and Lample, 2019;Conneau et al., 2020) to help LM differentiate source and target sequence. \n\nLM-PA adopts our proposed Partial Attention component in an LM. Different from PALM, it uses unidirectional attention and does not use the LE and SPE components. \n\nPreLM uses a Prefix Language Model (Liu et al., 2018;Dong et al., 2019;Raffel et al., 2020) to generate target sequences. Different from LM, it uses a fully-visible masking of the input source instead of using a unidirectional causal mask. The mask enables each source word depending on the whole input sequence instead of the previous one. Table 3: Early stop effect. \u2206L is calculated by length of PALM minus length of LM.",
            "score": 0.5120673646427449,
            "section_title": "Comparison Models",
            "char_start_offset": 25678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 62,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 769
                },
                {
                    "start": 772,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1359
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 153,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 179,
                    "end": 197,
                    "matchedPaperCorpusId": "91184134"
                },
                {
                    "start": 492,
                    "end": 518,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 518,
                    "end": 534,
                    "matchedPaperCorpusId": "54088698"
                },
                {
                    "start": 669,
                    "end": 695,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 695,
                    "end": 716,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 989,
                    "end": 1007,
                    "matchedPaperCorpusId": "147704286"
                },
                {
                    "start": 1007,
                    "end": 1027,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "232105052",
            "title": "Random Feature Attention",
            "text": "RFA builds on an unbiased estimate to exp( \u2022 , \u2022 ) from Theorem 1, which we begin with: \n\nThe last line does not have any nonlinear interaction between \u03c6(x) and \u03c6(y), allowing for a linear time/space approximation to attention. For clarity we assume the query and keys are unit vectors.2 \n\n= RFA (q t , {k i }, {v i }) . \n\n(5) \n\n\u2297 denotes the outer product between vectors, and \u03c3 2 corresponds to the temperature term \u03c4 in Eq. 1. \n\nRFA can be used as a drop-in-replacement for softmax-attention. \n\n(a) The input is revealed in full to cross attention and encoder self-attention. Here RFA calculates attention using Eq. 5. (b) In causal attention RFA attends only to the prefix. 3 This allows for a recurrent computation. Tuple (S t \u2208 R 2D\u00d7d , z t \u2208 R 2D ) is used as the \"hidden state\" at time step t to keep track of the history, similar to those in RNNs. Then RFA(q t , {k i } i\u2264t , {v i } i\u2264t ) = \u03c6(q t ) S t /(\u03c6(q t ) \u2022 z t ), where \n\n2D denotes the size of \u03c6(\u2022). Appendix A.1 summarizes the computation procedure of RFA, and Figure 1 compares it against the softmax attention. Appendix A.3 derives causal RFA in detail. \n\nAnalogously to the softmax attention, RFA has its multiheaded variant (Vaswani et al., 2017). In our experiments we use causal RFA in a transformer language model ( \u00a74.1), and both cross and causal RFA in the decoder of a sequence-to-sequence machine translation model.",
            "score": 0.5119720957934351,
            "section_title": "RANDOM FEATURE ATTENTION",
            "char_start_offset": 7968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 495
                },
                {
                    "start": 498,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 936
                },
                {
                    "start": 939,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1396
                }
            ],
            "ref_mentions": [
                {
                    "start": 1197,
                    "end": 1219,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04486083984375
        },
        {
            "corpus_id": "271974388",
            "title": "Legilimens: Practical and Unified Content Moderation for Large Language Model Services",
            "text": "Given the excellent parallelizability and capacity, the Transformer architecture [44,75] has become the de facto backbone to almost all LLMs, making it possible to scale language models to hundreds or thousands of billions of parameters. The vanilla Transformer [75] is a sequence-to-sequence model and consists of an encoder and a decoder, each of which is a stack of  identical blocks. Each encoder block is mainly composed of a multihead self-attention module and a position-wise feed-forward network (FFN). For building a deeper model, a residual connection [32] is employed around each module, followed by layer normalization [3] module. Compared to the encoder blocks, decoder blocks additionally insert cross-attention modules between the multi-head self-attention modules and the position-wise FFNs. Furthermore, the self-attention modules in the decoder are adapted to prevent each position from attending to subsequent positions in the training phase. \n\n2.1.2 Architecture. In general, the mainstream architectures of existing LLMs can be roughly categorized into two major types, namely encoder-decoder and decoder-only. \n\nEncoder-Decoder Architecture. The vanilla Transformer model is built on the encoder-decoder architecture [75], which consists of two stacks of Transformer blocks as the encoder and decoder, respectively. The encoder encodes the input sequence for generating its latent representations, while the decoder performs crossattention on these representations and generates the target sequence in an auto-regressive manner. So far, there are only a small number of LLMs that are built based on the encoder-decoder architecture, e.g., Flan-T5 [16]. \n\nDecoder-Only Architecture. Models of this type only have the decoder but no encoder. According to the self-attention mechanism used in the decoder, decoder-only models can be further divided into the causal decoder architecture and the prefix decoder architecture. \n\nThe causal decoder architecture incorporates the unidirectional attention mask, to guarantee that each input token can only attend to the past tokens and itself. The input and output tokens are processed in the same fashion through the decoder.",
            "score": 0.5114595443657912,
            "section_title": "Basic Component.",
            "char_start_offset": 9360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 262,
                    "end": 266,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 562,
                    "end": 566,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1239,
                    "end": 1243,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4892578125
        },
        {
            "corpus_id": "253736389",
            "title": "Transformers for Tabular Data Representation: A Survey of Models and Applications",
            "text": "The vanilla transformer (Vaswani et al., 2017) is a seq2seq model (Sutskever et al., 2014) consisting of an encoder and a decoder, each of which is a stack of N identical modules. The encoder block is composed of a multi-head self-attention module and a position-wise feed-forward network. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. Residual connections and layer-normalization modules are also used. Decoder blocks consist of cross-attention modules between the multi-head self-attention modules and the position-wise feed-forward networks, where masking is used to prevent each position from attending to subsequent positions. \n\nThe transformer architecture can be used as an encoder-decoder (Vaswani et al., 2017;Raffel et al., 2020), an encoder-only (Devlin et al., 2019;Liu et al., 2019), or decoder-only (Radford et al., 2019;Brown et al., 2020) model. The choice of the architecture depends on the final task. Encoder-only models are mainly used for classification and are the most popular choice for extensions for tabular data. In this case, pretraining is done with a masked language modeling (MLM) task, whose goal is to predict masked token(s) of an altered input. The encoder-decoder architecture is used for models that focus on sequence generation tasks (RPT, TAPEX).",
            "score": 0.5114542118109683,
            "section_title": "Vanilla Transformer",
            "char_start_offset": 19635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1374
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 45,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 66,
                    "end": 90,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 786,
                    "end": 808,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 846,
                    "end": 867,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 902,
                    "end": 924,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 924,
                    "end": 943,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.226318359375
        },
        {
            "corpus_id": "264426063",
            "title": "Masked Hard-Attention Transformers Recognize Exactly the Star-Free Languages",
            "text": "This kind of masking is common in decoder-only models and has been studied in encoder-only models as well (Bhattamishra et al., 2020). We also consider past masking (Yao et al., 2021). et al. (2021) show that average-hard attention transformer encoder-decoders, where the decoder runs for a polynomial number of steps before accepting or rejecting a string, recognize all of P (that is, all languages decidable by a deterministic Turing machine in polynomial time). Merrill and Sabharwal (2024) prove another version of this result, and further observe that all such transformers are in P. This result is the only other exact characterization of any transformer variant that we are aware of. Hao et al. (2022) show that (non-masked) hard-attention transformer encoders with arbitrary position embeddings have an upper bound of AC 0 (that is, languages defined by circuit families with polynomial size, unbounded fan-in, and bounded depth), and Barcel\u00f3 et al. (2024) show that they have a lower bound of LTL [Mon], which is linear temporal logic with all possible monadic numerical predicates. They leave open the question of whether these transformers are equivalent to LTL[Mon]-a question which, with suitable adjustments, we answer here in the affirmative.",
            "score": 0.5113469929093974,
            "section_title": "Transformer variants",
            "char_start_offset": 5685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1258
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 133,
                    "matchedPaperCorpusId": "222225236"
                },
                {
                    "start": 466,
                    "end": 494,
                    "matchedPaperCorpusId": "263909434"
                },
                {
                    "start": 692,
                    "end": 709,
                    "matchedPaperCorpusId": "248177889"
                },
                {
                    "start": 944,
                    "end": 965,
                    "matchedPaperCorpusId": "263829263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2073974609375
        },
        {
            "corpus_id": "201243320",
            "title": "Periodic Surface Defect Detection in Steel Plates Based on Deep Learning",
            "text": "The attention mechanism was implemented in the encoder-decoder framework throughout the network [20].The encoder-decoder structure without the attention mechanism usually takes the last state of the encoder as the input to the decoder; the subsequent decoder process has nothing to do with the previous input, and is only related to the last inputted state. The encoder-decoder structure with the attention mechanism has different input states of the decoder at different times. \n\nFigure 11 shows the structure of the implementation of the attention mechanism. The specific steps are as follows: \n\n(1) The similarity  between h 1 (the hidden layer output vector of LSTM at the current time) and Z 0 (the initial vector, the hidden state of the decoder) is calculated by the matching module \"match\". \n\n(2) The current output Z 0 needs to be matched with each input (h 1 ~h4 ) to obtain the similarity between the current output Z 0 and all inputs ( ~ )\u3002 \n\n(3) Normalize all similarities with the softmax function so that the sum of all similarities (i.e., weights) is 1.",
            "score": 0.5112691391392811,
            "section_title": "The Principle of an Improved Algorithm with the Attention Mechanism",
            "char_start_offset": 24167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1069
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142578125
        },
        {
            "corpus_id": "208248241",
            "title": "Improving Conditioning in Context-Aware Sequence to Sequence Models",
            "text": "While the sequential encoding outlined above has led to promising results on some tasks, (Liu et al. 2018) show that its performance starts decreasing when the input is longer than a few hundred tokens. One particular limitation is that the decoder treats the source and context in a roughly similar fashion even though they have different roles. Next, we propose several alternative architectures which separate the encoding of the source and context and combine them in various ways for decoding. The different models are illustrated in Figure 2. Combining Source and Context In all of the proposed architectures, we apply the encoder separately to S and C and leave the job of combining their information to the decoder. Note that while the source and context have different roles, they are both natural language sequences, so that we choose to use the same encoder for both. We denote as E(S) and E(C) the outputs of the encoder applied to the source and context respectively. We propose three approaches to combining these encodings in the decoder: concatenate, alternate, and interleave. \n\nThe most straight-forward way to combine these encodings is the concatenate approach: the decoder attention is applied to the concatenation of the source and context encoder, denoted as [E(S); E(C)]. Equation 5 then becomes: \n\nNote that while this approach looks similar to the sequential approach, the concatenate removes the cross-attention between the source and context, and the information in both sequences can only be combined by the decoder. Additionally, we can now apply different constraints to the source and context self-attentions as discussed later in this Section. \n\nThe alternate approach is inspired by the document-level NMT decoder of (Zhang et al. 2018), which adds an extra attention module in each decoder layer to attend over the context C before attending over the context S. This extends Equations 4-5 so that \u2200l \u2208 {1 . . . L D }: \n\nFinally, we propose a model to interleave the source attention and context attention. In this setting, some decoder layers only attend to the source S and others only attend to the context C after the self-attention module.",
            "score": 0.5110793407349943,
            "section_title": "Context-Aware Encoding Strategies",
            "char_start_offset": 9925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1093
                },
                {
                    "start": 1096,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 1952
                },
                {
                    "start": 1955,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 89,
                    "end": 106,
                    "matchedPaperCorpusId": "3608234"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11798095703125
        },
        {
            "corpus_id": "275336956",
            "title": "Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers",
            "text": "The evolution of attention mechanisms in neural networks has significantly influenced the field of artificial intelligence and machine learning, from early work [1,2,3,4,5] to more recent developments [6,7,8,9,10,11,12,13,14]. Originally vaguely inspired by cognitive processes in humans, attention mechanisms have become integral to modern neural network architectures like the Transformer [5]. These mechanisms dynamically allocate computational resources to the most relevant parts of input data, optimizing the processing of information and reducing computational redundancy. Decode-only Transformers [6], designed specifically for autoregressive tasks, have emerged as an efficient subclass of the Transformer architecture, focusing on multimodal tasks like text generation, image modeling, audio modeling, language modeling, and sequential data prediction, among many other modalities (Figure 1). These architectures utilize an embedding layer to convert discrete tokens (which can represent diverse types of data, like text, chemical structures, images/pixels, symbols, and others) into dense vector representations, enabling the model to process flexible inputs. The core of the architecture is the self-attention mechanism, which operates causally to ensure that each token attends only to its past context, maintaining the autoregressive property essential for tasks like generative AI. Multi-head self-attention enhances the model's ability to capture diverse relationships between tokens by allowing parallel attention computations. \n\nIn addition to attention, decode-only Transformers such as those used in LLama foundation models [10] integrate feedforward layers (FF), usually implemented via a multi-layer perceptron (MLP), following the attention mechanism. These layers expand and transform the attention outputs, introducing non-linearity and enabling the model to learn complex patterns. The architecture also employs several residual connections and layer normalization, ensuring stability during training and facilitating the flow of gradients. Positional encodings are incorporated to inject sequence order information into the embeddings [5,15], addressing the lack of inherent order in self-attention computations.",
            "score": 0.5109114453465264,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1544
                },
                {
                    "start": 1547,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "784288"
                },
                {
                    "start": 168,
                    "end": 170,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 204,
                    "end": 206,
                    "matchedPaperCorpusId": "235248316"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "259375902",
            "title": "Vision Language Transformers: A Survey",
            "text": "The first implementation of the transformer models was based on an encoder-decoder design. Formally, a given input sequence of symbol representations x = (x 1 , ..., x n ) is mapped to an intermediate representation z = (z 1 , ..., z n ) by the encoder module. With z as input, the decoder generates an output sequence y = (y 1 , ..., y n ). \n\nThe transformer encoder stack consists of N transformer layers of identical dimension. Each transformer layer, in turn, consists of a multi-head attention (MHA) sub-layer and a feed-forward network sub-layer (FFN), both of which are described in the following sub-sections. A residual connection (He et al., 2015) around each sub-layer is then followed by a layer normalization. Formally this amounts to LayerNorm(x + Sublayer(x)), where Sublayer(x) is the MHA or FFN sublayer function itself. \n\nThe decoder is also a stack of N layers of identical dimension. However, layers in the decoder stack contain a third multi-head attention sub-layer that attends to the output of the encoder stack. The self-attention mechanism in the decoder stack is also modified so that previous positions in the sequence are masked. Masking combined with an offset applied to the output embeddings ensure that decoder predictions are based only on previous outputs. These modifications make the decoder well suited to generative tasks. In the next subsection we describe the multi-head attention mechanism key to the transformer's operation.",
            "score": 0.5101620027223781,
            "section_title": "Encoder and Decoder Stacks",
            "char_start_offset": 6787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1467
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2254638671875
        },
        {
            "corpus_id": "269791266",
            "title": "When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models",
            "text": "In the context of LLMs, the \"encoder-decoder\" and \"decoderonly\" architectures are prominently used for NLP tasks.\n\nEncoder-decoder architectures [95,118,119] consist of two main components: an encoder f enc and a decoder f dec .The encoder and decoder components are typically implemented using transformers [95], which employ attention mechanisms to capture long-range dependencies in the input and output sequences.The encoder takes the input sequence X = (x 1 , x 2 , . . ., x N ) and maps it into a sequence of latent representations H = (h 1 , h 2 , . . ., h N ) that capture the contextual information, and the decoder generates the output sequence Y = (y 1 , y 2 , . . ., y T ) based on H. Mathematically, the encoding process can be expressed as H = f enc (X), and the entire latent sequence H is generated at once from X.The decoder, however, generates the output sequence Y sequentially: y t = f dec (y <t , H) where y <t = (y 1 , y 2 , . . ., y t\u22121 ).\n\nDecoder-only architectures [120,121,122], on the other hand, are a variant of the transformer architecture that uses only the decoder component.It is particularly suitable for language modeling tasks, where the goal is to predict the next token given the previous tokens.The decoder-only architecture can be mathematically expressed as y t = f dec (y <t ).\n\nTokenization is a preprocessing method to break the input text into a sequence of tokens, the basic data unit in language models.The number of tokens is finite, and each token can correspond to a word, sub-word, or a single letter.During inference, the input text is converted to a sequence of tokens and fed to the models, which predict the output tokens that are then converted back to text.The tokenization has a great impact on the performance of the language models, as it affects how the models perceive text.",
            "score": 0.5099516559483848,
            "section_title": "LLM Architectures",
            "char_start_offset": 9206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 115,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 417
                },
                {
                    "start": 417,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 560
                },
                {
                    "start": 560,
                    "end": 677
                },
                {
                    "start": 677,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 962
                },
                {
                    "start": 964,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1320
                },
                {
                    "start": 1322,
                    "end": 1451
                },
                {
                    "start": 1451,
                    "end": 1553
                },
                {
                    "start": 1553,
                    "end": 1715
                },
                {
                    "start": 1715,
                    "end": 1837
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1015625
        },
        {
            "corpus_id": "271534189",
            "title": "When to Stop? Towards Efficient Code Generation in LLMs with Excess Token Prevention",
            "text": "The decoder-only Transformer architecture is initially introduced in GPT [43]. In contrast to the original Transformer [51], the decoderonly architecture is composed solely of stacked decoder layers and does not include the encoder component [43]. The core of the decoder layers is its masked attention layer. This layer employs masks to allow the model to focus only on previously generated tokens and prevents it from attending to future tokens during the decoding process [65]. This mechanism determines the autoregressive generation characteristics of the decoder-only Transformer model. Given an input token sequence  = { 1 ,  2 , ...,   }, the model first maps them to context vectors  = { 1 ,  2 , ...,   } in word embedding layer, where  is the length of the sequence. We use the word vectors as the initial hidden states  0 = {\u210e 0 1 , \u210e 0 2 , ..., \u210e 0  } = , which serve as the input to the decoder layer. And then we take the hidden states output of the previous decoder layer as input to calculate the hidden states for the next decoder layer, following the Equation 1, where  represents the number of decoder layers,  represents the current index of decoder layer and M represents the mask matrix. \n\nAfter computing the hidden states of the final decoder layer, we choose the final hidden state of the last token \u210e   as the representation of the input sequence. Then, we input the representation into Language Modeling Head (LM Head) to predict the next token, following Equation2, where    is the LM Head Matrix. \n\nAfter obtaining probability vectors of next token  ( +1 ), we decode the next token  +1 using greedy decoding, which means selecting the token with the highest probability following Equation 3. \n\nWe update the output sequence  = { 1 } = { +1 }, and input token sequence  = { 1 ,  2 , ...,   ,  +1 }, and repeat the above inference process until the inference termination condition is reached. Here are two types of inference termination conditions: (1) Special end token is generated. When the special token <EOS> is generated, the model will stop generation. \n\n(2) The length of the generated tokens exceeds the maximum value.",
            "score": 0.5097106680794736,
            "section_title": "Inference of Code LLMs",
            "char_start_offset": 9021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1721
                },
                {
                    "start": 1724,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2087
                },
                {
                    "start": 2090,
                    "end": 2155
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.399658203125
        },
        {
            "corpus_id": "255941875",
            "title": "Ankh \u2625: Optimized Protein Language Model Unlocks General-Purpose Modelling",
            "text": "For our baseline model and throughout our experimentation, we utilize the encoder-decoder transformer originally proposed for machine translation and then for mapping an arbitrary input domain to a target domain [19]. The encoder learns to project the input domain sequences into a latent/embedding space, representing the \"context\". The decoder, however, learns to generate the target domain sequences given this context. Although later transformer releases abandoned the encoder-decoder combination and only utilized either of them, we draw upon Prot-Trans's experimentation promoting T5 [20] (the only encoder-decoder transformer analyzed) over encoder-only (e.g., BERT [21], ALBERT [22], and Electra [23]) and decoder-only transformers (e.g., TransformerXL [24] and XLNet [25]). However, the choice of encoder-decoder transformers in this work is not only motivated by the aforementioned top performance but also their compatibility with our experimentation independent variables such as, but not limited to, masking and architecture. Due to retaining both the encoder and decoder, T5-like transformers offer more compatibility with different masking strategies that are dependent on both masking and de-masking techniques. Furthermore and due to learning relative positional embeddings that are shared across layers for each attention head, T5-like transformers offer robustness against predictions surpassing the maximum length of the relative positional embedding dimension as it learns to combine the relative offset between lower layers' amino acid subsets [20]. For the masking strategy adopted by the baseline, we performed a 1-gram random token masking according to the default probability of 15% (i.e., 15% of the sequence tokens are masked at random) and performed a full demasking/reconstruction of the sequence (i.e., all tokens are reconstructed as individual tokens). \n\nFor the number of encoder-decoder layers, we used 36 layers for each. For the activation function, we used Gated Gaussian Error Linear Unit (Gated-GELU) [15].",
            "score": 0.5086474063708184,
            "section_title": "Pre-trained Model: Encoder-Decoder Transformer",
            "char_start_offset": 22528,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 1957
                },
                {
                    "start": 1958,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 590,
                    "end": 594,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1566,
                    "end": 1570,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25341796875
        },
        {
            "corpus_id": "250072246",
            "title": "BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on Fine-tuned CodeBERT",
            "text": "In the decoder module, we use Transformer's decoder pair as the framework's decoder. The autoregressive mechanism is used in the decoder, which can predict the next possible word based on the previous content. The specific implementation is that only the left part of the current word is known when decoding. Therefore, the masking mechanism is used to shield the influence of the right part of the current word and maintain the characteristics of autoregression. Each layer of the decoder performs additional cross-attention calculations on the final hidden layer of the encoder and is connected by cross attention. Each decoder layer performs an attention operation on the final hidden state of the encoder output, which can make the output of the generated model closer to the original output.\n\nSpecifically, the decoder uses the sentence sequence to predict the next word. That is the previous output is input to the Mask Multi-Head Attention layer. The function of Mask Multi-Head Attention is to block the following words to prevent information leakage. The next steps are almost the same as those in the encoder. The output vector is first passed to the Residual Connection and Layer Normalization layer, then is entered into the Feed-forward layer and is performed residual connection and layer normalization. The above steps are repeated N times, where N represents the number of layers of the decoder. Finally, the output of the decoder h t is sent to the fully connected neural network, which is then passed to the softmax layer to predict the probability of the next token.\n\nwhere y denotes the predicted token. We train our model parameters \u03b8 by the loss function L based on cross entropy:\n\nPrevious studies [6], [39], [40] showed that using neural networks' maximum probability distribution to generate text often leads to low-quality results. Recently, most studies [41]- [44] used beam search to achieve better performance on text generation tasks. Therefore, in our proposed method BASHEXPLAINER, we also use the beam search to generate comments for Bash codes. Specifically, the beam search is a compromise between the greedy strategy and the exhaustive strategy. It retains top-k high-probability words at each step of the prediction as to the input for the next time step, where k denotes the beam size. The larger the",
            "score": 0.5084431885066494,
            "section_title": "4) Decoder Module:",
            "char_start_offset": 17352,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1721,
                    "end": 1724,
                    "matchedPaperCorpusId": "49584534"
                },
                {
                    "start": 1726,
                    "end": 1730,
                    "matchedPaperCorpusId": "49584957"
                },
                {
                    "start": 1732,
                    "end": 1736,
                    "matchedPaperCorpusId": "207812088"
                },
                {
                    "start": 1881,
                    "end": 1885,
                    "matchedPaperCorpusId": "2783746"
                },
                {
                    "start": 1887,
                    "end": 1891,
                    "matchedPaperCorpusId": "231740679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307373046875
        },
        {
            "corpus_id": "235097256",
            "title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus",
            "text": "Similar to Syed et al. (2020), we first pre-train a Transformer-based language model with Masked Language Modeling (MLM) objective on English Wikipedia data extracted using WikiExtractor.1 This equips LM with the ability to predict masked words over a large corpus. Masked Language Modeling leverages bidirectional context of the input, thus enabling better language understanding. Following Masked Language Modeling objective from Devlin et al. (2019), we randomly sample 15% of the tokens from the text stream and replace them with the [MASK] token 80% of the time, by a random token 10% of the time and keep them unchanged 10% of the time, with the objective of predicting the original identity of the masked word based on its bidirectional context. To enable style transfer from a given sentence to target style, we use independently trained language models (LMs) to initialize the encoder and decoder and connect these with randomly initialized attention layers to arrive at a encoder-decoder setup. As discussed by Syed et al. (2020), the Transformer architecture (Vaswani et al., 2017) allows such independent initialization by implicitly aligning encoder-decoder layers via attention mechanism. \n\nPre-training an encoder only transformer on generative task and then leveraging it to initialize as both encoder and decoder as opposed to pretraining a joint encoder-decoder model has several advantages. Transformer-based models with encoder-only (Devlin et al., 2019) or decoder-only (Radford et al., 2018) blocks have been shown to perform well in generative pre-training task. Clearly, pre-training a single transformer block on generative task and then utilizing it as both encoder and decoder blocks has lower computational cost than training the entire encoder-decoder block jointly. Moreover, this also enables us to use the same pre-trained model to initialize both style transfer module and the discriminator models, explained in the following section. This is not only computationally more efficient but it also closely ties the underlying language distribution of the two modules.",
            "score": 0.5079432168800252,
            "section_title": "Pre-trained LM as Encoder-Decoder",
            "char_start_offset": 9535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2097
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 29,
                    "matchedPaperCorpusId": "202719307"
                },
                {
                    "start": 432,
                    "end": 452,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1021,
                    "end": 1039,
                    "matchedPaperCorpusId": "202719307"
                },
                {
                    "start": 1070,
                    "end": 1092,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1453,
                    "end": 1474,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.183837890625
        },
        {
            "corpus_id": "259063876",
            "title": "The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles",
            "text": "The transformer architecture [58] consists of an encoder and a decoder. An encoder-only architecture can be used for tasks like classification [15] and masked language modeling [14], whereas a decoder-only architecture can be used for generative tasks [7,50]. Both of these only require self-attention. For tasks like machine translation, an encoder-decoder architecture is used which additionally uses cross-attention in the decoder. We only focus on the self-attention mechanism of the transformer in this work. The key innovation of the transformer is the multihead attention mechanism, which can be expressed as: \n\nwhere Q, K, V are matrices containing rows of keys, queries and values. In the case of self-attention, all of them are formed by learnable projections of the embeddings.   is the dimensionality of the queries and the keys. A is known as the attention matrix. Element (, ) of this matrix is formed from the scaled dot product of query   and the key   followed by a softmax over all . The normalized weights at row  are used to aggregate the values   in updating the representation of position , thus allowing information to flow from  to . This process is done for multiple sets of queries, keys and values, where each is called an attention head. \n\nSeveral other terms may be added to the scaled dot product of queries and keys. A masking value    = \u2212\u221e may be added to prevent the model from attending to future positions (i.e.,  > ) for generative modeling or to padding tokens; the softmax function drives the attention matrix to zero at these positions. Another term may be added to encode relative positions. Although this may take different forms [12,44,48,53,56,61], we will discuss methods where a relative positional bias   \u2212  is added to the scaled dot-product, e.g., [39,48,53]. Our method should apply to other forms of relative positional encodings as well. With the inclusion of masking and relative positional encodings, the attention matrix becomes: \n\nWhere, M is the masking matrix and R is the relative positional bias matrix. We merge both of these into a single bias matrix B.",
            "score": 0.5076711788479193,
            "section_title": "METHOD 3.1 Background",
            "char_start_offset": 10221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 616
                },
                {
                    "start": 619,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1265
                },
                {
                    "start": 1268,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2114
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 252,
                    "end": 255,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "247476434"
                },
                {
                    "start": 1687,
                    "end": 1690,
                    "matchedPaperCorpusId": "236493453"
                },
                {
                    "start": 1796,
                    "end": 1800,
                    "matchedPaperCorpusId": "232352874"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330810546875
        },
        {
            "corpus_id": "203610517",
            "title": "Interrogating the Explanatory Power of Attention in Neural Machine Translation",
            "text": "\u2022 LastEncoderState: Here, we only use the final hidden state of the encoder as the context vector to be used in the decoder. Note that this is different from seq2seq without attention in which final hidden state of the encoder is used to initialize the decoder. When the focus is on the final hidden state of the encoder in the original attention weights, this method does not produce a counterfactual attention vector, which is why we don't intend to use this method to create a contradictory explanation, but rather to gain more insight into the sensitivity of the model to attention weights. \n\n\u2022 OnlyMax: In this and the following method, the source hidden state with the maximum attention still receives the highest attention, and so these two methods do not output counterfactual attention vectors. However we are curious to what extent other attention weights are required to preserve the model's output. Note that the weights produced by these methods can be counted as contradictory when multiple attention weights are used for justifying predictions. Because although the most attended source context is not changed, the relative ranking of the rest of the source context in terms of attention weights is changed. However, these kinds of justifications are mostly discussed in text classification. In this specific method we only keep the most attended source hidden state: \n\n\u2022 KeepMaxUniformOthers: Here, we set \n\n. This is to investigate if using other source hidden states uniformly has any added benefit.",
            "score": 0.5074112111788441,
            "section_title": "Encoder-Decoder Model with Attention Mechanism",
            "char_start_offset": 9339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1382
                },
                {
                    "start": 1385,
                    "end": 1421
                },
                {
                    "start": 1424,
                    "end": 1517
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "265150001",
            "title": "An Analysis and Mitigation of the Reversal Curse",
            "text": "Bi-Directional \n\nConverting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m . To address this issue, we propose a modification to the inner product between q m and k n for arbitrary values of m and n in a causal language model, as follows: \n\n(5) This adjustment ensures that when a query vector calculates an inner product with subsequent keys, there is no unexpected relative position information compared to training, as long as the relative distance between m and n does not exceed the maximum context length which is not within the scope of this paper. \n\nTo implement the Eq.5: when n \u2264 m, we calculate the attention weights as usual; In cases where n > m, we incorporate positional information with R \u22a4 \u03b8 , the transposition of R \u03b8 . Because R \u22a4 \u03b8,m is equivalent to R \u03b8,\u2212m for any given position m, we have: \n\n(6) Figure 3 illustrates this modification of attention calculation, where purple lines and squares denote that attention weights are calculated using the standard R \u03b8 matrix, and yellow indicates that the query attends to its succeeding keys within the extended bidirectional attention mechanism. The annotated numbers indicate the relative distance between a query and a key vector, with all values being non-positive.",
            "score": 0.5073022141088976,
            "section_title": "Extending Causal Attention to",
            "char_start_offset": 17265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 17,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1291
                },
                {
                    "start": 1294,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1714
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1787109375
        },
        {
            "corpus_id": "274235086",
            "title": "Improving Next Tokens via Second-to-Last Predictions with Generate and Refine",
            "text": "A large body of works [10,22] have built on the original transformer architecture [23]. The original transformer consists of an encoder and a decoder. The encoder maps input tokens to continuous representations, while the decoder uses these representations as well as inputs, along with self-attention to perform predictions such as the next token prediction in the output sequence. Encoder models like BERT [6] and its variants [11,19] focus on learning (universal) text representations and, therefore, include training tasks beyond next token prediction, i.e., identifying correct sentence order and predicting masked words, i.e., operating as auto-encoder to recover corrupted tokens. In contrast, autoregressive models for text generation, commonly employ decoder-only models as witnessed by the GPT series [17,3,13,14] and typically focus on next token prediction only -with exceptions like T5, which also employs an encoder and a decoder [18] and [27,1], which predict masked tokens. Thus, while the architecture of all of these models is similar to the original transformer [23,22], they differ significantly in their purpose and training objectives. In our work, we employ a model that predicts masked tokens like BERT using a decoder-only model like [27]. Aside from using a decoder model, we also differ from BERT in that we do not require a mask token and achieve higher training efficiency (from 15% up to 25%6 ) by using a static setup for predictions, i.e., we only predict tokens at fixed positions, such as every kth token, rather than predicting tokens at random positions. Our approach is also simpler than [27], which does not need a mask token. [27] uses attention masks to permute inputs and requires separate content and query representations, while at the same time facing challenges with conversion, effectively limiting training efficiency. That is, they only predict the last tokens in a permuted sequence. [1] (similarly to [18]) aims at a unified text language model by leveraging several training objectives, e.g., autoencoding using masking as well as autoregressive modeling.",
            "score": 0.5070500990664486,
            "section_title": "Related Work",
            "char_start_offset": 15560,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 22,
                    "end": 26,
                    "matchedPaperCorpusId": "235368340"
                },
                {
                    "start": 82,
                    "end": 86,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 815,
                    "end": 817,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 953,
                    "end": 957,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 957,
                    "end": 959,
                    "matchedPaperCorpusId": "211572655"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1259,
                    "end": 1263,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1625,
                    "end": 1629,
                    "matchedPaperCorpusId": "195069387"
                },
                {
                    "start": 1665,
                    "end": 1669,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.357421875
        },
        {
            "corpus_id": "17241828",
            "title": "On the Origin of Deep Learning",
            "text": "Attention Models are loosely based on a bionic design to simulate the behavior of human vision attention mechanism: when humans look at an image, we do not scan it bit by bit or stare at the whole image, but we focus on some major part of it and gradually build the context after capturing the gist. Attention mechanisms were first discussed by Larochelle and Hinton (2010) and Denil et al. (2012). The attention models mostly refer to the models that were introduced in  for machine translation and soon applied to many different domains like (Chorowski et al., 2015) for speech recognition and (Xu et al., 2015) for image caption generation.\n\nAttention models are mostly used for sequence output prediction. Instead of seeing the whole sequential data and make one single prediction (for example, language model), the model needs to make a sequential prediction for the sequential input for tasks like machine translation or image caption generation. Therefore, the attention model is mostly used to answer the question on where to pay attention to based on previously predicted labels or hidden states.\n\nThe output sequence may not have to be linked one-to-one to the input sequence, and the input data may not even be a sequence. Therefore, usually, an encoder-decoder framework  is necessary. The encoder is used to encode the data into representations and decoder is used to make sequential predictions. Attention mechanism is used to locate a region of the representation for predicting the label in current time step. Figure 25 shows a basic attention model under encoder-decoder network structure. The representation encoder encodes is all accessible to attention model, and attention model only selects some regions to pass onto the LSTM cell for further usage of prediction making. Therefore, all the magic of attention models is about how this attention module in Figure 25 helps to localize the informative representations.\n\nTo formalize how it works, we use r to denote the encoded representation (there is a total of M regions of representation), use h to denote hidden states of LSTM cell. Then, the attention module can generate the unscaled weights for ith region of the encoded representation as:\n\nwhere \u03b1 t\u22121 j is the attention weights computed at the previous time step, and can be computed at current time step as a simple softmax function:",
            "score": 0.5061852878765566,
            "section_title": "Attention Models",
            "char_start_offset": 100186,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 373,
                    "matchedPaperCorpusId": "9634512"
                },
                {
                    "start": 378,
                    "end": 397,
                    "matchedPaperCorpusId": "6661488"
                },
                {
                    "start": 544,
                    "end": 568,
                    "matchedPaperCorpusId": "1921173"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06488037109375
        },
        {
            "corpus_id": "210981845",
            "title": "AMR-To-Text Generation with Graph Transformer",
            "text": "In our model, the decoder has an architecture similar to that in the original Transformer model, which is composed of L 2 identical layers. \n\nEach layer has three sub-layers: a multi-head self-attention mechanism, multi-head attention mechanism over the output of the encoder stack, and position-wise feed-forward network. A residual connection is used for connecting adjacent sub-layers. The decoder generates the natural language sentence, and we denote the hidden state at position i of the t-th layer in the decoder stack as \u0125t i . Different from the input representation of the encoder, the position information is added and the sum of the embedding and position encoding is fed as the input, \n\nwhere e i and pe i \u2208 R d model are the embedding and positional encoding of the token at position i, respectively. \n\nThe self-attention sub-layer is used for encoding the information of the decoded subsequences. We use masking to ensure that the attention and prediction for position i depend only on the known words at positions preceding i, \n\nwhere \u0124t\u22121 is the package of hidden states \u0125t\u22121 i in the decoder. \n\nNext, the output of the self-attention is further fed into the multi-head attention and feed-forward network, expressed as follows: \n\nwhere H is the package of final node representations h i encoded by the graph encoder. \n\nFor convenience, we denote the final hidden state of the decoder at position i as \u0125i . Considering that numerous low-frequency open-class tokens such as named entities and numbers in an AMR graph appear in the corresponding sentence, we adopt the copy mechanism (Gu et al., 2016) to solve the problem. A gate is used over the decoder stack for controlling the generation of words from the vocabulary or directly copying them from the graph, expressed as \n\nwhere \n\nProbability distribution p g i of the words to be directly generated at time-step i is computed as \n\nwhere W g \u2208 R d model \u00d7d vocab and b g \u2208 R d vocab are the parameters and d vocab is the vocabulary size. Probability distribution p c i of the words to be copied at time-step i is computed as \n\nwhere z i * is the one-hot vector of node v i * .",
            "score": 0.5061205555018841,
            "section_title": "Sentence Decoder",
            "char_start_offset": 14492,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 142,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1898
                },
                {
                    "start": 1901,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2093
                },
                {
                    "start": 2096,
                    "end": 2145
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1746826171875
        },
        {
            "corpus_id": "27970287",
            "title": "Coarse-to-Fine Attention Models for Document Summarization",
            "text": "In HIER, we observe that the attention becomes washed out (in accord with its high entropy) and is essentially averaging all of the encoder hidden states. This is surprising because in theory, HIER should be able to replicate the same attention distribution as STANDARD. \n\nIf we examine the word-level attention (not pictured here), we find that the model focuses on stop words (e.g. punctuation marks, </s>) in the encoder. We posit this may be due to the LSTM \"saving\" information at these words, and so the soft attention model can best retrieve the information by averaging over these hidden states. Alternatively, the model may be ignoring the encoder and generating only from the decoder language model. In C2F, we see that we get very sharp attention on some rows as we had hoped. Unfortunately, the model has trouble deciding where to attend to, oscillating between the first and second-to-last rows. We partially alleviate this problem by allowing the model to attend to multiple rows in hard attention. Indeed, with +MULTI2 +POS, the model actually produces a very coherent output by focusing attention near the beginning. We believe that the improved result for this example is not only due to more flexibility in where to attend, but a better",
            "score": 0.5060797862816028,
            "section_title": "Attention Heatmaps For the document in",
            "char_start_offset": 24706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1254
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07684326171875
        },
        {
            "corpus_id": "258049081",
            "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
            "text": "Though the decoder-only Language Model (LM) is simply a decoder, it is still difficult to be compared with an Encoder-Decoder (ED) structure because this decoder handles both the source sequence and the target sequence together. To facilitate the comparison between the ED and LM structure, we propose to analyze a Regularized Encoder-Decoder (RED) framework as illustrated in Figure 2. It is a variant of the traditional ED framework while replicating the behaviors of an LM. Compared with the traditional ED structure, the RED framework mainly has the following different components: An unidirectional cross attention attends to both the source matrix and the target matrix simultaneously; a source auto-encoder recovers the input source; a parameter sharing mechanism shares the parameters between the encoder and the decoder; a layer-wise coordination component makes each decoder layer attending to the corresponding encoder layer output; a consecutive positional encoding utilizes a positional encoding starting from the length of the source tokens in the decoder. \n\nUnidirectional Cross Attention. The main difference between the ED framework and the LM is how the input source information is merged into the decoder. As illustrated in Figure 2, the ED framework first uses multiple Transformer blocks to extract features H E \u22121 from the source sequence s. Afterwards, it utilizes a self attention ATT D l to get the feature matrix G D l . It then uses an encoder attention ATT J l to take G D l as query and uses the encoder's final output H E \u22121 as the key and value to calculate Q D l . On the other hand, an LM uses an unidirectional attention to handle the concatenated features. To simulate this mechanism in the LM, as illustrated in Figure 2, the RED framework uses unidirectional cross attention ATT l which attends to both the source matrix G E l and the target matrix G D l simultaneously. Since it attends to all features with one attention, the output matrix Q D l of the attention layer becomes less sensitive to the input source matrix G E l especially when it has already generated many words and G D l becomes relatively long.",
            "score": 0.505354147484125,
            "section_title": "Regularized Encoder-Decoder",
            "char_start_offset": 9858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 1070
                },
                {
                    "start": 1073,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2150
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2685546875
        },
        {
            "paperId": "de3a0d172c7116336028d1b784f96b9eef5af49b",
            "corpusId": 274464654,
            "title": "AntLM: Bridging Causal and Masked Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 34,
            "citationCount": 1,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03275, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333893528",
                    "name": "Xinru Yu"
                },
                {
                    "authorId": "2334021829",
                    "name": "Bin Guo"
                },
                {
                    "authorId": "2333877832",
                    "name": "Shiwei Luo"
                },
                {
                    "authorId": "2297186132",
                    "name": "Jie Wang"
                },
                {
                    "authorId": "2309500964",
                    "name": "Tao Ji"
                },
                {
                    "authorId": "2309669217",
                    "name": "Yuanbin Wu"
                }
            ],
            "abstract": "Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two mainstream learning paradigms based on Transformer networks, specifically the Decoder-only and Encoder-only architectures. The strengths of each paradigm in downstream tasks have shown a mix of advantages and disadvantages. In the past BabyLM Challenge 2023, although the MLM paradigm achieved the best average performance, the CLM paradigm demonstrated significantly faster convergence rates. For the BabyLM Challenge 2024, we propose a novel language modeling paradigm named $\\textbf{AntLM}$, which integrates both CLM and MLM to leverage the advantages of these two classic paradigms. We chose the strict-small track and conducted experiments on two foundation models: BabyLlama, representing CLM, and LTG-BERT, representing MLM. During the training process for specific foundation models, we alternate between applying CLM or MLM training objectives and causal or bidirectional attention masks. Experimental results show that combining the two pretraining objectives leverages their strengths, enhancing overall training performance. Under the same epochs, $AntLM_{BabyLlama}$ improves Macro-average by 1%, and $AntLM_{LTG-BERT}$ achieves a 2.2% increase over the baselines.",
            "corpus_id": "274464654",
            "text": "Causal Language Modeling (CLM) and Masked Language Modeling (MLM) are two mainstream learning paradigms based on Transformer networks, specifically the Decoder-only and Encoder-only architectures. The strengths of each paradigm in downstream tasks have shown a mix of advantages and disadvantages. In the past BabyLM Challenge 2023, although the MLM paradigm achieved the best average performance, the CLM paradigm demonstrated significantly faster convergence rates. For the BabyLM Challenge 2024, we propose a novel language modeling paradigm named $\\textbf{AntLM}$, which integrates both CLM and MLM to leverage the advantages of these two classic paradigms. We chose the strict-small track and conducted experiments on two foundation models: BabyLlama, representing CLM, and LTG-BERT, representing MLM. During the training process for specific foundation models, we alternate between applying CLM or MLM training objectives and causal or bidirectional attention masks. Experimental results show that combining the two pretraining objectives leverages their strengths, enhancing overall training performance. Under the same epochs, $AntLM_{BabyLlama}$ improves Macro-average by 1%, and $AntLM_{LTG-BERT}$ achieves a 2.2% increase over the baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.333251953125
        },
        {
            "paperId": "ac5e62fc86586950576429fda824b5e1e1661872",
            "corpusId": 278740637,
            "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation",
            "venue": "",
            "year": 2025,
            "referenceCount": 31,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.11754, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1896870270",
                    "name": "Wenyu Huang"
                },
                {
                    "authorId": "7631872",
                    "name": "P. Vougiouklis"
                },
                {
                    "authorId": "1747893",
                    "name": "Mirella Lapata"
                },
                {
                    "authorId": "2311112356",
                    "name": "Jeff Z. Pan"
                }
            ],
            "abstract": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "corpus_id": "278740637",
            "text": "Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.32275390625
        },
        {
            "paperId": "c37236a6b98d0d1744c4687937d477544ba36434",
            "corpusId": 276580885,
            "title": "Enhancing DNA Foundation Models to Address Masking Inefficiencies",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 18,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.18405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265489832",
                    "name": "Monireh Safari"
                },
                {
                    "authorId": "2218058287",
                    "name": "Pablo Mill\u00e1n Arias"
                },
                {
                    "authorId": "2265494913",
                    "name": "Scott C. Lowe"
                },
                {
                    "authorId": "2347041819",
                    "name": "Lila Kari"
                },
                {
                    "authorId": "2303468198",
                    "name": "Angel X. Chang"
                },
                {
                    "authorId": "2265498974",
                    "name": "Graham W. Taylor"
                }
            ],
            "abstract": "Masked language modelling (MLM) as a pretraining objective has been widely adopted in genomic sequence modelling. While pretrained models can successfully serve as encoders for various downstream tasks, the distribution shift between pretraining and inference detrimentally impacts performance, as the pretraining task is to map [MASK] tokens to predictions, yet the [MASK] is absent during downstream applications. This means the encoder does not prioritize its encodings of non-[MASK] tokens, and expends parameters and compute on work only relevant to the MLM task, despite this being irrelevant at deployment time. In this work, we propose a modified encoder-decoder architecture based on the masked autoencoder framework, designed to address this inefficiency within a BERT-based transformer. We empirically show that the resulting mismatch is particularly detrimental in genomic pipelines where models are often used for feature extraction without fine-tuning. We evaluate our approach on the BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve substantial performance gains in both closed-world and open-world classification tasks when compared against causal models and bidirectional architectures pretrained with MLM tasks.",
            "corpus_id": "276580885",
            "text": "Masked language modelling (MLM) as a pretraining objective has been widely adopted in genomic sequence modelling. While pretrained models can successfully serve as encoders for various downstream tasks, the distribution shift between pretraining and inference detrimentally impacts performance, as the pretraining task is to map [MASK] tokens to predictions, yet the [MASK] is absent during downstream applications. This means the encoder does not prioritize its encodings of non-[MASK] tokens, and expends parameters and compute on work only relevant to the MLM task, despite this being irrelevant at deployment time. In this work, we propose a modified encoder-decoder architecture based on the masked autoencoder framework, designed to address this inefficiency within a BERT-based transformer. We empirically show that the resulting mismatch is particularly detrimental in genomic pipelines where models are often used for feature extraction without fine-tuning. We evaluate our approach on the BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve substantial performance gains in both closed-world and open-world classification tasks when compared against causal models and bidirectional architectures pretrained with MLM tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.03656005859375
        },
        {
            "paperId": "b9921a6a7ecf56340f6d98df35fd5d585a51a04d",
            "corpusId": 270764979,
            "title": "Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study",
            "venue": "Interspeech",
            "year": 2024,
            "referenceCount": 32,
            "citationCount": 3,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.18862, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249649693",
                    "name": "Peikun Chen"
                },
                {
                    "authorId": "2261393569",
                    "name": "Sining Sun"
                },
                {
                    "authorId": "10745604",
                    "name": "Changhao Shan"
                },
                {
                    "authorId": "2261362244",
                    "name": "Qing Yang"
                },
                {
                    "authorId": "2309898593",
                    "name": "Lei Xie"
                }
            ],
            "abstract": "Unified speech-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various speech-related tasks, especially in Automatic Speech Recognition (ASR). These models typically adopt a unified method to model discrete speech and text tokens, followed by training a decoder-only transformer. However, they are all designed for non-streaming ASR tasks, where the entire speech utterance is needed during decoding. Hence, we introduce a decoder-only model exclusively designed for streaming recognition, incorporating a dedicated boundary token to facilitate streaming recognition and employing causal attention masking during the training phase. Furthermore, we introduce right-chunk attention and various data augmentation techniques to improve the model's contextual modeling abilities. While achieving streaming speech recognition, experiments on the AISHELL-1 and -2 datasets demonstrate the competitive performance of our streaming approach with non-streaming decoder-only counterparts.",
            "corpus_id": "270764979",
            "text": "Unified speech-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various speech-related tasks, especially in Automatic Speech Recognition (ASR). These models typically adopt a unified method to model discrete speech and text tokens, followed by training a decoder-only transformer. However, they are all designed for non-streaming ASR tasks, where the entire speech utterance is needed during decoding. Hence, we introduce a decoder-only model exclusively designed for streaming recognition, incorporating a dedicated boundary token to facilitate streaming recognition and employing causal attention masking during the training phase. Furthermore, we introduce right-chunk attention and various data augmentation techniques to improve the model's contextual modeling abilities. While achieving streaming speech recognition, experiments on the AISHELL-1 and -2 datasets demonstrate the competitive performance of our streaming approach with non-streaming decoder-only counterparts.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.431640625
        },
        {
            "paperId": "0d5c1de59ad70445a9915718b01d2f2a5aa2f990",
            "corpusId": 278905007,
            "title": "Rethinking Causal Mask Attention for Vision-Language Inference",
            "venue": "",
            "year": 2025,
            "referenceCount": 50,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.18605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203812724",
                    "name": "Xiaohuan Pei"
                },
                {
                    "authorId": "2265957484",
                    "name": "Tao Huang"
                },
                {
                    "authorId": "2288104814",
                    "name": "Yanxiang Ma"
                },
                {
                    "authorId": "2292018438",
                    "name": "Chang Xu"
                }
            ],
            "abstract": "Causal attention has become a foundational mechanism in autoregressive vision-language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited from large language models (LLMs) where they are tailored for text-only decoding, and their adaptation to vision tokens is insufficiently addressed in the prefill stage. Strictly masking future positions for vision queries introduces overly rigid constraints, which hinder the model's ability to leverage future context that often contains essential semantic cues for accurate inference. In this work, we empirically investigate how different causal masking strategies affect vision-language inference and then propose a family of future-aware attentions tailored for this setting. We first empirically analyze the effect of previewing future tokens for vision queries and demonstrate that rigid masking undermines the model's capacity to capture useful contextual semantic representations. Based on these findings, we propose a lightweight attention family that aggregates future visual context into past representations via pooling, effectively preserving the autoregressive structure while enhancing cross-token dependencies. We evaluate a range of causal masks across diverse vision-language inference settings and show that selectively compressing future semantic context into past representations benefits the inference.",
            "corpus_id": "278905007",
            "text": "Causal attention has become a foundational mechanism in autoregressive vision-language models (VLMs), unifying textual and visual inputs under a single generative framework. However, existing causal mask-based strategies are inherited from large language models (LLMs) where they are tailored for text-only decoding, and their adaptation to vision tokens is insufficiently addressed in the prefill stage. Strictly masking future positions for vision queries introduces overly rigid constraints, which hinder the model's ability to leverage future context that often contains essential semantic cues for accurate inference. In this work, we empirically investigate how different causal masking strategies affect vision-language inference and then propose a family of future-aware attentions tailored for this setting. We first empirically analyze the effect of previewing future tokens for vision queries and demonstrate that rigid masking undermines the model's capacity to capture useful contextual semantic representations. Based on these findings, we propose a lightweight attention family that aggregates future visual context into past representations via pooling, effectively preserving the autoregressive structure while enhancing cross-token dependencies. We evaluate a range of causal masks across diverse vision-language inference settings and show that selectively compressing future semantic context into past representations benefits the inference.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.6083984375
        },
        {
            "paperId": "944165a43458f98e00f6a322ff8aefd08ed0fd64",
            "corpusId": 278595740,
            "title": "Token-Mol 1.0: tokenized drug design with large language models",
            "venue": "Nature Communications",
            "year": 2025,
            "referenceCount": 71,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s41467-025-59628-y",
                "status": "GOLD",
                "license": "CCBYNCND",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12075800, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237735396",
                    "name": "Jike Wang"
                },
                {
                    "authorId": "2310701527",
                    "name": "Rui Qin"
                },
                {
                    "authorId": "2142970644",
                    "name": "Mingyang Wang"
                },
                {
                    "authorId": "2310703435",
                    "name": "Meijing Fang"
                },
                {
                    "authorId": "2189515733",
                    "name": "Yangyang Zhang"
                },
                {
                    "authorId": "2307563764",
                    "name": "Yuchen Zhu"
                },
                {
                    "authorId": "2072402373",
                    "name": "Qun Su"
                },
                {
                    "authorId": "2310700922",
                    "name": "Qiaolin Gou"
                },
                {
                    "authorId": "2275714426",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2220403357",
                    "name": "Odin Zhang"
                },
                {
                    "authorId": "2037330967",
                    "name": "Zhenxing Wu"
                },
                {
                    "authorId": "2086994527",
                    "name": "Dejun Jiang"
                },
                {
                    "authorId": "2045701981",
                    "name": "Xujun Zhang"
                },
                {
                    "authorId": "2258696885",
                    "name": "Huifeng Zhao"
                },
                {
                    "authorId": "2300944193",
                    "name": "Jingxuan Ge"
                },
                {
                    "authorId": "2310779796",
                    "name": "Zhourui Wu"
                },
                {
                    "authorId": "2320730031",
                    "name": "Yu Kang"
                },
                {
                    "authorId": "2257984296",
                    "name": "Chang-Yu Hsieh"
                },
                {
                    "authorId": "2329895453",
                    "name": "Tingjun Hou"
                }
            ],
            "abstract": "The integration of large language models (LLMs) into drug design is gaining momentum; however, existing approaches often struggle to effectively incorporate three-dimensional molecular structures. Here, we present Token-Mol, a token-only 3D drug design model that encodes both 2D and 3D structural information, along with molecular properties, into discrete tokens. Built on a transformer decoder and trained with causal masking, Token-Mol introduces a Gaussian cross-entropy loss function tailored for regression tasks, enabling superior performance across multiple downstream applications. The model surpasses existing methods, improving molecular conformation generation by over 10% and 20% across two datasets, while outperforming token-only models by 30% in property prediction. In pocket-based molecular generation, it enhances drug-likeness and synthetic accessibility by approximately 11% and 14%, respectively. Notably, Token-Mol operates 35 times faster than expert diffusion models. In real-world validation, it improves success rates and, when combined with reinforcement learning, further optimizes affinity and drug-likeness, advancing AI-driven drug discovery.",
            "corpus_id": "278595740",
            "text": "The integration of large language models (LLMs) into drug design is gaining momentum; however, existing approaches often struggle to effectively incorporate three-dimensional molecular structures. Here, we present Token-Mol, a token-only 3D drug design model that encodes both 2D and 3D structural information, along with molecular properties, into discrete tokens. Built on a transformer decoder and trained with causal masking, Token-Mol introduces a Gaussian cross-entropy loss function tailored for regression tasks, enabling superior performance across multiple downstream applications. The model surpasses existing methods, improving molecular conformation generation by over 10% and 20% across two datasets, while outperforming token-only models by 30% in property prediction. In pocket-based molecular generation, it enhances drug-likeness and synthetic accessibility by approximately 11% and 14%, respectively. Notably, Token-Mol operates 35 times faster than expert diffusion models. In real-world validation, it improves success rates and, when combined with reinforcement learning, further optimizes affinity and drug-likeness, advancing AI-driven drug discovery.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06097412109375
        },
        {
            "paperId": "ffdd0b97b9ba996ba85e1a96f4a50ef86b796a4a",
            "corpusId": 267301268,
            "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 16,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2401.14556",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.14556, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218665654",
                    "name": "David Duki'c"
                },
                {
                    "authorId": "2269605426",
                    "name": "Jan \u0160najder"
                }
            ],
            "abstract": "Pre-trained language models based on masked language modeling (MLM) excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (LLMs) perform on par with smaller MLM-based encoders. Although their performance improves with scale, LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL). We hypothesize that LLMs' poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. Yet, how exactly and to what extent LLMs' performance on SL can be improved remains unclear. We explore techniques for improving the SL performance of open LLMs on IE tasks by applying layer-wise removal of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with state-of-the-art SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, demonstrating that open LLMs with layer-dependent CM removal outperform strong MLM-based encoders and even instruction-tuned LLMs.",
            "corpus_id": "267301268",
            "text": "Pre-trained language models based on masked language modeling (MLM) excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (LLMs) perform on par with smaller MLM-based encoders. Although their performance improves with scale, LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL). We hypothesize that LLMs' poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. Yet, how exactly and to what extent LLMs' performance on SL can be improved remains unclear. We explore techniques for improving the SL performance of open LLMs on IE tasks by applying layer-wise removal of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with state-of-the-art SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, demonstrating that open LLMs with layer-dependent CM removal outperform strong MLM-based encoders and even instruction-tuned LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.301025390625
        },
        {
            "paperId": "3586469d86a5ac204091d63d8bc709a0e3a4fd1b",
            "corpusId": 278789314,
            "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding",
            "venue": "",
            "year": 2025,
            "referenceCount": 90,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.16652, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338268075",
                    "name": "Feilong Tang"
                },
                {
                    "authorId": "2323386961",
                    "name": "Chengzhi Liu"
                },
                {
                    "authorId": "2290974030",
                    "name": "Zhongxing Xu"
                },
                {
                    "authorId": "2356583249",
                    "name": "Ming Hu"
                },
                {
                    "authorId": "2332482624",
                    "name": "Zelin Peng"
                },
                {
                    "authorId": "2362744561",
                    "name": "Zhiwei Yang"
                },
                {
                    "authorId": "2326633347",
                    "name": "Jionglong Su"
                },
                {
                    "authorId": "2363191762",
                    "name": "Minquan Lin"
                },
                {
                    "authorId": "2363317915",
                    "name": "Yifan Peng"
                },
                {
                    "authorId": "2305663348",
                    "name": "Xuelian Cheng"
                },
                {
                    "authorId": "2240180680",
                    "name": "Imran Razzak"
                },
                {
                    "authorId": "2274358979",
                    "name": "Zongyuan Ge"
                }
            ],
            "abstract": "Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in the decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention to capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.",
            "corpus_id": "278789314",
            "text": "Recent advancements in multimodal large language models (MLLMs) have significantly improved performance in visual question answering. However, they often suffer from hallucinations. In this work, hallucinations are categorized into two main types: initial hallucinations and snowball hallucinations. We argue that adequate contextual information can be extracted directly from the token interaction process. Inspired by causal inference in the decoding strategy, we propose to leverage causal masks to establish information propagation between multimodal tokens. The hypothesis is that insufficient interaction between those tokens may lead the model to rely on outlier tokens, overlooking dense and rich contextual cues. Therefore, we propose to intervene in the propagation process by tackling outlier tokens to enhance in-context inference. With this goal, we present FarSight, a versatile plug-and-play decoding strategy to reduce attention interference from outlier tokens merely by optimizing the causal mask. The heart of our method is effective token propagation. We design an attention register structure within the upper triangular matrix of the causal mask, dynamically allocating attention to capture attention diverted to outlier tokens. Moreover, a positional awareness encoding method with a diminishing masking rate is proposed, allowing the model to attend to further preceding tokens, especially for video sequence tasks. With extensive experiments, FarSight demonstrates significant hallucination-mitigating performance across different MLLMs on both image and video benchmarks, proving its effectiveness.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.276123046875
        },
        {
            "paperId": "59f9b7462f51d3b69585e68032f0806b221eea8e",
            "corpusId": 276259207,
            "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 42,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.07490, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2344834611",
                    "name": "Xialie Zhuang"
                },
                {
                    "authorId": "2345787070",
                    "name": "Zhikai Jia"
                },
                {
                    "authorId": "2344950174",
                    "name": "Jianjin Li"
                },
                {
                    "authorId": "2109338656",
                    "name": "Zhenyu (Allen) Zhang"
                },
                {
                    "authorId": "2346065531",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2345795443",
                    "name": "Zheng Cao"
                },
                {
                    "authorId": "2344983897",
                    "name": "Shiwei Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.",
            "corpus_id": "276259207",
            "text": "Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.27294921875
        },
        {
            "paperId": "23b0b9195e98cbf6e5904cf1e9a38c4a0f444f17",
            "corpusId": 258531094,
            "title": "Mask the Bias: Improving Domain-Adaptive Generalization of CTC-Based ASR with Internal Language Model Estimation",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2023,
            "referenceCount": 28,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2305.03837",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.03837, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3386912",
                    "name": "Nilaksh Das"
                },
                {
                    "authorId": "1796272030",
                    "name": "Monica Sunkara"
                },
                {
                    "authorId": "40696276",
                    "name": "S. Bodapati"
                },
                {
                    "authorId": "2115670967",
                    "name": "Jason (Jinglun) Cai"
                },
                {
                    "authorId": "38321735",
                    "name": "Devang Kulshreshtha"
                },
                {
                    "authorId": "32447397",
                    "name": "Jeffrey J. Farris"
                },
                {
                    "authorId": "1783839",
                    "name": "K. Kirchhoff"
                }
            ],
            "abstract": "End-to-end ASR models trained on large amount of data tend to be implicitly biased towards language semantics of the training data. Internal language model estimation (ILME) has been proposed to mitigate this bias for autoregressive models such as attention-based encoder-decoder and RNN-T. Typically, ILME is performed by modularizing the acoustic and language components of the model architecture, and eliminating the acoustic input to perform log-linear interpolation with the text-only posterior. However, for CTC-based ASR, it is not as straightforward to decouple the model into such acoustic and language components, as CTC log-posteriors are computed in a non-autoregressive manner. In this work, we propose a novel ILME technique for CTC-based ASR models. Our method iteratively masks the audio timesteps to estimate a pseudo log-likelihood of the internal LM by accumulating log-posteriors for only the masked timesteps. Extensive evaluation across multiple out-of-domain datasets reveals that the proposed approach improves WER by up to 9.8% and OOV F1-score by up to 24.6% relative to Shallow Fusion, when only text data from target domain is available. In the case of zero-shot domain adaptation, with no access to any target domain data, we demonstrate that removing the source domain bias with ILME can still outperform Shallow Fusion to improve WER by up to 9.3% relative.",
            "corpus_id": "258531094",
            "text": "End-to-end ASR models trained on large amount of data tend to be implicitly biased towards language semantics of the training data. Internal language model estimation (ILME) has been proposed to mitigate this bias for autoregressive models such as attention-based encoder-decoder and RNN-T. Typically, ILME is performed by modularizing the acoustic and language components of the model architecture, and eliminating the acoustic input to perform log-linear interpolation with the text-only posterior. However, for CTC-based ASR, it is not as straightforward to decouple the model into such acoustic and language components, as CTC log-posteriors are computed in a non-autoregressive manner. In this work, we propose a novel ILME technique for CTC-based ASR models. Our method iteratively masks the audio timesteps to estimate a pseudo log-likelihood of the internal LM by accumulating log-posteriors for only the masked timesteps. Extensive evaluation across multiple out-of-domain datasets reveals that the proposed approach improves WER by up to 9.8% and OOV F1-score by up to 24.6% relative to Shallow Fusion, when only text data from target domain is available. In the case of zero-shot domain adaptation, with no access to any target domain data, we demonstrate that removing the source domain bias with ILME can still outperform Shallow Fusion to improve WER by up to 9.3% relative.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07684326171875
        },
        {
            "paperId": "a729905f45b0ee19f5307d73cb69bbfc39f33a22",
            "corpusId": 269096348,
            "title": "Supervised Attention Multi-Scale Temporal Convolutional Network for monaural speech enhancement",
            "venue": "EURASIP Journal on Audio, Speech, and Music Processing",
            "year": 2024,
            "referenceCount": 35,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://asmp-eurasipjournals.springeropen.com/counter/pdf/10.1186/s13636-024-00341-x",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1186/s13636-024-00341-x?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1186/s13636-024-00341-x, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2118690365",
                    "name": "Zehua Zhang"
                },
                {
                    "authorId": "2156145527",
                    "name": "Lu Zhang"
                },
                {
                    "authorId": "2108895371",
                    "name": "Xuyi Zhuang"
                },
                {
                    "authorId": "2152102290",
                    "name": "Yukun Qian"
                },
                {
                    "authorId": "2292314709",
                    "name": "Mingjiang Wang"
                }
            ],
            "abstract": "Speech signals are often distorted by reverberation and noise, with a widely distributed signal-to-noise ratio (SNR). To address this, our study develops robust, deep neural network (DNN)-based speech enhancement methods. We reproduce several DNN-based monaural speech enhancement methods and outline a strategy for constructing datasets. This strategy, validated through experimental reproductions, has effectively enhanced the denoising efficiency and robustness of the models. Then, we propose a causal speech enhancement system named Supervised Attention Multi-Scale Temporal Convolutional Network (SA-MSTCN). SA-MSTCN extracts the complex compressed spectrum (CCS) for input encoding and employs complex ratio masking (CRM) for output decoding. The supervised attention module, a lightweight addition to SA-MSTCN, guides feature extraction. Experiment results show that the supervised attention module effectively improves noise reduction performance with a minor increase in computational cost. The multi-scale temporal convolutional network refines the perceptual field and better reconstructs the speech signal. Overall, SA-MSTCN not only achieves state-of-the-art speech quality and intelligibility compared to other methods but also maintains stable denoising performance across various environments.",
            "corpus_id": "269096348",
            "text": "Speech signals are often distorted by reverberation and noise, with a widely distributed signal-to-noise ratio (SNR). To address this, our study develops robust, deep neural network (DNN)-based speech enhancement methods. We reproduce several DNN-based monaural speech enhancement methods and outline a strategy for constructing datasets. This strategy, validated through experimental reproductions, has effectively enhanced the denoising efficiency and robustness of the models. Then, we propose a causal speech enhancement system named Supervised Attention Multi-Scale Temporal Convolutional Network (SA-MSTCN). SA-MSTCN extracts the complex compressed spectrum (CCS) for input encoding and employs complex ratio masking (CRM) for output decoding. The supervised attention module, a lightweight addition to SA-MSTCN, guides feature extraction. Experiment results show that the supervised attention module effectively improves noise reduction performance with a minor increase in computational cost. The multi-scale temporal convolutional network refines the perceptual field and better reconstructs the speech signal. Overall, SA-MSTCN not only achieves state-of-the-art speech quality and intelligibility compared to other methods but also maintains stable denoising performance across various environments.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.06610107421875
        },
        {
            "paperId": "9de900938e09166d71413950c3d273ccea00b45c",
            "corpusId": 271098088,
            "title": "Token-Mol 1.0: Tokenized drug design with large language model",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.07930, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237735396",
                    "name": "Jike Wang"
                },
                {
                    "authorId": "2310701527",
                    "name": "Rui Qin"
                },
                {
                    "authorId": "2142970644",
                    "name": "Mingyang Wang"
                },
                {
                    "authorId": "2310703435",
                    "name": "Meijing Fang"
                },
                {
                    "authorId": "2189515733",
                    "name": "Yangyang Zhang"
                },
                {
                    "authorId": "2307563764",
                    "name": "Yuchen Zhu"
                },
                {
                    "authorId": "2072402373",
                    "name": "Qun Su"
                },
                {
                    "authorId": "2310700922",
                    "name": "Qiaolin Gou"
                },
                {
                    "authorId": "2275714426",
                    "name": "Chao Shen"
                },
                {
                    "authorId": "2220403357",
                    "name": "Odin Zhang"
                },
                {
                    "authorId": "2037330967",
                    "name": "Zhenxing Wu"
                },
                {
                    "authorId": "2086994527",
                    "name": "Dejun Jiang"
                },
                {
                    "authorId": "2045701981",
                    "name": "Xujun Zhang"
                },
                {
                    "authorId": "2258696885",
                    "name": "Huifeng Zhao"
                },
                {
                    "authorId": "2297173438",
                    "name": "Xiaozhe Wan"
                },
                {
                    "authorId": "2310779796",
                    "name": "Zhourui Wu"
                },
                {
                    "authorId": "2202277409",
                    "name": "Liwei Liu"
                },
                {
                    "authorId": "2148946444",
                    "name": "Yu Kang"
                },
                {
                    "authorId": "2307736278",
                    "name": "Chang-Yu Hsieh"
                },
                {
                    "authorId": "2337540728",
                    "name": "Tingjun Hou"
                }
            ],
            "abstract": "Significant interests have recently risen in leveraging sequence-based large language models (LLMs) for drug design. However, most current applications of LLMs in drug discovery lack the ability to comprehend three-dimensional (3D) structures, thereby limiting their effectiveness in tasks that explicitly involve molecular conformations. In this study, we introduced Token-Mol, a token-only 3D drug design model. This model encodes all molecular information, including 2D and 3D structures, as well as molecular property data, into tokens, which transforms classification and regression tasks in drug discovery into probabilistic prediction problems, thereby enabling learning through a unified paradigm. Token-Mol is built on the transformer decoder architecture and trained using random causal masking techniques. Additionally, we proposed the Gaussian cross-entropy (GCE) loss function to overcome the challenges in regression tasks, significantly enhancing the capacity of LLMs to learn continuous numerical values. Through a combination of fine-tuning and reinforcement learning (RL), Token-Mol achieves performance comparable to or surpassing existing task-specific methods across various downstream tasks, including pocket-based molecular generation, conformation generation, and molecular property prediction. Compared to existing molecular pre-trained models, Token-Mol exhibits superior proficiency in handling a wider range of downstream tasks essential for drug design. Notably, our approach improves regression task accuracy by approximately 30% compared to similar token-only methods. Token-Mol overcomes the precision limitations of token-only models and has the potential to integrate seamlessly with general models such as ChatGPT, paving the way for the development of a universal artificial intelligence drug design model that facilitates rapid and high-quality drug design by experts.",
            "corpus_id": "271098088",
            "text": "Significant interests have recently risen in leveraging sequence-based large language models (LLMs) for drug design. However, most current applications of LLMs in drug discovery lack the ability to comprehend three-dimensional (3D) structures, thereby limiting their effectiveness in tasks that explicitly involve molecular conformations. In this study, we introduced Token-Mol, a token-only 3D drug design model. This model encodes all molecular information, including 2D and 3D structures, as well as molecular property data, into tokens, which transforms classification and regression tasks in drug discovery into probabilistic prediction problems, thereby enabling learning through a unified paradigm. Token-Mol is built on the transformer decoder architecture and trained using random causal masking techniques. Additionally, we proposed the Gaussian cross-entropy (GCE) loss function to overcome the challenges in regression tasks, significantly enhancing the capacity of LLMs to learn continuous numerical values. Through a combination of fine-tuning and reinforcement learning (RL), Token-Mol achieves performance comparable to or surpassing existing task-specific methods across various downstream tasks, including pocket-based molecular generation, conformation generation, and molecular property prediction. Compared to existing molecular pre-trained models, Token-Mol exhibits superior proficiency in handling a wider range of downstream tasks essential for drug design. Notably, our approach improves regression task accuracy by approximately 30% compared to similar token-only methods. Token-Mol overcomes the precision limitations of token-only models and has the potential to integrate seamlessly with general models such as ChatGPT, paving the way for the development of a universal artificial intelligence drug design model that facilitates rapid and high-quality drug design by experts.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.046478271484375
        },
        {
            "paperId": "4b27f18bff43d605805c92696a979714ced0b805",
            "corpusId": 247315559,
            "title": "UniXcoder: Unified Cross-Modal Pre-training for Code Representation",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "referenceCount": 37,
            "citationCount": 591,
            "influentialCitationCount": 111,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.03850",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.03850, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2278834796",
                    "name": "Daya Guo"
                },
                {
                    "authorId": "2115338656",
                    "name": "Shuai Lu"
                },
                {
                    "authorId": "46429989",
                    "name": "Nan Duan"
                },
                {
                    "authorId": "2108975906",
                    "name": "Yanlin Wang"
                },
                {
                    "authorId": "92660691",
                    "name": "Ming Zhou"
                },
                {
                    "authorId": "2152938031",
                    "name": "Jian Yin"
                }
            ],
            "abstract": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
            "corpus_id": "247315559",
            "text": "Pre-trained models for programming languages have recently demonstrated great success on code intelligence. To support both code-related understanding and generation tasks, recent works attempt to pre-train unified encoder-decoder models. However, such encoder-decoder framework is sub-optimal for auto-regressive tasks, especially code completion that requires a decoder-only manner for efficient inference. In this paper, we present UniXcoder, a unified cross-modal pre-trained model for programming language. The model utilizes mask attention matrices with prefix adapters to control the behavior of the model and leverages cross-modal contents like AST and code comment to enhance code representation. To encode AST that is represented as a tree in parallel, we propose a one-to-one mapping method to transform AST in a sequence structure that retains all structural information from the tree. Furthermore, we propose to utilize multi-modal contents to learn representation of code fragment with contrastive learning, and then align representations among programming languages using a cross-modal generation task. We evaluate UniXcoder on five code-related tasks over nine datasets. To further evaluate the performance of code fragment representation, we also construct a dataset for a new task, called zero-shot code-to-code search. Results show that our model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1378173828125
        },
        {
            "paperId": "529a3164a4ef5c227b6a775f73936866cb51d72f",
            "corpusId": 265609875,
            "title": "Object Recognition as Next Token Prediction",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "referenceCount": 129,
            "citationCount": 9,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.02142",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.02142, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "39826117",
                    "name": "Kaiyu Yue"
                },
                {
                    "authorId": "2291074375",
                    "name": "Borchun Chen"
                },
                {
                    "authorId": "8284185",
                    "name": "Jonas Geiping"
                },
                {
                    "authorId": "2269668184",
                    "name": "Hengduo Li"
                },
                {
                    "authorId": "2237426712",
                    "name": "Tom Goldstein"
                },
                {
                    "authorId": "2269711487",
                    "name": "Ser-Nam Lim"
                }
            ],
            "abstract": "We present an approach to pose object recognition as next token prediction. The idea is to apply a language decoder that auto-regressively predicts the text tokens from image embeddings to form labels. To ground this prediction process in auto-regression, we customize a non-causal attention mask for the decoder, incorporating two key features: modeling tokens from different labels to be independent, and treating image tokens as a prefix. This masking mechanism inspires an efficient method - one-shot sampling - to simultaneously sample tokens of multiple labels in parallel and rank generated labels by their probabilities during inference. To further enhance the efficiency, we propose a simple strategy to construct a compact decoder by simply discarding the intermediate blocks of a pretrained language model. This approach yields a decoder that matches the full model's performance while being notably more efficient. The code is available at github.com/kaiyuyue/nxtp.",
            "corpus_id": "265609875",
            "text": "We present an approach to pose object recognition as next token prediction. The idea is to apply a language decoder that auto-regressively predicts the text tokens from image embeddings to form labels. To ground this prediction process in auto-regression, we customize a non-causal attention mask for the decoder, incorporating two key features: modeling tokens from different labels to be independent, and treating image tokens as a prefix. This masking mechanism inspires an efficient method - one-shot sampling - to simultaneously sample tokens of multiple labels in parallel and rank generated labels by their probabilities during inference. To further enhance the efficiency, we propose a simple strategy to construct a compact decoder by simply discarding the intermediate blocks of a pretrained language model. This approach yields a decoder that matches the full model's performance while being notably more efficient. The code is available at github.com/kaiyuyue/nxtp.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.46630859375
        }
    ],
    "quotes": {
        "cost": 0.178137,
        "quotes": [
            {
                "idx": 0,
                "key": "[204838007 | Raffel et al. | 2019 | Citations: 20336]",
                "snippets": "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern...Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern",
                        "pdf_hash": ""
                    },
                    {
                        "quote": "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247618909 | Hua et al. | 2022 | Citations: 16]",
                "snippets": "The decoder of a transformer masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 ,..., x n ) autoregressively...In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The decoder of a transformer masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 ,",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "MASKED TRANSFORMER FOR SEGMENT PREDICTION",
                        "pdf_hash": "",
                        "start": 434,
                        "end": 458,
                        "sentence_offsets": [
                            {
                                "start": 242,
                                "end": 459
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ", x n ) autoregressively"
                    },
                    {
                        "section_title": "MASKED TRANSFORMER FOR SEGMENT PREDICTION",
                        "pdf_hash": "",
                        "start": 1125,
                        "end": 1426,
                        "sentence_offsets": [
                            {
                                "start": 1125,
                                "end": 1252
                            },
                            {
                                "start": 1253,
                                "end": 1425
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[248118752 | Wang et al. | 2022 | Citations: 175]",
                "snippets": "A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[253080830 | Gao et al. | 2022 | Citations: 4]",
                "snippets": "In the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[13756489 | Vaswani et al. | 2017 | Citations: 132444]": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                },
                "metadata": [
                    {
                        "section_title": "On the Attention Mechanism",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1151,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 243
                            },
                            {
                                "start": 244,
                                "end": 316
                            },
                            {
                                "start": 317,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 685
                            },
                            {
                                "start": 686,
                                "end": 866
                            },
                            {
                                "start": 867,
                                "end": 1151
                            }
                        ],
                        "ref_mentions": [
                            "13756489"
                        ],
                        "quote": "In the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[260886785 | Kim et al. | 2023 | Citations: 15]",
                "snippets": "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Quantization Challenges on GLMs",
                        "pdf_hash": "",
                        "start": 193,
                        "end": 857,
                        "sentence_offsets": [
                            {
                                "start": 193,
                                "end": 244
                            },
                            {
                                "start": 245,
                                "end": 379
                            },
                            {
                                "start": 380,
                                "end": 500
                            },
                            {
                                "start": 501,
                                "end": 716
                            },
                            {
                                "start": 717,
                                "end": 857
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[263829839 | Saha et al. | 2023 | Citations: 54]",
                "snippets": "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[263831258 | Woo et al. | 2023 | Citations: 14]",
                "snippets": "Causal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. (Dong et al., 2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
                },
                "metadata": [
                    {
                        "section_title": "D.3 ATTENTION MASKS",
                        "pdf_hash": "",
                        "start": 337,
                        "end": 1134,
                        "sentence_offsets": [
                            {
                                "start": 337,
                                "end": 487
                            },
                            {
                                "start": 488,
                                "end": 616
                            },
                            {
                                "start": 617,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1134
                            }
                        ],
                        "ref_mentions": [
                            "147704286"
                        ],
                        "quote": "Causal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. (Dong et al., 2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[264805730 | Gong et al. | 2023 | Citations: 2]",
                "snippets": "Causal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Causal Language Modeling",
                        "pdf_hash": "",
                        "start": 524,
                        "end": 1102,
                        "sentence_offsets": [
                            {
                                "start": 524,
                                "end": 752
                            },
                            {
                                "start": 753,
                                "end": 862
                            },
                            {
                                "start": 865,
                                "end": 1005
                            },
                            {
                                "start": 1006,
                                "end": 1102
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Causal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[266755678 | Liu et al. | 2024 | Citations: 74]",
                "snippets": "The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[268041362 | Yi et al. | 2024 | Citations: 72]",
                "snippets": "Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. (Raffel et al., 2019)",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "GENERAL METHODS",
                        "pdf_hash": "",
                        "start": 495,
                        "end": 976,
                        "sentence_offsets": [
                            {
                                "start": 495,
                                "end": 502
                            },
                            {
                                "start": 503,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 750
                            },
                            {
                                "start": 751,
                                "end": 971
                            },
                            {
                                "start": 972,
                                "end": 1246
                            }
                        ],
                        "ref_mentions": [
                            "204838007"
                        ],
                        "quote": "Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. (Raffel et al., 2019)"
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[268157336 | Patil et al. | 2024 | Citations: 80]",
                "snippets": "For example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Prefix (Non-Causal) Language Model",
                        "pdf_hash": "",
                        "start": 246,
                        "end": 1595,
                        "sentence_offsets": [
                            {
                                "start": 246,
                                "end": 424
                            },
                            {
                                "start": 425,
                                "end": 505
                            },
                            {
                                "start": 506,
                                "end": 643
                            },
                            {
                                "start": 644,
                                "end": 752
                            },
                            {
                                "start": 753,
                                "end": 924
                            },
                            {
                                "start": 925,
                                "end": 988
                            },
                            {
                                "start": 989,
                                "end": 1034
                            },
                            {
                                "start": 1035,
                                "end": 1182
                            },
                            {
                                "start": 1183,
                                "end": 1329
                            },
                            {
                                "start": 1330,
                                "end": 1436
                            },
                            {
                                "start": 1437,
                                "end": 1595
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "For example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[268201845 | Chin et al. | 2024 | Citations: 0]",
                "snippets": "Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens...First, they pursue a denoising objective (see, (Devlin et al., 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[52967399 | Devlin et al. | 2019 | Citations: 95215]": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                },
                "metadata": [
                    {
                        "section_title": "Model Architecture and Objective",
                        "pdf_hash": "",
                        "start": 925,
                        "end": 1191,
                        "sentence_offsets": [
                            {
                                "start": 925,
                                "end": 1007
                            },
                            {
                                "start": 1008,
                                "end": 1192
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens"
                    },
                    {
                        "section_title": "Model Architecture and Objective",
                        "pdf_hash": "",
                        "start": 289,
                        "end": 807,
                        "sentence_offsets": [
                            {
                                "start": 289,
                                "end": 439
                            },
                            {
                                "start": 440,
                                "end": 606
                            },
                            {
                                "start": 607,
                                "end": 745
                            },
                            {
                                "start": 748,
                                "end": 806
                            }
                        ],
                        "ref_mentions": [
                            "52967399"
                        ],
                        "quote": "First, they pursue a denoising objective (see, (Devlin et al., 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268264275 | Sia et al. | 2024 | Citations: 0]",
                "snippets": "For Causal Decoder-only Transformer Language Models, given each position i, the Attention weight \u03b1 ij over context positions j, j < i can be computed by a\n\nIn other words, the goal is to characterise where the model has \"located\" the task of translation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "For Causal Decoder-only Transformer Language Models, given each position i, the Attention weight \u03b1 ij over context positions j, j < i can be computed by a\n\nIn other words, the goal is to characterise where the model has \"located\" the task of translation.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[270214176 | Jiang et al. | 2024 | Citations: 197]",
                "snippets": "The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Pre-training",
                        "pdf_hash": "",
                        "start": 128,
                        "end": 782,
                        "sentence_offsets": [
                            {
                                "start": 128,
                                "end": 280
                            },
                            {
                                "start": 280,
                                "end": 497
                            },
                            {
                                "start": 497,
                                "end": 680
                            },
                            {
                                "start": 680,
                                "end": 765
                            },
                            {
                                "start": 765,
                                "end": 782
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[270702559 | Yin et al. | 2024 | Citations: 12]",
                "snippets": "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[270832367 | Busto-Castineira et al. | 2024 | Citations: 1]",
                "snippets": "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder.\n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[209832341 | Wu | 2020 | Citations: 4]": "We describe a method for training accurate Transformer machine-translation models to run inference using 8-bit integer (INT8) hardware matrix multipliers, as opposed to the more costly single-precision floating-point (FP32) hardware. Unlike previous work, which converted only 85 Transformer matrix multiplications to INT8, leaving 48 out of 133 of them in FP32 because of unacceptable accuracy loss, we convert them all to INT8 without compromising accuracy. Tested on the newstest2014 English-to-German translation task, our INT8 Transformer Base and Transformer Big models yield BLEU scores that are 99.3\u2013100% relative to those of the corresponding FP32 models. Our approach converts all matrix-multiplication tensors from an existing FP32 model into INT8 tensors by automatically making range-precision trade-offs during training. To demonstrate the robustness of this approach, we also include results from INT6 Transformer models.",
                    "[219463386 | Chen et al. | 2020 | Citations: 23]": "Source input information plays a very important role in the Transformer-based translation system. In practice, word embedding and positional embedding of each word are added as the input representation. Then self-attention networks are used to encode the global dependencies in the input representation to generate a source representation. However, this processing on the source representation only adopts a single source feature and excludes richer and more diverse features such as recurrence features, local features, and syntactic features, which results in tedious representation and thereby hinders the further translation performance improvement. In this paper, we introduce a simple and efficient method to encode more diverse source features into the input representation simultaneously, and thereby learning an effective source representation by self-attention networks. In particular, the proposed grouped strategy is only applied to the input representation layer, to keep the diversity of translation information and the efficiency of the self-attention networks at the same time. Experimental results show that our approach improves the translation performance over the state-of-the-art baselines of Transformer in regard to WMT14 English-to-German and NIST Chinese-to-English machine translation tasks.",
                    "[234785837 | Nguyen et al. | 2021 | Citations: 22]": "Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings."
                },
                "metadata": [
                    {
                        "section_title": "A. CAUSALITY IN GENERATIVE TRANSFORMER LANGUAGE MODELS",
                        "pdf_hash": "",
                        "start": 571,
                        "end": 1529,
                        "sentence_offsets": [
                            {
                                "start": 510,
                                "end": 635
                            },
                            {
                                "start": 636,
                                "end": 738
                            },
                            {
                                "start": 739,
                                "end": 1012
                            },
                            {
                                "start": 1015,
                                "end": 1129
                            },
                            {
                                "start": 1132,
                                "end": 1211
                            },
                            {
                                "start": 1212,
                                "end": 1355
                            },
                            {
                                "start": 1356,
                                "end": 1441
                            },
                            {
                                "start": 1442,
                                "end": 1514
                            },
                            {
                                "start": 1515,
                                "end": 1700
                            }
                        ],
                        "ref_mentions": [
                            "219463386",
                            "209832341",
                            "234785837"
                        ],
                        "quote": "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder.\n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271600495 | Lu et al. | 2024 | Citations: 0]",
                "snippets": "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021)...based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself...Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[147704286 | Dong et al. | 2019 | Citations: 1560]": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
                    "[204960716 | Lewis et al. | 2019 | Citations: 10856]": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[247519241 | Du et al. | 2021 | Citations: 1554]": "There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 580,
                        "end": 922,
                        "sentence_offsets": [
                            {
                                "start": 580,
                                "end": 923
                            }
                        ],
                        "ref_mentions": [
                            "204838007",
                            "204960716",
                            "147704286",
                            "218971783",
                            "247519241"
                        ],
                        "quote": "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021)"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1066,
                        "end": 1387,
                        "sentence_offsets": [
                            {
                                "start": 1053,
                                "end": 1264
                            },
                            {
                                "start": 1265,
                                "end": 1387
                            }
                        ],
                        "ref_mentions": [
                            "218971783",
                            "247519241"
                        ],
                        "quote": "based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1702,
                        "end": 1819,
                        "sentence_offsets": [
                            {
                                "start": 1702,
                                "end": 1818
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[271903802 | Chen et al. | 2024 | Citations: 1]",
                "snippets": "In the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Position-weighted Mean Pooling",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 248,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 168
                            },
                            {
                                "start": 169,
                                "end": 248
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[272423598 | Gao et al. | 2024 | Citations: 2]",
                "snippets": "In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Frame-wise Block Causal Attention Mask",
                        "pdf_hash": "",
                        "start": 126,
                        "end": 616,
                        "sentence_offsets": [
                            {
                                "start": 126,
                                "end": 429
                            },
                            {
                                "start": 430,
                                "end": 616
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[272593363 | Liu et al. | 2024 | Citations: 5]",
                "snippets": "Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking.\n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Dense Tokenizer",
                        "pdf_hash": "",
                        "start": 192,
                        "end": 1092,
                        "sentence_offsets": [
                            {
                                "start": 146,
                                "end": 258
                            },
                            {
                                "start": 259,
                                "end": 448
                            },
                            {
                                "start": 449,
                                "end": 632
                            },
                            {
                                "start": 633,
                                "end": 748
                            },
                            {
                                "start": 751,
                                "end": 901
                            },
                            {
                                "start": 902,
                                "end": 1097
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking.\n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273025546 | Ewer et al. | 2024 | Citations: 4]",
                "snippets": "In contrast, the causal decoder-only model (Brown et al., 2020)(Chowdhery et al., 2022) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2019)Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.",
                    "[218971783 | Brown et al. | 2020 | Citations: 42437]": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
                    "[247951931 | Chowdhery et al. | 2022 | Citations: 6293]": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 547,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 254
                            },
                            {
                                "start": 255,
                                "end": 547
                            }
                        ],
                        "ref_mentions": [
                            "218971783",
                            "247951931",
                            "204838007"
                        ],
                        "quote": "In contrast, the causal decoder-only model (Brown et al., 2020)(Chowdhery et al., 2022) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2019)Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273233776 | Li et al. | 2024 | Citations: 10]",
                "snippets": "Unlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;(Zhang et al., 2023) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[232134936 | Dong et al. | 2021 | Citations: 387]": "Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards\"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.",
                    "[255942203 | Zhang et al. | 2023 | Citations: 356]": "In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation. Our implementation is available on the project page: https://mael-zys.github.io/T2M-GPT/."
                },
                "metadata": [
                    {
                        "section_title": "AUTOREGRESSIVE GENERATION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1437,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 145
                            },
                            {
                                "start": 146,
                                "end": 340
                            },
                            {
                                "start": 341,
                                "end": 552
                            },
                            {
                                "start": 555,
                                "end": 618
                            },
                            {
                                "start": 619,
                                "end": 890
                            },
                            {
                                "start": 891,
                                "end": 1000
                            },
                            {
                                "start": 1001,
                                "end": 1160
                            },
                            {
                                "start": 1161,
                                "end": 1280
                            },
                            {
                                "start": 1281,
                                "end": 1437
                            }
                        ],
                        "ref_mentions": [
                            "255942203",
                            "232134936"
                        ],
                        "quote": "Unlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;(Zhang et al., 2023) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274992300 | Katz et al. | 2024 | Citations: 1]",
                "snippets": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering...Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures...The most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2019). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 177,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 178
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering"
                    },
                    {
                        "quote": "Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 1188,
                        "end": 1500,
                        "sentence_offsets": [
                            {
                                "start": 1188,
                                "end": 1300
                            },
                            {
                                "start": 1301,
                                "end": 1498
                            },
                            {
                                "start": 1499,
                                "end": 1681
                            }
                        ],
                        "ref_mentions": [
                            "204838007"
                        ],
                        "quote": "The most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2019). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[275336136 | Lam et al. | 2025 | Citations: 0]",
                "snippets": "In standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,...,i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259501685 | Wu et al. | 2023 | Citations: 136]": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \u201cdecoder-only\u201c architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion."
                },
                "metadata": [
                    {
                        "section_title": "Audio Causal Masking",
                        "pdf_hash": "",
                        "start": 387,
                        "end": 649,
                        "sentence_offsets": [
                            {
                                "start": 387,
                                "end": 535
                            },
                            {
                                "start": 536,
                                "end": 726
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,"
                    },
                    {
                        "section_title": "Audio Causal Masking",
                        "pdf_hash": "",
                        "start": 652,
                        "end": 1230,
                        "sentence_offsets": [
                            {
                                "start": 536,
                                "end": 726
                            },
                            {
                                "start": 727,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 924
                            },
                            {
                                "start": 925,
                                "end": 965
                            },
                            {
                                "start": 968,
                                "end": 1015
                            },
                            {
                                "start": 1016,
                                "end": 1230
                            }
                        ],
                        "ref_mentions": [
                            "259501685"
                        ],
                        "quote": ",i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276771845 | Suganthan et al. | 2025 | Citations: 1]",
                "snippets": "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2019), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[204838007 | Raffel et al. | 2019 | Citations: 20336]": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
                },
                "metadata": [
                    {
                        "section_title": "Attention Masking",
                        "pdf_hash": "",
                        "start": 286,
                        "end": 1538,
                        "sentence_offsets": [
                            {
                                "start": 286,
                                "end": 405
                            },
                            {
                                "start": 406,
                                "end": 592
                            },
                            {
                                "start": 595,
                                "end": 692
                            },
                            {
                                "start": 693,
                                "end": 801
                            },
                            {
                                "start": 802,
                                "end": 934
                            },
                            {
                                "start": 935,
                                "end": 1028
                            },
                            {
                                "start": 1031,
                                "end": 1137
                            },
                            {
                                "start": 1138,
                                "end": 1233
                            },
                            {
                                "start": 1234,
                                "end": 1347
                            },
                            {
                                "start": 1348,
                                "end": 1538
                            }
                        ],
                        "ref_mentions": [
                            "204838007"
                        ],
                        "quote": "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2019), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.029238,
        "cot": "To answer the query \"How does the attention masking mechanism differ between causal decoder-only, non-causal (prefix) decoder-only, and encoder-decoder language models?\", I'll organize the information into clear dimensions that address the key aspects of attention masking across these different model architectures.\n\nFirst, I need a background/introduction section to establish what attention masking is and why it's important in language models. This will provide the foundation for understanding the differences between the architectures.\n\nThen, I'll create specific dimensions for each of the three model architectures mentioned in the query:\n1. Causal Decoder-Only Models: This section will explain the unidirectional attention masking mechanism used in these models.\n2. Non-Causal (Prefix) Decoder-Only Models: This section will cover how the prefix decoder modifies the masking mechanism to allow bidirectional attention over prefix tokens while maintaining causal masking elsewhere.\n3. Encoder-Decoder Models: This section will describe how attention masking works differently in the encoder and decoder components.\n\nFinally, I'll include a comparative analysis section to directly address the differences between these architectures, which is the core of the query.\n\nFor the Introduction/Background, I'll use a synthesis format to provide a cohesive explanation of attention masking fundamentals.\n\nFor the three architecture-specific sections, I'll use synthesis format to explain how attention masking works in each model type, as these require coherent explanations rather than lists.\n\nFor the Comparative Analysis section, I'll also use synthesis format to bring together the key differences in a cohesive way.\n\nLet me now assign quotes to each dimension:",
        "plan": {
            "Introduction: Fundamentals of Attention Masking (synthesis)": [
                0,
                7,
                9
            ],
            "Causal Decoder-Only Models (synthesis)": [
                2,
                5,
                7,
                8,
                12,
                13,
                14,
                17,
                18,
                19,
                21
            ],
            "Non-Causal (Prefix) Decoder-Only Models (synthesis)": [
                0,
                2,
                5,
                8,
                10,
                14,
                16,
                20,
                22,
                24
            ],
            "Encoder-Decoder Models (synthesis)": [
                1,
                2,
                3,
                5,
                11,
                14,
                15,
                22,
                23
            ],
            "Comparative Analysis of Attention Masking Mechanisms (synthesis)": [
                0,
                2,
                4,
                6,
                8,
                9,
                10,
                16,
                20,
                24
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction: Fundamentals of Attention Masking",
                "tldr": "Attention masking is a key mechanism that controls which input tokens can be attended to when generating each output token in transformer-based language models. The three primary masking patterns\u2014fully-visible, causal, and causal with prefix\u2014fundamentally shape how different language model architectures process and generate text. (3 sources)",
                "text": "\nAttention masking serves as a critical component in transformer-based language models, determining the information flow within the self-attention mechanism. The self-attention operation takes a sequence as input and produces a new sequence of the same length, where each output element is computed as a weighted average of the input elements. The weights are determined by the self-attention mechanism based on the relationships between the elements, and the attention mask constrains which input elements can be attended to when generating each output element <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.\n\nThere are three fundamental attention masking patterns that distinguish different language model architectures. First, the fully-visible mask allows the self-attention mechanism to attend to the entire input sequence when generating any output element. This pattern is visualized as a fully populated matrix and is typically used when processing a prefix or context that will later inform predictions <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. Second, the causal mask implements unidirectional attention by preventing an output element from attending to any future input elements. Formally, this is represented as a lower triangular binary matrix where m_ij = 0 for all i < j, ensuring that tokens can only attend to themselves and previous tokens in the sequence <Paper corpusId=\"264805730\" paperTitle=\"(Gong et al., 2023)\" isShortName></Paper>. Finally, the causal mask with prefix combines these approaches, applying fully-visible masking to a portion of the input sequence while maintaining causal masking for the remainder <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.\n\nThese attention masking patterns are visually represented as matrices where dark cells indicate allowed attention connections and light cells indicate restricted connections. The pattern choice significantly impacts model behavior\u2014fully-visible masking allows bidirectional context awareness, causal masking enforces left-to-right generation by preventing \"seeing into the future,\" and the hybrid approach with a prefix enables special treatment of contextual information <Paper corpusId=\"268041362\" paperTitle=\"(Yi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern",
                            "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Gong et al., 2023)",
                        "snippets": [
                            "Causal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j."
                        ],
                        "paper": {
                            "corpus_id": 264805730,
                            "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
                            "authors": [
                                {
                                    "authorId": "2165228008",
                                    "name": "Zhuocheng Gong"
                                },
                                {
                                    "authorId": "2261393008",
                                    "name": "Jiahao Liu"
                                },
                                {
                                    "authorId": "2261393439",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "2258759716",
                                    "name": "Jingang Wang"
                                },
                                {
                                    "authorId": "2259620212",
                                    "name": "Xunliang Cai"
                                },
                                {
                                    "authorId": "2253232138",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "2249533146",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.6669921875
                    },
                    {
                        "id": "(Yi et al., 2024)",
                        "snippets": [
                            "Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. (Raffel et al., 2019)"
                        ],
                        "paper": {
                            "corpus_id": 268041362,
                            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
                            "authors": [
                                {
                                    "authorId": "2287925430",
                                    "name": "Zihao Yi"
                                },
                                {
                                    "authorId": "2287922728",
                                    "name": "Jiarui Ouyang"
                                },
                                {
                                    "authorId": "2288039936",
                                    "name": "Yuwen Liu"
                                },
                                {
                                    "authorId": "2287923878",
                                    "name": "Tianhao Liao"
                                },
                                {
                                    "authorId": "2288033664",
                                    "name": "Zhe Xu"
                                },
                                {
                                    "authorId": "2288065597",
                                    "name": "Ying Shen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 72
                        },
                        "score": 0.7626953125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Causal Decoder-Only Models",
                "tldr": "Causal decoder-only models employ a unidirectional attention mask that restricts each token to attend only to itself and previous tokens in the sequence, enforcing strict left-to-right generation. This masking pattern, implemented as a lower triangular matrix, is fundamental to popular models like GPT and LLaMA, ensuring that information flows only from past to future tokens. (11 sources)",
                "text": "\nCausal decoder-only models represent one of the most prevalent architectures in modern large language models, including widely used systems like GPT and LLaMA <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. The defining characteristic of these models is their attention masking mechanism, which implements a strict unidirectional information flow. This causal attention mask ensures that each token in the sequence can only attend to preceding tokens and itself, but never to future tokens <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nFormally, the causal attention mask is implemented as a lower triangular binary matrix M \u2208 {0, 1}^(n\u00d7n), where n represents the total sequence length. Each element m_ij in this matrix indicates whether the j-th token can attend to the i-th token, with m_ij = 0 for all i < j <Paper corpusId=\"264805730\" paperTitle=\"(Gong et al., 2023)\" isShortName></Paper>. In practical implementations, this is often achieved by setting the lower triangular part of the attention matrix to 0 and all other elements to \u2212\u221e, effectively preventing any attention to future tokens <Paper corpusId=\"270214176\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.\n\nThe causal masking pattern has important implications for how these models process and generate text. Unlike encoder-decoder or prefix decoder architectures, causal decoder-only models treat all tokens equivalently\u2014there is no separate mechanism for processing input and output sequences differently <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This uniform processing approach means that conditioning is based solely on past tokens, with the model predicting the next token in the sequence autoregressively <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>.\n\nA key consequence of this architecture is that in causal decoder-only models, only the last token in a sequence contains information from the entire input, as each token can only attend to what came before it <Paper corpusId=\"271903802\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This property shapes how these models must be used in practice, particularly for tasks requiring bidirectional context understanding.\n\nThe causal attention mechanism provides advantages for autoregressive text generation by maintaining the integrity of the prediction task\u2014each token must be predicted without \"peeking\" at future information <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. Additionally, there are technical benefits to this approach: the attention matrix in causal attention masks is guaranteed to be full-rank, potentially providing greater expressiveness compared to some bidirectional attention mechanisms that may suffer from rank collapse issues <Paper corpusId=\"273233776\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"232134936\" paperTitle=\"(Dong et al., 2021)\" isShortName></Paper>.\n\nFor specialized applications, variations on the standard causal attention mask have been developed. For instance, some approaches implement cascaded attention masking schemes that preserve causal attention within blocks of tokens while allowing controlled attention patterns between blocks <Paper corpusId=\"272593363\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper>. These modifications demonstrate how the fundamental causal masking pattern can be adapted for particular use cases while maintaining its essential unidirectional character.",
                "citations": [
                    {
                        "id": "(Gao et al., 2024)",
                        "snippets": [
                            "In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text."
                        ],
                        "paper": {
                            "corpus_id": 272423598,
                            "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations",
                            "authors": [
                                {
                                    "authorId": "2319805585",
                                    "name": "Mingze Gao"
                                },
                                {
                                    "authorId": "2302790279",
                                    "name": "Jingyu Liu"
                                },
                                {
                                    "authorId": "2302785092",
                                    "name": "Mingda Li"
                                },
                                {
                                    "authorId": "2319964588",
                                    "name": "Jiangtao Xie"
                                },
                                {
                                    "authorId": "2258682951",
                                    "name": "Qingbin Liu"
                                },
                                {
                                    "authorId": "2304448412",
                                    "name": "Bo Zhao"
                                },
                                {
                                    "authorId": "2302990371",
                                    "name": "Xi Chen"
                                },
                                {
                                    "authorId": "2319814814",
                                    "name": "Hui Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.70947265625
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.654296875
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.6611328125
                    },
                    {
                        "id": "(Gong et al., 2023)",
                        "snippets": [
                            "Causal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j."
                        ],
                        "paper": {
                            "corpus_id": 264805730,
                            "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning",
                            "authors": [
                                {
                                    "authorId": "2165228008",
                                    "name": "Zhuocheng Gong"
                                },
                                {
                                    "authorId": "2261393008",
                                    "name": "Jiahao Liu"
                                },
                                {
                                    "authorId": "2261393439",
                                    "name": "Qifan Wang"
                                },
                                {
                                    "authorId": "2258759716",
                                    "name": "Jingang Wang"
                                },
                                {
                                    "authorId": "2259620212",
                                    "name": "Xunliang Cai"
                                },
                                {
                                    "authorId": "2253232138",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "2249533146",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 2
                        },
                        "score": 0.6669921875
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder."
                        ],
                        "paper": {
                            "corpus_id": 270214176,
                            "title": "A Survey on Large Language Models for Code Generation",
                            "authors": [
                                {
                                    "authorId": "2294682530",
                                    "name": "Juyong Jiang"
                                },
                                {
                                    "authorId": "2304542351",
                                    "name": "Fan Wang"
                                },
                                {
                                    "authorId": "2305041631",
                                    "name": "Jiasi Shen"
                                },
                                {
                                    "authorId": "2304525068",
                                    "name": "Sungju Kim"
                                },
                                {
                                    "authorId": "2257349580",
                                    "name": "Sunghun Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 197
                        },
                        "score": 0.53857421875
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.765625
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.6494140625
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "In the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence."
                        ],
                        "paper": {
                            "corpus_id": 271903802,
                            "title": "WPN: An Unlearning Method Based on N-pair Contrastive Learning in Language Models",
                            "authors": [
                                {
                                    "authorId": "2316443362",
                                    "name": "Guitao Chen"
                                },
                                {
                                    "authorId": "2316551557",
                                    "name": "Yunshen Wang"
                                },
                                {
                                    "authorId": "2316544717",
                                    "name": "Hongye Sun"
                                },
                                {
                                    "authorId": "2316516363",
                                    "name": "Guang Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "European Conference on Artificial Intelligence",
                            "n_citations": 1
                        },
                        "score": 0.79248046875
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Unlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;(Zhang et al., 2023) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness."
                        ],
                        "paper": {
                            "corpus_id": 273233776,
                            "title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning",
                            "authors": [
                                {
                                    "authorId": "2325264641",
                                    "name": "Zhe Li"
                                },
                                {
                                    "authorId": "2268513823",
                                    "name": "Weihao Yuan"
                                },
                                {
                                    "authorId": "2281417686",
                                    "name": "Yisheng He"
                                },
                                {
                                    "authorId": "73555520",
                                    "name": "Lingteng Qiu"
                                },
                                {
                                    "authorId": "2325327454",
                                    "name": "Shenhao Zhu"
                                },
                                {
                                    "authorId": "2268724718",
                                    "name": "Xiaodong Gu"
                                },
                                {
                                    "authorId": "2323499860",
                                    "name": "Weichao Shen"
                                },
                                {
                                    "authorId": "2218391810",
                                    "name": "Yuan Dong"
                                },
                                {
                                    "authorId": "2268646063",
                                    "name": "Zilong Dong"
                                },
                                {
                                    "authorId": "2261796256",
                                    "name": "Laurence T. Yang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.78857421875
                    },
                    {
                        "id": "(Dong et al., 2021)",
                        "snippets": [
                            "Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards\"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures."
                        ],
                        "paper": {
                            "corpus_id": 232134936,
                            "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth",
                            "authors": [
                                {
                                    "authorId": "145595795",
                                    "name": "Yihe Dong"
                                },
                                {
                                    "authorId": "51440515",
                                    "name": "Jean-Baptiste Cordonnier"
                                },
                                {
                                    "authorId": "1966031",
                                    "name": "Andreas Loukas"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 387
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al._1, 2024)",
                        "snippets": [
                            "Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking.\n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty."
                        ],
                        "paper": {
                            "corpus_id": 272593363,
                            "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM",
                            "authors": [
                                {
                                    "authorId": "150270469",
                                    "name": "Qijiong Liu"
                                },
                                {
                                    "authorId": "2290237904",
                                    "name": "Jieming Zhu"
                                },
                                {
                                    "authorId": "2147259441",
                                    "name": "Lu Fan"
                                },
                                {
                                    "authorId": "2265936086",
                                    "name": "Zhou Zhao"
                                },
                                {
                                    "authorId": "2187512110",
                                    "name": "Xiao-Ming Wu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.833984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Non-Causal (Prefix) Decoder-Only Models",
                "tldr": "Non-causal (prefix) decoder-only models employ a hybrid attention masking approach that allows bidirectional attention over designated prefix tokens while maintaining unidirectional attention for generated tokens. This architecture combines the comprehensive context understanding of encoder models with the autoregressive generation capabilities of causal decoder models. (10 sources)",
                "text": "\nPrefix decoder-only models represent an architectural variation that modifies the attention masking mechanism used in standard causal decoders <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This hybrid approach enables bidirectional attention over a designated portion of the input sequence (the prefix) while maintaining unidirectional attention for the remainder of the sequence <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. The design was notably explored in the T5 framework as \"PrefixLM,\" offering a middle ground between fully-visible and strictly causal masking patterns <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n\nThe attention masking pattern in prefix decoder models can be visualized as a matrix with two distinct regions: the prefix region employs fully-visible masking (allowing bidirectional attention), while the remaining sequence uses causal masking <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. This configuration enables the model to build richer non-causal representations of the input or conditioning text <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. For example, when translating an English sentence to French, the model would apply fully-visible masking to the prefix (\"translate English to French: I am doing well. Target:\"), followed by causal masking while predicting the target (\"je vais bien\") <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nUnlike causal decoder-only models that use a targets-only paradigm, prefix models employ an input-to-target paradigm <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This distinction is crucial as it allows the model to process the prefix information comprehensively before generating the output sequence. The bidirectional attention over the prefix enables the model to encode contextual information in a manner similar to an encoder, while the causal masking for subsequent tokens preserves the autoregressive nature required for text generation <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of the prefix decoder architecture is that it combines the benefits of both encoder-decoder and causal decoder-only approaches <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. By allowing bidirectional attention for the prefix tokens, these models can develop a more comprehensive understanding of the input context <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>. At the same time, the causal masking for the output portion maintains the model's ability to generate text autoregressively <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. This makes prefix decoder models particularly well-suited for tasks that require both deep understanding of input context and controlled text generation.",
                "citations": [
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.765625
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.6494140625
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021)",
                            "based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself",
                            "Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."
                        ],
                        "paper": {
                            "corpus_id": 271600495,
                            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                            "authors": [
                                {
                                    "authorId": "2314473248",
                                    "name": "Mingcong Lu"
                                },
                                {
                                    "authorId": "2314649002",
                                    "name": "Jiangcai Zhu"
                                },
                                {
                                    "authorId": "2314113733",
                                    "name": "Wang Hao"
                                },
                                {
                                    "authorId": "2314323587",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2314311430",
                                    "name": "Shusheng Zhang"
                                },
                                {
                                    "authorId": "2314110211",
                                    "name": "Kailai Shao"
                                },
                                {
                                    "authorId": "2314192630",
                                    "name": "Chao Chen"
                                },
                                {
                                    "authorId": "2314343132",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2324104105",
                                    "name": "Feng Wang"
                                },
                                {
                                    "authorId": "2324103820",
                                    "name": "Xin Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.744140625
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern",
                            "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Katz et al., 2024)",
                        "snippets": [
                            "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering",
                            "Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures",
                            "The most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2019). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder."
                        ],
                        "paper": {
                            "corpus_id": 274992300,
                            "title": "Segment-Based Attention Masking for GPTs",
                            "authors": [
                                {
                                    "authorId": "121254633",
                                    "name": "Shahar Katz"
                                },
                                {
                                    "authorId": "2186740854",
                                    "name": "Liran Ringel"
                                },
                                {
                                    "authorId": "2335566528",
                                    "name": "Yaniv Romano"
                                },
                                {
                                    "authorId": "2284763723",
                                    "name": "Lior Wolf"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.79931640625
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "For example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.6611328125
                    },
                    {
                        "id": "(Ewer et al., 2024)",
                        "snippets": [
                            "In contrast, the causal decoder-only model (Brown et al., 2020)(Chowdhery et al., 2022) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2019)Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants)."
                        ],
                        "paper": {
                            "corpus_id": 273025546,
                            "title": "ENTP: Encoder-only Next Token Prediction",
                            "authors": [
                                {
                                    "authorId": "2323781863",
                                    "name": "Ethan Ewer"
                                },
                                {
                                    "authorId": "2253659910",
                                    "name": "Daewon Chae"
                                },
                                {
                                    "authorId": "2323820473",
                                    "name": "Thomas Zeng"
                                },
                                {
                                    "authorId": "2323851531",
                                    "name": "Jinkyu Kim"
                                },
                                {
                                    "authorId": "2323790154",
                                    "name": "Kangwook Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.57177734375
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.654296875
                    },
                    {
                        "id": "(Suganthan et al., 2025)",
                        "snippets": [
                            "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2019), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."
                        ],
                        "paper": {
                            "corpus_id": 276771845,
                            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2348489099",
                                    "name": "Le Yan"
                                },
                                {
                                    "authorId": "2261361394",
                                    "name": "Junru Wu"
                                },
                                {
                                    "authorId": "2348507846",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2348488953",
                                    "name": "Jay Han"
                                },
                                {
                                    "authorId": "1954563",
                                    "name": "I. Zitouni"
                                },
                                {
                                    "authorId": "1727837",
                                    "name": "Enrique Alfonseca"
                                },
                                {
                                    "authorId": "2348422460",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92333984375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Encoder-Decoder Models",
                "tldr": "Encoder-decoder models employ distinct attention masking mechanisms in each component: fully-visible (bidirectional) attention in the encoder and a combination of causal self-attention and cross-attention in the decoder. This architecture allows for comprehensive bidirectional understanding of input sequences while maintaining controlled autoregressive generation of output sequences. (13 sources)",
                "text": "\nThe original Transformer architecture introduced by Vaswani et al. represents the foundational encoder-decoder model design, featuring two separate stacks of Transformer blocks\u2014the encoder and the decoder\u2014each with distinct attention masking patterns <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. In this architecture, attention mechanisms are applied in three distinct places: bidirectional self-attention in the encoder, causal self-attention in the decoder, and cross-attention between encoder and decoder <Paper corpusId=\"253080830\" paperTitle=\"(Gao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>.\n\nThe encoder component employs fully-visible (bidirectional) attention masking, which allows each token to attend to all other tokens in the input sequence <Paper corpusId=\"268201845\" paperTitle=\"(Chin et al., 2024)\" isShortName></Paper>. This bidirectional attention enables the encoder to build comprehensive representations of the input by capturing dependencies in both forward and backward directions <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>. The fully-visible mask in the encoder is fundamental to models like BERT that are designed for understanding tasks rather than generation, as it allows for complete contextual awareness <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nIn contrast, the decoder component implements two different types of attention mechanisms. First, it uses masked multi-head self-attention with a causal mask that prevents each position from attending to future positions <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>. This causal masking ensures that when predicting the output sequence, each token can only attend to preceding tokens and itself, maintaining the autoregressive nature of text generation <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Second, the decoder incorporates cross-attention layers that allow it to attend to the encoder's output representations <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. This cross-attention mechanism enables each token in the decoder to access information from all tokens in the encoded input sequence, regardless of position <Paper corpusId=\"253080830\" paperTitle=\"(Gao et al., 2022)\" isShortName></Paper>.\n\nThe distinct masking patterns in encoder-decoder models offer specific advantages for various NLP tasks. The bidirectional attention in the encoder allows for rich contextual representations of the input sequence, while the causal masking in the decoder self-attention ensures proper autoregressive generation <Paper corpusId=\"247618909\" paperTitle=\"(Hua et al., 2022)\" isShortName></Paper>. This combination makes encoder-decoder models particularly effective for sequence-to-sequence tasks like machine translation, where understanding the entire source sentence is crucial before generating the target translation <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.\n\nSome variations of encoder-decoder architectures have been explored to enhance their capabilities. For instance, alternative causal masking strategies have been proposed for specific applications, such as in speech processing, where speech tokens are allowed to attend to all other speech tokens in the sequence while maintaining causal masking for text tokens <Paper corpusId=\"275336136\" paperTitle=\"(Lam et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259501685\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. These modifications demonstrate the flexibility of the encoder-decoder architecture to accommodate different attention patterns for specialized tasks.\n\nWhile encoder-decoder models have been foundational in NLP development, their complexity and computational requirements have led to the rise of decoder-only architectures for many text generation tasks <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. Nevertheless, the encoder-decoder approach remains valuable for tasks that benefit from its clear separation of bidirectional understanding and unidirectional generation components <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yin et al., 2024)",
                        "snippets": [
                            "The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."
                        ],
                        "paper": {
                            "corpus_id": 270702559,
                            "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics",
                            "authors": [
                                {
                                    "authorId": "2265383225",
                                    "name": "Kai Yin"
                                },
                                {
                                    "authorId": "2308073678",
                                    "name": "Chengkai Liu"
                                },
                                {
                                    "authorId": "2258714985",
                                    "name": "Ali Mostafavi"
                                },
                                {
                                    "authorId": "2308068627",
                                    "name": "Xia Hu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 12
                        },
                        "score": 0.6611328125
                    },
                    {
                        "id": "(Gao et al., 2022)",
                        "snippets": [
                            "In the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations."
                        ],
                        "paper": {
                            "corpus_id": 253080830,
                            "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
                            "authors": [
                                {
                                    "authorId": "66122912",
                                    "name": "Yingbo Gao"
                                },
                                {
                                    "authorId": "1474566597",
                                    "name": "Christian Herold"
                                },
                                {
                                    "authorId": "48598969",
                                    "name": "Zijian Yang"
                                },
                                {
                                    "authorId": "145322333",
                                    "name": "H. Ney"
                                }
                            ],
                            "year": 2022,
                            "venue": "AACL",
                            "n_citations": 4
                        },
                        "score": 0.56640625
                    },
                    {
                        "id": "(Vaswani et al., 2017)",
                        "snippets": [
                            "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
                        ],
                        "paper": {
                            "corpus_id": 13756489,
                            "title": "Attention is All you Need",
                            "authors": [
                                {
                                    "authorId": "40348417",
                                    "name": "Ashish Vaswani"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "3877127",
                                    "name": "Niki Parmar"
                                },
                                {
                                    "authorId": "39328010",
                                    "name": "Jakob Uszkoreit"
                                },
                                {
                                    "authorId": "145024664",
                                    "name": "Llion Jones"
                                },
                                {
                                    "authorId": "19177000",
                                    "name": "Aidan N. Gomez"
                                },
                                {
                                    "authorId": "40527594",
                                    "name": "Lukasz Kaiser"
                                },
                                {
                                    "authorId": "3443442",
                                    "name": "I. Polosukhin"
                                }
                            ],
                            "year": 2017,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 132444
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chin et al., 2024)",
                        "snippets": [
                            "Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens",
                            "First, they pursue a denoising objective (see, (Devlin et al., 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other."
                        ],
                        "paper": {
                            "corpus_id": 268201845,
                            "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem",
                            "authors": [
                                {
                                    "authorId": "2289612023",
                                    "name": "Samuel J. K. Chin"
                                },
                                {
                                    "authorId": "2289612073",
                                    "name": "Matthias Winkenbach"
                                },
                                {
                                    "authorId": "2289782615",
                                    "name": "Akash Srivastava"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.51171875
                    },
                    {
                        "id": "(Busto-Castineira et al., 2024)",
                        "snippets": [
                            "While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder.\n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]."
                        ],
                        "paper": {
                            "corpus_id": 270832367,
                            "title": "Predictability and Causality in Spanish and English Natural Language Generation",
                            "authors": [
                                {
                                    "authorId": "2222734467",
                                    "name": "Andrea Busto-Casti\u00f1eira"
                                },
                                {
                                    "authorId": "2323809078",
                                    "name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o"
                                },
                                {
                                    "authorId": "1405165681",
                                    "name": "Silvia Garc\u00eda-M\u00e9ndez"
                                },
                                {
                                    "authorId": "2034282614",
                                    "name": "Francisco de Arriba-P\u00e9rez"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Access",
                            "n_citations": 1
                        },
                        "score": 0.505859375
                    },
                    {
                        "id": "(Katz et al., 2024)",
                        "snippets": [
                            "Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering",
                            "Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures",
                            "The most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2019). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder."
                        ],
                        "paper": {
                            "corpus_id": 274992300,
                            "title": "Segment-Based Attention Masking for GPTs",
                            "authors": [
                                {
                                    "authorId": "121254633",
                                    "name": "Shahar Katz"
                                },
                                {
                                    "authorId": "2186740854",
                                    "name": "Liran Ringel"
                                },
                                {
                                    "authorId": "2335566528",
                                    "name": "Yaniv Romano"
                                },
                                {
                                    "authorId": "2284763723",
                                    "name": "Lior Wolf"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.79931640625
                    },
                    {
                        "id": "(Devlin et al., 2019)",
                        "snippets": [
                            "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
                        ],
                        "paper": {
                            "corpus_id": 52967399,
                            "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                            "authors": [
                                {
                                    "authorId": "39172707",
                                    "name": "Jacob Devlin"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "3259253",
                                    "name": "Kristina Toutanova"
                                }
                            ],
                            "year": 2019,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 95215
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.765625
                    },
                    {
                        "id": "(Hua et al., 2022)",
                        "snippets": [
                            "The decoder of a transformer masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 ,",
                            ", x n ) autoregressively",
                            "In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask)."
                        ],
                        "paper": {
                            "corpus_id": 247618909,
                            "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)",
                            "authors": [
                                {
                                    "authorId": "1419971650",
                                    "name": "Tianyu Hua"
                                },
                                {
                                    "authorId": "2476765",
                                    "name": "Yonglong Tian"
                                },
                                {
                                    "authorId": "1823941979",
                                    "name": "Sucheng Ren"
                                },
                                {
                                    "authorId": "2146231364",
                                    "name": "Hang Zhao"
                                },
                                {
                                    "authorId": "144398147",
                                    "name": "L. Sigal"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 16
                        },
                        "score": 0.59228515625
                    },
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern",
                            "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Lam et al., 2025)",
                        "snippets": [
                            "In standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,",
                            ",i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models."
                        ],
                        "paper": {
                            "corpus_id": 275336136,
                            "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison",
                            "authors": [
                                {
                                    "authorId": "46211375",
                                    "name": "Tsz Kin Lam"
                                },
                                {
                                    "authorId": "1736801422",
                                    "name": "Marco Gaido"
                                },
                                {
                                    "authorId": "2006601535",
                                    "name": "Sara Papi"
                                },
                                {
                                    "authorId": "2486762",
                                    "name": "L. Bentivogli"
                                },
                                {
                                    "authorId": "2322986291",
                                    "name": "Barry Haddow"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 0
                        },
                        "score": 0.68798828125
                    },
                    {
                        "id": "(Wu et al., 2023)",
                        "snippets": [
                            "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion."
                        ],
                        "paper": {
                            "corpus_id": 259501685,
                            "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
                            "authors": [
                                {
                                    "authorId": "97569165",
                                    "name": "Jian Wu"
                                },
                                {
                                    "authorId": "2334648",
                                    "name": "Yashesh Gaur"
                                },
                                {
                                    "authorId": "145718850",
                                    "name": "Zhuo Chen"
                                },
                                {
                                    "authorId": "2135918679",
                                    "name": "Long Zhou"
                                },
                                {
                                    "authorId": "2117870253",
                                    "name": "Yilun Zhu"
                                },
                                {
                                    "authorId": "2118915113",
                                    "name": "Tianrui Wang"
                                },
                                {
                                    "authorId": "152319568",
                                    "name": "Jinyu Li"
                                },
                                {
                                    "authorId": "2107983441",
                                    "name": "Shujie Liu"
                                },
                                {
                                    "authorId": "2121381699",
                                    "name": "Bo Ren"
                                },
                                {
                                    "authorId": "2049319",
                                    "name": "Linquan Liu"
                                },
                                {
                                    "authorId": "49176273",
                                    "name": "Yu Wu"
                                }
                            ],
                            "year": 2023,
                            "venue": "Automatic Speech Recognition & Understanding",
                            "n_citations": 136
                        },
                        "score": 0
                    },
                    {
                        "id": "(Saha et al., 2023)",
                        "snippets": [
                            "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction."
                        ],
                        "paper": {
                            "corpus_id": 263829839,
                            "title": "LLM for SoC Security: A Paradigm Shift",
                            "authors": [
                                {
                                    "authorId": "2256992493",
                                    "name": "Dipayan Saha"
                                },
                                {
                                    "authorId": "2114625129",
                                    "name": "Shams Tarek"
                                },
                                {
                                    "authorId": "2256991081",
                                    "name": "Katayoon Yahyaei"
                                },
                                {
                                    "authorId": "2231854143",
                                    "name": "Sujan Kumar Saha"
                                },
                                {
                                    "authorId": "2257235852",
                                    "name": "Jingbo Zhou"
                                },
                                {
                                    "authorId": "145954982",
                                    "name": "M. Tehranipoor"
                                },
                                {
                                    "authorId": "1997019",
                                    "name": "Farimah Farahmandi"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Access",
                            "n_citations": 54
                        },
                        "score": 0.6494140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Analysis of Attention Masking Mechanisms",
                "tldr": "The three language model architectures\u2014causal decoder-only, non-causal (prefix) decoder-only, and encoder-decoder\u2014each employ distinctive attention masking patterns that fundamentally shape their capabilities and limitations. These architectural differences determine how models process context, balance understanding with generation, and handle various natural language processing tasks. (12 sources)",
                "text": "\nWhen comparing the attention masking mechanisms across the three major language model architectures, several key distinctions emerge that influence their performance characteristics and optimal use cases. The most fundamental difference lies in how information flows through each architecture, which is directly controlled by the attention masking patterns <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> <Paper corpusId=\"268041362\" paperTitle=\"(Yi et al., 2024)\" isShortName></Paper>.\n\nCausal decoder-only models implement strictly unidirectional attention, where each token can only attend to itself and previous tokens in the sequence. This architecture processes all tokens equivalently through a single masking pattern, making it conceptually simpler but limited in its ability to build comprehensive bidirectional representations <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. In contrast, non-causal (prefix) decoder-only models offer a middle ground by employing a hybrid approach\u2014bidirectional attention for prefix tokens and unidirectional attention for generated tokens\u2014combining aspects of both understanding and generation within a single architecture <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nEncoder-decoder models represent the most compartmentalized approach, with distinct components and masking patterns for understanding (encoder with fully-visible masking) and generation (decoder with causal self-attention and cross-attention) <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. This clear separation allows encoder-decoder models to develop rich bidirectional representations of input sequences while maintaining controlled autoregressive generation <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nFrom a technical perspective, these architectural differences impact model performance characteristics. Causal attention in decoder-only models creates a pattern of cumulative quantization errors that can affect generation quality, as errors propagate through the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. Additionally, the choice of attention mask directly influences a model's suitability for specific tasks. For instance, fully-visible masking enables comprehensive contextual understanding beneficial for tasks like question answering, while causal masking is essential for autoregressive text generation <Paper corpusId=\"263831258\" paperTitle=\"(Woo et al., 2023)\" isShortName></Paper> <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>.\n\nThe evolution of these architectures reflects a growing understanding of how attention masking can be optimized for different applications. The T5 framework's introduction of \"PrefixLM\" demonstrated that hybrid approaches can combine the strengths of different masking patterns <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. By allowing bidirectional attention over a designated prefix while maintaining causal attention for generation, prefix models offer a balance between the comprehensive context understanding of encoder models and the autoregressive generation capabilities of decoder models <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThe practical implications of these architectural differences become evident in real-world applications. For instance, in translation tasks, prefix decoder-only models can apply fully-visible masking to the source text (e.g., \"translate English to French: I am doing well. Target:\") followed by causal masking for generating the target language output (\"je vais bien\") <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This capability to process different segments of the input with distinct attention patterns provides greater flexibility than the uniform approach of causal decoder-only models.\n\nThese architectural distinctions have shaped the evolution of modern language models, with each approach finding its niche in the broader landscape of natural language processing. While encoder-decoder models were the original Transformer architecture, the simplicity and scaling properties of decoder-only models have made them dominant in recent large language model development <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>. Nevertheless, the diversity of attention masking mechanisms continues to play a crucial role in addressing the varied requirements of different natural language processing tasks <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Raffel et al., 2019)",
                        "snippets": [
                            "A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern",
                            "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."
                        ],
                        "paper": {
                            "corpus_id": 204838007,
                            "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
                            "authors": [
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "1846258",
                                    "name": "Noam M. Shazeer"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "3844009",
                                    "name": "Katherine Lee"
                                },
                                {
                                    "authorId": "46617804",
                                    "name": "Sharan Narang"
                                },
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2389316",
                                    "name": "Yanqi Zhou"
                                },
                                {
                                    "authorId": "2157338362",
                                    "name": "Wei Li"
                                },
                                {
                                    "authorId": "35025299",
                                    "name": "Peter J. Liu"
                                }
                            ],
                            "year": 2019,
                            "venue": "Journal of machine learning research",
                            "n_citations": 20336
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Yi et al., 2024)",
                        "snippets": [
                            "Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. (Raffel et al., 2019)"
                        ],
                        "paper": {
                            "corpus_id": 268041362,
                            "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
                            "authors": [
                                {
                                    "authorId": "2287925430",
                                    "name": "Zihao Yi"
                                },
                                {
                                    "authorId": "2287922728",
                                    "name": "Jiarui Ouyang"
                                },
                                {
                                    "authorId": "2288039936",
                                    "name": "Yuwen Liu"
                                },
                                {
                                    "authorId": "2287923878",
                                    "name": "Tianhao Liao"
                                },
                                {
                                    "authorId": "2288033664",
                                    "name": "Zhe Xu"
                                },
                                {
                                    "authorId": "2288065597",
                                    "name": "Ying Shen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 72
                        },
                        "score": 0.7626953125
                    },
                    {
                        "id": "(Wang et al., 2022)",
                        "snippets": [
                            "A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."
                        ],
                        "paper": {
                            "corpus_id": 248118752,
                            "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?",
                            "authors": [
                                {
                                    "authorId": "2135734748",
                                    "name": "Thomas Wang"
                                },
                                {
                                    "authorId": "145625142",
                                    "name": "Adam Roberts"
                                },
                                {
                                    "authorId": "80424302",
                                    "name": "Daniel Hesslow"
                                },
                                {
                                    "authorId": "1379806208",
                                    "name": "Teven Le Scao"
                                },
                                {
                                    "authorId": "3351938",
                                    "name": "Hyung Won Chung"
                                },
                                {
                                    "authorId": "46181066",
                                    "name": "Iz Beltagy"
                                },
                                {
                                    "authorId": "143945447",
                                    "name": "Julien Launay"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 175
                        },
                        "score": 0.765625
                    },
                    {
                        "id": "(Ewer et al., 2024)",
                        "snippets": [
                            "In contrast, the causal decoder-only model (Brown et al., 2020)(Chowdhery et al., 2022) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2019)Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants)."
                        ],
                        "paper": {
                            "corpus_id": 273025546,
                            "title": "ENTP: Encoder-only Next Token Prediction",
                            "authors": [
                                {
                                    "authorId": "2323781863",
                                    "name": "Ethan Ewer"
                                },
                                {
                                    "authorId": "2253659910",
                                    "name": "Daewon Chae"
                                },
                                {
                                    "authorId": "2323820473",
                                    "name": "Thomas Zeng"
                                },
                                {
                                    "authorId": "2323851531",
                                    "name": "Jinkyu Kim"
                                },
                                {
                                    "authorId": "2323790154",
                                    "name": "Kangwook Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.57177734375
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021)",
                            "based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself",
                            "Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."
                        ],
                        "paper": {
                            "corpus_id": 271600495,
                            "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                            "authors": [
                                {
                                    "authorId": "2314473248",
                                    "name": "Mingcong Lu"
                                },
                                {
                                    "authorId": "2314649002",
                                    "name": "Jiangcai Zhu"
                                },
                                {
                                    "authorId": "2314113733",
                                    "name": "Wang Hao"
                                },
                                {
                                    "authorId": "2314323587",
                                    "name": "Zheng Li"
                                },
                                {
                                    "authorId": "2314311430",
                                    "name": "Shusheng Zhang"
                                },
                                {
                                    "authorId": "2314110211",
                                    "name": "Kailai Shao"
                                },
                                {
                                    "authorId": "2314192630",
                                    "name": "Chao Chen"
                                },
                                {
                                    "authorId": "2314343132",
                                    "name": "Nan Li"
                                },
                                {
                                    "authorId": "2324104105",
                                    "name": "Feng Wang"
                                },
                                {
                                    "authorId": "2324103820",
                                    "name": "Xin Lu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.744140625
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."
                        ],
                        "paper": {
                            "corpus_id": 266755678,
                            "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
                            "authors": [
                                {
                                    "authorId": "2116426849",
                                    "name": "Yi-Hsueh Liu"
                                },
                                {
                                    "authorId": "2155082967",
                                    "name": "Haoyang He"
                                },
                                {
                                    "authorId": "2184719751",
                                    "name": "Tianle Han"
                                },
                                {
                                    "authorId": "2273584640",
                                    "name": "Xu Zhang"
                                },
                                {
                                    "authorId": "2210636248",
                                    "name": "Mengyuan Liu"
                                },
                                {
                                    "authorId": "2257433902",
                                    "name": "Jiaming Tian"
                                },
                                {
                                    "authorId": "2257095790",
                                    "name": "Yutong Zhang"
                                },
                                {
                                    "authorId": "2110238778",
                                    "name": "Jiaqi Wang"
                                },
                                {
                                    "authorId": "2277869261",
                                    "name": "Xiaohui Gao"
                                },
                                {
                                    "authorId": "2215167446",
                                    "name": "Tianyang Zhong"
                                },
                                {
                                    "authorId": "2221032216",
                                    "name": "Yi Pan"
                                },
                                {
                                    "authorId": "2211904452",
                                    "name": "Shaochen Xu"
                                },
                                {
                                    "authorId": "2263593041",
                                    "name": "Zihao Wu"
                                },
                                {
                                    "authorId": "2145977326",
                                    "name": "Zheng Liu"
                                },
                                {
                                    "authorId": "2257586495",
                                    "name": "Xin Zhang"
                                },
                                {
                                    "authorId": "2277750447",
                                    "name": "Shu Zhang"
                                },
                                {
                                    "authorId": "1742535",
                                    "name": "Xintao Hu"
                                },
                                {
                                    "authorId": "49104946",
                                    "name": "Tuo Zhang"
                                },
                                {
                                    "authorId": "2251076040",
                                    "name": "Ning Qiang"
                                },
                                {
                                    "authorId": "2254792886",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2257302793",
                                    "name": "Bao Ge"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neurocomputing",
                            "n_citations": 74
                        },
                        "score": 0.654296875
                    },
                    {
                        "id": "(Patil et al., 2024)",
                        "snippets": [
                            "For example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence."
                        ],
                        "paper": {
                            "corpus_id": 268157336,
                            "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)",
                            "authors": [
                                {
                                    "authorId": "2289385425",
                                    "name": "Rajvardhan Patil"
                                },
                                {
                                    "authorId": "117730513",
                                    "name": "Venkat Gudivada"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Sciences",
                            "n_citations": 80
                        },
                        "score": 0.97265625
                    },
                    {
                        "id": "(Kim et al., 2023)",
                        "snippets": [
                            "Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation."
                        ],
                        "paper": {
                            "corpus_id": 260886785,
                            "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models",
                            "authors": [
                                {
                                    "authorId": "2141320070",
                                    "name": "Minsoo Kim"
                                },
                                {
                                    "authorId": "2144376191",
                                    "name": "Sihwa Lee"
                                },
                                {
                                    "authorId": "2265920992",
                                    "name": "Janghwan Lee"
                                },
                                {
                                    "authorId": "2158125346",
                                    "name": "S. Hong"
                                },
                                {
                                    "authorId": "2180828053",
                                    "name": "Duhyeuk Chang"
                                },
                                {
                                    "authorId": "66936521",
                                    "name": "Wonyong Sung"
                                },
                                {
                                    "authorId": "2506452",
                                    "name": "Jungwook Choi"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 15
                        },
                        "score": 0.5078125
                    },
                    {
                        "id": "(Woo et al., 2023)",
                        "snippets": [
                            "Causal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. (Dong et al., 2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example."
                        ],
                        "paper": {
                            "corpus_id": 263831258,
                            "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
                            "authors": [
                                {
                                    "authorId": "151488390",
                                    "name": "Gerald Woo"
                                },
                                {
                                    "authorId": "2257133300",
                                    "name": "Chenghao Liu"
                                },
                                {
                                    "authorId": "2257090206",
                                    "name": "Akshat Kumar"
                                },
                                {
                                    "authorId": "36187119",
                                    "name": "Doyen Sahoo"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.56689453125
                    },
                    {
                        "id": "(Dong et al., 2019)",
                        "snippets": [
                            "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."
                        ],
                        "paper": {
                            "corpus_id": 147704286,
                            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
                            "authors": [
                                {
                                    "authorId": "145307652",
                                    "name": "Li Dong"
                                },
                                {
                                    "authorId": "144610884",
                                    "name": "Nan Yang"
                                },
                                {
                                    "authorId": "51456429",
                                    "name": "Wenhui Wang"
                                },
                                {
                                    "authorId": "49807919",
                                    "name": "Furu Wei"
                                },
                                {
                                    "authorId": "46522098",
                                    "name": "Xiaodong Liu"
                                },
                                {
                                    "authorId": "72682749",
                                    "name": "Yu Wang"
                                },
                                {
                                    "authorId": "1800422",
                                    "name": "Jianfeng Gao"
                                },
                                {
                                    "authorId": "143849609",
                                    "name": "M. Zhou"
                                },
                                {
                                    "authorId": "145058181",
                                    "name": "H. Hon"
                                }
                            ],
                            "year": 2019,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 1560
                        },
                        "score": 0
                    },
                    {
                        "id": "(Suganthan et al., 2025)",
                        "snippets": [
                            "Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2019), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."
                        ],
                        "paper": {
                            "corpus_id": 276771845,
                            "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks",
                            "authors": [
                                {
                                    "authorId": "1658871094",
                                    "name": "P. Suganthan"
                                },
                                {
                                    "authorId": "2165469946",
                                    "name": "Fedor Moiseev"
                                },
                                {
                                    "authorId": "2348489099",
                                    "name": "Le Yan"
                                },
                                {
                                    "authorId": "2261361394",
                                    "name": "Junru Wu"
                                },
                                {
                                    "authorId": "2348507846",
                                    "name": "Jianmo Ni"
                                },
                                {
                                    "authorId": "2348488953",
                                    "name": "Jay Han"
                                },
                                {
                                    "authorId": "1954563",
                                    "name": "I. Zitouni"
                                },
                                {
                                    "authorId": "1727837",
                                    "name": "Enrique Alfonseca"
                                },
                                {
                                    "authorId": "2348422460",
                                    "name": "Xuanhui Wang"
                                },
                                {
                                    "authorId": "2349772191",
                                    "name": "Zhe Dong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.92333984375
                    },
                    {
                        "id": "(Brown et al., 2020)",
                        "snippets": [
                            "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."
                        ],
                        "paper": {
                            "corpus_id": 218971783,
                            "title": "Language Models are Few-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "31035595",
                                    "name": "Tom B. Brown"
                                },
                                {
                                    "authorId": "2056658938",
                                    "name": "Benjamin Mann"
                                },
                                {
                                    "authorId": "39849748",
                                    "name": "Nick Ryder"
                                },
                                {
                                    "authorId": "2065894334",
                                    "name": "Melanie Subbiah"
                                },
                                {
                                    "authorId": "152724169",
                                    "name": "J. Kaplan"
                                },
                                {
                                    "authorId": "6515819",
                                    "name": "Prafulla Dhariwal"
                                },
                                {
                                    "authorId": "2072676",
                                    "name": "Arvind Neelakantan"
                                },
                                {
                                    "authorId": "67311962",
                                    "name": "Pranav Shyam"
                                },
                                {
                                    "authorId": "144864359",
                                    "name": "Girish Sastry"
                                },
                                {
                                    "authorId": "119609682",
                                    "name": "Amanda Askell"
                                },
                                {
                                    "authorId": "144517868",
                                    "name": "Sandhini Agarwal"
                                },
                                {
                                    "authorId": "1404060687",
                                    "name": "Ariel Herbert-Voss"
                                },
                                {
                                    "authorId": "2064404342",
                                    "name": "Gretchen Krueger"
                                },
                                {
                                    "authorId": "103143311",
                                    "name": "T. Henighan"
                                },
                                {
                                    "authorId": "48422824",
                                    "name": "R. Child"
                                },
                                {
                                    "authorId": "1992922591",
                                    "name": "A. Ramesh"
                                },
                                {
                                    "authorId": "2052152920",
                                    "name": "Daniel M. Ziegler"
                                },
                                {
                                    "authorId": "49387725",
                                    "name": "Jeff Wu"
                                },
                                {
                                    "authorId": "2059411355",
                                    "name": "Clemens Winter"
                                },
                                {
                                    "authorId": "144239765",
                                    "name": "Christopher Hesse"
                                },
                                {
                                    "authorId": "2108828435",
                                    "name": "Mark Chen"
                                },
                                {
                                    "authorId": "2064673055",
                                    "name": "Eric Sigler"
                                },
                                {
                                    "authorId": "1380985420",
                                    "name": "Ma-teusz Litwin"
                                },
                                {
                                    "authorId": "145565184",
                                    "name": "Scott Gray"
                                },
                                {
                                    "authorId": "1490681878",
                                    "name": "Benjamin Chess"
                                },
                                {
                                    "authorId": "2115193883",
                                    "name": "Jack Clark"
                                },
                                {
                                    "authorId": "133740015",
                                    "name": "Christopher Berner"
                                },
                                {
                                    "authorId": "52238703",
                                    "name": "Sam McCandlish"
                                },
                                {
                                    "authorId": "38909097",
                                    "name": "Alec Radford"
                                },
                                {
                                    "authorId": "1701686",
                                    "name": "I. Sutskever"
                                },
                                {
                                    "authorId": "2698777",
                                    "name": "Dario Amodei"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 42437
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.161313
    }
}