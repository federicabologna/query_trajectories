{
    "query": "lot specialized language models to build a general system combine abilities different models this unified model be good the single large LM",
    "user_id": "lib_user",
    "task_id": "db442306-2063-456e-856e-64c743f46a16",
    "timestamp": "2025-06-23T22:39:35.991588",
    "n_retrieval": 256,
    "n_retrieved": 273,
    "n_candidates": 10,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.187077,
    "decomposed_query": {
        "rewritten_query": "Using specialized language models to build a general system that combines abilities of different models, comparing if this unified model would be better than a single large language model.",
        "keyword_query": "specialized language models general system combine abilities unified model single large language model",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.00987,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "reference_count": 56,
            "citation_count": 29,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2023.findings-emnlp.552.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.14628, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152358188",
                    "name": "Chenglei Si"
                },
                {
                    "authorId": "3040379",
                    "name": "Weijia Shi"
                },
                {
                    "authorId": "145756130",
                    "name": "Chen Zhao"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "1389036863",
                    "name": "Jordan L. Boyd-Graber"
                }
            ],
            "abstract": "While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MoRE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output. We release all code and data to facilitate future work.",
            "corpus_id": 258865893,
            "sentences": [
                {
                    "corpus_id": "258865893",
                    "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
                    "text": "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively. \n\nTo obtain the most capable specialist models for each reasoning type, we leverage specialized prompting strategies such as Chain-of-Thought (Wei et al., 2022b) prompting and retrieval-augmented prompting. Experiments on our collection of 12 QA datasets across four diverse reasoning types confirm that our specialist models outperform the backbone model without specialization, but they achieve much lower accuracy on question types outside of their expertise. \n\nWith these specialized models, we propose our MORE framework to combine their strengths. MORE selects the best candidate answer from the pool of specialized models, and we teach MORE to abstain from answering if none of the candidate answers are correct. We design our answer selector based on these indicative features: (1) the match between the question type and each specialized model's expertise; (2) the confidence of each specialized model and the characteristics of their predictions; and (3) the agreement among all specialized models, which is a novel feature that we propose. Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types. \n\nApart from the improved generalizability of MORE, an important byproduct of cross-checking among specialized experts is to offer a useful signal for understanding the whole system's working mechanism. This is validated by the experimental results showing that incorporating agreement among different specialized experts leads to better selective QA results-where the system answers as many questions as possible while maintaining high accuracy-and presenting such internal decision processes to human annotators helps them determine the correctness of the system predictions more accurately and in a shorter time.",
                    "score": 0.44679986253757326,
                    "section_title": "Introduction",
                    "char_start_offset": 1761,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 221
                        },
                        {
                            "start": 222,
                            "end": 603
                        },
                        {
                            "start": 604,
                            "end": 704
                        },
                        {
                            "start": 707,
                            "end": 911
                        },
                        {
                            "start": 912,
                            "end": 1167
                        },
                        {
                            "start": 1170,
                            "end": 1258
                        },
                        {
                            "start": 1259,
                            "end": 1424
                        },
                        {
                            "start": 1425,
                            "end": 1755
                        },
                        {
                            "start": 1756,
                            "end": 1930
                        },
                        {
                            "start": 1933,
                            "end": 2133
                        },
                        {
                            "start": 2134,
                            "end": 2546
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 847,
                            "end": 866,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75390625
                },
                {
                    "corpus_id": "258865893",
                    "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
                    "text": "While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MoRE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output. We release all code and data to facilitate future work.",
                    "score": 0.39802521138919233,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74951171875
                }
            ],
            "relevance_judgement": 0.75390625,
            "relevance_judgment_input_expanded": "# Title: Getting MoRE out of Mixture of Language Model Reasoning Experts\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan L. Boyd-Graber\n## Abstract\nWhile recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MoRE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output. We release all code and data to facilitate future work.\n## Introduction\nTherefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively. \n\nTo obtain the most capable specialist models for each reasoning type, we leverage specialized prompting strategies such as Chain-of-Thought (Wei et al., 2022b) prompting and retrieval-augmented prompting. Experiments on our collection of 12 QA datasets across four diverse reasoning types confirm that our specialist models outperform the backbone model without specialization, but they achieve much lower accuracy on question types outside of their expertise. \n\nWith these specialized models, we propose our MORE framework to combine their strengths. MORE selects the best candidate answer from the pool of specialized models, and we teach MORE to abstain from answering if none of the candidate answers are correct. We design our answer selector based on these indicative features: (1) the match between the question type and each specialized model's expertise; (2) the confidence of each specialized model and the characteristics of their predictions; and (3) the agreement among all specialized models, which is a novel feature that we propose. Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types. \n\nApart from the improved generalizability of MORE, an important byproduct of cross-checking among specialized experts is to offer a useful signal for understanding the whole system's working mechanism. This is validated by the experimental results showing that incorporating agreement among different specialized experts leads to better selective QA results-where the system answers as many questions as possible while maintaining high accuracy-and presenting such internal decision processes to human annotators helps them determine the correctness of the system predictions more accurately and in a shorter time.",
            "reference_string": "[258865893 | Si et al. | 2023 | Citations: 29]"
        },
        {
            "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
            "venue": "International Conference on Machine Learning",
            "year": 2024,
            "reference_count": 72,
            "citation_count": 38,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.05859, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1582888954",
                    "name": "Mohammed Muqeeth"
                },
                {
                    "authorId": "48447436",
                    "name": "Haokun Liu"
                },
                {
                    "authorId": "2283449435",
                    "name": "Yufan Liu"
                },
                {
                    "authorId": "2269733851",
                    "name": "Colin Raffel"
                }
            ],
            "abstract": "Recently, there has been a widespread proliferation of\"expert\"language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range of specialized model collections and zero-shot generalization benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access). To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE's performance stems from its ability to make adaptive per-token and per-module expert choices. We release all of our code to support future work on improving zero-shot generalization by recycling specialized experts.",
            "corpus_id": 267547973,
            "sentences": [
                {
                    "corpus_id": "267547973",
                    "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
                    "text": "The allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;Wei et al., 2021;Mishra et al., 2022), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access. \n\nTo tackle this problem, most past work (Jang et al., 2023;Belofsky, 2023;Durbin, 2024;Maxine, 2023) learns a posthoc routing strategy by comparing an embedding of the input query to the average embedding of examples in each dataset used to train each expert. Such methods implicitly assume there is a single expert well-suited for the query and hope that the retrieval algorithm can accurately identify this best expert. However, Jang et al. (2023) showed that such approaches lag behind an \"oracle\" router that always chooses the best expert for a given query. To explore alternative routing approaches, we first note that many PEFT methods typically insert small trainable modules in many places across the model (e.g. at each weight matrix (Hu et al., 2021)). Meanwhile, many sparsely gated Mixture-of-Experts models make routing decisions separately for each token (Shazeer et al., 2016;Fedus et al., 2022;Du et al., 2022). In this work, we therefore explore the angle of improving zero-shot generalization through adaptive per-token and per-module routing.",
                    "score": 0.41533628383991983,
                    "section_title": "Introduction",
                    "char_start_offset": 2225,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 247
                        },
                        {
                            "start": 248,
                            "end": 496
                        },
                        {
                            "start": 497,
                            "end": 630
                        },
                        {
                            "start": 631,
                            "end": 953
                        },
                        {
                            "start": 956,
                            "end": 1214
                        },
                        {
                            "start": 1215,
                            "end": 1376
                        },
                        {
                            "start": 1377,
                            "end": 1517
                        },
                        {
                            "start": 1518,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1883
                        },
                        {
                            "start": 1884,
                            "end": 2017
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 788,
                            "end": 805,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 805,
                            "end": 825,
                            "matchedPaperCorpusId": "237421373"
                        },
                        {
                            "start": 1825,
                            "end": 1847,
                            "matchedPaperCorpusId": "12462234"
                        },
                        {
                            "start": 1866,
                            "end": 1882,
                            "matchedPaperCorpusId": "245124124"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.73095703125
                }
            ],
            "relevance_judgement": 0.73095703125,
            "relevance_judgment_input_expanded": "# Title: Learning to Route Among Specialized Experts for Zero-Shot Generalization\n# Venue: International Conference on Machine Learning\n# Authors: Mohammed Muqeeth, Haokun Liu, Yufan Liu, Colin Raffel\n## Abstract\nRecently, there has been a widespread proliferation of\"expert\"language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range of specialized model collections and zero-shot generalization benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access). To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE's performance stems from its ability to make adaptive per-token and per-module expert choices. We release all of our code to support future work on improving zero-shot generalization by recycling specialized experts.\n## Introduction\nThe allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;Wei et al., 2021;Mishra et al., 2022), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access. \n\nTo tackle this problem, most past work (Jang et al., 2023;Belofsky, 2023;Durbin, 2024;Maxine, 2023) learns a posthoc routing strategy by comparing an embedding of the input query to the average embedding of examples in each dataset used to train each expert. Such methods implicitly assume there is a single expert well-suited for the query and hope that the retrieval algorithm can accurately identify this best expert. However, Jang et al. (2023) showed that such approaches lag behind an \"oracle\" router that always chooses the best expert for a given query. To explore alternative routing approaches, we first note that many PEFT methods typically insert small trainable modules in many places across the model (e.g. at each weight matrix (Hu et al., 2021)). Meanwhile, many sparsely gated Mixture-of-Experts models make routing decisions separately for each token (Shazeer et al., 2016;Fedus et al., 2022;Du et al., 2022). In this work, we therefore explore the angle of improving zero-shot generalization through adaptive per-token and per-module routing.",
            "reference_string": "[267547973 | Muqeeth et al. | 2024 | Citations: 38]"
        },
        {
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 54,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.00985, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34920835",
                    "name": "Ayan Sengupta"
                },
                {
                    "authorId": "2345922770",
                    "name": "Yash Goel"
                },
                {
                    "authorId": "2249914540",
                    "name": "Tanmoy Chakraborty"
                }
            ],
            "abstract": "We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.",
            "corpus_id": 278310893,
            "sentences": [
                {
                    "corpus_id": "278310893",
                    "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                    "text": "This targeted training approach eliminates wasteful trial-and-error methods typically associated with domain adaptation, as each model receives only the most relevant data for its specialization. \n\nThe final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020;Yadav et al., 2024), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost. \n\nThe success of this pipeline relies on careful orchestration between components and systematic empirical validation. Our approach addresses three critical challenges in modern language model development: reducing computational requirements through strategic pruning, maintaining performance through targeted domain adaptation, and combining specialized capabilities through sophisticated ensemble methods. The result is a more efficient, domain-aware language model that achieves high performance without excessive computational demands or model scale.",
                    "score": 0.5129729490733902,
                    "section_title": "Proposed Downscaling Pipeline",
                    "char_start_offset": 25479,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 198,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 706
                        },
                        {
                            "start": 707,
                            "end": 861
                        },
                        {
                            "start": 862,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1134
                        },
                        {
                            "start": 1137,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1542
                        },
                        {
                            "start": 1543,
                            "end": 1689
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 499,
                            "end": 516,
                            "matchedPaperCorpusId": "220496384"
                        },
                        {
                            "start": 516,
                            "end": 535,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6923828125
                }
            ],
            "relevance_judgement": 0.6923828125,
            "relevance_judgment_input_expanded": "# Title: Position: Enough of Scaling LLMs! Lets Focus on Downscaling\n# Venue: arXiv.org\n# Authors: Ayan Sengupta, Yash Goel, Tanmoy Chakraborty\n## Abstract\nWe challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.\n## Proposed Downscaling Pipeline\nThis targeted training approach eliminates wasteful trial-and-error methods typically associated with domain adaptation, as each model receives only the most relevant data for its specialization. \n\nThe final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020;Yadav et al., 2024), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost. \n\nThe success of this pipeline relies on careful orchestration between components and systematic empirical validation. Our approach addresses three critical challenges in modern language model development: reducing computational requirements through strategic pruning, maintaining performance through targeted domain adaptation, and combining specialized capabilities through sophisticated ensemble methods. The result is a more efficient, domain-aware language model that achieves high performance without excessive computational demands or model scale.",
            "reference_string": "[278310893 | Sengupta et al. | 2025 | Citations: 0]"
        },
        {
            "title": "What Matters for Model Merging at Scale?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 89,
            "citation_count": 22,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46841632",
                    "name": "Prateek Yadav"
                },
                {
                    "authorId": "144244743",
                    "name": "Tu Vu"
                },
                {
                    "authorId": "2325489176",
                    "name": "Jonathan Lai"
                },
                {
                    "authorId": "2324583448",
                    "name": "Alexandra Chronopoulou"
                },
                {
                    "authorId": "1779225",
                    "name": "Manaal Faruqui"
                },
                {
                    "authorId": "2253396640",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2227827",
                    "name": "Tsendsuren Munkhdalai"
                }
            ],
            "abstract": "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.",
            "corpus_id": 273162841,
            "sentences": [
                {
                    "corpus_id": "273162841",
                    "title": "What Matters for Model Merging at Scale?",
                    "text": "Model merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
                    "score": 0.46900152188672606,
                    "section_title": "BACKGROUND",
                    "char_start_offset": 6956,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 84
                        },
                        {
                            "start": 85,
                            "end": 413
                        },
                        {
                            "start": 414,
                            "end": 563
                        },
                        {
                            "start": 566,
                            "end": 689
                        },
                        {
                            "start": 690,
                            "end": 795
                        },
                        {
                            "start": 796,
                            "end": 935
                        },
                        {
                            "start": 936,
                            "end": 1105
                        },
                        {
                            "start": 1106,
                            "end": 1289
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 257,
                            "end": 276,
                            "matchedPaperCorpusId": "270843326"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.640625
                }
            ],
            "relevance_judgement": 0.640625,
            "relevance_judgment_input_expanded": "# Title: What Matters for Model Merging at Scale?\n# Venue: arXiv.org\n# Authors: Prateek Yadav, Tu Vu, Jonathan Lai, Alexandra Chronopoulou, Manaal Faruqui, Mohit Bansal, Tsendsuren Munkhdalai\n## Abstract\nModel merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors -- like the base model quality and number of expert models -- , to affect the merged model's performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using 4 popular merging methods -- Averaging, Task~Arithmetic, Dare, and TIES -- across model sizes ranging from 1B-64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert's training tasks, and zero-shot generalization to unseen held-out tasks. Our experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging 8 large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.\n## BACKGROUND\nModel merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
            "reference_string": "[273162841 | Yadav et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.19610, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313634509",
                    "name": "Mohammed Al-Maamari"
                },
                {
                    "authorId": "2074055134",
                    "name": "Mehdi Ben Amor"
                },
                {
                    "authorId": "2259357506",
                    "name": "Michael Granitzer"
                }
            ],
            "abstract": "This research combines Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient multilingual language models. Key objectives include evaluating adaptive versus fixed alpha methods in KD and comparing modular MoE architectures for handling multi-domain inputs and preventing catastrophic forgetting. KD compresses large language models (LLMs) into smaller, efficient models, while MoE enhances modularity with specialized tasks. Experiments showed similar performance for both KD methods, with marginal improvements from adaptive alpha. A combined loss approach provided more stable learning. The router, trained to classify input sequences into English, French, German, or Python, achieved 99.95% precision, recall, and F1 score, with Logistic Regression being the most effective classifier. Evaluations of modular MoE architectures revealed that Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed similarly, while the MoE with Common Expert (MoE-CE) setup showed slightly lower performance. Including a common expert in MoE-CE improved its performance. Studies on catastrophic forgetting indicated that sequential training led to significant forgetting, while single-session training with balanced batches and the MoE approach mitigated this issue. The MoE architecture preserved knowledge across multiple languages effectively. The research contributes open-sourced resources including the dataset (https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation tool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the research codebase (https://github.com/ModMaamari/mixture-modular-experts).",
            "corpus_id": 271533631,
            "sentences": [
                {
                    "corpus_id": "271533631",
                    "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
                    "text": "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models. \n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7]. \n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]. \n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting.",
                    "score": 0.480645263487433,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 185
                        },
                        {
                            "start": 186,
                            "end": 334
                        },
                        {
                            "start": 335,
                            "end": 541
                        },
                        {
                            "start": 544,
                            "end": 692
                        },
                        {
                            "start": 693,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 984
                        },
                        {
                            "start": 985,
                            "end": 1172
                        },
                        {
                            "start": 1175,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1881
                        },
                        {
                            "start": 1882,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2144
                        },
                        {
                            "start": 2145,
                            "end": 2304
                        },
                        {
                            "start": 2307,
                            "end": 2613
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 132,
                            "end": 135,
                            "matchedPaperCorpusId": "11280500"
                        },
                        {
                            "start": 181,
                            "end": 184,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 637,
                            "end": 640,
                            "matchedPaperCorpusId": "211532645"
                        },
                        {
                            "start": 1384,
                            "end": 1387,
                            "matchedPaperCorpusId": "256808267"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.630859375
                }
            ],
            "relevance_judgement": 0.630859375,
            "relevance_judgment_input_expanded": "# Title: Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models\n# Venue: arXiv.org\n# Authors: Mohammed Al-Maamari, Mehdi Ben Amor, Michael Granitzer\n## Abstract\nThis research combines Knowledge Distillation (KD) and Mixture of Experts (MoE) to develop modular, efficient multilingual language models. Key objectives include evaluating adaptive versus fixed alpha methods in KD and comparing modular MoE architectures for handling multi-domain inputs and preventing catastrophic forgetting. KD compresses large language models (LLMs) into smaller, efficient models, while MoE enhances modularity with specialized tasks. Experiments showed similar performance for both KD methods, with marginal improvements from adaptive alpha. A combined loss approach provided more stable learning. The router, trained to classify input sequences into English, French, German, or Python, achieved 99.95% precision, recall, and F1 score, with Logistic Regression being the most effective classifier. Evaluations of modular MoE architectures revealed that Pre-trained Language Experts (PLE) and Joint Expert Embedding Training (JEET) performed similarly, while the MoE with Common Expert (MoE-CE) setup showed slightly lower performance. Including a common expert in MoE-CE improved its performance. Studies on catastrophic forgetting indicated that sequential training led to significant forgetting, while single-session training with balanced batches and the MoE approach mitigated this issue. The MoE architecture preserved knowledge across multiple languages effectively. The research contributes open-sourced resources including the dataset (https://zenodo.org/doi/10.5281/zenodo.12677631), a balanced dataset creation tool (https://github.com/padas-lab-de/multi-language-dataset-creator), and the research codebase (https://github.com/ModMaamari/mixture-modular-experts).\n## Introduction\nLanguage models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models. \n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7]. \n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]. \n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting.",
            "reference_string": "[271533631 | Al-Maamari et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 63,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262512474",
                    "name": "Zhenyi Lu"
                },
                {
                    "authorId": "2277238906",
                    "name": "Chenghao Fan"
                },
                {
                    "authorId": "2284721764",
                    "name": "Wei Wei"
                },
                {
                    "authorId": "2262446609",
                    "name": "Xiaoye Qu"
                },
                {
                    "authorId": "2182623368",
                    "name": "Dangyang Chen"
                },
                {
                    "authorId": "2284687448",
                    "name": "Yu Cheng"
                }
            ],
            "abstract": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}",
            "corpus_id": 270702345,
            "sentences": [
                {
                    "corpus_id": "270702345",
                    "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                    "text": "In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
                    "score": 0.4970069483341095,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 555
                        },
                        {
                            "start": 558,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1865
                        },
                        {
                            "start": 1868,
                            "end": 2036
                        },
                        {
                            "start": 2037,
                            "end": 2200
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 340,
                            "end": 343,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 550,
                            "end": 554,
                            "matchedPaperCorpusId": "232110907"
                        },
                        {
                            "start": 677,
                            "end": 681,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 681,
                            "end": 684,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 684,
                            "end": 687,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 687,
                            "end": 690,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 929,
                            "end": 932,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1592,
                            "end": 1596,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 1596,
                            "end": 1599,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 1646,
                            "end": 1650,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1650,
                            "end": 1653,
                            "matchedPaperCorpusId": "258832777"
                        },
                        {
                            "start": 1653,
                            "end": 1656,
                            "matchedPaperCorpusId": "263831551"
                        },
                        {
                            "start": 1656,
                            "end": 1659,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 2028,
                            "end": 2032,
                            "matchedPaperCorpusId": "258865647"
                        },
                        {
                            "start": 2032,
                            "end": 2035,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.623046875
                }
            ],
            "relevance_judgement": 0.623046875,
            "relevance_judgment_input_expanded": "# Title: Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging\n# Venue: Neural Information Processing Systems\n# Authors: Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, Yu Cheng\n## Abstract\nIn the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}\n## Introduction\nIn recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
            "reference_string": "[270702345 | Lu et al. | 2024 | Citations: 63]"
        },
        {
            "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 69,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.08281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46649145",
                    "name": "Ning Ding"
                },
                {
                    "authorId": "2135835258",
                    "name": "Yulin Chen"
                },
                {
                    "authorId": "52297757",
                    "name": "Ganqu Cui"
                },
                {
                    "authorId": "2221271501",
                    "name": "Xingtai Lv"
                },
                {
                    "authorId": "2257007994",
                    "name": "Ruobing Xie"
                },
                {
                    "authorId": "2218723159",
                    "name": "Bowen Zhou"
                },
                {
                    "authorId": "2273470196",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                }
            ],
            "abstract": "Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content. This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains.",
            "corpus_id": 268379027,
            "sentences": [
                {
                    "corpus_id": "268379027",
                    "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
                    "text": "However, in general, semantics in language, logic and structures in code, and abstract symbol manipulations in math intricately always create a situation of mutual weakening.To elaborate further, comparing highly specialized models (such as those for coding or mathematics) with general-purpose models capable of performing all tasks (like GPT-4) for their expertise is a trap that can easily lead to misinformation.This paper hopes to integrate specialized abilities into a general chat language model with as little loss as possible.\n\nMore specifically, we propose to leverage separate models that are already highly specialized via a fusing structure.In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. 1 To ensure that the fused model benefits from the Figure 1: Performance on three different domains of specialized models and our ULTRAFUSER.The performance for the text domain is computed by the average results on TruthfulQA (Acc) (Lin et al., 2021) and AlpacaEval (Win Rate) (Li et al., 2023b) datasets; the performance for the code domain is Pass@1 of HumanEval (Chen et al., 2021); and the performance for the math domain is the average result of GSM8K (Pass@1) (Cobbe et al., 2021), MATH (Pass@1) (Hendrycks et al., 2021), SAT-Math (Acc) (Zhong et al., 2023), and AQuA-RAT (Acc) (Ling et al., 2017) datasets.All numbers are zero-shot results.specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data.Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model.The key to functioning the model is to train the gating module.For example, when the model conducts code generation, we want the coding specialist to contribute more than the other two.This necessitates a mixed instruction tuning dataset that contains the three domains for the training.",
                    "score": 0.398533088984083,
                    "section_title": "Introduction",
                    "char_start_offset": 1854,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 174,
                            "end": 416
                        },
                        {
                            "start": 416,
                            "end": 535
                        },
                        {
                            "start": 537,
                            "end": 654
                        },
                        {
                            "start": 654,
                            "end": 925
                        },
                        {
                            "start": 925,
                            "end": 1397
                        },
                        {
                            "start": 1397,
                            "end": 1431
                        },
                        {
                            "start": 1431,
                            "end": 1676
                        },
                        {
                            "start": 1676,
                            "end": 1829
                        },
                        {
                            "start": 1829,
                            "end": 1892
                        },
                        {
                            "start": 1892,
                            "end": 2014
                        },
                        {
                            "start": 2014,
                            "end": 2116
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1368,
                            "end": 1387,
                            "matchedPaperCorpusId": "12777818"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61328125
                },
                {
                    "corpus_id": "268379027",
                    "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
                    "text": "If a piece of information can be serialized and tokenized, it is likely to be handled by large language models (LLMs) (Bommasani et al., 2021;Brown et al., 2020;Ope-nAI, 2023).LLMs, as one of the most advanced manifestations of artificial intelligence, have demonstrated proficiency in three representative symbol systems that are essential to human progress: natural language (Ouyang et al., 2022;Bai et al., 2022), which forms the cornerstone of human inter-action; programming code (Li et al., 2023a;Rozi\u00e8re et al., 2023), the backbone of our digital ecosystem; and mathematical reasoning, the framework underpinning scientific advancement (Luo et al., 2023a;Yang et al., 2023).The mastery of three domains would equip LLMs with unparalleled versatility.However, the intrinsic variability of data distribution across these domains presents a formidable challenge for an LLM to achieve consistently high performance at the same time.One awkward situation is that it is challenging to integrate professional-level coding and mathematical abilities into a general conversational language model without loss.In other words, these skills are more often reflected in the numbers on related benchmarks rather than a real-world user interface.\n\nFigure 1 (a-c) demonstrates such a struggle by presenting the performance of three specialized models on the aforementioned domains, all initially based on the Llama-2 (Touvron et al., 2023) 13B architecture.Our findings reveal a clear trade-off: specialized training in one domain often comes at the expense of performance in the others, whereas training on all three types of data at the same time results in a simultaneous suboptimal situation.Delving into this situation, such an issue may be partially mitigated by careful designs of data engineering, training strategy, or prompt construction.However, in general, semantics in language, logic and structures in code, and abstract symbol manipulations in math intricately always create a situation of mutual weakening.",
                    "score": 0.42591170365907827,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 176
                        },
                        {
                            "start": 176,
                            "end": 681
                        },
                        {
                            "start": 681,
                            "end": 757
                        },
                        {
                            "start": 757,
                            "end": 935
                        },
                        {
                            "start": 935,
                            "end": 1107
                        },
                        {
                            "start": 1107,
                            "end": 1238
                        },
                        {
                            "start": 1240,
                            "end": 1448
                        },
                        {
                            "start": 1448,
                            "end": 1687
                        },
                        {
                            "start": 1687,
                            "end": 1839
                        },
                        {
                            "start": 1839,
                            "end": 2013
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 142,
                            "end": 161,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 377,
                            "end": 398,
                            "matchedPaperCorpusId": "246426909"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.26220703125
                }
            ],
            "relevance_judgement": 0.61328125,
            "relevance_judgment_input_expanded": "# Title: Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models\n# Venue: arXiv.org\n# Authors: Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun\n## Abstract\nUnderlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content. This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains.\n## Introduction\nIf a piece of information can be serialized and tokenized, it is likely to be handled by large language models (LLMs) (Bommasani et al., 2021;Brown et al., 2020;Ope-nAI, 2023).LLMs, as one of the most advanced manifestations of artificial intelligence, have demonstrated proficiency in three representative symbol systems that are essential to human progress: natural language (Ouyang et al., 2022;Bai et al., 2022), which forms the cornerstone of human inter-action; programming code (Li et al., 2023a;Rozi\u00e8re et al., 2023), the backbone of our digital ecosystem; and mathematical reasoning, the framework underpinning scientific advancement (Luo et al., 2023a;Yang et al., 2023).The mastery of three domains would equip LLMs with unparalleled versatility.However, the intrinsic variability of data distribution across these domains presents a formidable challenge for an LLM to achieve consistently high performance at the same time.One awkward situation is that it is challenging to integrate professional-level coding and mathematical abilities into a general conversational language model without loss.In other words, these skills are more often reflected in the numbers on related benchmarks rather than a real-world user interface.\n\nFigure 1 (a-c) demonstrates such a struggle by presenting the performance of three specialized models on the aforementioned domains, all initially based on the Llama-2 (Touvron et al., 2023) 13B architecture.Our findings reveal a clear trade-off: specialized training in one domain often comes at the expense of performance in the others, whereas training on all three types of data at the same time results in a simultaneous suboptimal situation.Delving into this situation, such an issue may be partially mitigated by careful designs of data engineering, training strategy, or prompt construction.However, in general, semantics in language, logic and structures in code, and abstract symbol manipulations in math intricately always create a situation of mutual weakening.\n...\nHowever, in general, semantics in language, logic and structures in code, and abstract symbol manipulations in math intricately always create a situation of mutual weakening.To elaborate further, comparing highly specialized models (such as those for coding or mathematics) with general-purpose models capable of performing all tasks (like GPT-4) for their expertise is a trap that can easily lead to misinformation.This paper hopes to integrate specialized abilities into a general chat language model with as little loss as possible.\n\nMore specifically, we propose to leverage separate models that are already highly specialized via a fusing structure.In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. 1 To ensure that the fused model benefits from the Figure 1: Performance on three different domains of specialized models and our ULTRAFUSER.The performance for the text domain is computed by the average results on TruthfulQA (Acc) (Lin et al., 2021) and AlpacaEval (Win Rate) (Li et al., 2023b) datasets; the performance for the code domain is Pass@1 of HumanEval (Chen et al., 2021); and the performance for the math domain is the average result of GSM8K (Pass@1) (Cobbe et al., 2021), MATH (Pass@1) (Hendrycks et al., 2021), SAT-Math (Acc) (Zhong et al., 2023), and AQuA-RAT (Acc) (Ling et al., 2017) datasets.All numbers are zero-shot results.specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data.Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model.The key to functioning the model is to train the gating module.For example, when the model conducts code generation, we want the coding specialist to contribute more than the other two.This necessitates a mixed instruction tuning dataset that contains the three domains for the training.",
            "reference_string": "[268379027 | Ding et al. | 2024 | Citations: 7]"
        },
        {
            "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 69,
            "citation_count": 5,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03735, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2529182",
                    "name": "David Grangier"
                },
                {
                    "authorId": "2324421747",
                    "name": "Simin Fan"
                },
                {
                    "authorId": "2324783252",
                    "name": "Skyler Seto"
                },
                {
                    "authorId": "1763708",
                    "name": "Pierre Ablin"
                }
            ],
            "abstract": "Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead. We propose a novel method, ClusteRed Importance SamPling (CRISP). CRISP clusters the generalist dataset and samples from these clusters based on their frequencies in the smaller specialist dataset. It is scalable, suitable for both pretraining and continued pretraining, and works well in multi-task settings. CRISP performs favorably compared to other methods that adjust the training distribution of the generalist data with guidance from the limited domain-specific data. Our findings demonstrate improvements across different domains in terms of language modeling perplexity and accuracy on multiple-choice question tasks. We also present ablation studies that examine the impact of dataset sizes, clustering configurations, and model sizes.",
            "corpus_id": 273185866,
            "sentences": [
                {
                    "corpus_id": "273185866",
                    "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling",
                    "text": "Generalist language models (LMs) can address a wide variety of tasks, but this generality comes at a cost (Brown et al., 2020). It necessitates a large training set representative of all prospective tasks, as well as a large model to fit such a comprehensive dataset. Specialist models forgo this generality and fit a model for a limited domain or task. In their narrow specialty, such models can achieve better accuracy at a given model size (Kerner, 2024). \n\nPretraining a specialist is interesting when two conditions are met: (i) the targeted task justifies the cost of training a dedicated model and (ii) a specialist dataset large enough for pretraining is available. Condition (i) is dependent on the targeted application and its potential economic benefit. Condition (ii) is more limiting since modern LMs are commonly pre-trained on datasets larger than 100B tokens 1 , an amount that cannot be commissioned for most applications. This work considers relaxing condition (ii) and studies methods to train a specialist model when specialized data is scarce. Given a large generalist dataset and a small specialist dataset, we propose to modify the distribution over the generalist dataset guided by the scarce specialist dataset. Training a model on the modified distribution gives a specialist model with better accuracy than a generalist model of the same size. \n\nWe study this setting across different specialization tasks including domain-specific language modeling (medical, encyclopedic domains) and end-tasks (scholar exams in science and humanities, reasoning questions). We compare different strategies to manipulate the pretraining distribution. We evaluate strategies based on text classifiers, gradient-alignment and importance sampling (IS). Although IS is rarely used for LM data selection, we build upon on a simple IS recipe based on clustering (Grangier et al., 2024b) and report that the resulting method systematically outperforms alternatives. Our IS recipe clusters the generalist set and computes the cluster histogram over the specialist data. Then, for pretraining, generic data is sampled according to the specialist histogram, see Figure 1. We show the empirical benefit of this method varying model sizes (350m to 7B parameters), the amount of generalist data and the amount of specific data.",
                    "score": 0.4224700463657677,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 458
                        },
                        {
                            "start": 461,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 764
                        },
                        {
                            "start": 765,
                            "end": 939
                        },
                        {
                            "start": 940,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1236
                        },
                        {
                            "start": 1237,
                            "end": 1370
                        },
                        {
                            "start": 1373,
                            "end": 1586
                        },
                        {
                            "start": 1587,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1761
                        },
                        {
                            "start": 1762,
                            "end": 1970
                        },
                        {
                            "start": 1971,
                            "end": 2073
                        },
                        {
                            "start": 2074,
                            "end": 2173
                        },
                        {
                            "start": 2174,
                            "end": 2326
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 106,
                            "end": 126,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.587890625
                }
            ],
            "relevance_judgement": 0.587890625,
            "relevance_judgment_input_expanded": "# Title: Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling\n# Venue: International Conference on Learning Representations\n# Authors: David Grangier, Simin Fan, Skyler Seto, Pierre Ablin\n## Abstract\nSpecialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead. We propose a novel method, ClusteRed Importance SamPling (CRISP). CRISP clusters the generalist dataset and samples from these clusters based on their frequencies in the smaller specialist dataset. It is scalable, suitable for both pretraining and continued pretraining, and works well in multi-task settings. CRISP performs favorably compared to other methods that adjust the training distribution of the generalist data with guidance from the limited domain-specific data. Our findings demonstrate improvements across different domains in terms of language modeling perplexity and accuracy on multiple-choice question tasks. We also present ablation studies that examine the impact of dataset sizes, clustering configurations, and model sizes.\n## INTRODUCTION\nGeneralist language models (LMs) can address a wide variety of tasks, but this generality comes at a cost (Brown et al., 2020). It necessitates a large training set representative of all prospective tasks, as well as a large model to fit such a comprehensive dataset. Specialist models forgo this generality and fit a model for a limited domain or task. In their narrow specialty, such models can achieve better accuracy at a given model size (Kerner, 2024). \n\nPretraining a specialist is interesting when two conditions are met: (i) the targeted task justifies the cost of training a dedicated model and (ii) a specialist dataset large enough for pretraining is available. Condition (i) is dependent on the targeted application and its potential economic benefit. Condition (ii) is more limiting since modern LMs are commonly pre-trained on datasets larger than 100B tokens 1 , an amount that cannot be commissioned for most applications. This work considers relaxing condition (ii) and studies methods to train a specialist model when specialized data is scarce. Given a large generalist dataset and a small specialist dataset, we propose to modify the distribution over the generalist dataset guided by the scarce specialist dataset. Training a model on the modified distribution gives a specialist model with better accuracy than a generalist model of the same size. \n\nWe study this setting across different specialization tasks including domain-specific language modeling (medical, encyclopedic domains) and end-tasks (scholar exams in science and humanities, reasoning questions). We compare different strategies to manipulate the pretraining distribution. We evaluate strategies based on text classifiers, gradient-alignment and importance sampling (IS). Although IS is rarely used for LM data selection, we build upon on a simple IS recipe based on clustering (Grangier et al., 2024b) and report that the resulting method systematically outperforms alternatives. Our IS recipe clusters the generalist set and computes the cluster histogram over the specialist data. Then, for pretraining, generic data is sampled according to the specialist histogram, see Figure 1. We show the empirical benefit of this method varying model sizes (350m to 7B parameters), the amount of generalist data and the amount of specific data.",
            "reference_string": "[273185866 | Grangier et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2022,
            "reference_count": 58,
            "citation_count": 250,
            "influential_citation_count": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2212.09849",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2212.09849, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2148654757",
                    "name": "Xisen Jin"
                },
                {
                    "authorId": "1384550891",
                    "name": "Xiang Ren"
                },
                {
                    "authorId": "1398830377",
                    "name": "Daniel Preotiuc-Pietro"
                },
                {
                    "authorId": "2904366",
                    "name": "Pengxiang Cheng"
                }
            ],
            "abstract": "Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.",
            "corpus_id": 254877510,
            "sentences": [
                {
                    "corpus_id": "254877510",
                    "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
                    "text": "The dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021). This results in specialized models that are explicitly trained to run inference over a single domain and task. Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022). Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models. \n\nHowever, the multi-task learning setup suffers from two practical limitations. First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy or to guard intellectual property to annotations. Second, because a significant amount of data or task combinations are not beneficial to performance (Poth et al., 2021), building a single model requires training on all data set combinations to identify the optimal one, which can be prohibitive especially if there are many available source data sets or models. \n\nModel merging is defined as combining multiple models into a single one in parameter space without access to data (Matena & Raffel, 2021). This technique provides an alternative to building a single model while satisfying data privacy constraints. Weight merging algorithms usually also have a closed-form solution, making them very efficient as no retraining is necessary, thus enabling usage even when a large number of data sets or model combinations are available. Merging can be considered as an alternative to model ensembling (Opitz & Maclin, 1999;Rokach, 2010), where the outputs of individual models are combined to produce the final prediction.",
                    "score": 0.45277154345188597,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 218
                        },
                        {
                            "start": 219,
                            "end": 329
                        },
                        {
                            "start": 330,
                            "end": 581
                        },
                        {
                            "start": 582,
                            "end": 891
                        },
                        {
                            "start": 894,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1567
                        },
                        {
                            "start": 1570,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1817
                        },
                        {
                            "start": 1818,
                            "end": 2038
                        },
                        {
                            "start": 2039,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 180,
                            "end": 201,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 515,
                            "end": 542,
                            "matchedPaperCorpusId": "220045835"
                        },
                        {
                            "start": 542,
                            "end": 560,
                            "matchedPaperCorpusId": "233289699"
                        },
                        {
                            "start": 560,
                            "end": 580,
                            "matchedPaperCorpusId": "248780114"
                        },
                        {
                            "start": 699,
                            "end": 718,
                            "matchedPaperCorpusId": "233289699"
                        },
                        {
                            "start": 764,
                            "end": 784,
                            "matchedPaperCorpusId": "220045358"
                        },
                        {
                            "start": 1355,
                            "end": 1374,
                            "matchedPaperCorpusId": "233289699"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57373046875
                }
            ],
            "relevance_judgement": 0.57373046875,
            "relevance_judgment_input_expanded": "# Title: Dataless Knowledge Fusion by Merging Weights of Language Models\n# Venue: International Conference on Learning Representations\n# Authors: Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, Pengxiang Cheng\n## Abstract\nFine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.\n## INTRODUCTION\nThe dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021). This results in specialized models that are explicitly trained to run inference over a single domain and task. Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022). Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models. \n\nHowever, the multi-task learning setup suffers from two practical limitations. First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy or to guard intellectual property to annotations. Second, because a significant amount of data or task combinations are not beneficial to performance (Poth et al., 2021), building a single model requires training on all data set combinations to identify the optimal one, which can be prohibitive especially if there are many available source data sets or models. \n\nModel merging is defined as combining multiple models into a single one in parameter space without access to data (Matena & Raffel, 2021). This technique provides an alternative to building a single model while satisfying data privacy constraints. Weight merging algorithms usually also have a closed-form solution, making them very efficient as no retraining is necessary, thus enabling usage even when a large number of data sets or model combinations are available. Merging can be considered as an alternative to model ensembling (Opitz & Maclin, 1999;Rokach, 2010), where the outputs of individual models are combined to produce the final prediction.",
            "reference_string": "[254877510 | Jin et al. | 2022 | Citations: 250]"
        },
        {
            "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
            "venue": "Neural Information Processing Systems",
            "year": 2022,
            "reference_count": 102,
            "citation_count": 69,
            "influential_citation_count": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2206.04674",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2206.04674, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "150167730",
                    "name": "Jinguo Zhu"
                },
                {
                    "authorId": "2578924",
                    "name": "Xizhou Zhu"
                },
                {
                    "authorId": "71074736",
                    "name": "Wenhai Wang"
                },
                {
                    "authorId": "2155455424",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "49404547",
                    "name": "Hongsheng Li"
                },
                {
                    "authorId": "31843833",
                    "name": "Xiaogang Wang"
                },
                {
                    "authorId": "3304536",
                    "name": "Jifeng Dai"
                }
            ],
            "abstract": "To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon. To mitigate such interference, we introduce the Conditional Mixture-of-Experts (Conditional MoEs) to generalist models. Routing strategies under different levels of conditions are proposed to take both the training/inference cost and generalization ability into account. By incorporating the proposed Conditional MoEs, the recently proposed generalist model Uni-Perceiver can effectively mitigate the interference across tasks and modalities, and achieves state-of-the-art results on a series of downstream tasks via prompt tuning on 1% of downstream data. Moreover, the introduction of Conditional MoEs still holds the generalization ability of generalist models to conduct zero-shot inference on new tasks, e.g., video-text retrieval and video caption. Code and pre-trained generalist models shall be released.",
            "corpus_id": 249538647,
            "sentences": [
                {
                    "corpus_id": "249538647",
                    "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
                    "text": "Specialized Models. Previous research focuses on building specialized models for specific tasks. \n\nCNNs [47,26,70] and ViTs [20,53,76,80] are developed for image classification. Subsequent works re-design them to adapt to diverse downstream visual tasks, e.g., object detection [63] and segmentation [15,48]. In NLP, different architectures are specifically designed for neural machine translation [77], natural language understanding [19], and natural language generation [51]. As for vision-language tasks, previous works usually combined modality-specific encoders and representation fusion modules together [13,54]. Recently, [89,65,71] integrate several specialized models into a single one to handle diverse tasks. Such integrated specialized models are equipped with multiple task-specific modules to adapt to as many downstream tasks as possible. However, these methods still follow the task-specific paradigm, which conflicts with the objective of generalist models. \n\nVanilla Generalist Models. Vanilla generalist models handle different tasks and modalities with shared parameters. Uni-Perceiver [93] formulates various perception tasks as finding the maximum likelihood target for each input through the similarity of their representations. OFA [79], Flamingo [3] and SimVLM [84] attempt to unify different tasks into sequence-to-sequence generation. UniCORN [86] and Gato [62] further incorporate bounding box and reinforcement learning tasks into the unified formulation, respectively. These generalist models not only achieve competitive performance on pre-training tasks with shared parameters, but also can perform zero-shot inference on new tasks [62,93]. However, these methods rarely investigate the potential interference among different modalities and tasks, which could result in the performance degradation of generalist models. \n\nMulti-Task Learning. Multi-task learning [8,17] has been widely studied in the community of vision [27,74,72], language [25,16,50] and vision-language learning [10,55,29]. While multi-task training enables collaboration between tasks, it may also introduce the task interference problem [81,83,28,36,72].",
                    "score": 0.46347209221304975,
                    "section_title": "Related Works",
                    "char_start_offset": 4709,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 19
                        },
                        {
                            "start": 20,
                            "end": 96
                        },
                        {
                            "start": 99,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 308
                        },
                        {
                            "start": 309,
                            "end": 478
                        },
                        {
                            "start": 479,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 720
                        },
                        {
                            "start": 721,
                            "end": 854
                        },
                        {
                            "start": 855,
                            "end": 975
                        },
                        {
                            "start": 978,
                            "end": 1004
                        },
                        {
                            "start": 1005,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1252
                        },
                        {
                            "start": 1253,
                            "end": 1362
                        },
                        {
                            "start": 1363,
                            "end": 1499
                        },
                        {
                            "start": 1500,
                            "end": 1673
                        },
                        {
                            "start": 1674,
                            "end": 1852
                        },
                        {
                            "start": 1855,
                            "end": 1875
                        },
                        {
                            "start": 1876,
                            "end": 2026
                        },
                        {
                            "start": 2027,
                            "end": 2159
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 108,
                            "end": 111,
                            "matchedPaperCorpusId": "206594692"
                        },
                        {
                            "start": 131,
                            "end": 134,
                            "matchedPaperCorpusId": "229363322"
                        },
                        {
                            "start": 134,
                            "end": 137,
                            "matchedPaperCorpusId": "232035922"
                        },
                        {
                            "start": 278,
                            "end": 282,
                            "matchedPaperCorpusId": "10328909"
                        },
                        {
                            "start": 1896,
                            "end": 1899,
                            "matchedPaperCorpusId": "45998148"
                        },
                        {
                            "start": 1954,
                            "end": 1958,
                            "matchedPaperCorpusId": "54465873"
                        },
                        {
                            "start": 1958,
                            "end": 1961,
                            "matchedPaperCorpusId": "85544221"
                        },
                        {
                            "start": 1961,
                            "end": 1964,
                            "matchedPaperCorpusId": "159040666"
                        },
                        {
                            "start": 2019,
                            "end": 2022,
                            "matchedPaperCorpusId": "208637516"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5703125
                }
            ],
            "relevance_judgement": 0.5703125,
            "relevance_judgment_input_expanded": "# Title: Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs\n# Venue: Neural Information Processing Systems\n# Authors: Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, Jifeng Dai\n## Abstract\nTo build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon. To mitigate such interference, we introduce the Conditional Mixture-of-Experts (Conditional MoEs) to generalist models. Routing strategies under different levels of conditions are proposed to take both the training/inference cost and generalization ability into account. By incorporating the proposed Conditional MoEs, the recently proposed generalist model Uni-Perceiver can effectively mitigate the interference across tasks and modalities, and achieves state-of-the-art results on a series of downstream tasks via prompt tuning on 1% of downstream data. Moreover, the introduction of Conditional MoEs still holds the generalization ability of generalist models to conduct zero-shot inference on new tasks, e.g., video-text retrieval and video caption. Code and pre-trained generalist models shall be released.\n## Related Works\nSpecialized Models. Previous research focuses on building specialized models for specific tasks. \n\nCNNs [47,26,70] and ViTs [20,53,76,80] are developed for image classification. Subsequent works re-design them to adapt to diverse downstream visual tasks, e.g., object detection [63] and segmentation [15,48]. In NLP, different architectures are specifically designed for neural machine translation [77], natural language understanding [19], and natural language generation [51]. As for vision-language tasks, previous works usually combined modality-specific encoders and representation fusion modules together [13,54]. Recently, [89,65,71] integrate several specialized models into a single one to handle diverse tasks. Such integrated specialized models are equipped with multiple task-specific modules to adapt to as many downstream tasks as possible. However, these methods still follow the task-specific paradigm, which conflicts with the objective of generalist models. \n\nVanilla Generalist Models. Vanilla generalist models handle different tasks and modalities with shared parameters. Uni-Perceiver [93] formulates various perception tasks as finding the maximum likelihood target for each input through the similarity of their representations. OFA [79], Flamingo [3] and SimVLM [84] attempt to unify different tasks into sequence-to-sequence generation. UniCORN [86] and Gato [62] further incorporate bounding box and reinforcement learning tasks into the unified formulation, respectively. These generalist models not only achieve competitive performance on pre-training tasks with shared parameters, but also can perform zero-shot inference on new tasks [62,93]. However, these methods rarely investigate the potential interference among different modalities and tasks, which could result in the performance degradation of generalist models. \n\nMulti-Task Learning. Multi-task learning [8,17] has been widely studied in the community of vision [27,74,72], language [25,16,50] and vision-language learning [10,55,29]. While multi-task training enables collaboration between tasks, it may also introduce the task interference problem [81,83,28,36,72].",
            "reference_string": "[249538647 | Zhu et al. | 2022 | Citations: 69]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "In order to study the relative merits of merging for different objectives across a wide set of languages, we conduct extensive ablations. We detail some of the most critical experiment variants below: \n\nObjective-based merging. To evaluate the relative merits of merging on balancing dual-objectives, we merge models that have been separately optimized for general-purpose abilities and safety. This builds upon our multilingual 0% and 100% Safety Mixes (see Section 2.2) to balance the trade-offs between safety and general performance. Language-based merging. Multilinguality remains one of the most challenging tasks in language modeling. We aim to determine whether language-specific models can be used off-the-shelf to incorporate language capabilities and explore how merging models based exclusively on different languages affects their downstream performance. \n\nSpecifically, we investigate whether combining models optimized for both safety and general performance with a 15% language-specific safety mix for our target languages leads to better performance than training on a mixture of those languages. For clarity, to produce a multilingual model with safe and general-purpose abilities for English, French, and Spanish (referred to as the EN-FR-SP group later), we merge models optimized independently on a 15% Safety Mix for each of these languages. \n\nComparison of merging applied to DPO and SFT. Model merging is a highly adaptable technique that can be applied at any stage of the training process owing to its simple input requirement of model checkpoints. To determine the optimal stage for maximizing its benefits, we merge and evaluate SFT and DPO checkpoints independently as these techniques have shown great success towards the alignment of language models [Aakanksha et al., 2024;Shen et al., 2024]. \n\nSensitivity to hyperparameters. Previous works [Ilharco et al., 2023] have shown that merging is sensitive to the hyperparameters involved and have developed sophisticated algorithms [Akiba et al., 2024;Xiao et al., 2023;Davari & Belilovsky, 2024] to find the optimal values for the same. \n\nTo this end, we seek to find the impact of varying the weighting scheme of Linear merging on both general performance and safety.",
            "score": 0.5691555461562221,
            "section_title": "Key Ablations",
            "char_start_offset": 11237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259033203125
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "Figure 3 presents the Word Error Rate (WER) obtained after training the language model on the small FR and EN datasets respectively.These figures present the performance of the systems for 6 differents types of language models: words, characters, and k-multigrams (with 2 \u2264 k \u2264 5).\n\nOne can first notice that the unified optical model (US and UU) significantly improves the specialized framework whatever the test set (FR and EN) and the configuration of the system (specialized or unified).The optical model takes advantage of language similarities to be more robust and more efficient.\n\nBesides, multigram-based language models often outperform traditional language models based on words or characters.Especially, the 2-multigrams language models are always better than the traditional ones.This confirms our hypothesis that multigrams are a good trade-off between words and characters for a language modeling task.Compared to the specialized frameworks, the unified scheme generally provides similar results except for the FR dataset where a slight improvement is observed when the optical model is unified.\n\nFinally one can notice that the performance obtained on the IAM dataset is rather low for any of the language model type (word, character or k-multigrams), and system unification type (SS, SU, US, UU), when a small vocabulary is used for training the language models.This is a particular difficulty encountered on the IAM dataset because of the low lexicon coverage rate of the training dataset lexicon on the test dataset.This is the reason why most studies have come to use additional linguistic resources to get better performance.To the next, we analyze tour systems using a large vocabulary setting.",
            "score": 0.541526236928113,
            "section_title": "Evaluation with small EN and FR lexicons",
            "char_start_offset": 26526,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 281
                },
                {
                    "start": 283,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 587
                },
                {
                    "start": 589,
                    "end": 704
                },
                {
                    "start": 704,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1110
                },
                {
                    "start": 1112,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1535
                },
                {
                    "start": 1535,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1716
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0592041015625
        },
        {
            "corpus_id": "256390607",
            "title": "Specializing Smaller Language Models towards Multi-Step Reasoning",
            "text": "The surprising ability of Large Language Models (LLMs) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models (100+ billion parameters). We show that such abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5 variants ($\\le$ 11B). We propose model specialization, to specialize the model's ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power, but are spread on a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we concentrate their capacity on a specific target task, the model can achieve a decent improved performance. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1). there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities; (2). by paying the price of decreased generic ability, we can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability. We further give comprehensive discussions about important design choices for better generalization, including the tuning data format, the start model checkpoint, and a new model selection method. We hope our practice and discoveries can serve as an important attempt towards specialized smaller models in the new research paradigm set by LLMs.",
            "score": 0.5266238738839023,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2044677734375
        },
        {
            "corpus_id": "278310893",
            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
            "text": "This targeted training approach eliminates wasteful trial-and-error methods typically associated with domain adaptation, as each model receives only the most relevant data for its specialization. \n\nThe final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020;Yadav et al., 2024), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost. \n\nThe success of this pipeline relies on careful orchestration between components and systematic empirical validation. Our approach addresses three critical challenges in modern language model development: reducing computational requirements through strategic pruning, maintaining performance through targeted domain adaptation, and combining specialized capabilities through sophisticated ensemble methods. The result is a more efficient, domain-aware language model that achieves high performance without excessive computational demands or model scale.",
            "score": 0.5129729490733902,
            "section_title": "Proposed Downscaling Pipeline",
            "char_start_offset": 25479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1689
                }
            ],
            "ref_mentions": [
                {
                    "start": 499,
                    "end": 516,
                    "matchedPaperCorpusId": "220496384"
                },
                {
                    "start": 516,
                    "end": 535,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6923828125
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
            "score": 0.4970069483341095,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "232110907"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1592,
                    "end": 1596,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1596,
                    "end": 1599,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1650,
                    "end": 1653,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "263831551"
                },
                {
                    "start": 1656,
                    "end": 1659,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 2028,
                    "end": 2032,
                    "matchedPaperCorpusId": "258865647"
                },
                {
                    "start": 2032,
                    "end": 2035,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.623046875
        },
        {
            "corpus_id": "269757765",
            "title": "The Platonic Representation Hypothesis",
            "text": "AI systems are rapidly evolving into highly multifunctional entities. For example, whereas in the past we had specialpurpose solutions for different language processing tasks (e.g., sentiment analysis, parsing, dialogue), modern large language models (LLMs) are competent at all these tasks using a single set of weights (Srivastava et al., 2022). Unified systems are also being built across data modalities: instead of using a different architecture for processing images versus text, recent models, such as GPT4-V (Ope-nAI, 2023), Gemini (Google, 2023), and LLaVA (Liu et al., 2023), handle both modalities with a combined architecture. More and more systems are built off of general-purpose pretrained backbones, sometimes called foundation models (Bommasani et al., 2021), that support a large range of tasks, including robotics (Driess et al., 2023;Brohan Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).",
            "score": 0.4889922704296802,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 997
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41064453125
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "In chapter ve, we motivate the addition of knowledge to language models, develop di erent criteria to identify useful knowledge, and present methods to combine knowledge in a language model. We begin by pointing out three reasons for wanting to add knowledge to a language model. First, we would like to improve its performance. Second, if we apply current speech recognition technology to more complex tasks than the ones tackled today, the number of acoustically confusable hypotheses will increase, and we may well need a better language model in order to deal with the additional ambiguity. Third, adding knowledge is more satisfying than sticking to existing models on psychological grounds because humans seem to use knowledge to predict a word other than the knowledge used in current models, namely the immediately preceding two or three words. Hence, there is clearly a need for a language model which incorporates more linguistic knowledge.\n\nOnce we have decided to add knowledge to a language model, the following two questions come to mind. First, what knowledge should we add, and second, how can we combine di erent types of knowledge in a language model. We address both questions in turn.\n\nRather than trying to give a necessarily incomplete list of types of knowledge that we should add, we present four criteria that we think should be used to identify useful knowledge. First, the knowledge should restrict the number of possible words, otherwise it is not going to help in solving our task. Second, it should be applicable often enough to be of statistical signi cance. Third, it should be possible computationally to use this knowledge in real time speech recognition. Finally, we should be able to acquire and code this knowledge for use with unrestricted text.\n\nWe develop a classi cation of possibly useful knowledge and apply the criteria for identifying useful knowledge to one type of knowledge that promises to be useful for improving language models in general.\n\nWe then move on to the issue of combining di erent types of knowledge in a language model. We present three methods of combining knowledge and develop some of the advantages and disadvantages we see in each method. Following that, we conclude that it is very unlikely that we will have enough data to estimate distributions that depend on several knowledge sources directly, even with the availability of increasingly large corpora. Therefore, we think that methods that combine distributions from single knowledge sources in a Language Modeling for Speech Recognition In the last",
            "score": 0.4884840962405906,
            "section_title": "Chapter 5: Adding Linguistic Knowledge to Language Models",
            "char_start_offset": 29540,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "273228908",
            "title": "Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes",
            "text": "Model fusion, which involves the integration of capabilities from different models, can be mainly divided into two categories. Firstly, ensemble approaches combine the output of multiple models to enhance the overall prediction performance (Littlestone and Warmuth, 1994;Sagi and Rokach, 2018). Outputs are typically combined by weight averaging (Littlestone and Warmuth, 1994) or majority voting (Monteith et al., 2011). These ensemble approaches can improve the prediction performance of large-scale language models (Jiang et al., 2023). Secondly, weight merging approaches enable model fusion at the parameter level. Wortsman et al. (2022) show that weight averaging of multiple models fine-tuned with different hyperparameters improves prediction accuracy and robustness. Task vectors derived from model weights can be modified and combined together through arithmetic operations to steer the behavior of a resulting model (Ilharco et al., 2023). This approach can be enhanced by trimming task vectors and resolving sign conflicts before merging them (Yadav et al., 2023). In addition, Drop And Rescale (DARE) can be used as a general preprocessing technique for existing model merging methods to merge multiple task-specific fine-tuned models into a single model with diverse abilities (Yu et al., 2023).",
            "score": 0.48721007575623015,
            "section_title": "Model Fusion",
            "char_start_offset": 6437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 271,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 271,
                    "end": 293,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 346,
                    "end": 377,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 518,
                    "end": 538,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 620,
                    "end": 642,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 927,
                    "end": 949,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1055,
                    "end": 1075,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.293212890625
        },
        {
            "corpus_id": "7079619",
            "title": "Analyzing and Improving Statistical Language Models for Speech Recognition",
            "text": "In this chapter, we motivated the addition of knowledge to language models, developed di erent criteria for identifying useful knowledge, and presented methods for combining knowledge in a language model.\n\nWe began by pointing out three reasons for wanting to add knowledge to a language model. First, we would like to improve the model's performance. Second, if we apply current speech recognition technology to more complex tasks than the ones tackled today, the number of acoustically confusable hypotheses will increase, and we will need a better language model in order to deal with this ambiguity. Third, adding knowledge is more satisfying than sticking to existing models on psychological grounds because humans seem to use knowledge to predict a word other than the knowledge used in current models, namely the immediately preceding two or three words. Hence, there is clearly a need for a language model which incorporates more linguistic knowledge.\n\nOnce we had decided to add knowledge to a language model, the following two questions came to mind. First, what knowledge should we add, and second, how can we combine di erent types of knowledge in a language model. We addressed both issues in turn.\n\nRather than trying to give a necessarily incomplete list of types of knowledge that we should add, we presented four criteria that we think should be used to identify useful knowledge. First, the knowledge should restrict the number of possible words, otherwise it is not going to help in solving our task. Second, it should be applicable often enough to be of statistical signi cance. Third, it should be possible computationally to use this knowledge in real time speech recognition. Finally, we should be able to acquire and code this knowledge for use with unrestricted text. Moreover, we developed a classi cation of possibly useful knowledge and applied the criteria for identifying useful knowledge to one kind of knowledge, that promises to be useful to improve language models in general.\n\nWe then moved on to the issue of combining di erent knowledge in a language model. We presented three methods for combining knowledge and developed some of the advantages and disadvantages we see in each method. Following that, we concluded that it is very unlikely that we will have enough data to estimate distributions that depend on several knowledge sources directly, even with the availability of increasingly large corpora. Therefore, we think that methods that combine distributions from single knowledge sources in a meaningful fashion will be very useful and require",
            "score": 0.4845185361120572,
            "section_title": "Summary",
            "char_start_offset": 201139,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09967041015625
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "We have seen that unifying English and French optical models allows to significantly improve the recognition performance of the system.This may be explained by two facts: on the one hand, there is a lot of shared characters between the two languages, which increases the number of training samples for theses character classes; on the other hand, the shared characters appear in a different context (i.e.into different words), which gives more variability in the data as well.The model can benefit of it for increasing its generalization capacities.The unified optical model achieves a Character Error Rate (CER) of 10.0% alone (i.e.\n\nwithout language model) on the FR test set, while the specialized model has a CER of 12.3%.Similarly, we obtain a CER of 15.3% on the EN test set with the unified optical model while the CER of the specialized model is 18.9%.\n\nCompared to the specialized language model, the contribution of a unified language model appears rather moderate, as results are generally equivalent whatever the lexicon size.In fact, there is no reason to believe that some improvements should be gain by combining the English and French languages.Indeed, the language modeling task should become more complex due to some language similarities.The key point is that unifying both languages into the language model does not affect the system performance, which allows to design a system with only one language model whatever the languages.\n\nThere is only one case where unifying the language model improves the performance of the specialized system.This is for the UU setting on the FR test dataset with a small lexicon.In this case, the system benefits from both a better CER with a unified optical model and a better coverage of the unified language model.This improvement may be explained by the fact that the OOV rate, computed at a word-level, is reduced in the unified cases (compared to the specialized ones) and that the effective coverage rate, computed at a sub-lexical-level, is high.For instance, the OOV rate on the FR set for the 3-multigrams is of 1.4% in the specialized case and of 0.7% in the unified one, while the effective coverage rate for the unified dataset is equal to 98.5%.",
            "score": 0.48155341393021694,
            "section_title": "Discussion",
            "char_start_offset": 28971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 549
                },
                {
                    "start": 549,
                    "end": 633
                },
                {
                    "start": 635,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 860
                },
                {
                    "start": 862,
                    "end": 1038
                },
                {
                    "start": 1038,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1257
                },
                {
                    "start": 1257,
                    "end": 1451
                },
                {
                    "start": 1453,
                    "end": 1561
                },
                {
                    "start": 1561,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1770
                },
                {
                    "start": 1770,
                    "end": 2007
                },
                {
                    "start": 2007,
                    "end": 2212
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "271533631",
            "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
            "text": "Language models (LMs) are pivotal in Natural Language Processing (NLP), facilitating a variety of tasks such as machine translation [1], sentiment analysis [2], and text generation [3]. Despite their potential, large-scale models encounter challenges like computational inefficiency, limited adaptability, and catastrophic forgetting. Our study explores the amalgamation of Knowledge Distillation (KD) and Mixture of Experts (MoE) to mitigate these challenges, aiming to improve efficiency, modularity, and specialization in language models. \n\nTransformers, the backbone of many large models, require substantial computational resources [4], which hampers their scalability and accessibility. The increasing complexity and size associated with supporting more languages and domains adversely affect training durations and generalization abilities [5]. Additionally, fine-tuning for specific tasks consumes significant resources and often falls short of achieving optimal outcomes [6]. Catastrophic forgetting is a major hurdle, particularly in models handling multiple languages and domains, as they tend to lose previously acquired knowledge when exposed to new data [7]. \n\nSpecialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts [8]. Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]. \n\nOur research objectives include evaluating adaptive versus fixed alpha methods in KD, training a router to efficiently direct inputs to the appropriate experts, and comparing various MoE architectures to determine their effectiveness in handling multi-domain inputs and in averting catastrophic forgetting.",
            "score": 0.480645263487433,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2304
                },
                {
                    "start": 2307,
                    "end": 2613
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 135,
                    "matchedPaperCorpusId": "11280500"
                },
                {
                    "start": 181,
                    "end": 184,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 637,
                    "end": 640,
                    "matchedPaperCorpusId": "211532645"
                },
                {
                    "start": 1384,
                    "end": 1387,
                    "matchedPaperCorpusId": "256808267"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.630859375
        },
        {
            "corpus_id": "253080406",
            "title": "Composing Ensembles of Pre-trained Models via Iterative Consensus",
            "text": "Large pre-trained models have shown remarkable zero-shot generalization abilities, ranging from zero-shot image generation and natural language processing to machine reasoning and action planning. Such models are trained on large datasets scoured from the internet, often consisting of billions of datapoints. Individual pre-trained models capture different aspects of knowledge on the internet, with language models (LMs) capturing textual information in news, articles, and Wikipedia pages, and visual-language models (VLMs) modeling the alignments between visual and textual information. While it is desirable to have a single sizable pre-trained model capturing all possible modalities of data on the internet, such a comprehensive model is challenging to obtain and maintain, requiring intensive memory, an enormous amount of energy, months of training time, and millions of dollars. A more scalable alternative approach is to compose different pre-trained models together, leveraging the knowledge from different expert models to solve complex multimodal tasks.\n\nBuilding a unified framework for composing multiple models is challenging. Prior works (Alayrac et al., 2022;Zeng et al., 2022) have explored composing pre-trained models in two main ways:\n\n(jointly) finetuning models on large datasets, or using common interfaces such as language to combine different models. However, these works have several key limitations: First, simply combining models does not fully utilize each pre-trained model as there is no closed-loop feedback between models. Cascading models, such as Socratic models (Zeng et al., 2022), allows one-way communication but prevents information processed by later models from propagating back to earlier models to correct errors. Secondly, common interfaces are limited to particular types of models. Language is used as the intermediate connection in Socratic models (Zeng et al., 2022), but a language interface is insufficient to solve many real-world tasks, such as continuous robot control, which requires continuous representations. In addition, Socratic models require pre-designed language templates for the communication between models, which limits scalability. Thirdly, jointly finetuning multiple models (Alayrac et al., 2022) requires careful optimization to ensure that the model behaviors remain stable. Such models also require intensive memory and large datasets and can only be used for",
            "score": 0.4798222893110302,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.325439453125
        },
        {
            "corpus_id": "258840882",
            "title": "Automatic Model Selection with Large Language Models for Reasoning",
            "text": "Large language models (LLMs) have made impressive progresses in numerous fields (Devlin et al., 2019;Brown et al., 2020;OpenAI, 2023;Chowdhery et al., 2022;Bubeck et al., 2023;Wei et al., 2022a) and are often powerful enough to solve problems through a single unified method. While convenient, this approach tends to ignore the distinct structures and variations among the problems, which would benefit from using different methods. On the other hand, in human society, individuals with different skill sets excel in various roles, leading to a thriving world as a whole. \n\nIn the case of reasoning, Chain-of-Thought (CoT) (Wei et al., 2022b) and Program-Aided Language Models (PAL) (Gao et al., 2022; Chen et al.,   1 Our code, data and prompts are available at https:// github.com/XuZhao0/Model-Selection-Reasoning 2022) have emerged as two effective methods that offer different strengths and weaknesses. Essentially, CoT decomposes a reasoning problem into a series of intermediate steps using natural language, making it more general, flexible, and comprehensible. On the other hand, PAL offers solutions via Python functions, with its step-by-step programming code ensuring a more rigorous and structured methodology. The external Python interpreter further guarantees the computation precision. Intuitively, combining the strengths of both models and selecting between them based on their solutions to a specific problem would lead to enhanced performance. However, without access to the ground truth, choosing a better method itself becomes a machine learning problem. \n\nIn order to select among multiple solutions, previous studies have suggested training a ranker (Uesato et al., 2022). While training a dedicated model generally results in improved accuracy, it can also be somewhat cumbersome and entail significant costs. Conversely, large language models (LLMs) have demonstrated good calibration and have been used to assess the accuracy of their own outputs (Guo et al., 2017;Shinn et al., 2023;Xie et al., 2023). In light of this, we propose leveraging the in-context learning capabilities of LLMs for model selection.",
            "score": 0.4778507257767918,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2135
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 120,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 176,
                    "end": 194,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1974,
                    "end": 1992,
                    "matchedPaperCorpusId": "28671436"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12274169921875
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "Large Language Models (LLMs) have demonstrated unparalleled capability in a diverse array of natural language tasks, encompassing commonsense reasoning, question answering, and specialized domains such as mathematics and programming [39,43,52]. The effectiveness of LLMs is based on the scaling law, which posits that proportionally increasing model and training data size leads to enhanced model performance [27]. Nevertheless, the computation overhead and data requirement surge as LLM continues to scale. With the widespread of open-sourced general or specialized LLMs, aggregating existing models to construct a more versatile LLM emerges as an economical alternative to training a larger LLM from scratch [13,16,54]. This not only mitigates the computation cost but also leverages the collective advancements of previous efforts in building LLMs. Within different methods to combine existing LLMs, a major class is merging [2,4,22,24,35,59,63,64]. Model merging combines multiple models into a single one of the same size through weight-space transformation. Wortsman et al. [59] first propose to merge a few fine-tuned models as a training trick for the flat loss-landscape, and Ilharco et al. [22] extends it to multi-task scenario, both of which employ the simple averaging. Other works propose more complicated merging methods, leveraging weight sparsity [63,64] and non-uniform coefficient [4,35]. However, they assume that all candidate models are \"useful\" when merging. While this may hold for small-sized designed model collections, it may not be the case in real-world scenarios given a large and divergent model zoo. How to ensure the benefits of merging different model zoo sizes and similarities, and exclude \"harmful\" candidates, remains underexplored. Since merging is limited to the same model structures and initial weights, another alternative is Mixture-of-Experts (MoE) [16]. MoE is a conditional computation architecture that activates only a subset of model parameters for each specific input example [47]. MoE LLMs have already demonstrated performance and computational efficiency advantages over their dense counterparts [15,25,30,68].",
            "score": 0.4773858119107669,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "267061245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.360107421875
        },
        {
            "corpus_id": "439840",
            "title": "Using of heterogeneous corpora for training of an ASR system",
            "text": "During the course of the workshop, it became apparent that the three corpora actually do not combine well. The corpora \"A\" and \"B\" are closest, but even their combination for training did not produce better results -see Table 4. \n\nAnother piece of evidence can be gathered from Table 5. The language model created from the training data of the \"T\" dataset was not useful for the language model interpolation. \n\nAs the diversity of the data proved to be too high to allow for the training of a single model on all of the data that would function well, we decided to train data-set specific models, i.e. train three models, each of which would be specialized to that given dataset. Moreover, we tried to find out if there was a way to benefit from the fact that we had multiple (similar) corpora. The method we used to exploit this fact was the sharing (i.e. Table 5. Optimal mixing weights, and the resulting perplexities on three dev-test sets, for interpolating language models trained on the three corpora: \"A\" -Appen, \"B\" -Babel and \"T\" -TransTac. \n\nFig. 2. Scheme of sharing the layers for joint multi-corpora training. The best performance was achieved with the \"share all but first and last\" approach. training jointly) of the hidden layers and only having the last and the first layers be dataset-specific. The reason for doing this was two-fold. First, it allowed us to train a larger neural network with potentially better performance. Secondly, the shared layers would hopefully learn more general/robust hyperplane separations. See Figure 2 for an illustration of this method. We experimented with different sharing strategies. The best performance was obtained when the first and the last layer were shared. Sharing less or more layers (than the first and last one) has made the performance worse. \n\nThe training procedure was similar to the training of a single network. First, in each step, for each dataset, a new, updated network was obtained using model averaging. After that, these corpus-specific networks were averaged and the shared layers of this final network were copied back to the corpus-specific network. This represents one iteration of the joint-multi-corpora training.",
            "score": 0.47590225974745953,
            "section_title": "JOINT MULTI-CORPUS TRAINING",
            "char_start_offset": 10401,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1809
                },
                {
                    "start": 1812,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2198
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.144287109375
        },
        {
            "corpus_id": "249626024",
            "title": "Language Models are General-Purpose Interfaces",
            "text": "Some efforts investigate the general-purpose model that supports multiple tasks, transformations, and modalities in a shared module. MT-DNN (Liu et al., 2019a) trains on many tasks through multitask learning. Specific to language-only general-purpose, UniLM (Dong et al., 2019) and T5 (Raffel et al., 2020) unify understanding and generation ability in a single model. Moreover, language models are finetuned to follow instructions (Ouyang et al., 2022;Wei et al., 2021;Sanh et al., 2022), i.e., aligning language models with user intentions to implement the general-purpose capability. There are some work that support not only multitask but also multimodality. Jaegle et al. (2022) introduce Perceiver IO, a general architecture across multiple domains including language/visual understanding, multimodal and symbolic representations for games. Baevski et al. (2022) propose a unified learning framework for different modalities but still use modality specific encoders. Tsimpoukelli et al. (2021) demonstrate that the in-context learning ability of frozen language models can be transferred to a vision-language setting. Alayrac et al. ( 2022) also implement general-purpose understanding of image, video, and text by a large frozen language model. Reed et al. (2022) build a generalist agent that works as a multi-modal, multi-task, multi-embodiment generalist policy.",
            "score": 0.47469907226757246,
            "section_title": "General-Purpose Modeling",
            "char_start_offset": 45580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1372
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 159,
                    "matchedPaperCorpusId": "59523594"
                },
                {
                    "start": 285,
                    "end": 306,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 470,
                    "end": 488,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 973,
                    "end": 999,
                    "matchedPaperCorpusId": "235658331"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350341796875
        },
        {
            "corpus_id": "235829792",
            "title": "Importance-based Neuron Allocation for Multilingual Neural Machine Translation",
            "text": "Our goal is to build a unified model, which can achieve good performance on all language pairs. The main idea of our method is that different neurons have different importance to the translation of different languages. Based on this, we divide them into general and language-specific ones and make general neurons participate in the translation of all the languages while language-specific neurons focus on some specific languages. Specifically, the proposed approach involves the following steps shown in Figure 1. First, we pretrain the model on the combined data of all the language pairs following the normal paradigm in Johnson et al. (2017). Second, we evaluate the importance of different neurons on these language pairs and allocate them into general neurons and language-specific neurons. Last, we fine-tune the translation model on the combined data again. It should be noted that for a specific language pair only the general neurons and the language-specific neurons for this language pair will participate in the forward and backward computation when the model is trained on this language pair. Other neurons will be zeroed out during both training and inference.",
            "score": 0.4746266059851161,
            "section_title": "Approach",
            "char_start_offset": 5099,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 646,
                    "matchedPaperCorpusId": "260464809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1737060546875
        },
        {
            "corpus_id": "271916426",
            "title": "Clinical insights: A comprehensive review of language models in medicine",
            "text": "A critical milestone in the field of Natural Language Processing (NLP) was the introduction of the attention mechanism in neural machine translation [17]. This mechanism, linking the encoder and decoder in sequence-to-sequence models, paved the way for subsequent advancements by enabling the model to focus on different parts of the input sequence for each step of the output, substantially improving the handling of longer input sequences and complex dependencies. Notably, it led to the creation of the Transformer model [18], which exclusively relies on attention mechanisms. This innovation revolutionized not only the field of NLP but also the broader realms of AI and machine learning (ML). \n\nThe emergence of Transformer-based models has spurred the development of a wide array of language models, both in commercial and open formats. Among these, the GPT series [19] has advanced text generation capabilities while establishing the foundation for interactive applications. A notable milestone was the introduction of ChatGPT [20], a conversational agent that revolutionized chat-based language models by facilitating intuitive, human-like dialogue generation. The success of ChatGPT and similar systems [5,21] underscored the adaptability of Transformer architectures and catalyzed the development of language models optimized for interactive and specialized applications, including those in the medical domain. \n\nThe overall task of language modeling can be expressed as estimating the joint probability of a sequence of words w 1 , w 2 , \u2026 , w n in a sentence, drawn from large text corpora: \n\nDespite the simplicity of statistical approaches to language modeling and their initial lack of attention to the underlying rules of language, as critiqued by Chomsky [22], contemporary large language models (LLMs) like GPT-4 have demonstrated remarkable proficiency in a wide range of language understanding tasks [4], exhibiting emergent abilities and an advanced capacity to learn natural language patterns. \n\nModern language models have undergone a notable paradigm shift, moving from a pretraining and fine-tuning approach to embracing in-context learning (ICL) [23] and zero-shot learning [24]. Traditionally, language model development has followed a two-step process: pre-training and fine-tuning. During pre-training, a model learns general language representations from large text corpora in an unsupervised manner.",
            "score": 0.47400269069344736,
            "section_title": "Language models",
            "char_start_offset": 2536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 697
                },
                {
                    "start": 700,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1420
                },
                {
                    "start": 1423,
                    "end": 1602
                },
                {
                    "start": 1605,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2310
                },
                {
                    "start": 2311,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 153,
                    "matchedPaperCorpusId": "11212020"
                },
                {
                    "start": 524,
                    "end": 528,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "Large language models demonstrate strong multitask capabilities, effectively addressing a wide range of tasks across diverse domains [Brown et al., 2020;Radford et al., 2019]. \"Safety\" in a model can be viewed as another \"task-solving\" ability that a model can learn. It is well established that equipping a model with any kind of capabilities with the standard paradigm of training requires copious amounts of data. Multi-tasking abilities typically arise from fine-tuning models on mixed datasets, which combine data from various sources and across many tasks [Raffel et al., 2023;Wang et al., 2019;\u00dcst\u00fcn et al., 2024]. However, determining the optimal strategy for mixing datasets in multi-task training is often complex and resource-intensive, as it must ensure that all tasks benefit from the shared training process -especially in the context of safety, where the general performance of models often gets cannibalized in exchange for safety [Tsipras et al., 2019;Bianchi et al., 2024;Ray & Bhalani, 2024;\u00dcst\u00fcn et al., 2024]. \n\nMore recently, an emerging approach for enabling multi-tasking has focused on training distinct models for specific tasks, followed by a weight-merging process governed by a pre-defined algorithm [Tam et al., 2023;Yang et al., 2024;Li et al., 2024a;Wan et al., 2024;Zhou et al., 2024;Davari & Belilovsky, 2024]. This method has shown great promise in building models with new capabilities without incurring additional costs and challenges that accompany training from scratch. However, a We analyze the differences in merging models on trained with specialized multilingual datasets, particularly in the context of safety, in contrast to those trained directly on mixtures of these datasets. We follow the LLM-as-ajudge approach for evaluating the performance of these models along two axes -general and safety. \n\nkey question remains -how does it compare to traditional data mixing and weighting approaches?",
            "score": 0.4737715159620085,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 153,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1876220703125
        },
        {
            "corpus_id": "259202736",
            "title": "Revealing the structure of language model capabilities",
            "text": "Here, we investigated the structure of language model capabilities using performance data from a range of models on the HELM benchmark tasks. We found evidence that language model capabilities are multidimensional, and identified three factors that appear to correspond to abilities capturing comprehension, language modeling, and reasoning. \n\nTo the best of our knowledge, this is the first empirical investigation into the structure of language model capabilities. Our findings suggest that a large proportion of the variance in language model performance can be explained by a small number of latent capabilities. The fact that three factors alone explained so much of the variance in performance across the wide variety of models in the dataset is striking-the included tasks test everything from question answering and summarization to sentiment analysis and various kinds of reasoning [19]. This pattern indicates that although LLMs vary greatly in their capabilities, they might share a common underlying cognitive structure. If this is indeed the case, we may be able to learn a lot about the capabilities of future models by examining current and past systems-a fact that might provide some hope for efforts to understand these systems in the face of breakneck progress. \n\nThe data presented here provide useful insights into the underlying abilities needed to complete different kinds of tasks. For instance, the results suggest that various tasks involving comprehension, including question answering, summarization, and sentiment analysis all appear to rely on the same underlying ability, despite having quite different formats. Similarly, our data suggest that mathematical reasoning tasks and other inductive reasoning tasks might rely on a single underlying reasoning ability. \n\nThese findings also have a number of implications for benchmark construction. If the performance of LLMs on a wide range of tasks can be largely explained by a small number of broad capabilities, it should be possible to make benchmarks more efficient. Instead of requiring dozens or even hundreds of tasks, we may be able to obtain a good sense of a model's capabilities with only a small number of tasks that provide estimates of a model's reasoning, comprehension, and language modeling abilities. This approach would provide a much more cost-efficient way of gathering useful data on a wide range of models. Such an outcome would be in-keeping with wider efforts to make more streamlined, \"lite\" benchmarks to combat the prohibitive costs involved in benchmarking LLMs [19,30].",
            "score": 0.47283411153182897,
            "section_title": "Discussion",
            "char_start_offset": 18644,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1792
                },
                {
                    "start": 1795,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2295
                },
                {
                    "start": 2296,
                    "end": 2406
                },
                {
                    "start": 2407,
                    "end": 2576
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07147216796875
        },
        {
            "corpus_id": "209516059",
            "title": "An Empirical Study of Factors Affecting Language-Independent Models",
            "text": "In today's globalized world, companies need to be able to understand and analyze what is being said out there, about them, their products, services, or their competitors, regardless of the human language used. Many organizations have spent tremendous resources to develop cognitive applications and services for dealing with customers in different countries. For example, cognitive systems may use machine learning techniques to process input messages or statements to determine their meaning and to provide associated confidence scores based on knowledge acquired by the cognitive system. Typically, the use of such cognitive systems requires training individual nat-ural language understanding models in a specific human language. For example, a tone analyzer model can be built to predict tones from English conversations (Liu et al., 2018), but such model would not work effectively with other languages. While translation techniques can be applied to translate data from an existing language to another language, human translation is labor-intensive and time-consuming, and machine translation can be costly and unreliable. As a result, attempts to scale existing applications to multiple human languages has traditionally proven to be difficult, mainly due to the language-dependent nature of preprocessing and feature engineering techniques employed in traditional approaches (Akkiraju et al., 2018). \n\nIn this work, we empirically investigate the feasibility of multilingual representations to build language-independent models, which can be trained with data from multiple source languages and then serve multiple target languages (target languages can be different from source languages). We explore this question using a unified language model Multilingual BERT (Devlin et al., 2019), which is pre-trained on the combination of monolingual Wikipedia corpora from 104 languages. Through a series of experiments on multiple task types, language sets and data resources, we contribute empirical findings of how factors affect language-independent models: \n\nWe analyze and compare language-independent models on two most representative NLP tasks: sentence classification and sequence labeling. On both tasks, we show that language-independent models can be comparable to or even outperform the models trained using monolingual data. Language-independent models are generally more effective on sentence classification.",
            "score": 0.47066769827905464,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1407
                },
                {
                    "start": 1410,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2062
                },
                {
                    "start": 2065,
                    "end": 2200
                },
                {
                    "start": 2201,
                    "end": 2339
                },
                {
                    "start": 2340,
                    "end": 2424
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 842,
                    "matchedPaperCorpusId": "5072523"
                },
                {
                    "start": 1773,
                    "end": 1794,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056121826171875
        },
        {
            "corpus_id": "259211816",
            "title": "Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI",
            "text": "The ultimate goal of artificial intelligence is to achieve general intelligence where the machine is able to simulate human cognitive abilities. In theory, a general AI model has a wide range of capabilities and can solve a diverse set of problems, rather than being designed to perform a specific task. Thus, in this paper, we investigate whether pre-trained language models can perform well and improve the state-of-the-art on diverse tasks that are not directly related to language. This could imply that the models have gained general knowledge during pre-training that is useful on downstream tasks from different domains. \n\nWe chose to investigate transformers because since they were proposed [1], they replaced RNNs in most NLP tasks such as translation, language modeling, sentiment analysis etc. They were also applied to other domains including vision [2] and audio processing [3] where they also achieve state-of-the-art performance. \n\nPre-training transformers on large corpora have emerged as a standard practice before fine-tuning on downstream tasks. The architecture of these pre-trained language models varies between the standard encoder-decoder architecture such as T5 [4] and BART [5], an encoder only architecture like BERT [6], and a decoder only architecture like GPT [7]. Most of the prior work employ pre-trained models by fine-tuning them on specific NLP tasks. However, in this paper, we investigate whether pre-training models on language can help on tasks from different domains and hence test their ability to generalize. Pre-training on language is attractive as it can be performed in an unsupervised fashion with unlabeled text data, abundant on the web. Pre-training on language would be more important if models were enabled to generalize to tasks from different domains. Thus, we choose four well-known pre-trained transformers with different architectures including T5, BART, BERT, and GPT-2 and we fine-tune them on three different non-language tasks. Two tasks are taken from the Long Range Arena Dataset [8], which are Long Listops and CIFAR10-LRA, and the third is Remote Homology by TAPE [9]. We compare the performance of our pre-trained models to transformers trained from scratch from [8].",
            "score": 0.47048151151337725,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "52287921"
                },
                {
                    "start": 1189,
                    "end": 1192,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 2131,
                    "end": 2134,
                    "matchedPaperCorpusId": "195069360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.298095703125
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "As illustrated in Table 3, the number of states and transitions is highly reduced using multigram language models compared to the word-based language models.For instance, the 2-multigrams model reduces the sum of states and transitions of the search graph by 31%.The volume on the disk is also divided by 1.45.Similar comments can be made on a large lexicon with more significant results (Table 4).Dealing with 2-multigrams allows to reduce the sum of states and transitions of the search graph by 63% compared to word model.The volume on disk is also reduced by a factor in our experiments and it can perform sometimes poorly (as shown on the EN dataset in Figure 4).Table 5 shows the decoding time on the FR validation set, which relates to a measure computed on 1.7 millions of frames.The decoding time is reduced using multigram language models compared to traditional language models based on words.For instance, using a 5-multigram language model reduces the time by 18% and by 24% using 2-multigrams on the small lexicon while there is a reduction of 22% using 2-multigrams on the large lexicon.\n\nTo conclude, we compare our multilingual recognition system with state-of-the art systems (Table 6).In our system, the optical model has a simpler architecture compared to state-of-the-art systems while these systems rest on language models based on words or hybrid models of words and characters.Multilingual systems based on multigrams reach performance closed to most of the state-of-the-art systems, while the system complexity is reduced.The specialized system presented in Table 6 refer to a selective approach where the right specialized model is always selected (an ideal case).We show that, whatever the test set (FR or EN), our unified system makes profit of the combination of the languages to outperform the selective approach.",
            "score": 0.4695698404586627,
            "section_title": "Discussion",
            "char_start_offset": 32780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 157,
                    "end": 263
                },
                {
                    "start": 263,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 904
                },
                {
                    "start": 904,
                    "end": 1102
                },
                {
                    "start": 1104,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1401
                },
                {
                    "start": 1401,
                    "end": 1547
                },
                {
                    "start": 1547,
                    "end": 1690
                },
                {
                    "start": 1690,
                    "end": 1843
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07452392578125
        },
        {
            "corpus_id": "260464809",
            "title": "Google\u2019s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
            "text": "This section shows the result of combining 12 production language pairs having a total of 3B parameters (255M per single model) into a single multilingual model. A range of multilingual models were trained, starting from the same size as a single language pair model with 255M parameters (1024 nodes) up to 650M parameters (1792 nodes). As above, the input needs to be prepended with the target language token. We oversample the examples from the smaller language pairs to balance the data as explained above. \n\nThe results for single language pair models versus multilingual models with increasing numbers of parameters are summarized in Table 4. We find that the multilingual models are on average worse than the single models (about 5.6% to 2.5% relative depending on size, however, some actually get better) and as expected the average difference gets smaller when going to larger multilingual models. It should be noted that the largest multilingual model we have trained has still about five times less parameters than the combined single models. \n\nThe multilingual model also requires only roughly 1/12-th of the training time (or computing resources) to converge compared to the combined single models (total training time for all our models is still in the order of weeks). Another important point is that since we only train for a little longer than a standard single model, the individual language pairs can see as little as 1/12-th of the data in comparison to their single language pair models but still produce satisfactory results. The results are summarized in Table 4. We find that the multilingual model is reasonably close to the best single models and in some cases even achieves comparable quality. It is remarkable that a single model with 255M parameters can do what 12 models with a total of 3B parameters would have done. The multilingual model also requires one twelfth of the training time and computing resources to converge. Another important point is that since we only train for a little longer than the single models, the individual language pairs can see as low as one twelfth of the data in comparison to their single language pair models. Again we note that the comparison below is somewhat unfair for the multilingual model and we expect a larger model trained on all available data will likely achieve comparable or better quality than the baselines.",
            "score": 0.46924919502710966,
            "section_title": "Large-scale Experiments",
            "char_start_offset": 21841,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2387
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03131103515625
        },
        {
            "corpus_id": "273162841",
            "title": "What Matters for Model Merging at Scale?",
            "text": "Model merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
            "score": 0.46900152188672606,
            "section_title": "BACKGROUND",
            "char_start_offset": 6956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1289
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 276,
                    "matchedPaperCorpusId": "270843326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "243860766",
            "title": "FPM: A Collection of Large-scale Foundation Pre-trained Language Models",
            "text": "Recent work in language modeling has shown that train-\ning large-scale Transformer models has promoted the lat-\nest developments in natural language processing applica-\ntions. However, there is very little work to unify the cur-\nrent effective models. In this work, we use the current ef-\nfective model structure to launch a model set through the\ncurrent most mainstream technology.\nWe think this will\nbecome the basic model in the future.\nFor Chinese, us-\ning the GPT-2[9] model, a 10.3 billion parameter language\nmodel was trained on the Chinese dataset, and, in particu-\nlar, a 2.9 billion parameter language model based on dia-\nlogue data was trained; the BERT model was trained on the\nChinese dataset with 495 million parameters; the Trans-\nformer model has trained a language model with 5.6 bil-\nlion parameters on the Chinese dataset. In English, cor-\nresponding training work has also been done. Using the\nGPT-2 model, a language model with 6.4 billion param-\neters was trained on the English dataset; the BERT[3]\nmodel trained a language model with 1.24 billion param-\neters on the English dataset, and in particular, it trained a\n688 million parameter based on single card training tech-\nnology Language model; Transformer model trained a lan-\nguage model with 5.6 billion parameters on the English\ndataset.\nIn the TNEWS classification task evaluated by\nCLUE[13], the BERT-C model exceeded the 59.46% accu-\nracy of ALBERT-xxlarge with an accuracy rate of 59.99%,\nan increase of 0.53%. In the QQP classification task evalu-\nated by GLUE[11], the accuracy rate of 78.95% surpassed\nthe accuracy rate of BERT-Large of 72.1%, an increase of\n6.85%. Compared with the current accuracy rate of ERNIE,\nthe first place in the GLUE evaluation of 75.2%, an increase\nof 3.75%.",
            "score": 0.46762785855459027,
            "section_title": "abstract",
            "char_start_offset": 2,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058990478515625
        },
        {
            "corpus_id": "247155169",
            "title": "A Survey of Multilingual Models for Automatic Speech Recognition",
            "text": "3.1.1. Acoustic models (Heigold et al., 2013) presents an extensive study comparing three major approaches towards building multilingual ASR using the GMM or DNN (or combined) and HMM setup. \n\n\u2022 Language independent feature extraction: In this approach, feature extraction is learned independent of the languages, so the feature extractor can be used for any language, after which an ASR system can be built on top of it. A major drawback of such a system is that it is not optimised jointly. \n\n\u2022 Transfer learning: In this approach, a model's learnable weights are initialised from a model trained on a high-resource language, followed by a separate fine-tuning step. \n\n\u2022 Multi-task learning: In this setting, one unified model is optimized/trained to perform n tasks, without any separate fine-tuning step. In our case, n is the number of different languages i.e., and each language is treated as a separate task. (Heigold et al., 2013) conduct experiments on 11 languages with a total duration of around 10000 hours for training and 35 hours for testing. The authors first compare, on all the 11 languages, GMM-HMM and DNN-HMM based monolingual systems and observe that the later performs better. Therefore they use DNN-HMM architecture for cross-lingual and multilingual experiments. (i) In the cross-lingual experiments, the DNN-HMM model is first trained on 3000 hours of English and then fine-tuned in two settings, either fine-tuning the top two layers or all layers for each language. Finetuning the top two layers avoids over-fitting for low resource languages. (ii) In the multilingual experiments, the DNN-HMM model is trained on combined data of all the 11 languages. It performs better than the cross-lingual setup consistently on all the languages. This study shows that multilingual models trained by combining data of all languages performs better than fine-tuning an English model on the target languages.",
            "score": 0.4674246665212257,
            "section_title": "Models that use only labeled data",
            "char_start_offset": 9103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 190
                },
                {
                    "start": 193,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 23,
                    "end": 45,
                    "matchedPaperCorpusId": "14310182"
                },
                {
                    "start": 916,
                    "end": 938,
                    "matchedPaperCorpusId": "14310182"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0965576171875
        },
        {
            "corpus_id": "12477464",
            "title": "Local System Voting Feature for Machine Translation System Combination",
            "text": "Adding more linguistic informed models (e.g. language model or translation model) additionally to the standard models into system combination seems to yield no or only small improvements. The reason is that all these models should have already been applied during the decoding process of the individual systems (which serve as input hypotheses for system combination) and hence already fired before system combination. To improve system combination with additional models, we need to define a model which can not be applied by an individual system.\n\nIn state-of-the-art confusion network system combination the following models are usually applied:\n\nSystem voting (globalVote) models For each word the voting model for system i (1 \u2264 i \u2264 I) is 1 iff the word is from system i, otherwise 0. Binary primary system model (primary) A model that marks the primary hypothesis. Language model 3-gram language model (LM) trained on the input hypotheses. Word penalty Counts the number of words.\n\nTo gain improvements with additional models, it is better to define models which are not used by an individual system. A simple model which can not be applied by any individual system is the binary system voting model (globalVote). This model is the most important one during system combination decoding as it determines the impact of each individual system. Each system i is assigned one globalVote model which fires if the word is generated by system i. Nevertheless, this simple model is independent of the actual words and the score is only based on the global preferences of the individual systems. This disadvantage prevents system combination from producing words which have only been seen by systems with low system weights (low globalVote model weights). To give systems and words with low weights a chance to affect the final output, we define a new local system voting model (localVote) which makes decisions based on the current word options and not only on a general weight. The local system voting model allows system combination to prefer different system outputs at different word positions even for the same sentence.\n\nMotivated by the success of neural networks in language modelling (Bengio et al., 2006, Schwenk andGauvain, 2002) and translation modelling (Son et al., 2012), we choose feedforward neural networks to train the novel model. Instead of calculating the probabilities in a discrete space,",
            "score": 0.4668363655156385,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "262043747",
            "title": "OWL: A Large Language Model for IT Operations",
            "text": "Large Language Models (LLMs) (Chowdhery et al., 2022b;Touvron et al., 2023a;b) have emerged as powerful tools in the field of natural language processing (NLP) and Artificial Intelligence (AI). The release of GPT-3 (Brown et al., 2020a) in 2020 demonstrated the advantages of training large auto-regressive LLMs. With 175 billion parameters, GPT-3 surpassed previous models in various LLM tasks including reading comprehension, question answering, and code generation. Similar results have been achieved by other models as well. Furthermore, evidence suggests that larger models exhibit emergent behaviors and possess abilities not present in smaller models. For instance, they can learn tasks from a few examples, a phenomenon known as few-shot prompting. This capability expands the scope of supported tasks and facilitates the automation of new language tasks for users. However, the majority of research endeavors have been directed towards constructing general Large Language Models (LLMs) that encompass a wide spectrum of subjects, certain models trained on domain-specific data have exhibited exceptional performance within their specific domains, such as science and medicine. These discoveries underscore the necessity for additional advancements in domain-specific models. \n\nIn the field of IT operations (Du et al., 2017;Liu et al., 2023;Guo et al., 2023a), the significance of natural language processing (NLP) technologies is steadily on the rise. This paper undertakes the crucial task of delineating a set of specific assignments within the realm of IT operations, encompassing areas such as information security, system architecture, and other domains. However, the complexity and specific terminology of IT operations pose formidable challenges, including a unique set of terminologies, processes, and contextual nuances that are not easily decipherable by conventional NLP models. Therefore, it becomes increasingly evident that there is a pressing need for the development and deployment of a Large Language Model specifically tailored to the exigencies of IT operations within such specialized domains. The fine-tuned large language model customized to this purpose promises to be an invaluable asset in navigating the complexities of IT operations within these highly specialized domains.",
            "score": 0.46515638224918315,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 236,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1316,
                    "end": 1333,
                    "matchedPaperCorpusId": "4232579"
                },
                {
                    "start": 1350,
                    "end": 1368,
                    "matchedPaperCorpusId": "251741097"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1922607421875
        },
        {
            "corpus_id": "260316119",
            "title": "A critical review of large language models: Sensitivity, bias, and the path toward specialized AI",
            "text": "The observed deviation between the Specialized SDG model and OpenAI's GPT-3.5 underscores the need for careful consideration in the application of these models. It accentuates the trade-off between the vast coverage of general models like GPT-3.5 and the precision of specialized models. This disparity, resulting from differences in training data and model parameter tuning capabilities, warrants thoughtful contemplation. Importantly, despite some observed overlap in the results produced by large language models (LLMs) like GPT-3.5 and specialized models, their performance should not be considered interchangeable. The choice between a general or specialized model must be dictated by the specific requirements of a given task. Broad, catch-all classification tasks may benefit from LLMs, whereas precision-focused tasks necessitate the use of specialized models. \n\nWhile the progression of LLMs suggests the potential for more nuanced SDG detection in the future, it is contingent upon researchers engaging in more specific data training and further model parameter tuning. This approach could allow LLMs to match the precision of specialized models without compromising their broad coverage. However, this observation comes with a cautionary note. It is clear that LLMs, such as GPT-3.5, operate as black-box models, leaving us without a clear view of how they arrive at their conclusions. Consequently, their expansive, liberal application could lead to unpredicted and, in some cases, undesired outcomes. As such, a more reliable approach, especially when accuracy and transparency are of utmost importance, would be to utilize a specialized model tailored to the task at hand. \n\nOur experiment extends beyond the specific comparison of GPT-3.5 and the specialized model. It offers a unique vantage point on contrasting a highly specialized machine learning model with an autonomous LLM. It invites scholars to carefully consider the trade-offs of using LLMs, including their cost, complexity, and opacity. It also underlines that many applications, developing a specialized model tailored to the task at hand might be more straightforward, cost-efficient, and transparent. While LLMs are undoubtedly powerful and versatile, their use should not be considered a one-size-fits-all solution.",
            "score": 0.46474014975876554,
            "section_title": "Conclusion",
            "char_start_offset": 33301,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 868
                },
                {
                    "start": 871,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2298
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21240234375
        },
        {
            "corpus_id": "249538647",
            "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
            "text": "Specialized Models. Previous research focuses on building specialized models for specific tasks. \n\nCNNs [47,26,70] and ViTs [20,53,76,80] are developed for image classification. Subsequent works re-design them to adapt to diverse downstream visual tasks, e.g., object detection [63] and segmentation [15,48]. In NLP, different architectures are specifically designed for neural machine translation [77], natural language understanding [19], and natural language generation [51]. As for vision-language tasks, previous works usually combined modality-specific encoders and representation fusion modules together [13,54]. Recently, [89,65,71] integrate several specialized models into a single one to handle diverse tasks. Such integrated specialized models are equipped with multiple task-specific modules to adapt to as many downstream tasks as possible. However, these methods still follow the task-specific paradigm, which conflicts with the objective of generalist models. \n\nVanilla Generalist Models. Vanilla generalist models handle different tasks and modalities with shared parameters. Uni-Perceiver [93] formulates various perception tasks as finding the maximum likelihood target for each input through the similarity of their representations. OFA [79], Flamingo [3] and SimVLM [84] attempt to unify different tasks into sequence-to-sequence generation. UniCORN [86] and Gato [62] further incorporate bounding box and reinforcement learning tasks into the unified formulation, respectively. These generalist models not only achieve competitive performance on pre-training tasks with shared parameters, but also can perform zero-shot inference on new tasks [62,93]. However, these methods rarely investigate the potential interference among different modalities and tasks, which could result in the performance degradation of generalist models. \n\nMulti-Task Learning. Multi-task learning [8,17] has been widely studied in the community of vision [27,74,72], language [25,16,50] and vision-language learning [10,55,29]. While multi-task training enables collaboration between tasks, it may also introduce the task interference problem [81,83,28,36,72].",
            "score": 0.46347209221304975,
            "section_title": "Related Works",
            "char_start_offset": 4709,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2026
                },
                {
                    "start": 2027,
                    "end": 2159
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "229363322"
                },
                {
                    "start": 134,
                    "end": 137,
                    "matchedPaperCorpusId": "232035922"
                },
                {
                    "start": 278,
                    "end": 282,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 1896,
                    "end": 1899,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 1954,
                    "end": 1958,
                    "matchedPaperCorpusId": "54465873"
                },
                {
                    "start": 1958,
                    "end": 1961,
                    "matchedPaperCorpusId": "85544221"
                },
                {
                    "start": 1961,
                    "end": 1964,
                    "matchedPaperCorpusId": "159040666"
                },
                {
                    "start": 2019,
                    "end": 2022,
                    "matchedPaperCorpusId": "208637516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5703125
        },
        {
            "corpus_id": "264426172",
            "title": "LLM-Prop: Predicting Physical And Electronic Properties Of Crystalline Solids From Their Text Descriptions",
            "text": "T5 (Raffel et al., 2020) was the first unified language model that converts all textbased language problems into a text-to-text problem to perform both generative and predictive tasks. This framework is important for adapting and finetuning T5 on many tasks, and enables efficient multitask finetuning. Raffel et al. (2020) carefully compared different architectures, pretraining objectives, unlabeled datasets, transfer approaches, and more on dozens of natural language tasks and then combined the best-performing approaches in each comparison to pretrain T5. For instance, while MatBERT, a BERT-based model, was pretrained using a masked language model (MLM) objective (Devlin et al., 2018), T5 uses a span-masking objective (Joshi et al., 2020) as it was shown to outperform MLM objective in terms of predictive power and speed. These considerations motivate our choice to use T5 as our main pretrained model.",
            "score": 0.46063079881211805,
            "section_title": "T5",
            "char_start_offset": 9308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 913
                }
            ],
            "ref_mentions": [
                {
                    "start": 3,
                    "end": 24,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 303,
                    "end": 323,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 728,
                    "end": 748,
                    "matchedPaperCorpusId": "198229624"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12347412109375
        },
        {
            "corpus_id": "259924919",
            "title": "Software Testing With Large Language Models: Survey, Landscape, and Vision",
            "text": "Recently, pre-trained language models (PLMs) have been proposed by pretraining Transformer-based models over large-scale corpora, showing strong capabilities in solving various natural language processing (NLP) tasks [16]- [19]. \n\nStudies have shown that model scaling can lead to improved model capacity, prompting researchers to investigate the scaling effect through further parameter size increases. Interestingly, when the parameter scale exceeds a certain threshold, these larger language models demonstrate not only significant performance improvements but also special abilities such as in-context learning, which are absent in smaller models such as BERT. \n\nTo discriminate the language models in different parameter scales, the research community has coined the term large language models (LLM) for the PLMs of significant size. LLMs typically refer to language models that have hundreds of billions (or more) of parameters and are trained on massive text data such as GPT-3, PaLM, Codex, and LLaMA. LLMs are built using the Transformer architecture, which stacks multi-head attention layers in a very deep neural network. Existing LLMs adopt similar model architectures (Transformer) and pre-training objectives (language modeling) as small language models, but largely scale up the model size, pre-training data, and total compute power. This enables LLMs to better understand natural language and generate high-quality text based on given context or prompts. \n\nNote that, in existing literature, there is no formal consensus on the minimum parameter scale for LLMs, since the model capacity is also related to data size and total compute. In a recent survey of LLMs [17], the authors focus on discussing the language models with a model size larger than 10B. Under their criteria, the first LLM is T5 released by Google in 2019, followed by GPT-3 released by OpenAI in 2020, and there are more than thirty LLMs released between 2021 and 2023 indicating its popularity. In another survey of unifying LLMs and knowledge graphs [24], the authors categorize the LLMs into three types: encoder-only (e.g., BERT), encoder-decoder (e.g., T5), and decoder-only network architecture (e.g., GPT-3).",
            "score": 0.4569934752237978,
            "section_title": "Large Language Model (LLM)",
            "char_start_offset": 8191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 231,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1679,
                    "end": 1683,
                    "matchedPaperCorpusId": "257900969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11456298828125
        },
        {
            "corpus_id": "266844751",
            "title": "Fine-tuning and Utilization Methods of Domain-specific LLMs",
            "text": "To develop a finance-specific LLM, the choice of a pre-trained language model is crucial. The selected model should have language comprehension abilities that extend to the specific terminologies and contexts of the finance domain. Consider the following criteria when choosing a pre-trained LLM model: \n\n1) Model Size: The size of the model impacts its performance. Generally, larger models tend to deliver better results. \n\n2) Model Purpose: Different models may be more suitable depending on their intended use. \n\nFor tasks like sentiment analysis of financial news, a model specialized in natural language processing is preferable. \n\n3) Model Availability: If a model is not publicly available, training the model from scratch becomes necessary. This process can consume considerable time and resources. \n\nWhen selecting a pre-trained LLM model, consider the following elements: Once a pre-trained model is selected, the next step involves fine-tuning it to adapt to the intricacies of the finance domain. The fine-tuning process refines the model's understanding of financerelated language patterns, enhancing its applicability to tasks within this specific industry. \n\nSeveral pre-trained Language Models (LLMs) are available for applications in the finance domain. \n\nHere are some notable models: \n\n1) GPT-4: GPT-4 is a large-scale language model developed by OpenAI. It has 1.75 trillion parameters, making it one of the most powerful language models in the world. GPT-4 can be used in a variety of fields, including natural language processing, machine translation, and code generation. It has been shown to be capable of generating human-quality text, translating languages accurately, and writing different kinds of creative content. \n\n2) BERT: BERT is a large-scale language model developed by Google AI. It has 100 million parameters, making it smaller than GPT-4 but still very powerful. BERT can be used in a variety of fields, including natural language processing, question answering, and sentiment analysis. It has been shown to be very good at understanding the meaning of text and answering questions about it. \n\n3) LLaMA2: LLaMA2 is a large-scale language model developed by Meta. It has 7 to 65 billion parameters, making it a smaller model than GPT-4 but still very powerful.",
            "score": 0.45657617515028304,
            "section_title": "Selection of Pre-trained LLM Model",
            "char_start_offset": 16880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 302
                },
                {
                    "start": 305,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 514
                },
                {
                    "start": 517,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 807
                },
                {
                    "start": 810,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1271
                },
                {
                    "start": 1274,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1744
                },
                {
                    "start": 1747,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2130
                },
                {
                    "start": 2133,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2298
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1708984375
        },
        {
            "corpus_id": "266755751",
            "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
            "text": "To address the training and the data challenges mentioned above, we propose and study a practical setting for model composition: (i) we are given access to one (or more) augmenting model(s) and an anchor model, (ii) we are not allowed to modify the weights of either models, and (iii) we only have access to a small amount of data, representing the \"combined skills\" of the given models, e.g., code generation with complex logical reasoning. \n\nPrior work has largely approached the question of composition from either a routing or a merging standpoint, neither of which provide an effective solution to capture this setting. Routing between the given models, i.e., choosing an output of one model over the other (Ma et al., 2019), or performing a soft ensemble (Muqeeth et al., 2023) is not effective when neither of the models can demonstrate the desired capability. Another body of work creates a combined model by an arithmetic combination of base model parameters (Wortsman et al., 2022;Ilharco et al., 2022;Matena & Raffel, 2022). However, these settings are naturally restrictive and their efficacy is unclear when combining models with different sizes and pre-training objectives (Yadav et al., 2023). \n\nIn this work, we propose a novel Composition to Augment Language Models (CALM) framework to address the general model composition setting mentioned above. Rather than a shallow combination of the augmenting and anchor LMs (Wortsman et al., 2022;Ilharco et al., 2022), CALM introduces a small number of trainable parameters over both augmenting and anchor models' intermediate layer representations. CALM finds an effective combination of the given models to perform new challenging tasks more accurately than either of the models alone, while preserving the capabilities of individual models. Figure 1 highlights few motivating scenarios for CALM. \n\nWe study key practical applications of CALM: language inclusivity and code generation. For language inclusivity ( \u00a74.2), we use a model that has been trained on a set of low-resource languages.",
            "score": 0.456148486056424,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1208
                },
                {
                    "start": 1211,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2054
                }
            ],
            "ref_mentions": [
                {
                    "start": 712,
                    "end": 729,
                    "matchedPaperCorpusId": "58145688"
                },
                {
                    "start": 968,
                    "end": 991,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1012,
                    "end": 1034,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1433,
                    "end": 1456,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24169921875
        },
        {
            "corpus_id": "272987305",
            "title": "The Construction of Instruction-tuned LLMs for Finance without Instruction Data Using Continual Pretraining and Model Merging",
            "text": "Recently, large language models (LLMs) have demonstrated excellent performance. The latest models, such as ChatGPT [26] and GPT-4 [27], exhibit particularly high performance and significant generalization abilities. The basis of these models begins with the transformer [43]. BERT [8] and GPT series [4], [29], [30] were developed using the transformer. Other LLMs have also been proposed, such as Bard [9], LLaMA [41], [42], Dolly [7], BLOOM [34], Vicuna [44], PaLM [2], [5], and Gemini [39]. \n\nThe major difference between the latest LLMs and previous language models (e.g., BERT) is that one model can answer questions in multiple languages and domains and provide responses by following the instructions. While BERT was trained separately in different languages and domains [38], the latest LLMs, such as GPT4, can freely process multiple languages. Moreover, whereas BERT can only fill in incom-plete sentences, the latest LLMs can answer questions in the same manner as humans. \n\nNevertheless, even if LLMs can answer questions in multiple languages and domains, domain-specific models could still be useful. For example, Hirano et al. [15] tuned the Englishbased model to Japanese, achieving better outputs than the original model. Sukeda et al. [36] also tuned the English-based model to the Japanese medical domain. \n\nAlthough constructing instruction-tuning data is essential for building domain-specific LLMs, such data cannot be easily obtained. The LLMs that humans can naturally use are instruction-tuned LLMs. Instruction tuning [45] is a method to fine tune LLMs to answer questions conversationally, as humans do. It requires a large amount of difficult-to-obtain instruction data for domain-specific LLMs. For example, building an instruction-tuned LLM for finance necessitates the preparation of a large amount of financial instruction data. This is because instruction tuning is usually conducted after the LLMs' pretraining and, in the case of domain-specific ones, instruction tuning for general purposes is not compatible with the domain-specific pretrained ones.",
            "score": 0.4558474853670218,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 275
                },
                {
                    "start": 276,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1324
                },
                {
                    "start": 1327,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1630
                },
                {
                    "start": 1631,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 2086
                }
            ],
            "ref_mentions": [
                {
                    "start": 270,
                    "end": 274,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 281,
                    "end": 284,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 300,
                    "end": 303,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 414,
                    "end": 418,
                    "matchedPaperCorpusId": "257219404"
                },
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "259950998"
                },
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "253420279"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "258832318"
                },
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "264146504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147705078125
        },
        {
            "corpus_id": "2508203",
            "title": "Low-Cost Enrichment of Spanish WordNet with Automatically Translated Glosses: Combining General and Specialized Models",
            "text": "In order to improve results, in first place we turned our eyes to language modeling. In addition to the language model built from the Europarl corpus ('EU') and the specialized language model based on the small training set of parallel glosses ('WNG'), two specialized language models, based on the two large monolingual Spanish electronic dictionaries ('D1' and 'D2') were used. We tried several configurations. In all cases, language models are combined with equal probability. See results, for the development set, in Table 2.\n\nAs expected, the closer the language model is to the target domain, the better results. Observe how results using language models 'D1' and 'D2' outperform results using 'EU'. Note also that best results are in all cases consistently attained by using the 'WNG' language model. This means that language models estimated from small sets of indomain data are helpful. A second conclusion is that a significant gain is obtained by incrementally adding (in-domain) specialized language models to the baselines, according to all metrics but BLEU for which no combination seems to significantly outperform the 'WNG' baseline alone. Observe that best results are obtained, except in the case of BLEU, by the system using 'EU' as translation model and 'WNG' as language model. We interpret this result as an indicator that translation models estimated from out-of-domain data are helpful because they provide recall. A third interesting point is that adding an out-of-domain language model ('EU') does not seem to help, at least combined with equal probability than in-domain models. Same conclusions hold for the test set, too.",
            "score": 0.45531315939004274,
            "section_title": "Combining Sources: Language Models",
            "char_start_offset": 11072,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27490234375
        },
        {
            "corpus_id": "258832731",
            "title": "CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models",
            "text": "We are also interested in measuring how well the adapted models retain the general language modeling ability of the original large model. We use perplexity on The Pile (Gao et al., 2021) as a proxy of general language modeling performance, as it is a large collection of many datasets from different domains, often used to train generalist LMs (Black et al., 2022;Biderman et al., 2023). To this end, we also extract random train-fit and test subsets from The Pile. While some subsets of The Pile are also present in the training data for OPT, we do not measure performance on The Pile as a benchmark for model quality, and are only interested in it as a proxy for degradation in general language modeling ability of the adapted models. \n\nWe compare fitting the combination function on the target domain train-fit, as done throughout the paper, as well as on the combination of the target domain and The Pile train-fit sets. Table 5 reports results for Amazon Reviews, and full results can be found in Appendix B. \n\nWhen fitting the combination function on Amazon Reviews, we observe a significant degradation on The Pile. However, different combination methods behave differently in this regard. For example, entropy-scalar and full-vector perform similarly in Amazon Reviews (15.50 vs 15.43), but the former performs much better on The Pile (7.35 vs 10.07). \n\nIt is also remarkable that The Pile perplexity of the combined model remains far better than the small fine-tuned LM (e.g. 7.35 for entropy-scalar vs 19.78 for the small LM), while also performing better in-domain. \n\nWhen fitting the combination function on the mixin set, we observe that performance on The Pile is almost entirely preserved, at the expense of a slight degradation on Amazon Reviews. For example, for full-scalar, the combination fit on the mixin set achieves a perplexity of 15.45 on Amazon Reviews and 6.85 on The Pile, both within 0.1 of the best results for each dataset. \n\nOverall, these results show that a large model can be adapted to a particular domain while mitigating degradation in the general domain by mixing in-domain and general text to train the combination function.",
            "score": 0.454554411866238,
            "section_title": "Effect on general language modeling",
            "char_start_offset": 16479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1576
                },
                {
                    "start": 1579,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1954
                },
                {
                    "start": 1957,
                    "end": 2164
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2314453125
        },
        {
            "corpus_id": "254877510",
            "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
            "text": "The dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021). This results in specialized models that are explicitly trained to run inference over a single domain and task. Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022). Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models. \n\nHowever, the multi-task learning setup suffers from two practical limitations. First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy or to guard intellectual property to annotations. Second, because a significant amount of data or task combinations are not beneficial to performance (Poth et al., 2021), building a single model requires training on all data set combinations to identify the optimal one, which can be prohibitive especially if there are many available source data sets or models. \n\nModel merging is defined as combining multiple models into a single one in parameter space without access to data (Matena & Raffel, 2021). This technique provides an alternative to building a single model while satisfying data privacy constraints. Weight merging algorithms usually also have a closed-form solution, making them very efficient as no retraining is necessary, thus enabling usage even when a large number of data sets or model combinations are available. Merging can be considered as an alternative to model ensembling (Opitz & Maclin, 1999;Rokach, 2010), where the outputs of individual models are combined to produce the final prediction.",
            "score": 0.45277154345188597,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 201,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 515,
                    "end": 542,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 542,
                    "end": 560,
                    "matchedPaperCorpusId": "233289699"
                },
                {
                    "start": 560,
                    "end": 580,
                    "matchedPaperCorpusId": "248780114"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "233289699"
                },
                {
                    "start": 764,
                    "end": 784,
                    "matchedPaperCorpusId": "220045358"
                },
                {
                    "start": 1355,
                    "end": 1374,
                    "matchedPaperCorpusId": "233289699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "12982389",
            "title": "A bit of progress in language modeling",
            "text": "ing, trying 9 different techniques, finding a new clustering technique, fullibmpredict, that is a bit better than standard ibm clustering, and examining the limits of improvements from clustering. We also showed that clustering performance may depend on smoothing in some cases.\n\nOur word-error rate reduction of 8.9% from combining all techniques except caching is also very good.\n\nFinally, we put all the techniques together, leading to a 38%-50% reduction in perplexity, depending on training data size. The results compare favorably to other recently reported combination results (Martin et al., 1999), where, essentially using a subset of these techniques, from a comparable baseline (absolute discounting), the perplexity reduction is half as much. Our results show that smoothing can be the most important factor in language modeling, and its interaction with other techniques cannot be ignored.\n\nIn some ways, our results are a bit discouraging. The overall model we built is so complex, slow and large that it would be completely impractical for a product system. Despite this size and complexity, our word error rate improvements are modest. To us, this implies that the potential for practical benefit to speech recognizers from language model research is limited. On the other hand, language modeling is useful for many fields beyond speech recognition, and is an interesting test bed for machine learning techniques in general.\n\nFurthermore, parts of our results are very encouraging. First, they show that progress in language modeling continues to be made. For instance, one important technique in our system, sentence mixture models, is only a few years old, and, as we showed, its potential has only been partially tapped. Similarly, the combination of so many techniques is also novel. Furthermore, our results show that the improvements from these different techniques are roughly additive: one might expect an improvement of .9 bits for the largest training size based on simply adding up the results of Figure 11, and instead the total is about .8 bits -very similar. This means that further incremental improvements may also lead to improvements in the best models, rather than simply overlapping or being redundant.\n\nAs we noted in Section 10, there are many other promising language modeling techniques currently being pursued, such as maximum entropy models, neural networks, latent semantic analysis, and structured language models. Figuring out how",
            "score": 0.45200855882079016,
            "section_title": "Discussion",
            "char_start_offset": 99717,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 584,
                    "end": 605,
                    "matchedPaperCorpusId": "7837346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08465576171875
        },
        {
            "corpus_id": "243847307",
            "title": "Towards Building ASR Systems for the Next Billion Users",
            "text": "The Indian subcontinent is one of the most linguistically diverse regions in the world as well as one of the most populous regions in the world -little wonder that is called a 'subcontinent'. It is home to around 2 billion people from 7 countries who speak 100+ major languages (and thousands of minority languages and dialects) belonging to four major language families 1,2 . Of these the Indo-Aryan and Dravidian languages are spoken by a large section of the population. These language families have intermingled over a large period of time giving rise to the Indian linguistic area/sprachbund where languages across these families share many features (Emeneau 1956;Subb\u0101r\u0101o 2012;Kunchukuttan and Bhattacharyya 2020).\n\nBuilding high-quality ASR models for such a large and diverse pool of languages is challenging, even if we re-Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1 https://en.wikipedia.org/wiki/Languages of South Asia 2 https://en.wikipedia.org/wiki/Languages of India strict ourselves to 30 odd languages which have more than a million speakers. Many modern ASR models rely on large amounts of labeled data for each language to build highquality ASR systems. Such approaches are expensive and not scalable, thus limiting the reach of ASR technologies to some languages and a section of the population. In addition to these challenges on availability of labeled data, Indian languages also face a set of common challenges that need to be addressed. Collection on unlabeled data for pretraining can be undertaken as a joint effort since many sources might be shared amongst these languages. Most Indic scripts have a larger character set than English which can be addressed in a uniform way. The complex inflectional systems of Indian languages would make modelling phonotactics more challenging. The rich inflectional/agglutinative nature of Indian languages result in larger vocabulary sizes, presenting challenges to incorporating language models. At the same time there are opportunities from a unified perspective. A largely overlapping phoneme inventory, logically overlapping character sets and syntactic similarity can help utilize linguistic similarity at various levels to build multilingual models where transfer learning can be effective. In this",
            "score": 0.4519026259318663,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 655,
                    "end": 669,
                    "matchedPaperCorpusId": "52189296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01507568359375
        },
        {
            "corpus_id": "271915854",
            "title": "Great Memory, Shallow Reasoning: Limits of kNN-LMs",
            "text": "A foundational property of pretrained language modeling (Peters et al., 2018;Devlin et al., 2019) has been that improvements to the perplexity of the model lead to improvements on downstream tasks. This property is central to the scaling of large language models (LLMs) where researchers focus nearly exclusively on perplexity as a proxy metric for improved general purpose abilities (Kaplan et al., 2020). In recent years, this research has centered primarily on high-quality text data at greater and greater quantities as the limiting component for producing better language models (Hoffmann et al., 2022). \n\nThis increasing need for data to train language models has led to significant challenges. On one hand, including as much high-quality data as possible results in improved downstream performance. On the other hand, this data is often protected by licenses or copyright, which means training on such data brings legal issues. For example, the recent high-profile lawsuit from the New York Times notes the clear use of their data in OpenAI models (Grynbaum and Mac, 2023). \n\nIt would be ideal to circumvent this issue entirely with alternative approaches. If a model could be trained on lower-quality data but adapted to perform well on real tasks, it might provide a technical workaround. Non-parametric Language Models (NPLMs), such as kNN-LMs, have emerged as a promising approach in this space (Khandelwal et al., 2020). kNN-LMs extend neural LMs by linearly interpolating with simple k-nearest neighbor LMs. This approach can improve language modeling with its memory over a massive collection of texts, usually referred to as a datastore. Khandelwal et al. (2021) and Shi et al. (2022) validate that kNN-LMs achieve better performance on downstream tasks compared to standard LMs. The SILO model of Min et al. (2024) applies this approach further by training a LM exclusively on license-permissive data, and using a non-parametric datastore to improve the models during inference. \n\nIn this work, we study the limits of how kNN-LMs can be used to improve LLMs.",
            "score": 0.4516779584041551,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 56,
                    "end": 77,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 77,
                    "end": 97,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1406,
                    "end": 1431,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1653,
                    "end": 1677,
                    "matchedPaperCorpusId": "222125236"
                },
                {
                    "start": 1682,
                    "end": 1699,
                    "matchedPaperCorpusId": "249152130"
                },
                {
                    "start": 1813,
                    "end": 1830,
                    "matchedPaperCorpusId": "260704206"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0855712890625
        },
        {
            "corpus_id": "9777884",
            "title": "A Layered Grammar Model: Using Tree-Adjoining Grammars to Build a Common Syntactic Kernel for Related Dialects",
            "text": "The above-mentioned modelling choice may seem counter-intuitive in the theoretical frame of structural linguistics. One might object that if the language itself is the whole object of description, then it is absurd to include it as a category in the description. This view is justified as long as one does not wish to take into account dialectal variation as an internal system variable. If this is the case, then every single dialect must be considered an isolate and be given a holistic, unitary description. But in the context we are working in, several rationales lead us to think that it might be a good idea to include dialectal variation in the description. \n\nWe already have mentioned practical reasons (see above, in Introduction). The \"time saving\" and \"resource sharing\" rationales applies to our method as well as to others (like meta-grammars). A supplementary argument, which applies more specifically to our method, is the fact that in the cases we are studying, not only some syntactical properties of the languages are common, but also an important part of the vocabulary, until at the very surface level. This speaks for sharing bottomlevel structures. \n\nBut there is another, less practical, type of argument: if we have a modular grammatical system model which \"contains\" more than one language in itself, we are able to model the linguistic competence in one of the languages, but also to model multilingual (in the present case, multidialectal) linguistic competence. \n\nIf our goal is to model monolingual competence, this is easily done by unifying the lan parameter with one of its possible values, and then erasing the (now redundant) parameter from the description. \n\nHowever, in some cases, we might want to have a model of multidialectal variation. Considered from the E-language side, we then have a model of a dialectal continuum. Considered from the Ilanguage side, we have a model of the linguistic competence of a multilingual speaker of related dialects. The interplay of grammatical structures of a multidialectal system, the possibilities of combination and unification given different levels of instantiation of the lan parameter, might provide us with a model for such linguistic phenomena as: specialized repertoires, code switching, code mixing, or koin\u00ea emergence.",
            "score": 0.4514633832682049,
            "section_title": "Discussion",
            "char_start_offset": 18650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 664
                },
                {
                    "start": 667,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2305
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03265380859375
        },
        {
            "corpus_id": "273098762",
            "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
            "text": "Large Language Models (LLMs) have significantly revolutionized the natural language processing area, showcasing unparalleled abilities across various tasks (Achiam et al., 2023). Despite their versatility, LLMs exhibit limitations in specialized areas such as mathematics, programming, etc., which * Work done during an internship at Alibaba Cloud. \u2020 Tao Gong is the corresponding author. hinder the potential of wide-ranging applications. \n\nTo address these gaps, existing work (Liu et al., 2023;Wang et al., 2023) has sought to enhance the diverse skills of pre-trained LLMs through customized data strategies. However, they require extensive computational efforts and massive data volumes, challenging the widespread accessibility of LLM research. Furthermore, while Parameter-Efficient Fine-Tuning (PEFT) techniques offer a reduction in training requirements, their effectiveness tends to diminish (Biderman et al., 2024;Wu et al., 2024) compared to traditional fine-tuning methods, especially as the size of the model and the dataset grows. \n\nSubsequently, another line of research emerged, focusing on methods such as model merging (Akiba et al., 2024) and model expansion (Wu et al., 2024;Choi and Gazeley, 2024;Kim et al., 2023). Model merging methods strive to synthesize a multifaceted model that amalgamates insights from various pre-trained domain-specific LLMs, potentially crafting a model adept at addressing a multitude of tasks concurrently. However, the process of training multiple domain-specific LLMs is resource-intensive. On the other hand, model expansion methods, exemplified by Llama Pro, seek to refine pre-trained models for domain-specific applications in the post-pretraining phase by only fine-tuning the expanded layers. Therefore, it can employ significantly fewer trainable parameters than full model fine-tuning. \n\nHowever, present model expansion methods generally treat each part of LLMs equally, although different layers may exhibit varying sensitivity to incorporated knowledge. This lack of differentiation can result in less-than-ideal knowledge injection results. An intuitive idea is to inject knowledge into the most important layers so that the LLM can more sufficiently leverage the new knowledge without the overhead of redundant adjustments across all layers.",
            "score": 0.45138249259350394,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2047119140625
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "For instance, the OOV rate on the FR set for the 3-multigrams is of 1.4% in the specialized case and of 0.7% in the unified one, while the effective coverage rate for the unified dataset is equal to 98.5%.In contrast, the OOV rates on the EN set are very small (0.06% and 0.03% for the specialized and unified case respectively) and the effective coverage rate stay low enough even if the lexicon has been unified (89.9%).Besides, when lexicons are large, unifying the languages does not significantly reduce the OOV rate, which does not impact the recognition performance.For example, the unification of languages for the 3-multigrams only reduce the OOV rate from 0.5% to 0.4% on the FR dataset and from 2.2% to 1.6% on the EN dataset.\n\nMoreover, languages models based on multigrams often outperform traditional language models based on words or characters and there is always a system based on multigrams which is better than systems based on traditional models.Another advantage of dealing with multigrams is related to the lexicon size, which is highly reduced compared to word lexicons.For instance, there is 116k words in the large unified EN + FR lexicon while there is 71.6k 5-multigrams, 16.9k 3-multigrams and only 3.1k 2-multigrams.\n\nFinally, we analyze the system complexity according to the search graph size (number of states and transitions in the FST automaton S defined in equation 1), the search graph volume on disk and the decoding time.Besides, the memory and the processing time during the decoding process depend on the search graph size (Mohri et al. (2002)).Note that complexities are computed from the unified FR+EN lexicon and that the character language model is a 9-gram model (in contrast to previous results based on a 10-gram model) to provide comparable complexities.As illustrated in Table 3, the number of states and transitions is highly reduced using multigram language models compared to the word-based language models.",
            "score": 0.45093845961162615,
            "section_title": "Discussion",
            "char_start_offset": 30978,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 422
                },
                {
                    "start": 422,
                    "end": 573
                },
                {
                    "start": 573,
                    "end": 737
                },
                {
                    "start": 739,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1245
                },
                {
                    "start": 1247,
                    "end": 1459
                },
                {
                    "start": 1459,
                    "end": 1585
                },
                {
                    "start": 1585,
                    "end": 1802
                },
                {
                    "start": 1802,
                    "end": 1959
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0217742919921875
        },
        {
            "corpus_id": "244037178",
            "title": "Can You Tell? SSNet - A Biologically-Inspired Neural Network Framework for Sentiment Classifiers",
            "text": "One other potential concern here is that the models may be strongly correlated as they are trying to predict the same thing. We compensate for this by using fundamentally different language models. Each of the models has different limitations that are challenged by the text of the different reviews in the dataset, yielding different predictions on some of them. In addition, we apply random shuffling of the training dataset on each training epoch. These two factors alleviate this potential concern to a large extent, as confirmed by the computational results in Section 3. Thus, our approach is different than the classic leave-one-out training approach in [4,24] and better suited for the current state-of-the-art NLP models and their applications on large datasets of interest. \n\nIn principle, there are two potential approaches to combining the models: systematic and heuristic. A systematic approach avoids the use of heuristics in deciding how to combine the models. Instead, one states assumptions on the properties of an individual model and then builds a general rule to combine the models. Following this principle, we introduce a neural network combiner in Section 2.1 and analyze its properties. This approach delivers state-of-the-art accuracy results on the benchmark corpus of interest, which we provide in Section 3. For a baseline comparison we consider the systematic Bayesian combination rules in [22] (Sum, Majority Vote, Average, Max). A brief discription of this is given in Section 2.2. \n\nWe also propose a hybrid heuristic-systematic technique for combining models in Section 2.3, which makes use of the combination rules in [22] but with our heuristic way of choosing what and when to combine. This hybrid technique shows performance characteristics that are pretty close to the leading neural network combiner and outperforms the classic Bayesian rule combiners -see Section 3.",
            "score": 0.45057687131130164,
            "section_title": "Sagittal stratum-inspired neural network",
            "char_start_offset": 16913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1512
                },
                {
                    "start": 1515,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 661,
                    "end": 664,
                    "matchedPaperCorpusId": "11152531"
                },
                {
                    "start": 1419,
                    "end": 1423,
                    "matchedPaperCorpusId": "1991617"
                },
                {
                    "start": 1652,
                    "end": 1656,
                    "matchedPaperCorpusId": "1991617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1966552734375
        },
        {
            "corpus_id": "237503549",
            "title": "MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model",
            "text": "Pretrained language models are trained from random initialization on a large corpus C of unlabelled sentences. Each sentence is used to optimize the parameters of the model using a pretraining objective, for example, masked language modelling, where, for a given sentence, 15% of the tokens are masked in the input m, and the model is trained to predict those tokens J(\u03b8) = \u2212log p \u03b8 (x m | x \\m ) . C is usually a corpus of no specific domain, 3 e.g. Wikipedia or crawled web text. Domain-adaptive pretraining is the process of continuing to pretrain a language model to suit a specific domain (Gururangan et al., 2020;Han and Eisenstein, 2019). This process also uses the masked language modelling pretraining objective, but the model is trained using a domain-specific corpus S, e.g. biomedical text if the model should be suited to the biomedical domain. Our goal is to pretrain a single model, which will be used for downstream tasks in multiple languages within a specific domain, as opposed to having a separate model for each language. This single multilingual domain-specific model should, ideally, perform as well as language-specific domain-specific models in a domain-specific downstream task.\n\nIn pursuit of this goal, we use different types of corpora for domain adaptive pretraining of a single multilingual model. Each considered corpus has two properties: (1) a domain propertyit is a general or specific corpus; and (2) a language property -it is either monolinugal or multilingual. These properties can be combined, for example the multilingual Wikipedia is a multi-general corpus, while the abstracts of English biomedical publications would be a monospecific corpus. Recall that specific cor-pora are not always available in languages other than English, but they are useful for adapting to the intended domain; while multi-general are more readily available, and should help maintain the multilingual abilities of the adapted language model. In the remainder of this paper, we will explore the benefits of domain adaptive pretraining with mono-specific, multi-specific, and multi-general corpora. Figure 1 shows how MDAPT extends domain adaptive pretraining to a multiling",
            "score": 0.4485691311198017,
            "section_title": "Problem Formulation",
            "char_start_offset": 4407,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 594,
                    "end": 619,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 619,
                    "end": 644,
                    "matchedPaperCorpusId": "202541481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08477783203125
        },
        {
            "corpus_id": "271309982",
            "title": "Domain-Specific Pretraining of Language Models: A Comparative Study in the Medical Field",
            "text": "Large Language Models (LLMs) have gained increasing popularity in recent times. It is not unusual to see companies employ them as assistants on websites or employ them directly in workflows like the development of internal IT systems. They also have growing relevance in specialized fields like medicine. LLMs could, for example, assist doctors with diagnoses or explain complicated medical situations to patients in simple language. For models to be used reliably, it is important for them to perform well in the domains of their intended usage. High-performance general-purpose models currently include GPT-4 and claude-3-opus, which can be applied to most tasks with usually good results. \n\nHowever, due to their large size and most state-of-the-art (SOTA) general-purpose models being proprietary and only accessible through an API, it is generally not possible to run these LLMs locally. This is a big problem when reliability and privacy are the main concerns. When working with sensitive patient data or with an unreliable internet connection, using an external API is often not feasible. \n\nA possible solution to this problem would be to use smaller, specialized LLMs that perform well on tasks in a certain domain and whose performance in other domains is irrelevant. Small models benefit from comparatively fast inference times, lower latency, and less expensive training. Their reasonably small size allows them to be hosted on local, consumer-grade hardware. Due to their lower parameter count and limited learning capacity, small models inherently perform worse than large models, given similar training conditions such as the amount and quality of data used for training. Focusing the training on a specific domain and ignoring capabilities in all other domains lets us circumvent this limitation to some degree by not forcing the model to learn irrelevant or out-of-domain information and allowing it to fully focus on memorizing domain-related information. \n\nBut how can we create specialized models, and can they really be as good as large general-purpose models in their domain? To answer these questions, we will take a look at different methods of training LLMs, domain-specific datasets and compare the benchmark results of specialized models to general-purpose models in benchmarks related to their domain. This paper will focus on the medical domain for all examples and comparisons.",
            "score": 0.4479752240843201,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 691
                },
                {
                    "start": 694,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1972
                },
                {
                    "start": 1975,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2328
                },
                {
                    "start": 2329,
                    "end": 2406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.373046875
        },
        {
            "corpus_id": "258865893",
            "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
            "text": "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively. \n\nTo obtain the most capable specialist models for each reasoning type, we leverage specialized prompting strategies such as Chain-of-Thought (Wei et al., 2022b) prompting and retrieval-augmented prompting. Experiments on our collection of 12 QA datasets across four diverse reasoning types confirm that our specialist models outperform the backbone model without specialization, but they achieve much lower accuracy on question types outside of their expertise. \n\nWith these specialized models, we propose our MORE framework to combine their strengths. MORE selects the best candidate answer from the pool of specialized models, and we teach MORE to abstain from answering if none of the candidate answers are correct. We design our answer selector based on these indicative features: (1) the match between the question type and each specialized model's expertise; (2) the confidence of each specialized model and the characteristics of their predictions; and (3) the agreement among all specialized models, which is a novel feature that we propose. Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types. \n\nApart from the improved generalizability of MORE, an important byproduct of cross-checking among specialized experts is to offer a useful signal for understanding the whole system's working mechanism. This is validated by the experimental results showing that incorporating agreement among different specialized experts leads to better selective QA results-where the system answers as many questions as possible while maintaining high accuracy-and presenting such internal decision processes to human annotators helps them determine the correctness of the system predictions more accurately and in a shorter time.",
            "score": 0.44679986253757326,
            "section_title": "Introduction",
            "char_start_offset": 1761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2546
                }
            ],
            "ref_mentions": [
                {
                    "start": 847,
                    "end": 866,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "273404376",
            "title": "Qtok: A Comprehensive Framework for Evaluating Multilingual Tokenizer Quality in Large Language Models",
            "text": "To avoid analyzing each tokenizer individually, we decided to combine all the tokens into a unified tokenizer. This artificial unified tokenizer allows us to examine and annotate tokens independently of any specific tokenizer. Later, this unified tokenizer can be used as a metric for more detailed analysis of individual tokenizers. \n\nThe process of consolidating tokens into a single unified tokenizer is relatively straightforward. First, the vocabulary from each unique tokenizer is collected. Then, for each token in an individual tokenizer's vocabulary, a check is performed to determine whether it is already present in the collective set of tokens. If the token is absent, it is added to the unified file; if it is already included, it is disregarded. Additionally, for each token, we maintain a list of its ranks across all 13 tokenizers. This allows us to track the relative importance and usage of each token within different tokenization systems. For individual models within the same group, there may be slight differences in tokens, primarily due to instruction-tuning, which introduces additional tags. However, we chose not to account for these specific tokens, and as a result, the total number of tokens across tokenizers is approximate rather than absolute. \n\nThis approach ensures that the \"unified tokenizer\" consists exclusively of non-duplicate (i.e., unique) tokens, enabling more effective general analysis. Moreover, only unique tokenizers were used to construct this overall vocabulary. We had a question regarding how much tokenizers contribute to the overall addition of tokens. To explore this, we analyzed the cumulative growth of our unified tokenizer as we added new tokenizers to it. The tokenizers were sorted by size, allowing us to observe how each new tokenizer impacts the total number of unique tokens. We found the rapid advancement in language model development, with newer models incorporating significantly larger token vocabularies. This trend likely correlates with improvements in model performance, capability, and potentially, the ability to understand and generate more nuanced and diverse language. The exponential growth also raises questions about the computational resources required for training and running these increasingly large models, as well as their potential capabilities and limitations (Figure 4). Figure 4 illustrates the cumulative growth of unique tokens across language models.",
            "score": 0.4464029249994503,
            "section_title": "Unified tokenizer",
            "char_start_offset": 7091,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1977
                },
                {
                    "start": 1978,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08709716796875
        },
        {
            "corpus_id": "231718643",
            "title": "KoreALBERT: Pretraining a Lite BERT Model for Korean Language Understanding",
            "text": ", text classification, and machine reading comprehension. At a high level, there are two approaches to use pre-trained language models. First, pretrained models can provide additional feature representations for a downstream task. More importantly, pre-trained models can be a baseline upon which the downstream task is finetuned.\n\nBy having an expensive, but shareable pre-training followed by much smaller fine-tuning, it is a powerful paradigm to focus on optimizing the performance of a downstream NLP task. Self-supervised learning with large corpora allows a suitable starting point for an outer task-specific layer being optimized from scratch while reusing the pre-trained model parameters.\n\nSince its introduction, BERT has achieved state-of-the-art accuracy performances for natural language understanding tasks such as GLUE [5], MultiNLI [6], SQuAD v1.1 [7] & SQuAD v2.0 [8], and CoNLL-2003 NER [9]. Despite having fewer parameters than BERT, ALBERT has been able to achieve new state-of-the-art results on the GLUE, RACE [10], and SQuAD benchmarks.\n\nIt is important to remark that a large network is crucial in pushing state-of-the-art results for downstream tasks. While BERT gives a sound choice to build a general language model trained on large corpora, it is difficult to experiment with training large BERT models due to the memory limitations and computational constraints. Training BERT-large in fact is a lengthy process of consuming significant hardware resources. Besides, there are already a wide variety of languages pretrained in BERT, which include the multilingual BERT and monolingual models pre-trained in 104 different languages. ALBERT, however, gives a much narrower choice in languages.\n\nAsserting an argument that having a better language model is roughly equivalent to pre-train a large model, all without imposing too much memory and computational requirements, we choose to go with ALBERT. In this paper, we develop and train KoreALBERT, a monolingual ALBERT model for Korean language understanding. Compared to a multilingual model, monolingual language models are known to optimize the performance for a specific language in every aspect, including downstream tasks critical to build modern NLP systems and applications.\n\nIn addition to the original ALBERT MLM and SOP training objectives",
            "score": 0.44606860974590806,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2112,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 835,
                    "end": 838,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 849,
                    "end": 852,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 865,
                    "end": 868,
                    "matchedPaperCorpusId": "11816014"
                },
                {
                    "start": 882,
                    "end": 885,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "2470716"
                },
                {
                    "start": 1033,
                    "end": 1037,
                    "matchedPaperCorpusId": "6826032"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1346435546875
        },
        {
            "corpus_id": "271855813",
            "title": "Building Decision Making Models Through Language Model Regime",
            "text": "Recent works explored how to use paradigm of language modeling to solve decision making problems (Chen et al., 2021;Janner et al., 2021), but they have largely focused on sequence modeling with the paradigm of supervised tuning directly. However, the generalization and emergent abilities of large language models are not a product of supervised learning on specific tasks (Wei et al., 2022). Rather, they are achieved through self-supervised pre-training, learning from vast amounts of data that are not entirely aligned with any specific tasks (OpenAI, 2024;Anthropic, 2024). But ultimately, these models are capable of tackling specific downstream tasks and perform exceedingly well at them. \n\nInspired by how large language model is trained and used, we propose a new method to develop a decision making model that goes beyond the limitations of task-specific models. This method seeks to improve both effectiveness and adaptability with which decision making tasks can be approached. Unlike supervised tuning paradigm, where models are trained in a supervised way and then are expected to encounter similar data and make correct decisions during application scenarios (Ouyang et al., 2022), we introduce a new training strategy called \"Learning then Using\" (LTU). This is a two-step method starts with a continue pre-training (CT) phase (Gupta et al., 2023), which aims to build a foundation decision making model via infusing a wide range of knowledge across various related decision domains and contexts. This phase is responsible for learning. Following this, the supervised fine-tuning (SFT) phase further refines the foundation model to reach requirements of a certain decision making task. The SFT phase is responsible for using, aiming to solve specific downstream tasks. \n\nBy combining the broad potential of large language models with a carefully designed training process, we study whether this new paradigm can provide better decision making abilities than supervised learning and exhibit stronger generalization. We conduct experiments in two classic e-commerce scenarios, advertising and searching. Across a total of four tasks with various ablation studies, LTU method not only outperforms SFT on decision making tasks but also makes the foundation model built with learning phase remarkably versatile. When applied to different decision making tasks, the foundation model consistently performs better than those trained only through supervised learning regimes.",
            "score": 0.4458258760948154,
            "section_title": "Introduction",
            "char_start_offset": 1895,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 988
                },
                {
                    "start": 989,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2116
                },
                {
                    "start": 2117,
                    "end": 2321
                },
                {
                    "start": 2322,
                    "end": 2481
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 116,
                    "matchedPaperCorpusId": "235294299"
                },
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "235313679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12158203125
        },
        {
            "corpus_id": "15392825",
            "title": "Inducing a Multilingual Dictionary from a Parallel Multitext in Related Languages",
            "text": "Modern statistical natural language processing techniques require large amounts of human-annotated data to work well. For practical reasons, the required amount of data exists only for a few languages of major interest, either commercial or governmental. As a result, many languages have very little computational research done in them, especially outside the borders of the countries in which these languages are spoken. Some of these languages are, however, major languages with hundreds of millions of speakers. Of the top 10 most spoken languages, Linguistic Data Consortium at University of Pennsylvania, the premier U.S. provider of corpora, offers text corpora only in 7 (The World Factbook (2004), 2000 estimate) Only a few of the other languages (French, Arabic, and Czech) have resources provided by LDC. Many Asian and Eastern European languages number tens of millions of speakers, yet very few of these seem to have any related compu-tational linguistics work, at least as presented at the international conferences, such as the ACL. 1 he situation is not surprising, nor is it likely to significantly change in the future. Luckily, most of these less-represented languages belong to language families with several prominent members. As a result, some of these languages have siblings with more resources and published research. 2 Interestingly, the better-endowed siblings are not always the ones with more native speakers, since political considerations are often more important. 3 If one is able to use the resources available in one language (henceforth referred to as source) to facilitate the creation of tools and resource in another, related language (target), this problem would be alleviated. This is the ultimate goal of this project, but in the first stage we focus on multi-language dictionary induction. \n\nBuilding a high-quality dictionary, or even better, a joint word distribution model over all the languages in a given family is very important, because using such a model one can use a variety of techniques to project information across languages, e.g. to parse or to translate. Building a unified model for more than a pair of languages improves the quality over building several unrelated pairwise models, because relating them to each other provides additional information.",
            "score": 0.44551199846878664,
            "section_title": "Introduction and Motivation",
            "char_start_offset": 30,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01090240478515625
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "The observations given in input of our system are gray-scales images of text-lines.The input images are normalized to a fixed height of 100 pixels, preserving the aspect ratio.The gray scale is normalized to zero mean and unity variance (standardization).\n\nThe unified optical model is a Recurrent Neural Network (RNN) composed of 4 Bi-directional Long Short Term Memory (BLSTM)  We used 9-gram language models of k-multigrams or words, and 10-gram language models of characters in any of our experiments.We explored k-multigrams models for 2 \u2264 k \u2264 5. To ensure that long distance dependencies are estimated in the model, all text lines are concatenated in order to form a single text line.Every language model is estimated using the MIT language modeling toolkit from Hsu (2009), where the back-off coefficients are estimated using the modified Kneser-Ney smoothing method (Kneser and Ney (1995)).\n\nViterbi's two-pass decoding algorithm is applied at the paragraph level to track contextual dependencies as much as possible.\n\nThe beam search parameter has been fixed to 0.8 in all the experimentations while language model scale and word insertion penalty are optimized for each scenario (see below).\n\nDuring test, we are interested in quantifying the contribution of each stage (i.e.optical model and language model) to the performance of the unified system.In this respect, we conducted four experiments (scenarios).The first one (UU) consists in having a whole unified system (both the optical model and the language model are unified).The second experiment (US) consists in combining a unified optical model with a specialized language model.The third experiment (SU) combines a specialized optical model with a unified language model.Finally, the last one (SS) is composed of specialized models (both the optical model and the language model).In the following section we report and analyze the performance of these four configuration of the system.",
            "score": 0.44439705094942855,
            "section_title": "System configurations and Evaluation protocol",
            "char_start_offset": 24212,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 255
                },
                {
                    "start": 257,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 690
                },
                {
                    "start": 690,
                    "end": 898
                },
                {
                    "start": 900,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1201
                },
                {
                    "start": 1203,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1360
                },
                {
                    "start": 1360,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1540
                },
                {
                    "start": 1540,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1740
                },
                {
                    "start": 1740,
                    "end": 1849
                },
                {
                    "start": 1849,
                    "end": 1954
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "12982389",
            "title": "A bit of progress in language modeling",
            "text": "It turns out that language modeling is an ideal place to use object oriented methods, and inheritance. In particular, our language modeling tool was based on a virtual base class called Model. The single most important member function of a model provides the probability of a word or a cluster given a context. Models have all sorts of other useful functions; for instance, they can return the set of parameters that can be optimized. They can go through the test data and determine which counts they will need. They can go through the training data and accumulate those counts. From this base class, we can construct all sorts of other subclasses. In particular, we can construct models that store probabilities -DistributionModels -and models that contain other models -ListModels.\n\nThe DistributionModels are, for instance, Kneser-Ney smoothed models, Katz smoothed models, simple interpolated models, and absolute discounting models. Even within these models, there is some substructure; for instance, absolute discounting backoff models, Kneser-Ney backoff models, and Katz models can share most of their code, while interpolated Kneser-Ney models and interpolated absolute discounting models can also share code.\n\nThe other type of model is a model containing other models. For instance, an InterpolateModel contains a list of other models, whose probability it interpolates. For clustered models of the form P (Z|XY ) \u00d7 P (z|Z), there is a model that contains a list of other models, and multiplies their probabilities. There is also a SentenceInterpolateModel that contains a list of other models, and interpolates them at the sentence level, instead of the word level. All of these container type models have similar behavior in many ways, and much of their code can be shared as well.\n\nIt was in part this architecture, providing a set of basic building blocks and tools for combining them, that allowed so many different experiments to be performed.",
            "score": 0.4439982908604859,
            "section_title": "B.2 System architecture",
            "char_start_offset": 113473,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1229248046875
        },
        {
            "corpus_id": "9087527",
            "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches",
            "text": "While most work on multilingual learning focuses on bilingual analysis, some models operate on more than one pair of languages. For instance, Genzel (2005) describes a method for inducing a multilingual lexicon from a group of related languages. This work first induces bilingual models for each pair of languages and then combines them. We take a different approach by simultaneously learning from all languages, rather than combining bilingual results. \n\nA related thread of research is multi-source machine translation (Och & Ney, 2001;Utiyama & Isahara, 2006;Cohn & Lapata, 2007;Chen, Eisele, & Kay, 2008;Bertoldi, Barbaiani, Federico, & Cattoni, 2008) where the goal is to translate from multiple source languages to a single target language. By using multi-source corpora, these systems alleviate sparseness and increase translation coverage, thereby improving overall translation accuracy. Typically, multi-source translation systems build separate bilingual models and then select a final translation from their output. For instance, a method developed by Och and Ney (2001) generates several alternative translations from source sentences expressed in different languages and selects the most likely candidate. Cohn and Lapata (2007) consider a different generative model: rather than combining alternative sentence translations in a post-processing step, their model estimates the target phrase translation distribu-tion by marginalizing over multiple translations from various source languages. While their model combines multilingual information at the phrase level, at its core are estimates for phrase tables that are obtained using bilingual models. \n\nIn contrast, we present an approach for unsupervised multilingual learning that builds a single joint model across all languages. This makes maximal use of unlabeled data and sidesteps the difficult problem of combining the output of multiple bilingual systems without supervision.",
            "score": 0.4435346262559613,
            "section_title": "BEYOND BILINGUAL LEARNING",
            "char_start_offset": 11472,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 454
                },
                {
                    "start": 457,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 155,
                    "matchedPaperCorpusId": "15392825"
                },
                {
                    "start": 522,
                    "end": 539,
                    "matchedPaperCorpusId": "16088175"
                },
                {
                    "start": 539,
                    "end": 563,
                    "matchedPaperCorpusId": "8030425"
                },
                {
                    "start": 563,
                    "end": 583,
                    "matchedPaperCorpusId": "9334744"
                },
                {
                    "start": 583,
                    "end": 609,
                    "matchedPaperCorpusId": "964751"
                },
                {
                    "start": 609,
                    "end": 656,
                    "matchedPaperCorpusId": "18966569"
                },
                {
                    "start": 1064,
                    "end": 1082,
                    "matchedPaperCorpusId": "16088175"
                },
                {
                    "start": 1220,
                    "end": 1242,
                    "matchedPaperCorpusId": "9334744"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0616455078125
        },
        {
            "corpus_id": "245723446",
            "title": "A Metaverse: Taxonomy, Components, Applications, and Open Challenges",
            "text": "Reference games illustrate the functional use of language for communication and provide a basic learning environment for neural agents. Languages are inherently biased by the underlying capabilities of agents. Dagan et al. [287] introduced the Language Transmission Simulator to model agent populations' cultural and architectural evolution. They emphasize the importance of studying basic agent architectures and propose coevolution of languages and agents in the study of language emergence. With the recent development of LM, it is widely used for various tasks of natural language and various modals. Recurrent Neural Network Transducer (RNN-T) is a famous method in automatic speech recognition due to its simplicity, conciseness, and general transcription, but it lacks an external language model and is more vulnerable to rare long-tail words (e.g., entity names). Le et al. [288] proposed RNN-T to model intractable rare WordPieces by injecting additional information into the encoder and using alternative letter pronunciations. Deep fusion with personalized language models for stronger biasing. Weber et al. [289] considered language modeling as a multi-task problem, combining three studies: multitask learning, linguistics, and interpretability, to analyze the generalization behavior of language models in Negative Polarity Items (NPIs). \n\nQA is the most basic solution for communicating with NPCs in the Metaworld. Annotated data sets are difficult and expensive to collect and rarely exist in languages other than English. That is the reason it is hard to build a QA system that works well in other languages. Lewis et al. [290] proposed a multi-dimensionally ordered extractive QA evaluation benchmark MLQA. Xiong et al. [291] proposed a simple and efficient multi-hop dense search approach to answer complex open-domain questions, achieving state-of-the-art performance in two multi-hop data sets, HotpotQA and multievidence FEVER. Min et al. [292] proposed a model to build a system that can predict correct answers in open QA that receives natural language questions as input and returns natural language answers while meeting strict disk memory budgets. Memory budgets encourage agents to explore a balance between storing parameters for large and redundant search corpora and large training models.",
            "score": 0.4433800357371533,
            "section_title": "a: LANGUAGE INTERACTION",
            "char_start_offset": 112068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 871
                },
                {
                    "start": 872,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2320
                }
            ],
            "ref_mentions": [
                {
                    "start": 882,
                    "end": 887,
                    "matchedPaperCorpusId": "226965322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01406097412109375
        },
        {
            "corpus_id": "244873212",
            "title": "Text-to-speech system for low-resource language using cross-lingual transfer learning and data augmentation",
            "text": "Gutkin et al. [11] constructed a multi-speaker, acoustic database using crowdsourcing, and then used it to bootstrap a statistical, parametric speech synthesis system. These studies all used multispeaker datasets which were in the same language as the target speech. \n\nMultilingual models: Since high-quality, multi-speaker data is generally unavailable in most low-resource languages, multilingual or multilingual/multi-speaker models can be used to address data availability issues. Yu et al. [12] proposed a multilingual bi-directional long shortterm memory (BLSTM)-based speech synthesis method which transforms the input linguistic features into acoustic features. The input layer and hidden layers of the BLSTM were shared across different languages for speech synthesis of low-resource languages, but the output layer was not shared. The input feature vectors of different languages were combined to form a single, uniform representation of the input features. The shared hidden layers transform the uniform input features into an internal representation that can benefit low-resource TTS. Their proposed multilingual BLSTM-based speech synthesis method was able to more accurately predict acoustic features than a monolingual BLSTM. Li and Zen [13] built a long short-term memory (LSTM) recurrent neural network based, multi-language/multi-speaker (MLMS) statistical parametric speech synthesis system using six languages. Their proposed MLMS model achieved similar performance to that of conventional language-dependent and speaker-dependent models. They also demonstrated that adapting their proposed system to new languages using limited training data achieved better performance than building low-resource language models from scratch. Korte et al. [14] conducted experiments to compare the naturalness of speech from single-speaker models with speech from multilingual models when different amounts of the target speaker's data were used for training. They also compared the naturalness of speech from monolingual, multi-speaker models with speech from multilingual, multi-speaker models when larger amounts of non-target language training data were used.",
            "score": 0.44301801373792304,
            "section_title": "Introduction",
            "char_start_offset": 4032,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 266
                },
                {
                    "start": 269,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2168
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06671142578125
        },
        {
            "corpus_id": "246959349",
            "title": "Computational intelligence in processing of speech acoustics: a survey",
            "text": "Kipyatkova et al. [156] proposed a language model that integrated statistical and syntactical text analysis. HMMs were used for acoustic modeling and the phonemes were modeled using continuous HMM. A speech corpus consisted of 100 continuous utterances and 1068 words. Ronzhin et al. [270] presented a comprehensive survey of the Russian language, the applied methods, and models for speech recognition techniques in Russia and foreign countries. Karpov et al. [142] described an ASR for large vocabulary systems in the Russian language. A hybrid of knowledge-based and statistical approaches was being used to build an acoustic model. To develop the language model, a novel method combines the syntactical and statistical aspects of the text for training data. The results were computed on two distinct Russian databases. The proposed language model was evaluated on 204 thousand words vocabulary and proved to be efficient than the existing models. On similar grounds, a speech recognition system based on phonetic decoding technique was developed by Savchenko [287]. Vazhenina and Markov [341] focused on a technique to select the phonemes based on the hybrid of phonological and statistical analysis. The proposed technique when applied to the IPA Russian phonetic set with the reduced number of phonemes achieved better results. \n\nKarpov et al. [143] build the acoustic model with the combination of knowledge-based and statistical approaches to create several different phoneme sets. The analysis was conducted with 204 thousand words vocabulary and the performance of standard statistical n-gram LMs and the language models created using our syntactico-statistical method were compared. The results confirmed that the proposed language modeling approach is reducing word recognition errors. Kapralova et al. [141] re-decode speech logged by production recognizer to improve the quality of ground truth transcripts used for training alignments. A fully unsupervised approach to the acoustic model was described that took advantage of a large amount of traffic of Google's speech recognition products. Yanzhou and Mianzhu [360] optimized the recognition algorithm by implementing different feature extraction techniques. Prudnikov et al. [254] and Smirnov et al. [309] proposed a system to detect keywords from LVCSR systems where the experiments were conducted on the CMU-Sphinx platform.",
            "score": 0.44297828049924287,
            "section_title": "Russian",
            "char_start_offset": 31706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2394
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 23,
                    "matchedPaperCorpusId": "9255946"
                },
                {
                    "start": 461,
                    "end": 466,
                    "matchedPaperCorpusId": "26646950"
                },
                {
                    "start": 1063,
                    "end": 1068,
                    "matchedPaperCorpusId": "51908087"
                },
                {
                    "start": 1091,
                    "end": 1096,
                    "matchedPaperCorpusId": "15871325"
                },
                {
                    "start": 1350,
                    "end": 1355,
                    "matchedPaperCorpusId": "17102065"
                },
                {
                    "start": 1815,
                    "end": 1820,
                    "matchedPaperCorpusId": "5917427"
                },
                {
                    "start": 2127,
                    "end": 2132,
                    "matchedPaperCorpusId": "62737726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.021087646484375
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "Designing a unified language model is less straightforward than designing a unified optical model.This is due to the word lexicons sizes which are often large (to model as much as possible each language).Thus, combining word lexicons from various languages strongly increases the model complexity and may become intractable.One solution is to consider a lexicon of characters but language models based on characters perform often poorly (Pl\u00f6tz and Fink (2009)).\n\nDesigning a unified language model is attractive for taking into account similarities between languages of the same origin that exist at the lexical, morphological or syntactic levels (Kalindra (2004)).In this respect, language models based on sub-lexical units have been recently proposed for handwritten recognition (Swaileh et al. (2017), Swaileh et al. (2016)).Dealing with sub-lexical units has a number of advantages: on the one hand, it allows to significantly reduce the lexicon size and, on the other hand, to improve the recognition rate, as it partly tackles the out-of-vocabulary words problem.\n\nIn this paper, we proposes an end-to-end unified multilingual recognition system for handwriting recognition, where both the optical model and the language model combine various languages.Both models rest on state-of-the-art methods.On the one hand, we make use of an optical model based on deep recurrent neural networks.On the other hand, our language model uses sequences of sub-lexical units, called multigrams.Dealing with multigrams allows to reduce the lexicon size from each language and thus to build a unified language model of reasonable size.We evaluate our approach on languages of the same origin (French and English), as the similarities between the languages may impact the capacity of the system.In our experiments, we show that a) combining languages of the same origin in a unified framework allows to strengthen the capacity of the optical model; b) combining the languages does not significantly affect the robustness of the language model.This allows to build a language model estimated on all the training corpus, without the need to separate the languages; c) build a unified multilingual system seems better than dealing with specific-language systems, where one of them must be selected to provide a prediction.",
            "score": 0.44286884907234186,
            "section_title": "Introduction",
            "char_start_offset": 2063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 98,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 461
                },
                {
                    "start": 463,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 828
                },
                {
                    "start": 828,
                    "end": 1069
                },
                {
                    "start": 1071,
                    "end": 1259
                },
                {
                    "start": 1259,
                    "end": 1304
                },
                {
                    "start": 1304,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1486
                },
                {
                    "start": 1486,
                    "end": 1625
                },
                {
                    "start": 1625,
                    "end": 1784
                },
                {
                    "start": 1784,
                    "end": 2032
                },
                {
                    "start": 2032,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 437,
                    "end": 459,
                    "matchedPaperCorpusId": "27527196"
                },
                {
                    "start": 647,
                    "end": 663,
                    "matchedPaperCorpusId": "16320533"
                },
                {
                    "start": 781,
                    "end": 803,
                    "matchedPaperCorpusId": "24024363"
                },
                {
                    "start": 805,
                    "end": 826,
                    "matchedPaperCorpusId": "9959581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04193115234375
        },
        {
            "corpus_id": "260485459",
            "title": "Text mining biomedical literature to identify extremely unbalanced data for digital epidemiology and systematic reviews: dataset and methods for a SARS-CoV-2 genomic epidemiology study",
            "text": "This strong limitation has been recently lifted with the availability of better hardware and algorithms, leading to the release of very large language models by the community. For example, GPT-4 which powers chatGP, 21,22 accepts inputs of 8000 tokens and it can be extended to 32k. 23 We plan, as future work, to encode the entire paragraphs as input for our classifier module by replacing the BERT model with a larger language model in open-access such as BLOOM. 24 ntegrating a large language model, like BLOOM, in our pipeline would have another benefit besides the improvement of its precision. We hypothesize that a large language model will increase the genericity and re-usability of our pipeline. The current pipeline screens the literature using a BERT classifier. This classifier needs to be trained on a large number of examples to achieve acceptable performance. This training limits the re-usability of our pipeline if one wants to change the selection criteria for the need of another study. Annotations still involve human annotators, have a high cost, and once the examples are obtained, retraining the classifier on these examples involves specialized technicians and hardware availability. Instead, the larger size of the layers composing the networks of large language models allows them to encode long-range dependencies between sentences of a paragraph, thus capturing the meaning of the text. When large language models are pre-trained on massive corpora composed of documents from multiple domains, they learn linguistic and semantic knowledge they can recall on demand to perform prediction tasks that they have never been explicitly trained on. 25 This property makes some authors, such as Moor et al. in, 26 compare these models with generalist intelligent agents. LLMs can learn in the context of a few examples provided by a researcher to perform new tasks and achieve noticeable performance. We plan to add a user interface in our pipeline to help researchers interact with a large language model and define some examples of relevant and irrelevant articles for their study which should be enough for the model to screen the literature without being explicitly trained to do so.",
            "score": 0.44285585631497654,
            "section_title": "Discussion",
            "char_start_offset": 26579,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 1671,
                    "end": 1673,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04534912109375
        },
        {
            "corpus_id": "16467845",
            "title": "Web Augmentation of Language Models for Continuous Speech Recognition of SMS Text Messages",
            "text": "In Table 1, the prediction abilities of the in-domain and web mixture language models are compared. As an evaluation measure we use perplexity calculated on test sets consisting of in-domain text. The comparison is performed on FSTs of different sizes. The FSTs contain the acoustic models, language model and lexicon, but the LM makes up for most of the size. The availability of data varies for the different languages, and therefore the FST sizes are not exactly the same across languages. \n\nThe LMs have been created using the SRI LM toolkit (Stolcke, 2002). Good-Turing smoothing with Katz backoff (Katz, 1987) has been used, and the different model sizes are obtained by pruning down the full models using entropy-based pruning (Stolcke, 1998) ture models, whereas the best in-domain models are 4-or 5-grams. \n\nFor every language and model size, the web mixture model performs better than the corresponding in-domain model. The perplexity reductions obtained increase with the size of the model. Since it is possible to create larger mixture models than in-domain models, there are no in-domain results for the largest model sizes. \n\nEspecially if large models can be afforded, the perplexity reductions are considerable. The largest improvements are observed for French (between 10.2 % and 22.6 % relative). This is not surprising, as the French in-domain set is the smallest, which leaves much room for improvement.",
            "score": 0.4427730552263227,
            "section_title": "Test set perplexities",
            "char_start_offset": 21018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 929
                },
                {
                    "start": 930,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1423
                }
            ],
            "ref_mentions": [
                {
                    "start": 546,
                    "end": 561,
                    "matchedPaperCorpusId": "1988103"
                },
                {
                    "start": 603,
                    "end": 615,
                    "matchedPaperCorpusId": "6555412"
                },
                {
                    "start": 734,
                    "end": 749,
                    "matchedPaperCorpusId": "8150809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040924072265625
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "We presented an end-to-end unified multilingual system for handwriting recognition where both the optical model and the language model are trained on datasets composed of examples from several languages.Our proposal allows, on the one hand, to optimize a unique system, whatever the languages that are in training and, on the other hand, to avoid the use of a decision process for selecting one specialized system trained on a specific language.\n\nOur unified optical model is optimized to recognize a unified character set.In case of languages of the same origin, unifying the character set reduces the system complexity and increases the number of training examples per character classes that are shared between the languages.While traditional language models are based on words, which can become intractable in case of unified lexicons, we proposed to build a language model based on sub-lexical units, called multigrams.Dealing with multigrams has many advantages: the multigrams are obtained using a data-driven process without the need of linguistic expertise; it reduces the model complexity compared to words; finally, it allows a better modeling of long dependencies than with characters.\n\nOur experiments on English and French languages with small or large lexicons, highlighted that the optical model benefits from the language unification and provides significant improvements compared to specialized systems.A major contribution is to show that unifying languages that have some similarities does not affect the language models which provide similar results than the specialized language models.In addition, dealing with multigrams allows to improve the traditional language models based on words or characters.Finally, our system reaches state-of-the art performance with a unique and less complex system.",
            "score": 0.4415805467074545,
            "section_title": "Conclusion",
            "char_start_offset": 34890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 203,
                    "end": 445
                },
                {
                    "start": 447,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 727
                },
                {
                    "start": 727,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1196
                },
                {
                    "start": 1198,
                    "end": 1420
                },
                {
                    "start": 1420,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "268876005",
            "title": "MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in Cross-Lingual Textual Relatedness",
            "text": "Our main results are presented in Table 2 and are discussed in the following section.\n\nSingle-source versus multi-source transfer.We first compare the performance of a zero-shot STR model trained on English (XLM-R eng , Furina eng ) against a multi-source model trained on the concatenation of all available languages from Track A (XLM-R MS-All , Furina MS-All ).Our results reveal that knowledge transfer from multiple source languages (RQ1) improves STR models, affirming the potential of multi-source training to enhance crosslingual capabilities.On average, both MS-All models outperform their single-source counterparts by 0.02 and 0.09 respectively.This is expected since  the multi-source training dataset is with 15,123 instances almost three times larger than the English dataset with 5,500 instances (cf.Table 1).When trained solely on English data, FURINA performs substantially worse than XLM-R.However, this performance gap narrows when transitioning from single-source to multi-source training.\n\nTransfer from language families.After showing that models trained on all languages outperform the single-source baseline, we now investigate the effect of training on languages from the same family as source languages.Here we experiment with two multi-source models specialized only on Indo-European and Afro-Asiatic languages respectively (MS-Fam).Importantly, for each target language we train a multi-source model on all other languages in the same language family.6On Indo-European languages, we find that XLM-R MS-Fam and Furina MS-Fam yield similar results with much less training data (i.e., 4,913 fewer instances belonging to other language families).For Spanish, our models show performance gains of +0.8 and +0.13 for XLM-R and FURINA respectively, when compared to models trained on all languages.This underscores the presence of language interference (Wang et al., 2020) in multilingual STR models when the training data from dissimilar languages are combined (RQ2).",
            "score": 0.44142014983976097,
            "section_title": "Results and Discussion",
            "char_start_offset": 13010,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 87,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 363
                },
                {
                    "start": 363,
                    "end": 550
                },
                {
                    "start": 550,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 814
                },
                {
                    "start": 814,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 1008
                },
                {
                    "start": 1010,
                    "end": 1042
                },
                {
                    "start": 1042,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1359
                },
                {
                    "start": 1359,
                    "end": 1479
                },
                {
                    "start": 1479,
                    "end": 1669
                },
                {
                    "start": 1669,
                    "end": 1818
                },
                {
                    "start": 1818,
                    "end": 1988
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03955078125
        },
        {
            "corpus_id": "259727499",
            "title": "A Speech Recognition Method Based on Domain-Specific Datasets and Confidence Decision Networks",
            "text": "Domainspecific speech recognition techniques [10] aim to develop specialized speech recognition models for specific professional domains, such as medical [11], legal, and financial fields. In domain-specific speech recognition, the recognition performance can be enhanced through the use of custom language models [12] or domain-adaptive methods [13,14]. These approaches employ techniques such as data augmentation [15], domain-adaptive training, and domain-specific model fine-tuning. Custom language models can be trained using a corpus of domain-specific vocabulary or adjusted by combining a general-purpose language model with domain-specific vocabulary. This helps reduce domain differences and improves recognition accuracy. In domain-specific contexts, the terms and expressions used in speech inputs differ from those in general-purpose speech recognition techniques. This disparity poses challenges for accurately recognizing domain-specific content. To address this issue, this paper proposes a method for creating a speech network using domain-specific datasets. \n\nIn this paper, we propose a joint modeling [16] approach that leverages acoustic and language models to collaborate with each other in parsing and translating the speech signal, leading to more accurate recognition results. The main contributions of this work can be summarized as follows: \n\n(1) We propose an acoustic modeling approach that combines the speech spectrogram, DFCNN, and CTC. This approach utilizes the rich information provided by the speech spectrogram, the powerful feature extraction capability of DFCNN, and the sequence modeling capabilities of CTC without alignment. By handling speech signals of varying lengths, we achieved improved speech recognition results; (2) To address unfamiliar words in new domains, we present a comprehensive system based on N-gram technology and the construction of domain-specific datasets incorporated into language models. This enables speech recognition in new domains; (3) To optimize the model, we propose a speech confidence-based determination method that dynamically adjusts the use of the language model, thereby enhancing the accuracy of the speech recognition model; (4) We designed experimental comparisons using different domain datasets to verify the effectiveness of the proposed method in addressing domain-specific speech recognition. These experiments follow a step-by-step incremental model experimental approach.",
            "score": 0.4413228260748421,
            "section_title": "Introduction",
            "char_start_offset": 2359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1367
                },
                {
                    "start": 1370,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2382
                },
                {
                    "start": 2383,
                    "end": 2463
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 49,
                    "matchedPaperCorpusId": "52272147"
                },
                {
                    "start": 314,
                    "end": 318,
                    "matchedPaperCorpusId": "258580049"
                },
                {
                    "start": 346,
                    "end": 350,
                    "matchedPaperCorpusId": "253421536"
                },
                {
                    "start": 350,
                    "end": 353,
                    "matchedPaperCorpusId": "257068455"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "245624441"
                },
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "216298020"
                },
                {
                    "start": 2004,
                    "end": 2007,
                    "matchedPaperCorpusId": "257493603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34423828125
        },
        {
            "corpus_id": "271161894",
            "title": "The sociolinguistic foundations of language modeling",
            "text": "token prediction), the type of language being modeled by language models is only ever defined in the most general terms, for example, \"a broad swath of internet data\" [27].Models are often trained on corpora based at least in part on the CommonCrawl dataset [5,28,29], but otherwise, in most cases, the nature of the language being modeled is not described at all [10].In large part, this is a natural consequence of the need for massive amounts of data to train base models, making the sources of these corpora of secondary concern.However, even when these models are adapted for more specific contexts [30], the type of language used for further training is generally only loosely defined.For example, ChatGPT was developed by adapting a GPT-3.5 base model for dialogue [31], but the form of dialogue actually being modeled by ChatGPT is something much less diverse and much more artificial than everyday English conversation, as anyone who interacts with ChatGPT knows.\n\nDrawing on modern sociolinguistic theory, in this paper, we therefore provide an answer to the question what is being modeled by language models?Language models are models of varieties of language.\n\nWe argue that any language model is inherently modeling the variety of language represented by the corpus on which it is trained, even if that variety of language is unknown and even if that corpus is a poor representation of that variety of language.Our view is that this simple insight can inform, at a fundamental level, how language models are developed and deployed in the real world.Given rapid advances in language modeling in recent years and the increasing societal impact and risk associated with LLMs, we believe the sociolinguistic perspective we are advocating for in this paper is especially important at this time -not only to improve the performance, evaluation, and applicability of LLMs, but to guide the creation of safe and ethical AI systems and to help us better understand their underlying nature.\n\nIn the rest of this paper, we expand on our basic claim that language models represent varieties of language and consider the implications of this claim for the future of language modeling.We first provide a technical definition of the sociolinguistic concept of a variety of language and argue that this concept inherently underpins the task of language modeling.",
            "score": 0.4411387193771215,
            "section_title": "Introduction",
            "char_start_offset": 1636,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 533
                },
                {
                    "start": 533,
                    "end": 691
                },
                {
                    "start": 691,
                    "end": 972
                },
                {
                    "start": 974,
                    "end": 1119
                },
                {
                    "start": 1119,
                    "end": 1171
                },
                {
                    "start": 1173,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1562
                },
                {
                    "start": 1562,
                    "end": 1993
                },
                {
                    "start": 1995,
                    "end": 2184
                },
                {
                    "start": 2184,
                    "end": 2359
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 261,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "270286602"
                },
                {
                    "start": 364,
                    "end": 368,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01560211181640625
        },
        {
            "corpus_id": "235265670",
            "title": "An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers",
            "text": "We combined instances from all the language pairs and built a single word-level QE model. Our results, displayed in section II (\"All\") of Table 2, show that multilingual models perform on par with bilingual models or even better for some language pairs. We also investigate whether combining language pairs that share either the same domain or MT type can be more beneficial, since it is possible that the learning process is better when language pairs share certain characteristics. However as shown in sections III and IV of Table 2, for the majority of the language pairs, specialised multilingual models built on certain domains or MT types do not perform better than multilingual models which contain all the data.  Table 2: Target F1-Multi between the algorithm predictions and human annotations. Best results for each language by any method are marked in bold. Sections I, II and III indicate the different evaluation settings. Section IV shows the results of the state-of-the-art methods and the best system submitted for the language pair in that competition. NR implies that a particular result was not reported by the organisers. Zero-shot results are coloured in grey and the value shows the difference between the best result in that section for that language pair and itself.",
            "score": 0.44089226872398324,
            "section_title": "Multilingual QE",
            "char_start_offset": 8038,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06982421875
        },
        {
            "corpus_id": "258187362",
            "title": "Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task",
            "text": "Creating a unified, general-purpose model to solve problems in all fields versus creating local specific models for different fields is an important issue. Especially at this particular point in time, where LLMs represented by ChatGPT and GPT-4 have achieved unprecedented success in different tasks, bringing new hope for creating a unified model. In this study, we aimed to explore this issue further by comparing the performance of ChatGPT/GPT-4 and locally fine-tuned models in a highly specialized task -radiology language inference. The results indicated that local models' performance falls short of that of ChatGPT/GPT-4 when the amount of specialized data is limited. However, with sufficient data, local models outperform the unified large models. Therefore, we are faced with the dilemma of choosing between building a unified, general-purpose model or creating effective datasets, both of which require substantial resources when reaching a certain scale (details can be seen in Table 5). Alternatively, a more promising approach to this issue is to combine existing unified LLMs with limited specific datasets to better solve tasks in specialized domains. \n\nIn conclusion, in this work, we evaluated the performance of ChatGPT/GPT-4 on a radiology NLI task and compared it to other models fine-tuned specifically on task-related data samples. The results showed that GPT-4 outperformed ChatGPT in the radiology NLI task, and other specifically fine-tuned models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings demonstrate that constructing a unified model capable of solving various tasks across different domains is feasible. However, further research is needed to determine whether this approach is preferable to creating domain-specific models, and how to effectively utilize limited specialized datasets with large LLMs.",
            "score": 0.44077449753336256,
            "section_title": "Uniform vs Domain-Specific Models",
            "char_start_offset": 37846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1168
                },
                {
                    "start": 1171,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1900
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.286376953125
        },
        {
            "corpus_id": "235828734",
            "title": "A Configurable Multilingual Model is All You Need to Recognize All Languages",
            "text": "For example, if we want to have bilingual and trilingual support of 10 languages, we have to build C 2 10 = 45 and C 3 10 = 120 specific models with such solution. In this work, we design a configurable multilingual model (CMM) that can be configured to recognize speeches from any combination of languages based on user selection. We formulate the hidden output as the weighted combination of the output from a universal multilingual model and the outputs from all language-specific modules. The universal model is language independent, modeling the shared information of all languages. The residue of any language from the shared one only carries much less information. Therefore, it only needs a very small number of parameters to model the residue for every language. At runtime, the universal model together with corresponding language-specific modules are activated based on the user selection. \n\nCMM is different from the multilingual ASR model with 1-hot LID vector which can only recognize the pre-selected single language. CMM also differs from the recent multilingual ASR models using mixture of experts (MoE) [11,12], in which every expert has the same amount of parameters as the universal model. Therefore, it is very hard for MoE to scale up with multiple languages given the very large model size. In contrast, CMM is only slightly larger than the universal model due to the residue modeling. More importantly, to our best knowledge, there is no work of configuring a single model for better recognition of any combination of languages selected by multilingual users.",
            "score": 0.4398773737207989,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 900
                },
                {
                    "start": 903,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1408
                },
                {
                    "start": 1409,
                    "end": 1583
                }
            ],
            "ref_mentions": [
                {
                    "start": 1121,
                    "end": 1125,
                    "matchedPaperCorpusId": "235779961"
                },
                {
                    "start": 1125,
                    "end": 1128,
                    "matchedPaperCorpusId": "235780751"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0343017578125
        },
        {
            "corpus_id": "252872964",
            "title": "Graph-Based Neural Modules to Inspect Attention-Based Architectures, A Position Paper",
            "text": "In an ideal scenario, the only information available during training should be sufficient to build meaningful representations. Even with available linguistic resources, expanding the entry point representation with tokens and concepts from the training data could allow the use of a more robust and extensible representational language such as a graph, hypergraph, or natural language. \n\nTo develop a pre-trained zoo of entry point representations for specific architectures that can be further specialized. It should be possible to use a library of visualization modules pre-trained on commonplace text corpora as a baseline, and specialize it with specific domain corpora. This process might accelerate the inspection process and also diminish the learning curve of experts from different domains on how to use those tools. At this point, we are aiming at modularity. We intend not only to have a methodology but also specific modules that experts from diverse backgrounds can reuse. \n\nTo define a method comprising both local and global interpretation scope. In opposition to local scope interpretations, global interpretations aim at providing insight into the overall behavior of the model considering an entire task. \n\nFor instance, understanding what parts of a large language model are responsible for summing two numbers or performing sentiment analysis. \n\nTo achieve those objectives, one should build a robust methodology to edit and interact with large language models at multiple levels of abstraction processed by the model. This methodology might enable further insight into the inner workings of such models.",
            "score": 0.43962169850873534,
            "section_title": "Representation Module",
            "char_start_offset": 10555,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 385
                },
                {
                    "start": 388,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 985
                },
                {
                    "start": 988,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1222
                },
                {
                    "start": 1225,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1538
                },
                {
                    "start": 1539,
                    "end": 1624
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2073974609375
        },
        {
            "corpus_id": "265101195",
            "title": "Architectural Considerations for Conversational Systems -- The Verbmobil/INTARC Experience",
            "text": "A new issue in Verbmobil's second phase are investigations on speaker style. It is well known that system performance depends on the perplexity of the language models involved. Consequently, one of the main problems is to reduce the perplexity of the models in question. The common way to approach this problem is to specialize the models by additional knowledge about contexts. The traditional n-gram model uses a collection of conditional distributions instead of one single probability distribution. Normally, a fixed length context of immediately preceding words is used. Since the length of the word contexts is bound by data and computational resources, practicable models could only be achieved by restricting the application domain of a system. Commonly used n-gram models define P (w|C, D) where C is a context of preceding words and D is an application domain. But also finer grained restrictions have been tested in the last decade, e.g. a cache-based n-gram [11]. \n\nIntuitively, every speaker has its own individual speaking style. The question is whether it is possible to take advantage of this fact. The first step towards specialized speaker models is to prove whether sets of utterances sorted by speakers show significant differences in the use of syntactic structure at all. So, first of all the whole corpus has been tagged with POS-categories grounded on syntactic properties of words (for tagger and POS-categories see [14]). Using the whole corpus, we determined an empirical distribution D all over these categories. In order to separate the corpus in typical and non typical speakers we checked the distribution D s of every speaker s against D all using the Chi-square test. While we can't say anything about the usage of syntax by non-typical speakers, there is evidence that typical speakers make a similar use of syntax in a rough sense. With a significance level of 0.01 the test rejects 23.6% of the speakers. \n\nBi-and trigram models were estimated on the basis of the typical speakers and on the whole corpus in comparison. On a test set of normal speakers only the specialized models showed a slightly higher perplexity than the more general models. In contrast to this the specialization explored with automatic clustering using the K-means method shows a slightly better perplexity on most of the test set speakers.",
            "score": 0.4387133187670956,
            "section_title": "Speaker Style",
            "char_start_offset": 16924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2182
                },
                {
                    "start": 2183,
                    "end": 2350
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11163330078125
        },
        {
            "corpus_id": "265607944",
            "title": "Mixed-domain Language Modeling for Processing Long Legal Documents",
            "text": "The application of Natural Language Processing (NLP) to specialized domains, such as the law, has recently received a surge of interest. As many legal services rely on processing and analyzing large collections of documents, automating such tasks with NLP tools such as language models emerges as a key challenge since legal documents may contain specialized vocabulary from other domains, such as medical terminology in personal injury text. However, most language models are general-purpose models, which either have limited reasoning capabilities on highly specialized legal terminology and syntax, such as BERT or ROBERTA, or are expensive to run and tune, such as GPT-3.5 and Claude. Thus, in this paper, we propose a specialized language model for personal injury text, LEGALRELECTRA, which is trained on mixed-domain legal and medical corpora. We show that as a small language model, our model improves over general-domain and single-domain medical and legal language models when processing mixed-domain (personal injury) text. Our training architecture implements the ELECTRA framework but utilizes REFORMER instead of BERT for its generator and discriminator. We show that this improves the model\u2019s performance on processing long passages and results in better long-range text comprehension.",
            "score": 0.4381519653068693,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1778564453125
        },
        {
            "corpus_id": "227231697",
            "title": "Evaluating Cross-Lingual Transfer Learning Approaches in Multilingual Conversational Agent Models",
            "text": "In this paper, we propose a framework for building general multilingual NLU models, which can be used across different marketplaces and languages. \n\nTo choose the model with the best performance, we use language-specific test sets to evaluate the candidate models and their corresponding baseline models (e.g. English biLSTM-CRF model vs. monolingual English MaxEnt-CRF model) along four metrics, domain accuracy, intent accuracy, slot F1, and frame accuracy. The models which win in most of the evaluation metrics are the final picks. We find that models built from a simple multi-task biLSTM-CRF model setup are comparable to standard production models in terms of latency constraints required for on-the-fly voice assistant conversational models. \n\nWe observe performance improvements in all models with the introduction of transfer learning. Encoder transfer produced the greatest improvements whereas the transfer of the decoder did not bring much change when compared to the baseline model performance, except when tested on an English test set, when the transfer learning is performed from the model trained on English data. This is due to the fact that the target non-English language contains slots or intents which are not included in the pre-trained model, thus the decoder fails to predict correct classes simply because they are missing in the vocabulary. To mitigate this effect, a decoder with default initialization gives better performance because it now can embrace all available slots and intents in the target language realm. Furthermore, we find that a model pre-trained in a multilingual setup performs better than the one trained on a monolingual data set. This confirms that a multilingual model built based on lexically and orthographically similar languages may provide more beneficial context information to any similar target language. Experimental result on Hindi show that such a multilingual model can work even for non-alike languages with the same or better performance improvement. This confirms that a common multilingual model can be used to support multiple language with better results than a set of monolingual models. \n\nWith a single general multilingual NLU model, bootstrapping new languages can be faster as we can use cross-lingual contextual information from all existing high-resource languages.",
            "score": 0.43807834366717957,
            "section_title": "Conclusions",
            "char_start_offset": 14686,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 149,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2157
                },
                {
                    "start": 2160,
                    "end": 2341
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.097412109375
        },
        {
            "corpus_id": "6786749",
            "title": "Statistical Language Modeling with Performance Benchmarks using Various Levels of Syntactic-Semantic Information",
            "text": "We presented the effect of incorporating various levels of syntactic information in a statistical language model that uses the mathematical framework called syntactically enhanced LSA. \n\nSELSA is an attempt to develop a unified framework where syntactic and semantic dependencies can be jointly represented. It generalizes the LSA framework by incorporating various levels of the syntactic information along with the current word. This provides a mechanism for statistical language modeling where the probability of a word given the semantics of the preceding words is constrained by the adjacent syntax. The results on WSJ corpus sets a set of benchmarks for the performance improvements possible with these types of syntactic information. The supertag based information is very fine-grained and thus leads to a large reduction in perplexity if correct supertag is known. It is also observed that the knowledge of the phrase type also helps to reduce the perplexity compared to LSA. Even the knowledge of the content/function word type helps additionally in each of the SELSA based language models. These benchmarks can be approached with better algorithms for predicting the necessary syntactic information. Our experiments are still continuing in this direction as well as toward better understanding of the overall statistical language modeling problem with applications to speech recognition.",
            "score": 0.43790124190454305,
            "section_title": "Conclusions and Research Direction",
            "char_start_offset": 22503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 187,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1397
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "255749525",
            "title": "Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks",
            "text": "DaVinci (Diao et al., 2022) combines prefix language modeling and prefix image modeling to learn a general foundation model for a wide range of tasks. Uni-Perceiver (Zhu et al., 2021(Zhu et al., , 2022) builds a unified perception architecture that processes various modalities and tasks with a single Transformer and shared parameters. \n\nPrevious studies on general foundation models have shown that different capabilities can be established with only one model. Still, few studies demonstrate that the best performance can be achieved in all tasks with one model. In this paper, we propose a new method for training general foundation model and show that it can perform the best for all the understanding tasks of language, vision, and vision-language. We compare our model extensively with recent general foundation models on multiple dimensions, as shown in Appendix A. \n\nSeveral super-large foundation models (over 1B parameters) are proposed recently, most of which are trained on super-large in-house datasets (over 900M image-text pairs). The authors do not report results at the base (about 300M parameters) scale on public datasets, which we consider in this paper. CoCa (Yu et al., 2022) pre-trains an image-text sequence-to-sequence model with contrastive loss and captioning loss. BEiT-3 (Wang et al., 2022d) uses a multi-way Transformer and a unified objective of masked \"language\" modeling for learning from image, text, and image-text pair data. Flamingo (Alayrac et al., 2022) makes use of a large language model in vision-language pre-training to solve the \"in-context learning\" problem for vision-language tasks. PaLI (Chen et al., 2022) jointly scales up the vision encoder and language encoder to cover a variety of language, vision, vision-language, and multilingual tasks. 3 Method",
            "score": 0.4377075252309912,
            "section_title": "Related Work",
            "char_start_offset": 9406,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1804
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2841796875
        },
        {
            "corpus_id": "271039884",
            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "text": "In this work, we mainly study two categories of common roadmaps, CT-then-SFT and model merging, to transfer the task-solving ability from a highresource language to a low-resource one. We aim to investigate following research questions. RQ1: What is the viability of constructing task-solving  LLMs in low-resource languages via model merging? RQ2: Is model merging always a better choice than CT-then-SFT? \n\nWe take the settings of BASE, SFT-flan, and CT-X as the baselines. For CT-then-SFT, we investigate two common methods to build task-solving models, CTSFT-X-flan and CTSFT-X-mt. For model merging, we study two effective algorithms to combine the abilities of language modeling and task solving: weighted averaging and TIES. \n\nTable 3 illustrates the overall performance of different models or setups, i.e., the average scores over all tasks, for each language. For the experiments involving continual pre-training, we report the results based on the last checkpoints of CT-X6 . See the model performance on individual tasks in Appendix C.1.",
            "score": 0.43768883656636987,
            "section_title": "Results and Analysis",
            "char_start_offset": 15135,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 406
                },
                {
                    "start": 409,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1048
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07977294921875
        },
        {
            "corpus_id": "233481877",
            "title": "Hidden Backdoors in Human-Centric Language Models",
            "text": "Language Models assign probability to sequences of words [26]. \n\nThe probability of a sequence of  words { 1 , ...,   } is denoted as  ( 1 , ...,   ). To compute  ( 1 , ...,   ), the problem is decomposed with the chain rule of probability: \n\nEq. ( 2) is useful for determining whether a word sequence is accurate and natural, e.g., Eq. ( 2) would give a higher probability to \"the apple is red\" compared to \"red the apple is\". Neural Language Models. Neural network based language models have many advantages over the aforementioned -gram language models. Bengio et al. [3] first introduced a simple feedforward neural language model. As the model and dataset complexity continues to grow, modern neural language models are generally Recurrent or Transformer [64] architectures. \n\nLong short-term memory (LSTM) networks [19] remove information no longer needed from the context flow while adding information likely to be needed for future decision making. To accomplish this, the network controls the flow of information in and out of the network layers through specialized gated neural units. \n\nTransformer-based language models, e.g. Bert [12] or GPT-2 [49], take word embeddings of individual tokens of a given sequence and generate the embedding of the entire sequence. Transformer models rely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution. Self-attention relates different positions of a single sequence in order to compute a representation of the full sequence.",
            "score": 0.4371562209151408,
            "section_title": "Language Models",
            "char_start_offset": 9226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 65,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1542
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 574,
                    "matchedPaperCorpusId": "221275765"
                },
                {
                    "start": 760,
                    "end": 764,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 821,
                    "end": 825,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1142,
                    "end": 1146,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03338623046875
        },
        {
            "corpus_id": "263310813",
            "title": "SCALE: Synergized Collaboration of Asymmetric Language Translation Engines",
            "text": "Large Language Models (LLMs) have recently revolutionized the field of natural language processing (OpenAI, 2023;Touvron et al., 2023;Peng et al., 2023), significantly influencing machine translation (MT) by delivering exceptional performance without requiring a bilingual corpus, particularly in high-resource languages (Brown et al., 2020;Garcia et al., 2023). Moreover, as a unified multi-task learner, LLMs represent a substantial step towards artificial general intelligence (Bubeck et al., 2023), with the potential to overcome not only language barriers but also cultural boundaries simultaneously through a simple \"translate and explain\" prompt. \n\nDespite their advancements, LLM-based translation systems still confront several challenges. Firstly, there exists a significant language bias towards English (e.g., 92.1% of the GPT-3 pretraining corpus is English, while French, the second largest, represents only 1.8%2 ), which significantly constraints multilingual translation performance, especially for those low-resource languages (Scao et al., 2022;Hendy et al., 2023). Secondly, as a practical approach for system improvement, fine-tuning LLM poses great challenges. These include (1) the trade-off between speciality and generality (Cheng et al., 2023a;Lin et al., 2023), and (2) the prohibitively high cost associated with tuning large-scale models (Hu et al., 2021;Dettmers et al., 2023). In contrast, traditional Specialized Translation Models (STMs)-those based on encoder-decoder architecture, trained with supervision and significantly smaller in size (Sutskever et al., 2014;Vaswani et al., 2017)-serve as specialists for specific translation tasks and could be efficiently fine-tuned. However, these models lack general language capabilities and are potentially susceptible to parallel data bias, such as the memorization of low-quality samples (Raunak et al., 2022). \n\nIn this paper, we demonstrate for the first time the possibility to unify these two asymmetric translation engines in a single framework.",
            "score": 0.43690155134497966,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 1249,
                    "end": 1270,
                    "matchedPaperCorpusId": "263867307"
                },
                {
                    "start": 1575,
                    "end": 1599,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1870,
                    "end": 1891,
                    "matchedPaperCorpusId": "248965209"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35302734375
        },
        {
            "corpus_id": "248377869",
            "title": "Language & Coding Creativity",
            "text": "Human speech works this same way. When you walk into a room and say \"I need a ____,\" a relatively narrow list of words would make sense in the blank. As the context becomes more detailed-for instance, walking into a kitchen covered in mud-that list shrinks further. Our minds develop this sorting naturally through experiences, but to train GPT-3's mind, the system has to review hundreds of billions of different data points and work out the patterns among them. \n\nSince Markov's contributions, mathematicians and computer scientists have been laying the theoretical groundwork for today's NLP models. But it took recent advances in computing to make these theories reality: now processors can handle billions of inputs and outputs in milliseconds. For the first time, machines can perform any general language task. From a computer architecture sense, this has helped unify NLP architectures. Previously, there were myriad architectures across mathematical frameworks-recurrent neural networks, convolutional neural networks, and recursive neural networks-built for specific tasks. For a machine answering a phone call, previously, the software relied upon one mathematical framework to translate the language, another to dictate a response. Now, GPT architecture has unified NLP research under one system. \n\nGPT-3 is the latest iteration of generative pretrained transformer models, which were developed by scientists at OpenAI in 2018. On the surface, it may be difficult to see the difference between these models and more narrow or specific AI models. Historically, most AI models were trained through supervised machine learning, which means humans labeled data sets to teach the algorithm to understand patterns. Each of these models would be developed for a specific task, such as translating or suggesting grammar. Every model could only be used for that specific task and could not be repurposed even for seemingly similar applications. As a result, there would be as many models as there were tasks. \n\nTransformer machine learning models change this paradigm of specific models for specific tasks to a general model that can adapt to a wide array of tasks. In 160 Daedalus, the Journal of the American Academy of Arts & Sciences",
            "score": 0.43660904299104053,
            "section_title": "In the darkness The uncertainty",
            "char_start_offset": 8020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 463
                },
                {
                    "start": 466,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1308
                },
                {
                    "start": 1311,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2011
                },
                {
                    "start": 2014,
                    "end": 2168
                },
                {
                    "start": 2169,
                    "end": 2240
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "225639057",
            "title": "Ablations over transformer models for biomedical relationship extraction",
            "text": "Although performance gains from larger models and lengthier pretraining is an interesting phenomenon, this represents practical issues for those working within niche domains who desire models pretrained on specific styles of document. With the rapid evolution of new architectures, and substantial costs involved in pretraining, the investment in performing domain specific pretraining becomes hard to justify when the end result may be obsolete within months. Thus, it is desirable to know whether the performance gains from domain specific pretraining outlive the original model architecture (compared to newer architectures that do not benefit from learning better representations of a domain, but perhaps benefit from learning better representations of domain independent, fundamental aspects of language). \n\nA second aspect of language modelling concerns how model are fine-tuned to perform certain tasks. For instance, sentence classification tasks with the original BERT model is possible by passing the sentence representation token (denoted [CLS]) through a linear layer. More recent work (specific to the task of relationship extraction) has explored how combining embedded entity information with such sentence representations can lead to significant performance boosts (the RBERT head) 10 . However, evidence has since emerged 11 that at least some of the perceived performance gains of transformer style models is due to so-called 'Clever Hans' type effects, where the model is fine-tuned to learn unintended correlations in datasets rather than a generalised representation of the task. This in turn raises questions about the validity of such approaches in the task of relationship extraction, and how to manufacture appropriate datasets. \n\nThe goal of this article is to attempt to address some of these questions via ablation studies of a range of popular masked language models and classification heads, to determine their performance on the task of biological relationship extraction.",
            "score": 0.4349552965784496,
            "section_title": "Introduction",
            "char_start_offset": 2249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 2003
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03912353515625
        },
        {
            "corpus_id": "273901458",
            "title": "Enhancing Legal Expertise in Large Language Models through Composite Model Integration: The Development and Evaluation of Law-Neo",
            "text": "In this paper, we introduced Law-Neo, an innovative approach to enhancing large language models (LLMs) for the legal domain by leveraging the concept of composition to augment existing models. Our methodology focused on integrating Qwen2-72B-Chat with a legal domain-specific model based on Qwen2-7B. Our comprehensive experiments, conducted on multiple legal benchmarks including LawBench, the Unified Qualification Exam for Legal Professionals (UQELP), and CAIL2018, demonstrate the efficacy of our approach. The Law-Neo model outperformed several existing models, including general-purpose LLMs and specialized legal LLMs. Our results indicate that integrating models through shared parameters can effectively enhance their specialized knowledge without sacrificing the foundational abilities of the base models.",
            "score": 0.43485527893739845,
            "section_title": "Conclusion",
            "char_start_offset": 11132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 815
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1304931640625
        },
        {
            "corpus_id": "272593336",
            "title": "Scaling Law Hypothesis for Multimodal Model",
            "text": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance. The initial scaling laws proposed by OpenAI [1] suggested that larger models are more sample-efficient, leading to the creation of massive models like GPT-3. However, subsequent research from DeepMind, notably the Chinchilla study [2], revealed that many large models were undertrained. Their findings indicated that smaller models trained on more data could outperform larger models when the compute budget is held constant. Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8]. \n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]. This approach introduces bits per character (BPC) as a performance metric that reflects the model's compression efficiency [9]. BPC has been shown to correlate linearly with model performance across various modalities (Figure 1). This perspective reveals a linear relationship between BPC and the logarithm of compute used, which can be formalized as: \n\nwhere N is the number of training tokens, and P is the number of model parameters (Figure 2). \n\nThis unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant. In multimodal systems, diverse types of data, such as text, audio, images and video, are processed through specialized tokenization techniques, each with varying levels of compression efficiency. \n\nText is tokenized using methods like Byte Pair Encoding (BPE) [10], which offers relatively stable compression efficiency.",
            "score": 0.4347534660933112,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2179
                },
                {
                    "start": 2182,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061309814453125
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "This allows to build a language model estimated on all the training corpus, without the need to separate the languages; c) build a unified multilingual system seems better than dealing with specific-language systems, where one of them must be selected to provide a prediction.\n\nThe rest of the paper is organized as follows: Section 2 introduces the related works on multilingual handwriting recognition systems; In section 3, we present the framework of the unified multilingual handwriting recognition system that we propose here; Then, we show and discuss experimental results where the English and French languages are combined (section 4) before concluding.",
            "score": 0.4340517331236374,
            "section_title": "Introduction",
            "char_start_offset": 4095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 278,
                    "end": 662
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289764404296875
        },
        {
            "corpus_id": "220496447",
            "title": "Is Machine Learning Speaking my Language? A Critical Look at the NLP-Pipeline Across 8 Human Languages",
            "text": "Corpora of human language are regularly fed into machine learning systems as a key way to learn about the world. Systems are taught to identify spam email, suggest medical articles or diagnoses related to a patient's symptoms, sort resumes based on relevance for a given position and many other tasks that form key components of critical decision making systems in areas such as criminal justice, credit, hiring, housing, allocation of public resources, medical decisions and more. Much like facial recognition systems are often trained to represent white men more than black women (Buolamwini, J., 2018), machine learning systems are often trained to represent human expression in languages such as English more than in languages such as Urdu or Wolof. In this paper, a team including speakers of 8 languages (native speakers of 5) ask what it would take to see all the languages we speak fully included in NLP-based research? \n\nA typical NLP-pipeline includes steps such as gathering corpora, processing them into text format, identifying key language elements, training models, and then using these models to answer predictive questions. For some languages, there are well-developed resources available throughout the stages of this pipeline. For some languages, pre-trained models even exist allowing research or development teams to jump right to the last step. \n\nPre-training from scratch using the large corpora necessary for meaningful NLP-results is expensive (i.e. days on a dozen CPUs). When a team can download a pre-trained model, they avoid this substantial overhead. Fine-tuning is much less expensive (i.e. hours on a single CPU). This makes NLPbased results accessible to a wider range of people, but only if such a pre-trained model is available for their language. When these easy to use pre-trained models exist for only a few languages, it further exacerbates the disparity in representation and participation. \n\nIt is increasingly common to use these pre-trained models without a clear evaluation/understanding of the ingredients used to build them.",
            "score": 0.4328258490510863,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2071
                }
            ],
            "ref_mentions": [
                {
                    "start": 582,
                    "end": 604,
                    "matchedPaperCorpusId": "3298854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026763916015625
        },
        {
            "corpus_id": "272922197",
            "title": "The Impact of Prompting Techniques on the Security of the LLMs and the Systems to Which They Belong",
            "text": "To avoid or at least diminish these risks, we need a clear understanding of what a large language model is, how it behaves, and what are the factors that have the greatest impact on its performance and behavior. \n\nPrior to discussing the concept of a large language model, it is essential to first comprehend what constitutes a language model. A language model serves as a probabilistic representation of a natural language [10]. The functionality of a language model is based on the principle that the likelihood of the subsequent word in a sequence is determined solely by a specific number of preceding words, which is referred to as context. To facilitate this, language models utilize methodologies such as N-grams [10], Hidden Markov models [11], or neural networks to identify patterns and interrelations among words within a text corpus. \n\nA large language model (LLM) is nothing more than a language model, but being larger allows it to gain the ability to perform different kinds of natural language processing (NLP) tasks. Architecture-wise, except for the size, the difference between the two lies in the fact that the underlying model of an LLM is an artificial neural network using a transformer-based architecture. The transformer architecture is based on the \"Attention is all you need\" [12] paper that we will not cover in this paper, but we recommend checking it out. The ability of LLMs to perform general purpose NLP tasks makes them flexible, turning them into perfect components for our systems, since they are useful on almost every level, from interacting with our end user to doing different kinds of task under the hood. This strength can also be a weakness, because once we find a use case in our system for an LLM, we would like to be very good at it and do nothing other than what it is supposed to do. Another problem that arises is that LLMs do not have any kind of internal memory; therefore, they are strongly dependent on the context they are fed. Of course, these downsides can be overcome at least partially using different prompting techniques, some of which are covered in the papers we will discuss in Section 2.2. Looking at these techniques, we will observe that good prompting can achieve in some cases even better performance than fine tuning the model for a specific task.",
            "score": 0.4322615200201313,
            "section_title": "Large Language Models",
            "char_start_offset": 5348,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 845
                },
                {
                    "start": 848,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2316
                }
            ],
            "ref_mentions": [
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "11358505"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041778564453125
        },
        {
            "corpus_id": "268379355",
            "title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
            "text": "In the pre-LLM era, pre-trained language models (PLMs) such as BERT Devlin et al. (2018), RoBERTa Y. Liu et al. (2019), BART Lewis et al. (2019) etc. have been used extensively as language encoders.These PLMs are essentially transformer-based language models that are pre-trained on a large corpus of unlabeled text data (mostly webtext) and often fine-tuned on downstream task datasets to perform classification or detection.Given the extensive pre-training that these language models go through, PLMs are often used as general language encoders in a classification task, with additional classification layers or classification heads added to facilitate task-specific fine-tuning Howard and Ruder (2018); Arslan et al. (2021).\n\nFor example, authors in Kant et al. (2018) first pre-train and then fine-tune an encoderdecoder type language model on task specific data for the task of multi-dimensional sentiment classification and compare their method with a pre-trained ELMo Peters et al. (1802), which is then further fine-tuned on their tasks-specific dataset.BERT Devlin et al. (2018), which is a bidirectional transformer-based language model, has shown impressive performance on many natural language understanding tasks.Authors in Sun et al. (2019) in-vestigate the training regimes and different fine-tuning settings to understand how to get the most out of fine-tuning BERT for the task of text-classification.Through their experiments they advise that text classification using BERT can be improved via the following best practices: further pre-training on task-specific in-domain data, multi-task fine-tuning rather than single task fine-tuning etc.\n\nGiven the smaller sizes of pre-trained language models as compared to more recent models like ChatGPT or Llama, these models have been used in several other text classification tasks, often with task-specific fine-tuning or in conjunction with other specialized architecture or training regimes Min et al. (2023).",
            "score": 0.4314886782470191,
            "section_title": "Pre-LLM Era",
            "char_start_offset": 3811,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 727
                },
                {
                    "start": 729,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1659
                },
                {
                    "start": 1661,
                    "end": 1974
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 726,
                    "matchedPaperCorpusId": "235324756"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0784912109375
        },
        {
            "corpus_id": "268428263",
            "title": "ChatGPT Alternative Solutions: Large Language Models Survey",
            "text": "Language models like ChatGPT, including Large Language Models (LLMs), have limitations despite their impressive performance.These limitations stem from factors such as biases in training data, a lack of common-sense reasoning, and a reliance on statistical patterns.LLMs can produce factually incorrect outputs and struggle with complex prompts, making contextual understanding and misinformation challenges.Ongoing research is essential to improve their accuracy, ensuring more reliable and trustworthy results in various applications and mitigating these limitations.\n\nFuture research holds promise in the development of specialized datasets that extend beyond simple data collection from existing sources.Instead, these datasets should be carefully crafted to meet specific requirements, audiences, or problem domains, guided by thoughtful consideration.For instance, in the legal field, constructing a comprehensive dataset would necessitate a deep understanding of legal history and previous cases, serving as a valuable resource for training AI models in legal tasks.Creating such datasets requires meticulous planning in data collection methods to represent real-world complexities accurately, and techniques like data augmentation, transfer learning, and fine-tuning should enhance diversity and quality.Domain-specific datasets tailored to unique needs will be pivotal in achieving high-performance AI models for practical applications and advancing solutions to real-world challenges.\n\nMeanwhile, for future work, it is also essential to ask the question of who the target audience of the model is.Identifying the specific audience categories, such as medical, law, education, etc., will help determine the necessary adaptations and additions to the model to meet their unique requirements.For instance, if the model is intended for the medical field, it should be tailored to handle medical terminology, understand complex medical concepts, and ensure trustworthy results for diagnostic purposes.As a state-of-the-art review, considering future work is imperative.Moving beyond simple word prediction, the next level of development involves incorporating reasoning capabilities and applying the model to knowledge graphs using APIs.This extension would allow the model to exhibit advanced understanding and reasoning abilities, making it more adept at handling complex and nuanced tasks.In the context of education, exploring the potential of using the model as a teaching tool opens new avenues for research and development.",
            "score": 0.43141590495412696,
            "section_title": "Open Problems",
            "char_start_offset": 41774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 569
                },
                {
                    "start": 571,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 857
                },
                {
                    "start": 857,
                    "end": 1073
                },
                {
                    "start": 1073,
                    "end": 1312
                },
                {
                    "start": 1312,
                    "end": 1494
                },
                {
                    "start": 1496,
                    "end": 1608
                },
                {
                    "start": 1608,
                    "end": 1800
                },
                {
                    "start": 1800,
                    "end": 2007
                },
                {
                    "start": 2007,
                    "end": 2075
                },
                {
                    "start": 2075,
                    "end": 2243
                },
                {
                    "start": 2243,
                    "end": 2398
                },
                {
                    "start": 2398,
                    "end": 2536
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09808349609375
        },
        {
            "corpus_id": "231802467",
            "title": "Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",
            "text": "The following research questions were inspired by the discussion: \n\n\u2022 Can we better understand why language models improve so much with scale? Can this enable us to build models which scale more efficiently? \n\n\u2022 What are the limits of scaling? Will scale lead to strong causal reasoning, symbolic manipulation, commonsense understanding, and robustness to a wider class of inputs? Or will different techniques be necessary? \n\n\u2022 How can we understand the limits of what large language models are capable of? Can we enable models to ask for help or clarification, or abstain when they are unsure? \n\n\u2022 How can we develop new neural network architectures and algorithms that enable efficient learning from diverse, multimodal data beyond text? \n\n\u2022 What are the opportunities and tradeoffs involved in different approaches to steering the outputs of large-scale language models to be more aligned with human values? \n\n\u2022 How should access to models like GPT-3 be allocated, balancing considerations like security, replicability, and fairness? What kinds of tests do we need to develop in order to qualify language models like GPT-3 as being safe or unsafe for use in particular contexts? \n\n\u2022 What can academia do to best position itself to develop guardrails for the industrial development of such models -including advocating for sufficient funding to replicate the compute resources required to train them? \n\n\u2022 How can we best foster cross-disciplinary collaboration to understand and manage the biases in large datasets and model representations of such datasets? \n\n\u2022 How can we best characterize the potential \"threat landscape\" for such models; e.g., do we need to spend more time worrying about how models like this could be used by profit-driven actors to generate lots of low-grade spam, or should we be more worried about state-based actors using models to generate persuasive text for use in disinformation campaigns? \n\n\u2022 How cost-effective and skill-intensive would it be for malicious actors to misuse language models for various purposes, compared to alternative methods of achieving the same goals?",
            "score": 0.431203563546816,
            "section_title": "Future Research Directions",
            "char_start_offset": 15813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 68,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 207
                },
                {
                    "start": 210,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1560
                },
                {
                    "start": 1563,
                    "end": 1921
                },
                {
                    "start": 1924,
                    "end": 2106
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036163330078125
        },
        {
            "corpus_id": "272922197",
            "title": "The Impact of Prompting Techniques on the Security of the LLMs and the Systems to Which They Belong",
            "text": "The general-purpose ability to generate text or perform different kinds of NLP tasks makes the LLMs a perfect starting point for a lot of use cases we might be interested in. But once we have our task defined, we would like perhaps to increase the performance of the model on that specific job. The traditional way of doing so is usually to fine tune the model for our use case. We adapt our general-purpose model to a specific task by updating the parameters on a new dataset specific to our task. Most of the time, this approach yields the best results, but there are a few problems with it. One of these problems is the fact that the amount of NLP tasks and different variations for these jobs are too big to have for every one of them a dataset that fits them, especially if we want something custom-made for our system. \n\nThe \"Language Models are Few-Shot Learners\" [1] paper presents a different method of tweaking an LLM to increase its performance on an NLP job. This paper explores the ability of the LLMs to conduct in-context learning and to adapt to new kinds of NLP tasks more or less on the fly. Inspired by human behavior, the authors observe that most of the time, humans need a few examples or instructions to understand how to perform an NLP task. This type of approach was not very effective on the smaller language models, but as the size of the model scales, they notice that this method \"improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches\" [1]. The model used to test this hypothesis is GPT-3, which is an auto-regressive language model with 175 billion parameters without any kind of fine tuning or gradient updates. They test three different settings for it-zero-shot (the model is only given a natural language instruction describing the task, but no demonstration is provided), one-shot (the model is given a natural language instruction describing the task alongside a single demonstration of how should the task be performed) and few-shot (the model is given a few demonstrations of the task at inference time as conditioning together with the natural language instruction describing the task)-and compare them with the traditional fine-tuned models specific for each task.",
            "score": 0.430718636196448,
            "section_title": "Language Models Are Few-Shot Learners",
            "char_start_offset": 8764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "268041436",
            "title": "Prediction-Powered Ranking of Large Language Models",
            "text": "During the last years, large language models (LLMs) have shown a remarkable ability to generate and understand general-purpose language [1]. As a result, there has been an increasing excitement in their potential to help humans solve a variety of open-ended, complex tasks across many application domains such as coding [2], healthcare [3] and scientific discovery [4], to name a few. However, evaluating and comparing the performance of different LLMs has become very challenging [5]. The main reason is that, in contrast to traditional machine learning models, LLMs can solve a large number of different tasks and, in many of these tasks, there is not a unique, structured solution. As a consequence, there has been a paradigm shift towards evaluating their performance according to their level of alignment with human preferences-a model is better than other models if its outputs are more frequently preferred by humans [6][7][8][9][10]. \n\nOne of the most popular paradigms to rank a set of LLMs according to their level of alignment with human preferences utilizes pairwise comparisons [10][11][12][13][14][15][16][17]. Under this paradigm, each pairwise comparison comprises the outputs of two different models picked uniformly at random to an input sampled from a given distribution of inputs. Moreover, the pairwise comparisons are used to rank the models with a variety of methods such as the Elo rating [18][19][20][21][22], the Bradley-Terry model [10,17,23] or the win-rate [12,17,23]. While it is widely agreed that, given a sufficiently large set of pairwise comparisons, higher (lower) ranking under this paradigm corresponds to better (worse) human alignment, there have also been increasing concerns that this paradigm is too costly and time-consuming to be practical, especially given the pace at which models are updated and new models are developed.",
            "score": 0.43063828149486216,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 941
                },
                {
                    "start": 944,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "253117056"
                },
                {
                    "start": 336,
                    "end": 339,
                    "matchedPaperCorpusId": "257764798"
                },
                {
                    "start": 365,
                    "end": 368,
                    "matchedPaperCorpusId": "266223700"
                },
                {
                    "start": 481,
                    "end": 484,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 930,
                    "end": 933,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1103,
                    "end": 1107,
                    "matchedPaperCorpusId": "263829791"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "259360619"
                },
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1119,
                    "end": 1123,
                    "matchedPaperCorpusId": "268264163"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 1429,
                    "end": 1433,
                    "matchedPaperCorpusId": "258841681"
                },
                {
                    "start": 1463,
                    "end": 1466,
                    "matchedPaperCorpusId": "268264163"
                },
                {
                    "start": 1486,
                    "end": 1490,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1490,
                    "end": 1493,
                    "matchedPaperCorpusId": "268264163"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070556640625
        },
        {
            "corpus_id": "248780278",
            "title": "Findings of the IWSLT 2022 Evaluation Campaign",
            "text": "primary end-to-end and a primary cascaded system, in 6 out of 8 times, the cascaded system performed better than the end-to-end system. Whether this is partly due to the integration of pre-trained models has to be evaluated in further experiments.\n\nPre-trained models It is difficult to measure the impact of pre-trained models since there is no participant submitting both, a translation system with and without pre-trained models. However, there are some indications of the usefulness of pre-trained models. First, nearly all participants submitted systems with pre-trained models. Typically, these are audio encoders like wav2vec or Hubert for the encoder and text models like mBart for the decoder. Secondly, all winning systems are using this technology. And finally, we see large gains in translation quality compared to last year, where this technique was not allowed. Consequently, these models seem to be an interesting knowledge source. However, it should be noted that the models are rather large and therefore can also be a limiting factor for teams to participate in the evaluation campaign.\n\nMulti-lingual models For the first time, since several years, this year's edition of the offline task included several language directions. Interestingly, this did not lead to a partition of participants into different language pairs, but most participants submitted translations for all three language pairs. While the best performing systems were individually optimized for each language, we also see multilingual models submitted to the tasks. Especially, the integration of pre-trained models, which are typically multi-lingual, made it easier to build translation systems for all three conditions. While the ranking between the languages is not the same, it is still very similar. This indicates that a good system in one language direction typically will also result in good performance in the other directions. While the amount of training resources is at least comparable, this is interesting since the languages are rather different.",
            "score": 0.42969859425296375,
            "section_title": "Automatic Evaluation",
            "char_start_offset": 30897,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0321044921875
        },
        {
            "corpus_id": "263605804",
            "title": "Testing the Limits of Unified Sequence to Sequence LLM Pretraining on Diverse Table Data Tasks",
            "text": "Tables stored in databases and tables which are present in web pages and articles account for a large part of semi-structured data that is available on the internet. It then becomes pertinent to develop a modeling approach with large language models (LLMs) that can be used to solve diverse table tasks such as semantic parsing, question answering as well as classification problems. Traditionally, there existed separate models specialized for each task individually. It raises the question of how far can we go to build a unified model that works well on some table tasks without significant degradation on others. To that end, we attempt at creating a shared modeling approach in the pretraining stage with encoder-decoder style LLMs that can cater to diverse tasks. We evaluate our approach that continually pretrains and finetunes different model families of T5 with data from tables and surrounding context, on these downstream tasks at different model scales. Through multiple ablation studies, we observe that our pretraining with self-supervised objectives can significantly boost the performance of the models on these tasks. As an example of one improvement, we observe that the instruction finetuned public models which come specialized on text question answering (QA) and have been trained on table data still have room for improvement when it comes to table specific QA. Our work is the first attempt at studying the advantages of a unified approach to table specific pretraining when scaled from 770M to 11B sequence to sequence models while also comparing the instruction finetuned variants of the models.",
            "score": 0.42953413528089374,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321533203125
        },
        {
            "corpus_id": "259095948",
            "title": "Turning large language models into cognitive models",
            "text": "We have demonstrated that large language models can be turned into cognitive models by finetuning their final layer. This process led to models that achieved state-of-the-art performance in two domains. Furthermore, these models were able to capture behavioral differences at the individual participant level. Finally, we have shown that our approach generalizes to previously unseen tasks. In particular, a model that was finetuned on two tasks also exhibited human-like behavior on a third, hold-out task. \n\nThese results complement earlier work showing that large language model embeddings allow us to predict behavior and neural activations in linguistic settings [Schrimpf et al., 2021, Kumar et al., 2022, Tuckute et al., 2023, Antonello et al., 2023]. For example, Schrimpf et al. [2021] showed that large language models can predict neural and behavioral responses in tasks that involved reading short passages with an accuracy that was close to noise ceiling. While it may be expected that large language models explain human behavior in linguistic domains (after all these models are trained to predict future word occurrences), the observation that these results also transfer to more cognition domains like the ones studied here is highly non-trivial. \n\nWe are particularly excited about one feature of CENTaUR: embeddings extracted for different tasks all lie in a common space. This property allows finetuned large language models to solve multiple tasks in a unified architecture. We have presented preliminary results in this direction, showing that a model finetuned on two tasks can predict human behavior on a third. However, we believe that our current results only hint at the potential of this approach. Ideally, we would like to scale up our approach to finetuning on a larger number of tasks from the psychology literature. If one would include enough tasks in the training set, the resulting system should -in principle -generalize to any hold-out task. Therefore, our approach provides a towards a domain-general model of human cognition, which has been the goal of theoreticians for decades [Newell, 1992, Yang et al., 2019, Riveland and Pouget, 2022, Binz et al., 2023]. We believe that having access to such a model would transform psychology and the behavioral sciences more generally.",
            "score": 0.42830169891331527,
            "section_title": "Discussion",
            "char_start_offset": 14658,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1263
                },
                {
                    "start": 1266,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2315
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 690,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 690,
                    "end": 710,
                    "matchedPaperCorpusId": "249613319"
                },
                {
                    "start": 710,
                    "end": 732,
                    "matchedPaperCorpusId": "258216421"
                },
                {
                    "start": 772,
                    "end": 794,
                    "matchedPaperCorpusId": "222359195"
                },
                {
                    "start": 2131,
                    "end": 2150,
                    "matchedPaperCorpusId": "58006968"
                },
                {
                    "start": 2150,
                    "end": 2177,
                    "matchedPaperCorpusId": "247109479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1617431640625
        },
        {
            "corpus_id": "220523407",
            "title": "Adaptive Language Processing Based on Deep Learning in Cloud Computing Platform",
            "text": "is section introduces a domain adaptive approach based on massive corpus. e main idea is that the training of our model is mainly divided into three steps: first, selecting the data in the domain is according to the predefined domain; second, training the domain model and the general model is to construct the statistical machine translation system; third, using massive corpus technology makes joint adjustments to multiple domain systems. \n\nAccording to the above, the first step in this work is to select the in-domain bilingual control data from all the bilingual training data to train the translation model. Since the monolingual data in a specific field can be obtained in large quantities, we draw on the method of bilingual cross section data selection [22] to obtain bilingual data in the field: \n\nis bilingual cross-entropy-based criterion tends to choose a sentence pair that is more similar to the data distribution in the domain but different from the general data distribution. erefore, this method considers that the sentence pair with larger cross-entropy difference should be selected. \n\nIn the second step, we use the training data in the selected domain to build a statistical machine translation system based on the hybrid model. Specifically, we adopted the idea of a hybrid model to build N machine translation systems for N predefined fields; each of which is a log-linear model. For each system, the optimal translation result f is given by \n\nFor each machine translation system, two translation models and two language models are included. e translation model of a specific field is trained by the bilingual data selected by the data selection method introduced in the previous section, and the translation model of the general domain is trained using all bilingual data. For the language model, we reuse the language-specific and general-language models of the specific domain trained for data selection in the previous section. Compared to a translation system that does not do domain migration, this system with a hybrid model can better balance the general translation knowledge and domain-specific translation knowledge and can benefit from two aspects. \n\nIn the third step, it is necessary to adjust the feature weights in different machine translation systems. e traditional method of arranging is generally directed to a single system. \n\ne method described in this section regards translation systems in different fields as related translation tasks, and joints are coordinated under the framework of massive corpus. \n\nere are two reasons for using massive corpora:",
            "score": 0.4276616625532569,
            "section_title": "Statistical Machine Translation Adaptation.",
            "char_start_offset": 12439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2185
                },
                {
                    "start": 2188,
                    "end": 2294
                },
                {
                    "start": 2295,
                    "end": 2370
                },
                {
                    "start": 2373,
                    "end": 2551
                },
                {
                    "start": 2554,
                    "end": 2600
                }
            ],
            "ref_mentions": [
                {
                    "start": 763,
                    "end": 767,
                    "matchedPaperCorpusId": "205591725"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1795654296875
        },
        {
            "corpus_id": "9820466",
            "title": "The NLP Engine: A Universal Turing Machine for NLP",
            "text": "The purpose of this paper is to suggest a unified framework in which modern NLP research can quantitatively describe and compare NLP tasks. Even though everyone agrees that some NLP tasks are more complex than others, e.g., machine translation is 'harder' than syntactic parsing, which in turn is 'harder' than part-of-speech tagging, we cannot compute the relative complexities of different NLP tasks and subtasks. \n\nIn the typical current NLP paradigm, researchers apply several machine learning algorithms to a problem, report on their performance levels, and establish the winner as setting the level to beat in the future. We have no single overall model of NLP that subsumes and regularizes its various tasks. If you were to ask NLP researchers today they would say that no such model is possible, and that NLP is a collection of several semiindependent research directions that all focus on language and mostly use machine learning techniques. Researchers will tell you that a good summarization system on DUC/TAC dataset obtains a ROUGE score of 0.40, a good French-English translation system achieves a BLUE score of 37.0, 20-news classifiers can achieve accuracy of 0.85, and named entity recognition systems a recall of 0.95, and these numbers are not comparable. Further, we usually pay little attention to additional important factors such as the performance curve with respect to the amount of training data, the amount of preprocessing required, the size and complexity of auxiliary information required, etc. And even when some studies do report such numbers, in NLP we don't know how to characterize these aspects in general and across applications, how to quantify them in relationship to each other. \n\nWe here describe our first attempt to develop a single generic high-level model of NLP. We adopt the model of a universal machine, akin to a Turing Machine but specific to the concerns of language processing, and show how it can be instantiated in different ways for different applications. We employ Shannon Entropy within the machine to measure the complexity of each NLP task. \n\nIn his epoch-making work, Shannon (1951) demonstrated how to compute the amount of information in a message.",
            "score": 0.42759609377362917,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2100
                },
                {
                    "start": 2103,
                    "end": 2211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05987548828125
        },
        {
            "corpus_id": "235474507",
            "title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks",
            "text": "Knowledge consolidation by logit concatenation. In the service phase, PoE assembles the library component and the multiple expert components required for a given composite task in realtime, and thereby builds a single task-specific model without any training process. Our main strategy for this train-free model generation is to organize the task-specific model to have multiple branches from library, one for each expert, yet with a single final layer where the knowledge is finally consolidated. More specifically, as shown in Figure 3, we just put the library component at the front of the model, and make the input of all the necessary experts to be connected to library. Finally, the output logits of experts (i.e., s ) are concatenated into a unified logit, denoted by s , to be an input of the final softmax function. Somewhat surprisingly, this simple way of logit concatenation works very well for our problem of building a task-specific model. To be experimentally shown in Section 5.3, however, this does not necessarily imply that specialized models distilled by using only L can be easily unified by logit concatenation even though L effectively addresses the high confidence problem as mentioned in Section 4.1. Obviously, we cannot simply combine multiple neural networks by concatenating their logits either if they are independently built by any other training methods.\n\nLogit scale problem. The problem here is the fact that logits of different models can be in arbitrary scales and hence cannot simply be concatenated into a unified logit. Thus, we have to normalize logits in different scales, which we call the logit scale problem. To illustrate, consider a composite task consisting of 1 = { , } and 2 = { , } in Figure 4. Also, let us say we have built two expert models for 1 and 2 , which can be combined to the unified task-specific model ( ). Even if both ( 1 ) and ( 2 ) are properly trained to be not so confident about out-of-distribution samples as shown in Figure 4(a), the unified model ( ) can make a wrong inference when their logits are simply concatenated without considering their scales as shown in Figure 4(b). This can happen because the standard KD loss L as well as our L loss only focuses on trying",
            "score": 0.42756896156767465,
            "section_title": "Service Phase: Train-Free Knowledge Consolidation",
            "char_start_offset": 19339,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "269214633",
            "title": "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM",
            "text": "LLooM also presents critical design challenges, especially given its use of large language model outputs and its specific use of OpenAI's GPT models.These point to important future work directions.7.2.1 Uncertain LLM behaviors: risks of uneven cross-domain performance.One core limitation of this work, and any work that builds upon large language models, is that we currently lack reliability and performance guarantees.LLM performance can vary widely across domains and greatly depends on the training data, which is often withheld from public knowledge.While we can expect LLMs like GPT-4 to perform strongly on text similar to the distribution of large-scale Internet text data on which they were trained, performance may decline in specialized domains such as law, medicine, and fields requiring technical expertise.Novel techniques may be needed to enable concept induction in areas underrepresented in LLM training data.LLMs often err in following instructions, struggle with logical statements, or produce outputs with hallucinations that are not faithful to the original data.We cannot entirely remove the possibility of such foundational errors, but our system additionally mitigates the risk of downstream harm by heavily incorporating human review: analysts can trace concepts back to lower-level concepts and original data examples, and they can review concept scores and rationales to catch when models fail.7.2.2Drawbacks of closed-source LLMs: cost and lack of transparency.Compounded on the uncertainties of large language models in general, there are additional downsides of closed-source models like OpenAI's GPT models, which we use in our LLooM implementation.Since we lack transparency on both the data on which these models were trained and the design of the models themselves, we have limited ability to anticipate blindspots that would impact LLooM's functionality.Additionally, the use of OpenAI models presents barriers to reproducibility: the model versions underlying the APIs may change at any time without our knowledge, and we lack the control to invoke the same model version we may have used in the past.We opt to use the closed-source OpenAI GPT models because they represent the state-of-the-art; our preliminary testing with other models could not reliably execute the synthesis operations central to our approach.",
            "score": 0.4273522999488457,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 82967,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 149,
                    "end": 197
                },
                {
                    "start": 197,
                    "end": 269
                },
                {
                    "start": 269,
                    "end": 421
                },
                {
                    "start": 421,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 927
                },
                {
                    "start": 927,
                    "end": 1085
                },
                {
                    "start": 1085,
                    "end": 1422
                },
                {
                    "start": 1422,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1490
                },
                {
                    "start": 1490,
                    "end": 1681
                },
                {
                    "start": 1681,
                    "end": 1890
                },
                {
                    "start": 1890,
                    "end": 2138
                },
                {
                    "start": 2138,
                    "end": 2351
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06256103515625
        },
        {
            "corpus_id": "273662078",
            "title": "SandboxAQ's submission to MRL 2024 Shared Task on Multi-lingual Multi-task Information Retrieval",
            "text": "Our work on multilingual QA and NER and the variability in results across languages and tasks indicates that no single approach served as a universal solution across all scenarios. \n\nThe effectiveness of techniques like DSPy and translation varied between models and tasks. We challenged the assumption that larger models always perform better, suggesting that factors such as model architecture and prompt development play a significant role. Our results show that models specifically designed for multilingual tasks did not consistently outperform general-purpose LLMs, suggesting that specialized architectures may not be necessary for effective multilingual processing. \n\nA key advantage of our approach is achieving high scores (often above 95% for QA and 90% for NER) with minimal system building. This shows the potential for efficient, yet highly effective multilingual NLP systems through simple utilization of pretrained models. \n\nOur research also suggests the potential of using open-source LLMs beyond the Aya models, including LLama 3 16 , Mixtral (Jiang et al., 2023(Jiang et al., , 2024)), and Gemma 2 (Team et al., 2024). These models, which have often demonstrated performance comparable to closed-source alternatives, may facilitate new research and practical applications for working with limited resources in future studies. \n\nOther future research may focus on approaches that better handle the diverse requirements of different languages and tasks. These efforts, combined with the use of language specific datasets, or texts in languages of related language families to the languages we examine, for model finetuning, will be crucial in building truly effective and resourceefficient systems. \n\n\u2022 English Translation prompt: \n\n\"Translate the following text to English:\"",
            "score": 0.4266833100816976,
            "section_title": "Conclusions",
            "char_start_offset": 25207,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 183,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1793
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "259095527",
            "title": "The Two Word Test: A Semantic Benchmark for Large Language Models",
            "text": "Large Language Models (LLMs; also called Large Pre-Trained Models or Foundation Models) (Bommasani et al. 2021) are deep neural networks with billions or trillions or parameters that are trained on massive natural language corpora. They have shown remarkable and surprising abilities spanning many different tasks. Some examples include the ability to pass examinations required for advanced degrees, such as those in law (Choi et al. 2023), business (Terwiesch 2023), and medicine (Kung et al. 2023). Strong performance on benchmarks such as General Language Understanding Evaluation (GLUE) and its successor (SuperGLUE) have also been obtained (Brown et al. 2020, Chowdhery et al. 2022). Bubeck et al. (2023) investigated an early version of GPT-4, and reported that it can solve difficult tasks in mathematics, coding, vision, medicine, law, and psychology, music, and exhibited \"mastery of language.\" With such breadth of human-level (or better) performance, they suggested that it shows \"sparks\" of Artificial General Intelligence (AGI). \n\nSuch achievements have led many researchers to conclude that LLMs have achieved or are close to achieving real or humanlike understanding of language. Others remain skeptical. A recent survey (Michael et al. 2022) asked active researchers whether such models, trained only on text, could in principle understand natural language someday. About half (51%) agreed, while other half (49%) disagreed. This stark divide is closely tied to the question of what constitutes true understanding and is subject of intense debate (Michell and Karkauer 2023). \n\nThe skeptics have pointed out examples where LLMs produce less-than-satisfactory performance. Hallucinations (Lee et al. 2018, Raunak et al. 2021), inaccurate number comparisons, and reasoning errors are commonly cited problems, and failures in individual cases are frequently reported (e.g., https://github.com/giuven95/chatgptfailures). It is argued that while LLMs exhibit formal linguistic competence, they lack functional linguistic competence, which is the ability to robustly understand and use language in the real world (Mahowald et al. 2023).",
            "score": 0.4266168275325504,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 501
                },
                {
                    "start": 502,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1592
                },
                {
                    "start": 1595,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 422,
                    "end": 440,
                    "matchedPaperCorpusId": "256409866"
                },
                {
                    "start": 482,
                    "end": 500,
                    "matchedPaperCorpusId": "254876189"
                },
                {
                    "start": 646,
                    "end": 664,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05389404296875
        },
        {
            "corpus_id": "277740884",
            "title": "Linguistic Interpretability of Transformer-based Language Models: a systematic review",
            "text": "Also, by providing an insight into some aspects of the pre-training of these models, we might also be able to understand and even manually control some parts of it, something that could help alleviate the environmental and budgeting issues that are commonly associated with the pre-training from scratch of ever-larger models [15]. It may also help detect issues with existing machine translation systems based on the Transformer architecture. Additionally, the acquisition of linguistic knowledge in Transformer-based models may give some insights into how humans process and acquire language themselves, thus contributing to a combined study area in linguistic competence in humans and machines -e.g. [120]. \n\nThe information on linguistic competence in Transformer language models, while potentially useful and interesting, is unfortunately sparse and spread across many research works in the literature. As such, recovering information on the topic can become difficult for the researcher, since the general tendency is for an individual paper to either simply report on a single linguistic phenomenon -e.g. subject-verb agreement-or, even when discussing many linguistic phenomena, only provide results on a limited number of languages -mostly English-or on very few Transformer-based models. This leads to a plethora of studies that report individual instances of language discovery in these models, but without any sort of unifying vision on the overall linguistic competence of the Transformer architecture across languages. \n\nIn this survey we aim to provide a unified vision of the conclusions reached by a large body of work aiming to discuss the topic of 'linguistic interpretability' -i.e. discovery of linguistic knowledge-in Transformer-based pretrained language models, also known as PLMs, or large language models (LLMs) when they are of significant size. \n\nWe analyze a series of research papers on the topic, amounting to a total of 160 works, across different architectures and typologically-different languages, in order to give a general conclusion on how linguistic information may be present and processed internally within these models. We also present the methodologies commonly used by these works to discover this kind of knowledge, as well as the type of linguistic knowledge and phenomena that are usually investigated.",
            "score": 0.42652854363826953,
            "section_title": "Introduction",
            "char_start_offset": 6033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1872
                },
                {
                    "start": 1875,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2349
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 330,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 703,
                    "end": 708,
                    "matchedPaperCorpusId": "254823167"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037200927734375
        },
        {
            "corpus_id": "237604959",
            "title": "Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models",
            "text": "The number of V&L model works that focus on both V&L tasks and language-only tasks has increased (Ororbia et al., 2019;Lin et al., 2021;Hu and Singh, 2021). Ororbia et al. (2019) proposed a V&L neural architecture and trained it on a language model in a visual context. They demonstrated that their architecture outperforms its equivalent trained on language alone in perplexity and stated that language is inseparable from its physical context. Although it is not clear whether methods that improve the perplexity of language modeling can also apply to maintain the performance of downstream tasks, the strategy of improving models with reference to human cognition would be an important direction. More recently,  achieved better performance on language-only tasks than their base model with pre-training on three types of corpora (text, image, and image-text pairs) at the same time. Lin et al. (2021) reported that adding separated extractors for vision and language on top of a single-stream encoder can help maintain language knowledge. In this paper, we fine-tuned V&L models extended from a language model (LM) to an NLU benchmark to compare their NLU performance. We used five V&L models, including single-stream and dual-stream models, pre-trained in the same setup. The benchmark scores of those models decreased compared with their source LM. We also found that the single-stream models tended to retain (slightly) more language knowledge than the dual-stream models, and that the main cause of the drop in the NLU tasks can be pre-training. Our observations suggest that adopting a single stream and devising pre-training strategies could be effective, at least for preserving the language knowledge.  Table 6: Training dataset statistics. CC: The Conceptual Captions dataset (Sharma et al., 2018). CAP: image captioning, P/S: paraphrase/similarity task, SS: single-sentence task.  Table 7: Standard deviations of our results in the performance on the GLUE tasks' development sets (Table 2). SDs are shown in parentheses below each value. We ran three experiments for each task.",
            "score": 0.4264732089495257,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 13572,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 119,
                    "matchedPaperCorpusId": "174798289"
                },
                {
                    "start": 157,
                    "end": 178,
                    "matchedPaperCorpusId": "174798289"
                },
                {
                    "start": 1789,
                    "end": 1810,
                    "matchedPaperCorpusId": "51876975"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049041748046875
        },
        {
            "corpus_id": "257687695",
            "title": "Capabilities of GPT-4 on Medical Challenge Problems",
            "text": "Large language models (LLMs) have exhibited a remarkable ability to interpret and generate sequences across a wide array of domains, such as natural language, computer code, and protein sequences. Numerous powerful models are based on the transformer architecture [VSP + 17], adapted to language and trained in a self-supervised manner [RNS + 18, DCLT18]. Scores on a variety of benchmarks have generally improved with scale, involving increasing model size, dataset size, and the amount of training computation in tandem [KMH + 20, LBL + 22]. The empirical findings resonate with a theoretical analysis [BS21] which shows the necessity of scale for robustness of inferences from large neural models [BS21]. \n\nOver the last several years, LLMs trained on massive, cross-disciplinary corpora have become potent building blocks in the creation of task-focused systems [BHA + 21]. Methods for refining the models toward a particular domain include fine-tuning with specialized datasets drawn from target applications and general methods for steering the behavior of the models, such as reinforcement learning with human feedback (RLHF), which guides the system toward a better understanding of end-users' requests [CLB + 17, BJN + 22]. \n\nThere has also been great interest in the ability of LLMs to make useful inferences for a broad range of specialized tasks without dedicated fine-tuning. The performance of general-purpose LLMs using few-or even zero-shot prompting highlights their potential for assisting with tasks across problem types, specialty areas, and disciplines [BMR + 20]. Recently, researchers have investigated benchmarks that provide insight into how LLMs encode clinical knowledge and might be harnessed to augment the practice of medicine. Here we compare the performance of the recently released (text-only) GPT-4 model with its predecessors in the GPT family on medical challenge problems. While details on measures of scale for GPT-4, including the number of model parameters and the size and scope of training data, have not been made public, it has been reported that both dimensions are significantly bigger than for GPT-3.5, the model behind ChatGPT [Ope23].",
            "score": 0.4262794787594171,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1232
                },
                {
                    "start": 1235,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1909
                },
                {
                    "start": 1910,
                    "end": 2183
                }
            ],
            "ref_mentions": [
                {
                    "start": 604,
                    "end": 610,
                    "matchedPaperCorpusId": "14190268"
                },
                {
                    "start": 700,
                    "end": 706,
                    "matchedPaperCorpusId": "14190268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1473388671875
        },
        {
            "corpus_id": "268379027",
            "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
            "text": "If a piece of information can be serialized and tokenized, it is likely to be handled by large language models (LLMs) (Bommasani et al., 2021;Brown et al., 2020;Ope-nAI, 2023).LLMs, as one of the most advanced manifestations of artificial intelligence, have demonstrated proficiency in three representative symbol systems that are essential to human progress: natural language (Ouyang et al., 2022;Bai et al., 2022), which forms the cornerstone of human inter-action; programming code (Li et al., 2023a;Rozi\u00e8re et al., 2023), the backbone of our digital ecosystem; and mathematical reasoning, the framework underpinning scientific advancement (Luo et al., 2023a;Yang et al., 2023).The mastery of three domains would equip LLMs with unparalleled versatility.However, the intrinsic variability of data distribution across these domains presents a formidable challenge for an LLM to achieve consistently high performance at the same time.One awkward situation is that it is challenging to integrate professional-level coding and mathematical abilities into a general conversational language model without loss.In other words, these skills are more often reflected in the numbers on related benchmarks rather than a real-world user interface.\n\nFigure 1 (a-c) demonstrates such a struggle by presenting the performance of three specialized models on the aforementioned domains, all initially based on the Llama-2 (Touvron et al., 2023) 13B architecture.Our findings reveal a clear trade-off: specialized training in one domain often comes at the expense of performance in the others, whereas training on all three types of data at the same time results in a simultaneous suboptimal situation.Delving into this situation, such an issue may be partially mitigated by careful designs of data engineering, training strategy, or prompt construction.However, in general, semantics in language, logic and structures in code, and abstract symbol manipulations in math intricately always create a situation of mutual weakening.",
            "score": 0.42591170365907827,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 176,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1107
                },
                {
                    "start": 1107,
                    "end": 1238
                },
                {
                    "start": 1240,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1687
                },
                {
                    "start": 1687,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 142,
                    "end": 161,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 377,
                    "end": 398,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.26220703125
        },
        {
            "corpus_id": "233324191",
            "title": "The NLP Cookbook: Modern Recipes for Transformer Based Deep Learning Architectures",
            "text": "We provide a comprehensive and detailed summary of the major language models that have led to the current SOTA in NLP performance. Since the launch of the Attention mechanism and Transformer architecture, NLP has advanced exponentially. We presented a high-level mind map of model classifications via a taxonomy. These classifications are primarily based on Transformer derivative architectures, built for specialized tasks like Language Understanding and Generation, Model Size Reduction via Distillation, Quantization and Pruning, Information Retrieval, Long Sequence Modeling, and other Generalized Model Reduction techniques. Recent language models are primarily driven by attaining higher NLP performance requiring huge computing resources. Thus, model scaling has been the natural pathway in industry. This exponential scaling coupled with higher attention complexity makes these models infeasible to access at a global scale. Subsequently, significant efforts have been made to engineer reasonably sized models and an efficient attention computation to speed up model convergence leading to lower latency in models. Incorporating a Mixture of Expert (MoE) [98] methodology is an effective way for large models to achieve computational efficiency, as only a subset of the neural network is activated for every input. Consequently, this leads to sparsity, and although sparsity training is an active research area, current GPUs are better suited for dense matrix computations. While MoE models have demonstrated promise in training sparse matrices, their communication costs and complexity impede wide-scale deployment. Further, larger models are prone to memorize training data leading to overfitting and reduced learning [99]. To overcome this, models are only trained for a single epoch on de-duplicated instances on huge datasets, thereby exhibiting minimal overfitting. Thus, MoE design coupled with a robust training paradigm in the future might lead to highly scalable and efficient models. These models will possess superior language understanding, as data memorization would be minimized. The current approach in SOTA models relies on supervised learning on huge datasets. A promising area of future enhancements in NLP would be incorporating reinforcement learning in Machine Translation, text summarization, and Q&A tasks.",
            "score": 0.4254020166733422,
            "section_title": "XII. CONCLUSION AND FUTURE DIRECTIONS",
            "char_start_offset": 83055,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.174072265625
        },
        {
            "corpus_id": "274023704",
            "title": "MM-Eval: A Hierarchical Benchmark for Modern Mongolian Evaluation in LLMs",
            "text": "Large language models (LLMs) excel in high-resource languages but face notable challenges in low-resource languages like Mongolian. This paper addresses these challenges by categorizing capabilities into language abilities (syntax and semantics) and cognitive abilities (knowledge and reasoning). To systematically evaluate these areas, we developed MM-Eval, a specialized dataset based on Modern Mongolian Language Textbook I and enriched with WebQSP and MGSM datasets. Preliminary experiments on models including Qwen2-7B-Instruct, GLM4-9b-chat, Llama3.1-8B-Instruct, GPT-4, and DeepseekV2.5 revealed that: 1) all models performed better on syntactic tasks than semantic tasks, highlighting a gap in deeper language understanding; and 2) knowledge tasks showed a moderate decline, suggesting that models can transfer general knowledge from high-resource to low-resource contexts. The release of MM-Eval, comprising 569 syntax, 677 semantics, 344 knowledge, and 250 reasoning tasks, offers valuable insights for advancing NLP and LLMs in low-resource languages like Mongolian. The dataset is available at https://github.com/joenahm/MM-Eval.",
            "score": 0.4253119683799508,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03533935546875
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "Similarly to standard handwriting recognition system, our proposed system is composed of two components: 1) an optical model which is trained to recognize a sequence of characters from a sequence of observations; 2) a language model which is trained to model language constraints in sequences of words (Figure 1).\n\nDesigning a unified optical model can be done easily by combining the character sets.Combining the character sets may have many advantages when the languages are of the same origin, i.e.where some characters are shared between the languages.On the one hand, the system complexity is reduced as the shared characters are modeled only once.In addition, the number of occurrences of the shared characters is larger in the training set and the optical model can take benefit from this.Moreover, we also expect the optical model to generalize better as the shared characters come from various languages, so they appear in different contexts in the text.\n\nIn contrast to optical models, designing a unified language model is less straightforward.This is especially due to the model complexity that increases.Indeed, if one expects the multilingual language model to perform equally well for every language, then the size of the working lexicon is the sum of each lexicon size.This explains why multilingual unified systems are generally based on a unified optical model only and they use several specific language models, as it is the case in Kozielski et al. (2014a), Moysset et al. (2014) and Bluche and Messina (2017).\n\nIn this paper, we propose a unified language model based on sub-lexical units, called multigrams.Here, we consider multigrams as sequences of characters of variable length.Some examples of word decomposition into multigrams are illustrated in Table 1 and the proposed system architecture is illustrated in Figure 1.Working with a language model based on multigrams has many advantages compared to language model based on words.First, the lexicon size is strongly reduced, as we only consider subparts of words i.e. short sequences of characters.This allows to build a unified language model based on a lexicon of reasonable size.Second, sub-lexical units can generate an open-vocabulary (i.e.",
            "score": 0.42484034073863736,
            "section_title": "Unified recognition system proposal",
            "char_start_offset": 11039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 315,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 556
                },
                {
                    "start": 556,
                    "end": 653
                },
                {
                    "start": 653,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 963
                },
                {
                    "start": 965,
                    "end": 1055
                },
                {
                    "start": 1055,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1530
                },
                {
                    "start": 1532,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 1959
                },
                {
                    "start": 1959,
                    "end": 2077
                },
                {
                    "start": 2077,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1452,
                    "end": 1475,
                    "matchedPaperCorpusId": "14288490"
                },
                {
                    "start": 1475,
                    "end": 1499,
                    "matchedPaperCorpusId": "365150"
                },
                {
                    "start": 1504,
                    "end": 1529,
                    "matchedPaperCorpusId": "4764898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06646728515625
        },
        {
            "corpus_id": "263909146",
            "title": "Language Models are Universal Embedders",
            "text": "In the large language model (LLM) revolution, embedding is a key component of various systems. For example, it is used to retrieve knowledge or memories for LLMs, to build content moderation filters, etc. As such cases span from English to other natural or programming languages, from retrieval to classification and beyond, it is desirable to build a unified embedding model rather than dedicated ones for each scenario. In this work, we make an initial step towards this goal, demonstrating that multiple languages (both natural and programming) pre-trained transformer decoders can embed universally when finetuned on limited English data. We provide a comprehensive practice with thorough evaluations. On English MTEB, our models achieve competitive performance on different embedding tasks by minimal training data. On other benchmarks, such as multilingual classification and code search, our models (without any supervision) perform comparably to, or even surpass heavily supervised baselines and/or APIs. These results provide evidence of a promising path towards building powerful unified embedders that can be applied across tasks and languages.",
            "score": 0.424161092798097,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2098388671875
        },
        {
            "corpus_id": "9087527",
            "title": "Multilingual Part-of-Speech Tagging: Two Unsupervised Approaches",
            "text": "Next we consider the performance of the bilingual merged node model when the lexicon is reduced for only one of the two languages. This condition may occur when dealing with two languages with asymmetric resources, in terms of unannotated text. As shown in Table 6, the merged models on average scores 5.7 points higher than the monolingual model when both tag dictionaries are reduced, but 14.3 points higher when the partner language has a full tag dictionary. This suggests that the bilingual models effectively transfer the additional lexical information available for the resource-rich language to the resource-poor language, yielding substantial performance improvements. \n\nPerhaps the most surprising result is that the resource-rich language gains as much on average from pairing with the resource-poor partner language as it would have gained from pairing with a language with a full lexicon. In both cases, an average accuracy of 93.2% is achieved, compared to the 91.1% monolingual baseline.",
            "score": 0.4238206795509668,
            "section_title": "Avg",
            "char_start_offset": 65089,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 677
                },
                {
                    "start": 680,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1002
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0210418701171875
        },
        {
            "corpus_id": "218470403",
            "title": "Cross-Linguistic Syntactic Evaluation of Word Prediction Models",
            "text": "As such, we have a limited understanding of the effect of the cross-linguistic variability in neural networks' syntactic prediction abilities. In this paper, we introduce the Cross-Linguistic Assessment of Models on Syntax (CLAMS) data set, which extends the subject-verb agreement component of the Marvin and Linzen (2018) challenge set to French, German, Hebrew and Russian. By focusing on a single lin-guistic phenomenon in related languages, 1 we can directly compare the models' performance across languages. We see the present effort as providing a core data set that can be expanded in future work to improve coverage to other languages and syntactic constructions. To this end, we release the code for a simple grammar engineering framework that facilitates the creation and generation of syntactic evaluation sets. 2  We use CLAMS to test two hypotheses. First, we hypothesize that a multilingual model would show transfer across languages with similar syntactic constructions, which would lead to improved syntactic performance compared to monolingual models. In experiments on LSTM language models (LMs), we do not find support for this hypothesis; contrarily, accuracy was lower for the multilingual model than the monolingual ones. Second, we hypothesize that language models would be better able to learn hierarchical syntactic generalizations in morphologically complex languages (which provide frequent overt cues to syntactic structure) than in morphologically simpler languages (Gulordava et al., 2018;Lorimor et al., 2008;McCoy et al., 2018). We test this using LSTM LMs we train, and find moderate support for this hypothesis. \n\nIn addition to our analysis of LSTM LMs, we demonstrate the utility of CLAMS for testing pretrained word prediction models. We evaluate multilingual BERT (Devlin et al., 2019), a bidirectional Transformer model trained on a multilingual corpus, and find that this model performs well on English, has mixed syntactic abilities in French and German, and performs poorly on Hebrew and Russian. Its syntactic performance in English was somewhat worse than that of monolingual English BERT, again suggesting that interference between languages offsets any potential syntactic transfer. \n\n2 Background and Previous Work",
            "score": 0.42376750657925377,
            "section_title": "Introduction",
            "char_start_offset": 2111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1646
                },
                {
                    "start": 1649,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2229
                },
                {
                    "start": 2232,
                    "end": 2262
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 323,
                    "matchedPaperCorpusId": "52113185"
                },
                {
                    "start": 1496,
                    "end": 1520,
                    "matchedPaperCorpusId": "4460159"
                },
                {
                    "start": 1541,
                    "end": 1560,
                    "matchedPaperCorpusId": "3580012"
                },
                {
                    "start": 1803,
                    "end": 1824,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0313720703125
        },
        {
            "corpus_id": "274822367",
            "title": "Read Like a Radiologist: Efficient Vision-Language Model for 3D Medical Imaging Interpretation",
            "text": "Notable examples include PPKED, MedViLL, and Medical X-VL (Liu et al., 2021;Moon et al., 2022;Park et al., 2024). \n\nMore recently, the impressive capabilities of large language models (LLMs) have driven efforts to develop multi-modal models with a generalist architecture. These models aim to handle multiple modalities using a single framework, enhancing flexibility and simplifying the integration process compared to specialized encoder modules. Med-PaLM M (Tu et al., 2024) is a pioneering effort in this area, demonstrating the ability to process text, imaging, and genomic data within a unified framework. This design maximizes flexibility and information transfer across modalities, providing a strong foundation for future multi-modal AI systems. Med-PaLM M has also shown potential to outperform specialized models in specific tasks, suggesting that generalist models may offer superior performance even in domain-specific applications. Subsequent models, such as BioMedGPT and PathChat (Zhang et al., 2024a;Lu et al., 2024), have further built on these capabilities, underscoring the potential of foundational models to provide generalized solutions in the medical domain.",
            "score": 0.42366169357625394,
            "section_title": "Multi-modal Foundation Model in Medical Domain",
            "char_start_offset": 7320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1182
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 76,
                    "matchedPaperCorpusId": "235421693"
                },
                {
                    "start": 76,
                    "end": 94,
                    "matchedPaperCorpusId": "235166527"
                },
                {
                    "start": 94,
                    "end": 112,
                    "matchedPaperCorpusId": "265093622"
                },
                {
                    "start": 996,
                    "end": 1017,
                    "matchedPaperCorpusId": "258947331"
                },
                {
                    "start": 1017,
                    "end": 1033,
                    "matchedPaperCorpusId": "270437603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228271484375
        },
        {
            "corpus_id": "254405065",
            "title": "Comparative Analysis of Cross-lingual Contextualized Word Embeddings",
            "text": "In this paper we have compared currently popular alignment techniques using multilingual and monolingual models of various architectures from different aspects by utilizing two word level tasks (BLI and word retrieval) and one sentence level task XNLI with one low resource (Bengali-English) and one high resource language pair (German-English). We also have proposed a time, data and parameter efficient alignment technique. Our experimental results demonstrate that multilinguality always lead to better performance in cross-lingual transfer tasks. When the resources (computational and data) are available, bigger models are always preferred over smaller models, but when the resources are not accessible, smaller but specialized multilingual models should be chosen, since they are capable of performing similarly to or better than the large multilingual models on the languages the model is specialized for. A large set of supported languages and a large vocabulary does not always assist in all types of tasks in contrast to models specifically trained for a limited number of target languages. Large language models are sensitive regarding batch size and learning rate. Finally, high resource languages and large multilingual models perform well with our proposed approach. In future work we aim to develop alignment techniques capable of performing well even on low resource unseen languages.",
            "score": 0.42342266992167144,
            "section_title": "Conclusion",
            "char_start_offset": 29096,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1400
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1807861328125
        },
        {
            "corpus_id": "53775637",
            "title": "Translating and Evolving: Towards a Model of Language Change in DisCoCat",
            "text": "Language allows us to communicate, and to compose words in a huge variety of ways to obtain different meanings. It is also constantly changing. The compositional distributional model of [7] describes how to use compositional methods within a vector space model of meaning. However, this model, and others that are similar [4,20], do not have a built in notion of language change, or of translation between languages.\n\nIn contrast, many statistical machine translation systems currently use neural models, where a large network is trained to be able to translate words and phrases [21,8]. This approach does not make use of the grammatical structure which allows you to build translations of phrases from the translations of individual words. In this paper we define a notion of translation between two compositional distributional models of meaning which constitutes a first step towards unifying these two approaches.\n\nModeling translation between two languages also has intrinsic value, and doing so within the DisCo-Cat framework means that we can use its compositional power. In section 3.1, we provide a categorical description of translation between two languages that encompasses both updating or amending a language model and translating between two distinct natural languages.\n\nIn order to provide this categorical description, we must first introduce some preliminary concepts. In section 3.2 we propose a unification of the product space representation of a language model of [7] and the functorial representation of [16]. This allows us to formalize the notion of lexicon in section 3.3 which had previously been only loosely defined in the DisCoCat framework. We then show how to build a dictionary between two lexicons and give an example showing how translations can be used to model an update or evolution of a compositional distributional model of meaning. In section 3.4 we give a concrete suggestion for automated translation between corpora in English to corpora in Spanish.",
            "score": 0.4228215474358126,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 322,
                    "end": 325,
                    "matchedPaperCorpusId": "8360910"
                },
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "2887284"
                },
                {
                    "start": 584,
                    "end": 586,
                    "matchedPaperCorpusId": "10473972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03271484375
        },
        {
            "corpus_id": "226283530",
            "title": "Combining BERT with Static Word Embeddings for Categorizing Social Media",
            "text": "In this paper, we have presented a simple approach to combine static word embeddings with BERTbased language models. Intuitively, the reason why this hybrid approach can outperform the BERTbased models themselves is because the latter were not trained on Wikipedia. The alternative solution would be to train language models on a relevant social media corpus, as in the BERTweet model (Nguyen et al., 2020). While such a strategy is likely to lead to a better overall performance, in principle, this is not always feasible in practice. For instance, using static word vectors could play an important role in dealing with emerging terms, such as trending hashtags, as continuously updating language models (for many different languages) would be too expensive. Similarly, incorporating static word vectors seems to be a promising strategy for improving language models for low-resource languages, as specialized language models (e.g. trained on social media) are unlikely to become available for such languages.",
            "score": 0.42249248867012446,
            "section_title": "Conclusions",
            "char_start_offset": 12780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1010
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0640869140625
        },
        {
            "corpus_id": "273185866",
            "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling",
            "text": "Generalist language models (LMs) can address a wide variety of tasks, but this generality comes at a cost (Brown et al., 2020). It necessitates a large training set representative of all prospective tasks, as well as a large model to fit such a comprehensive dataset. Specialist models forgo this generality and fit a model for a limited domain or task. In their narrow specialty, such models can achieve better accuracy at a given model size (Kerner, 2024). \n\nPretraining a specialist is interesting when two conditions are met: (i) the targeted task justifies the cost of training a dedicated model and (ii) a specialist dataset large enough for pretraining is available. Condition (i) is dependent on the targeted application and its potential economic benefit. Condition (ii) is more limiting since modern LMs are commonly pre-trained on datasets larger than 100B tokens 1 , an amount that cannot be commissioned for most applications. This work considers relaxing condition (ii) and studies methods to train a specialist model when specialized data is scarce. Given a large generalist dataset and a small specialist dataset, we propose to modify the distribution over the generalist dataset guided by the scarce specialist dataset. Training a model on the modified distribution gives a specialist model with better accuracy than a generalist model of the same size. \n\nWe study this setting across different specialization tasks including domain-specific language modeling (medical, encyclopedic domains) and end-tasks (scholar exams in science and humanities, reasoning questions). We compare different strategies to manipulate the pretraining distribution. We evaluate strategies based on text classifiers, gradient-alignment and importance sampling (IS). Although IS is rarely used for LM data selection, we build upon on a simple IS recipe based on clustering (Grangier et al., 2024b) and report that the resulting method systematically outperforms alternatives. Our IS recipe clusters the generalist set and computes the cluster histogram over the specialist data. Then, for pretraining, generic data is sampled according to the specialist histogram, see Figure 1. We show the empirical benefit of this method varying model sizes (350m to 7B parameters), the amount of generalist data and the amount of specific data.",
            "score": 0.4224700463657677,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2173
                },
                {
                    "start": 2174,
                    "end": 2326
                }
            ],
            "ref_mentions": [
                {
                    "start": 106,
                    "end": 126,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "259360724",
            "title": "Text Alignment Is An Efficient Unified Model for Massive NLP Tasks",
            "text": "Recent large language models (LLMs) have demonstrated exceptional generalizability in a wide range of natural language processing (NLP) tasks. As the underlying formulation of these LLMs, next-word prediction is proven to be a general function applicable to diverse language problems. However, it is often not being an efficient solution for many tasks. LLMs often need to scale up to over tens of billions of parameters to achieve meaningful performance [1], with popular models like GPT-3 boasting as many as 175B parameters [2]. Additionally, even with their extreme scale, LLMs sometimes still find themselves outperformed by smaller models. For example, ChatGPT/GPT-3.5 falls behind existing finetuned baselines on most classical natural language understanding tasks [3]. \n\nAs a result, in many cases it is desirable to navigate the spectrum of generality-vs-efficiency tradeoff, for example, by developing smaller but general-purpose models that excel in a substantial subset of tasks. Despite being less versatile than the extreme-scale LLMs, these models are more efficient and Figure 1: Our alignment model (125M and 355M) achieves substantially better efficiency and performance compared to much larger models on a wide range of tasks, including (left) diverse text pair understanding tasks on over 20 datasets and (middle) factual consistency evaluation, and (right) improves existing LLMs on question answering by detecting unanswerable questions. See Section 4 for more details. provide superior performance, making them more usable on the set of tasks that they are designed to handle. Previous work has attempted to build natural language inference (NLI) models as an efficient solution for broad tasks [4][5][6]. But with limited NLI data (e.g., MNLI [7]) for training, the models exhibit limited performance and applicability across diverse domains. Another related line of research trains general text representation models with pretraining and multi-task learning [8][9][10]. Those models need to be specifically finetuned (with task-specific head) for each downstream task, instead of functioning as ready-to-use solutions.",
            "score": 0.42134264698702567,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 776
                },
                {
                    "start": 779,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 458,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 527,
                    "end": 530,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1718,
                    "end": 1721,
                    "matchedPaperCorpusId": "222142136"
                },
                {
                    "start": 1724,
                    "end": 1727,
                    "matchedPaperCorpusId": "235097442"
                },
                {
                    "start": 1767,
                    "end": 1770,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 1983,
                    "end": 1986,
                    "matchedPaperCorpusId": "59523594"
                },
                {
                    "start": 1986,
                    "end": 1989,
                    "matchedPaperCorpusId": "231718729"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1729736328125
        },
        {
            "corpus_id": "271212682",
            "title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMs",
            "text": "Large language models have shown extraordinary zero-shot capabilities in various tasks and domains, such as text classification, summarization, question answering, and chatbot [22,1,25,32,21,29]. Recent works [13,5,10] suggest exhaustively scaling the model size and training data size to improve the performance of language models and provoke their emergent abilities; for example, GPT-4, with over 1.74 trillion parameters, achieves superior performance in several tasks but also incur high economic costs. While this trend of scaling language models will continue in the future, there is also a growing diversification in recent models in terms of task or (sub-)domain specialization and computational costs. Consequently, for model users, identifying which LLM is best suited for their applications has become a daunting task. When factoring in the cost constraints, either computational resources or API service pricing, this task becomes even more complex. \n\nIn this paper, we imagine a world with several LLM providers, such as OpenAI 3 or Amazon's Bedrock 4 ; each provides service access to a diverse suite of LLMs with heterogeneous capabilities and cost structures. Here, an LLM user asks an important question: How do I select an LLM i (out of k LLMs) for optimal performance and usage cost in my application? One option is combining multiple LLMs as seen in existing ensemble methods [12,30], but this approach will yield significantly higher service costs; another option is cascading over a set of LLMs [4], but such an approach still requires querying the LLMs until we could find the best one. On the other hand, defaulting to a single LLM to avoid the extra cost of querying multiple models -by predicting the performance of LLMs [27,17] -may not also be optimal; for some queries, a less expensive model may also provide correct answers. Furthermore, as different LLMs exhibit very distinctive abilities on different tasks and data distribution [12,30], for a query, it is possible for LLM i to perform better than LLM j even though the cost of LLM i is noticeably lower than the cost of j.",
            "score": 0.42128763662215196,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1856
                },
                {
                    "start": 1857,
                    "end": 2109
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 180,
                    "end": 182,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 182,
                    "end": 185,
                    "matchedPaperCorpusId": "210838924"
                },
                {
                    "start": 185,
                    "end": 188,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 191,
                    "end": 194,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 213,
                    "end": 215,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1397,
                    "end": 1401,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1401,
                    "end": 1404,
                    "matchedPaperCorpusId": "263608332"
                },
                {
                    "start": 1748,
                    "end": 1752,
                    "matchedPaperCorpusId": "263151991"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.270263671875
        },
        {
            "corpus_id": "52056191",
            "title": "Translational Grounding: Using Paraphrase Recognition and Generation to Demonstrate Semantic Abstraction Abilities of MultiLingual NMT",
            "text": "An appealing property of encoder-decoder models for machine translation is the effect of compressing information into dense vector-based representations to map source language input onto adequate translations in the target language. However, it is not clear to what extent the model actually needs to find semantic abstractions to perform that task; especially for related languages, it is often not necessary to acquire a deep understanding of the input to translate in an adequate way. The intuition that we would like to explore in this paper is based on the assumption that an increasingly difficult training objective will enforce stronger abstractions. In particular, we would like to see whether multilingual machine translation models learn representations that are closer to language-independent meaning representations than bilingual models do. Hence, our hypothesis is that representations learned from multilingual data sets covering a larger linguistic diversity better reflect semantics than representations learned from less diverse material. Figure 1: An illustration of the assumption that multilingual models enforce stronger abstractions due to the increase of linguistic diversity that needs to be handled by one single model whereas simpler bilingual models can use \"cross-lingual shortcuts\" that do not require the same level of understanding. \n\nFigure 1 illustrates this intuition in relation to the conceptual setup of machine translation and the classical Vauquois triangle (Vauquois, 1976). Encoder-decoder models reflect the idea of combining language understanding and generation, where the encoder maps the incoming signal to some internal representation, from which the decoder generates text in the target language. On the basis of this illustration, we expect multilingual models to build a stronger and more language-independent representation layer, which ought to be closer to the abstract meaning behind the input. In order to test this, we add linguistically diverse material to the training data and measure the abstractions that the model achieves when confronted with paraphrased sentences in one language. \n\nOur basic framework consists of a standard attentional sequence-to-sequence model as commonly used for neural machine translation (Sennrich et al., 2017), with the multilingual extension proposed by Johnson et al. (2016). This extension allows a single system to learn machine translation for several language pairs, and crucially also for language pairs that have not been seen during training (this configuration is commonly referred to as zero-shot translation).",
            "score": 0.42126998723446973,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2146
                },
                {
                    "start": 2149,
                    "end": 2370
                },
                {
                    "start": 2371,
                    "end": 2614
                }
            ],
            "ref_mentions": [
                {
                    "start": 1499,
                    "end": 1515,
                    "matchedPaperCorpusId": "185221379"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.063232421875
        },
        {
            "corpus_id": "260334118",
            "title": "When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities",
            "text": "Tool learning is an emerging research field that aims to enhance task-solving capabilities by combining specialized tools with foundational models, which has been understood by [216] as two perspectives: \n\n1) Tool-augmented learning treats specialized tools as assistants in order for improving the quality and accuracy of tasks, or Tool for AI; 2) Tool-oriented learning focuses more on training models to effectively use tools, controlling and optimizing tool-applying processes, or AI for Tool. \n\nTool learning has found applications in various fields, and this section primarily focuses on tool learning paradigms based on large language models (LLMs). While recent works often involve a combination of these two perspectives, we do not specifically categorize each work into one type. LLMs, such as GPT, are well-suited for tool learning applications [217]. With their powerful natural language processing capabilities, LLMs can break down complex tasks into smaller sub-tasks and convert them into executable instructions. Specialized tools allow LLMs to access knowledge that is beyond their own understanding. By integrating specialized tools, LLMs can better understand and address complex problems, offering more accurate and efficient solutions. \n\nLLMs are commonly applied as controllers to select and manage various existing AI models to solve complex tasks, User-specified Tasks \n\nVisual ChatGPT [228] Taskmatrix.AI [229] Visual Foundation models Customized models with unified API form text-davinci-003 Visual Customized Tasks ReAct [214] Wikipedia API PaLM-540B Question Answering Face Verificaiton \n\nToolformer [230] Calculator Q&A system Search Engine Translation System Calendar GPT-J Downstream Tasks which rely on user input and language interfaces on making summarizations. They act as the central component, responsible for comprehending problem statements and making decisions regarding which actions to execute. Additionally, they aggregate the outcomes based on the results of the executed actions. In that case, HuggingGPT [227] leverages existing models from the Hugging Face community 1 to assist in task-solving.",
            "score": 0.4212389943775481,
            "section_title": "LLM-based Tool Learning",
            "char_start_offset": 65857,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 206,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1256
                },
                {
                    "start": 1259,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1614
                },
                {
                    "start": 1617,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2142
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21337890625
        },
        {
            "corpus_id": "250526239",
            "title": "Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model",
            "text": "Especially interesting would be the use of unlabeled texts as an in-domain language model, rather than the Transformers-based language models such as BERT and T5 that were trained on an immense variety of different domains. Sun et al. (2020) found that employing such an in-domain language model made the use of BERT unnecessary, and that the combination of indomain pretraining and pseudo-labeling resulted in the best model for a text classification task. The impact of training big Transformers models such as BERT and T5 on CO 2 emissions is well documented (Strubell, Ganesh, & McCallum, 2019). If the use of unlabeled texts for (generally smaller-scale) in-domain pretraining and pseudo-labeling is indeed also effective for NLG, this might help to focus development on more efficient and eco-friendly development of single-domain language models, rather than unsustainably striving for larger and larger all-encompassing Transformers models. \n\nThe findings of the current study highlight the feasibility of automatically extending datasets in a way that neural data-to-text systems benefit from the extension, even if the original dataset is small in size. This can be beneficial for robot journalism, for instance, that still mostly relies on rule-based NLG. Newspaper organisations oftentimes have large archives of texts and data, but aligning the two manually on a scale that permits a neural model to be trained on it, requires an amount of manpower that most newsrooms do not have available. By creating a small starting point and then using semi-supervised learning to create the full dataset, would be a less intensive undertaking-thus a smaller hurdle-for many companies to employ neural data-to-text systems. Police say an adult female was found dead at the scene from apparent gunshot wounds.",
            "score": 0.4212025329379455,
            "section_title": "Future Work",
            "char_start_offset": 87872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1810
                }
            ],
            "ref_mentions": [
                {
                    "start": 562,
                    "end": 598,
                    "matchedPaperCorpusId": "174802812"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.020416259765625
        },
        {
            "corpus_id": "3984122",
            "title": "Polyglot Semantic Parsing in APIs",
            "text": "In addition, each English description (Figure 1. 1-1.3) describes max differently using the synonyms greater, maximum, largest. In this case, it would seem that training models on multiple datasets, as opposed to single language pairs, might make learning more robust, and help to capture various linguistic alternatives. \n\nWith the software QA application in mind, an additional limitation is that their approach does not allow one to freely translate a given description to multiple output languages, which would be useful for comparing how different programming languages represent the same functionality. The model also cannot translate between natural languages and programming languages that are not observed during training. While software documentation is easy to find in bulk, if a particular API is not already documented in a language other than English (e.g., Haskell in de), it is unlikely that such a translation will appear without considerable effort by experienced translators. Similarly, many individual APIs may be too small or poorly documented to build individual models or QA applications, and will in some way need to bootstrap off of more general models or resources. \n\nTo deal with these issues, we aim to learn more general text-to-code translation models that are trained on multiple datasets simultaneously. Our ultimate goal is to build polyglot translation models (cf. Johnson et al. (2016)), or models with shared representations that can translate any input text to any output programming language, regardless of whether such language pairs were encountered explicitly during training. Inherent in this task is the challenge of building an efficient polyglot decoder, or a translation mechanism that allows such crossing between input and output languages. A key challenge is ensuring that such a decoder generates well-formed code representations, which is not guaranteed when one simply applies standard decoding strategies from SMT and neural MT (cf. Cheng et al. (2017)). Given our ultimate interest in API QA, such a decoder must also facilitate monolingual translation, or being able to translate to specific output languages as needed. \n\nTo solve the decoding problem, we introduce a new graph-based decoding and representation framework that reduces to solving shortest path problems in directed graphs.",
            "score": 0.42118404193133047,
            "section_title": "Introduction",
            "char_start_offset": 2122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2174
                },
                {
                    "start": 2177,
                    "end": 2343
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04443359375
        },
        {
            "corpus_id": "216144648",
            "title": "Multiple Segmentations of Thai Sentences for Neural Machine Translation",
            "text": "In this work we have explored whether the duplication of training sentences with different splits is useful to build NMT models with improved performance. The experiments show that the combination of different splits on the target side does improve NMT models involving a lowresource language such as Thai. The experiments also reveal that combining the same dataset using different merge operations of BPE not only improves the model trained on the dataset using the single configuration (regardless of the number of merge operations), but also the model trained on data that has been split using a tool trained on supervised data such as Deepcut. \n\nIn the future, we plan to conduct more fine-grained experiments to explore which configurations of BPE perform better. For example, would the combination of BPE 10000 and BPE 20000 (those with the highest number of operations explored) perform better than the model's original setup? And what would the results be if only BPE 1000 and BPE 20000 (those with the lowest and highest number of operation) are combined? Furthermore, as all the experiments with different splits have been applied on the target side, we plan to investigate NMT models when Thai is on the source side. Similarly, we will also investigate whether these improvements will be achieved using other languages. Another variation we are interested in exploring is not to replicate all the sentences but to use data-selection algorithms to find a subset of sentences that may boost the performance of the models trained on the subset [15; 17]. Finally, we would like to investigate the applicability of the method of employing several segmentations to other NLP tasks such as text classification [2; 27].",
            "score": 0.4211337454576217,
            "section_title": "Conclusions and Future Work",
            "char_start_offset": 12164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 648
                },
                {
                    "start": 651,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1723
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0289306640625
        },
        {
            "corpus_id": "229717763",
            "title": "Cross-Domain Generalization and Knowledge Transfer in Transformers Trained on Legal Data",
            "text": "It can be difficult to evaluate how well a model would perform if applied to other domains, and if it is able to generalize beyond the domains captured in the dataset. For example, it is possible that a model has only learned a specific vocabulary characteristic of a single domain or jurisdiction to describe factual circumstances, rather than the general idea of what a sentence describing the circumstances looks like. \n\nEvaluating the models on multiple datasets, however, is not straightforward. Firstly, publicly available datasets are scarce in AI & Law. Further, the datasets that are available typically use custom type systems, making cross-dataset evaluation challenging. In this paper, we identify an example meta type system, that generalizes across three datasets. We use the meta type system to train models on data from different domains and jurisdictions and assess performance of the models across the domains. \n\nWe compare the ability of different types of ML models to generalize from one domain to another. First, as a baseline we use a Support Vector Machines classifier which learns the correlation between word and/or n-gram occurrences and a certain label. Then, we evaluate the more recently developed BERT model (bidirectional encoder representation from transformers). This model has been pre-trained on massive corpora of text, to learn a language model. It can then be further fine-tuned on a down-stream task on a specialized dataset, achieving strong results with little training data by leveraging the previously learned language representation.",
            "score": 0.4209940666484615,
            "section_title": "Background",
            "char_start_offset": 3475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 421
                },
                {
                    "start": 424,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 928
                },
                {
                    "start": 931,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1578
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11376953125
        },
        {
            "corpus_id": "265607874",
            "title": "On Enhancing Fine-Tuning for Pre-trained Language Models",
            "text": "Natural Language Processing (NLP) is an essential branch of artificial intelligence that delves into the intricate realm of human language. Its primary objective is to empower computers with the ability to comprehend, interpret, and manipulate text and words in a manner that mirrors human understanding (Liddy, 2001). The definition of NLP covers a variety of aspects: There are several computational methods for NLP, and they essentially fall into four categories; symbolic, statistical, connectionist, and hybrid. Symbolic methods use a deep analysis of linguistic phenomena,and they are based on the explicit representation of linguistic facts using well-known knowledge representation schemes. Statistical approaches build models of linguistic phenomena using a variety of mathematical techniques and a large text corpus. The major source of evidence for these methods is observable data, with no linguistic or general knowledge added. The connectionist approach construct generalised models using examples of linguistic phenomena, and they employ also variety of representational theories. The text being analysed must come from a language that people use to communicate, and it may be in any language,and in any format oral or written. \n\nIn NLP, humans utilize various levels of language to comprehend the content of a document. These levels include Phonology (the study of speech sounds), Morphology (the study of word forms and structure), Lexical (the study of words and their meanings), Syntactic (the study of sentence structure), Semantic (the study of meaning in language), Discourse (the study of how sentences are connected and organized), and Pragmatic (the study of language use in context). The more capable an NLP system is, the more of these levels it will employ to understand and process language effectively. For instance, a sophisticated NLP system will take into account not only the words in a sentence but also their meanings, how they are arranged grammatically, and how the sentences relate to each other in a larger context. However, in practice, current NLP systems often utilize separate modules to handle different levels of language processing. These modules work together to process the language and extract meaningful information.",
            "score": 0.42068903212043185,
            "section_title": "Natural Language Processing",
            "char_start_offset": 3977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1242
                },
                {
                    "start": 1245,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 2055
                },
                {
                    "start": 2056,
                    "end": 2179
                },
                {
                    "start": 2180,
                    "end": 2267
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0635986328125
        },
        {
            "corpus_id": "273877967",
            "title": "DELIFT: Data Efficient Language model Instruction Fine Tuning",
            "text": "Fine-tuning large language models (LLMs) is pivotal for adapting these powerful architectures (Devlin et al., 2019;Brown et al., 2020a;Touvron et al., 2023) to specialized tasks such as intricate reasoning, precise question-answering, and the seamless integration of new information (Ouyang et al., 2022). This transformation-from a general-purpose model to a task-specific agent-heavily relies on the quality and nature of the data employed during fine-tuning, which critically determines the model's subsequent performance (Wei et al., 2022;Zhou et al., 2023;Hoffmann et al., 2024). \n\nThe effectiveness of fine-tuning hinges on the quality, diversity, and relevance of the selected data (Gururangan et al., 2020;Wei et al., 2022;Zhou et al., 2023). High-quality data ensures accurate learning, diverse data enhances generalization, and relevant data aligns the model's capabilities with specific application needs. However, optimizing data selection across different fine-tuning phases remains a significant challenge, leading to our central research question: \n\nHow can we create a unified framework for efficient data selection across all fine-tuning stages of LLMs, while optimizing performance and maximizing data efficiency? \n\nTo address this challenge, we present DELIFT (Data Efficient Language model Instruction Fine-Tuning), a novel, unified, and computationally efficient algorithm engineered to optimize data selection across all stages of the fine-tuning process. The key innovation of DELIFT lies in its pairwise utility metric, which assesses the informational value of data samples relative to both the model's current capabilities and other samples within the dataset. This metric, combined with submodular optimization techniques, allows DELIFT to efficiently select optimal data subsets that precisely address the model's learning requirements without incurring unnecessary computational costs.",
            "score": 0.42023851735064643,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 115,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 115,
                    "end": 135,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 283,
                    "end": 304,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 561,
                    "end": 583,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 689,
                    "end": 714,
                    "matchedPaperCorpusId": "216080466"
                },
                {
                    "start": 714,
                    "end": 731,
                    "matchedPaperCorpusId": "237416585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1422119140625
        },
        {
            "corpus_id": "251363760",
            "title": "Code Generation Using Machine Learning: A Systematic Review",
            "text": "In this section, we list three suggestions for future work that would contribute to the field of code generation using ML. \n\nImproving language model efficiency is our first suggestion for future work. Models such as transformers [2] are good for general code generation tasks but are extremely datahungry [4], [95]. Training and evaluating these models are therefore computationally expensive. Similarly, improving the energy efficiency of language models would also lower the barrier of entry for research. High energy consumption leads to high monetary costs. References [43], [71], [75], [83] mention computation costs as restrictive to their research. \n\nEnsemble learning is our second suggestion for future work. Some models excel in specific contexts while performing poorly in general. References [69], [71] are examples of this as they found that different models performed better on certain bug types in the context of APR. Reference [75] gives an example of how training and evaluating a model for three specific types of cross-PL translations required half as much data to achieve similar performance compared to a model trained and evaluated on four types of translations. Studying ensembles that combine the strengths of different models to improve performance over a variety of cases is a promising direction for future work. \n\nNew ways of using Abstract Syntax Trees (AST) representations of source code is our final suggestion for future work. Multiple studies discussed in this review, such as [21], [95], use AST representations of code for their models. Further exploitation of this data structure, which is characteristic of PLs, is recommended for future research. Generalized ASTs over multiple programming languages could lead to greater transfer learning capabilities and models that generalize to multiple languages. Code-specific decoding methods for sequential output models remain unexplored to the best of our knowledge. A decoding method that exploits AST or other syntax information could lead to more efficient and syntactically correct synthesized code.",
            "score": 0.4194182112036128,
            "section_title": "G. FUTURE WORK",
            "char_start_offset": 31818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1340
                },
                {
                    "start": 1343,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1686
                },
                {
                    "start": 1687,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2087
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 315,
                    "matchedPaperCorpusId": "235765336"
                },
                {
                    "start": 580,
                    "end": 584,
                    "matchedPaperCorpusId": "221321491"
                },
                {
                    "start": 586,
                    "end": 590,
                    "matchedPaperCorpusId": "227064320"
                },
                {
                    "start": 592,
                    "end": 596,
                    "matchedPaperCorpusId": "232290517"
                },
                {
                    "start": 805,
                    "end": 809,
                    "matchedPaperCorpusId": "214802082"
                },
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "221321491"
                },
                {
                    "start": 944,
                    "end": 948,
                    "matchedPaperCorpusId": "227064320"
                },
                {
                    "start": 1512,
                    "end": 1516,
                    "matchedPaperCorpusId": "49584534"
                },
                {
                    "start": 1518,
                    "end": 1522,
                    "matchedPaperCorpusId": "235765336"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158935546875
        },
        {
            "corpus_id": "254018006",
            "title": "Comparison Study Between Token Classification and Sequence Classification In Text Classification",
            "text": "In comparison to statistical models with handmade characteristics, neural network models have recently been utilized for text classifiers [9,10,4] have produced better results. Word representations in particular are utilized as the input to a neural network model which produces a single dense vector for each word representation in the whole text. Based on a non-linear transformation in each layer on the representation, a label is given. It has been demonstrated that across several domains, neural network models are more reliable than statistical models without the use of handmade features. \n\nText classifiers engines have been using both convolutional neural networks [11] and recurrent neural networks [12] modles. A single-layer LSTM also has been used over the word sequence to model the text, and they separately model sentences and documents using a two-level hierarchical CNN structure. It is well known that LSTMs excel at modeling temporal and ordered data while CNNs excel at capturing local information. The effectiveness of LSTMs and CNNs using the identical text classification settings has been discussed [10]. \n\nPre-trained language models have recently been used to significantly increase the performance of phrase prediction tasks by combining representations from many layers, creating an auxiliary sentence, using multitask learning, etc. \n\nLarge pre-trained language models have recently demonstrated amazing abilities in representation and generalization, like GPT [13], BERT [14], XLNet [15], etc. These models now perform a variety of downstream tasks including text summarization, generation, classification, and regression with enhanced accuracy. There are numerous innovative methods for improving alreadytrained language models. Auxiliary sentence construction was suggested by [16] to improve sentiment classification problems. To address sequential sentence classification tasks, Cohan [17] introduced additional distinct tokens to obtain representations of each sentence. A work in [16] on several fine-tuning techniques, such as combining text representations from various layers and leveraging multi-task learning has been done. \n\nIt makes sense to use BERT type models or generally transformer-based models to learn text representations given the enormous success of pre-trained language models like BERT in learning text representations with deep semantics. The BERT approach, which relies heavily on self-attention, can capture interactions between any two words throughout the entire body of articles (long texts).",
            "score": 0.4191174359173683,
            "section_title": "I. Introduction",
            "char_start_offset": 2450,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 596
                },
                {
                    "start": 599,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1130
                },
                {
                    "start": 1133,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2166
                },
                {
                    "start": 2169,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2556
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 144,
                    "matchedPaperCorpusId": "12345746"
                },
                {
                    "start": 144,
                    "end": 146,
                    "matchedPaperCorpusId": "17515275"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "17048224"
                },
                {
                    "start": 1125,
                    "end": 1129,
                    "matchedPaperCorpusId": "12345746"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "43059601"
                },
                {
                    "start": 1811,
                    "end": 1815,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 2018,
                    "end": 2022,
                    "matchedPaperCorpusId": "153312532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.109130859375
        },
        {
            "corpus_id": "1320875",
            "title": "Weighted Rational Transductions and their Application to Human Language Processing",
            "text": "In other words, the phone, word, and language models are, in effect, compiled together into one large transducer which is then applied to the input observation sequence [12]. On the other hand, one can use a more modular, staged computation,",
            "score": 0.4185848365485682,
            "section_title": "For instance, because composition, o, is associative, we see that the computation of max,o(O o A o D o M)(w) can be organized in several ways. A conventional integrated-search, speech recognizer computes maxw(O o (A o D o M))(w).",
            "char_start_offset": 14969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 241
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "14789841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.014617919921875
        },
        {
            "corpus_id": "41161308",
            "title": "Class-Dependent Modeling for Dialog Translation",
            "text": "In this paper we have presented a technique for combining all models from multiple SMT engines into a single decoding process. This technique allows for topic-dependent decoding with probabilistic soft weighting between the component models. We demonstrated the effectiveness of our approach on conversational data by building class-specific models to handle both questions and declarations. We carried out an extensive evaluation of the technique using a large number of language pairs and MT evaluation metrics. In most cases, we were able to show significant improvements over a system without model interpolation, and for some language pairs this approach excelled. The best improvement of all the language pairs was for Malaysian (Malay)-English, which outperformed the baseline system by 4.7 BLEU points (from 0.463 to 0.510). In future research we would like to try the approach with larger sets of models as well as (possibly overlapping) subsets of the data produced using automatic clustering methods.",
            "score": 0.4184660124803529,
            "section_title": "Conclusion",
            "char_start_offset": 24487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0970458984375
        },
        {
            "corpus_id": "269936543",
            "title": "A Historical Survey of Advances in Transformer Architectures",
            "text": "The authors hypnotized that a large enough model would learn tasks embedded within Using this relatively simple conceptual approach, the BERT model was able to obtain state-of-the-art results on eleven natural language processing (NLP) tasks, thereby establishing it as a notable work which numerous consequent models have been built upon.\n\nIt was soon after that, in the beginning of 2019, that Radford et al. followed up their proposed GPT model with a model they called GPT-2 which followed a similar philosophy of multi-task learning which they based on a framework proposed by Caruana [53].In their work, Radford et al. aimed to unify the two dominant approaches, namely, pre-training followed by supervised fine-tuning as well as a technique with unsupervised approaches towards specific tasks such as commonsense reasoning [55] and sentiment analysis [24].They achieve this by performing language modeling where, in addition to conditioning a model on the input, it is also conditioned on the task.They train their model in an unsupervised manner on a dataset consisting of millions of web pages, called WebText, producing GPT-2, which is an enormous 1.5 billion parameter model which achieved stateof-the-art results on seven language modeling tasks in a zero-shot system.The authors hypnotized that a large enough model would learn tasks embedded within language and would not require explicit, supervised training, which was proven by their results.\n\nMeanwhile, Wang et al. [25], in 2019, proposed a direct improvement upon the transformer model itself by formulating a deep transformer model which they claimed would bypass the prevalent big transformer counterpart.They achieved this using a dual approach where, firstly, they implemented the proper use of layer normalization in addition to introducing a novel way to pass the combinations of previous layers to the next ones.Furthermore, they trained a 30-layer encoder, which they claim was the deepest at the time.Using this approach, the authors were able to outperform the results of both the shallow and the big transformers on the WMT'16 EN-DE, the NIST OpenMT'12 Chinese-English, and the WMT'18 Chinese-English tasks.",
            "score": 0.41838385256518124,
            "section_title": "Introductory Works",
            "char_start_offset": 24960,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 339
                },
                {
                    "start": 341,
                    "end": 595
                },
                {
                    "start": 595,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 1005
                },
                {
                    "start": 1005,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1459
                },
                {
                    "start": 1461,
                    "end": 1677
                },
                {
                    "start": 1677,
                    "end": 1889
                },
                {
                    "start": 1889,
                    "end": 1980
                },
                {
                    "start": 1980,
                    "end": 2188
                }
            ],
            "ref_mentions": [
                {
                    "start": 830,
                    "end": 834,
                    "matchedPaperCorpusId": "7962030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06304931640625
        },
        {
            "corpus_id": "259501685",
            "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration",
            "text": "In recent times, the large language models (LLMs) have showcased remarkable achievements across various natural language benchmarks, encompassing question answering, machine translation, language understanding and more [1,2,3,4,5]. By employing a Transformer-based architecture [6] and training to anticipate forthcoming tokens within a sequence, this language model excels in contextual learning abilities. Not only does this significantly enhance its modeling prowess, but more importantly, it enables seamless user interaction that effectively connects cutting-edge research with real-world applications. \n\nAs speech represents the most innate and instinctive mode of human communication, integrating speech and LLMs will further boost the user experience of human-machine interaction. Based on this intuition, several attempts in combining speech signals and large language models were carried out [7,8,9,10]. Among them, the cascaded approach is the most straightforward solution. In these systems, the speech signal is firstly transformed into word tokens through existing automatic speech recognition (ASR) [11] models, and LLM processes the recognized words for downstream tasks. Later, inspired by the integration of image information to LLMs [12,13,14,15], researchers also explored the deep combination of speech signals [9,10,16,17,18,19]. In [16], the authors proposed to jointly model the speech and text tasks through a unified decoder only network. Similarly, in [19], the authors proposed to optimize the audio token conversion module together with a off-the-shelf LLM. Instead of word pieces, discrete tokens of speech representation from a selfsupervised model are used in [17]. \n\nWhile there have been promising outcomes, several crucial challenges regarding the integration of speech and LLMs still require further exploration. Initially, aligning the two modalities (speech and text) using a pretrained LLM poses challenges due to the typically longer sequence length of speech signals compared to text sequences. Moreover, given the costly nature of training LLMs, finding ways to minimize the overall integration cost while maintaining exceptional performance continues to be a challenging task.",
            "score": 0.41831168044749534,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1697
                },
                {
                    "start": 1700,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 226,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 278,
                    "end": 281,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1114,
                    "end": 1118,
                    "matchedPaperCorpusId": "240419899"
                },
                {
                    "start": 1262,
                    "end": 1265,
                    "matchedPaperCorpusId": "248476411"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.165283203125
        },
        {
            "corpus_id": "269930367",
            "title": "Code-mixed Sentiment and Hate-speech Prediction",
            "text": "Code-mixed discourse combines multiple languages in a single text. It is commonly used in informal discourse in countries with several official languages, but also in many other countries in combination with English or neighboring languages. As recently large language models have dominated most natural language processing tasks, we investigated their performance in code-mixed settings for relevant tasks. We first created four new bilingual pre-trained masked language models for English-Hindi and English-Slovene languages, specifically aimed to support informal language. Then we performed an evaluation of monolingual, bilingual, few-lingual, and massively multilingual models on several languages, using two tasks that frequently contain code-mixed text, in particular, sentiment analysis and offensive language detection in social media texts. The results show that the most successful classifiers are fine-tuned bilingual models and multilingual models, specialized for social media texts, followed by non-specialized massively multilingual and monolingual models, while huge generative models are not competitive. For our affective problems, the models mostly perform slightly better on code-mixed data compared to non-code-mixed data.",
            "score": 0.41809068067450406,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06610107421875
        },
        {
            "corpus_id": "266306794",
            "title": "MediBioDeBERTa: Biomedical Language Model With Continuous Learning and Intermediate Fine-Tuning",
            "text": "The emergence of large language models (LLMs) marked a significant milestone in the evolution of natural language processing (NLP) [1], [2], [3]. LLMs have been utilized to build a universal knowledge of a language by pre-training on large amounts of data and then being fine-tuned for specific downstream tasks. Typical transformer encoder-based language models (LMs) have been pre-trained to acquire general knowledge and contextual characteristics from large \n\nThe associate editor coordinating the review of this manuscript and approving it for publication was Agostino Forestiero . corpora, such as Wikipedia, news articles, and books [2], [4], [5], [6], [7]. This knowledge transfer to downstream tasks is the cornerstone of the successful application of a pre-trained language model (PLM) to many problems. The advantage of transfer learning has been robustly validated in many NLP applications. \n\nHowever, it is premised on the assumption that the language features of the downstream task are similar to those used in pretraining. Language features vary in terms of language type, style, terminology, etc.; the larger the gap between the pretraining information and the downstream problem, the more difficult it is to take advantage of them [8], [9], [10], [11], [12]. This culminated in the development of domain-specific PLMs that relied on domain-specific data for pretraining. This trend also manifests in generative LMs employing transformer decoders. OpenAI's GPT model [13], as a representative example, utilizes in-context learning to acquire general task knowledge through prompts. This allows it to demonstrate proficiency across a wide range of tasks without the need for fine-tuning. However, its performance on specialized domains for which it hasn't been trained is comparatively less effective. Even when fine-tuned, it cannot surpass the performance of domain-specific pretrained models [14]. \n\nRegarding the development of PLMs for specific domains, two primary approaches have been identified.",
            "score": 0.4178273366114005,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 461
                },
                {
                    "start": 464,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 902
                },
                {
                    "start": 905,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1916
                },
                {
                    "start": 1919,
                    "end": 2019
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 640,
                    "end": 643,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 655,
                    "end": 658,
                    "matchedPaperCorpusId": "198229624"
                },
                {
                    "start": 660,
                    "end": 663,
                    "matchedPaperCorpusId": "219531210"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 1254,
                    "end": 1257,
                    "matchedPaperCorpusId": "215416146"
                },
                {
                    "start": 1259,
                    "end": 1263,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 1265,
                    "end": 1269,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 1484,
                    "end": 1488,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2548828125
        },
        {
            "corpus_id": "259204134",
            "title": "Fine-tuning Large Enterprise Language Models via Ontological Reasoning",
            "text": "With the recent soar of AI-based chatbots, currently led by OpenAI's Chat-GPT, the field of Natural Language Processing (NLP) and, in particular, Large Language Models (LLMs), faced a major turning point and transcended its relevance in academia and industry, steering the attention of the general public towards generative AI. While many approaches are being proposed that exploit powerful pre-trained LLMs, such as T5 [18] and GPT [16], to address a plethora of industrial tasks, current solutions show limited effectiveness at specializing the models on enterprise domains, from finance to genomics. In our community, such domain-specific knowledge can be captured by combining factual data from corporate databases with business-level definitions as ontologies in Enterprise Knowledge Graphs (EKGs), and further augmented via ontological reasoning. In this paper, we build upon this domain representation and propose a novel solution to accurately specialize LLMs on core enterprise NLP tasks. Limits of task-specific fine-tuning. LLMs can be pre-trained on extensive datasets and, often, specialized with a fine-tuning process that customizes them so as to perform given NLP tasks [20], such as question-answering, language translation, named-entity recognition, document summarization, sentiment analysis, and more [7]. According to a very common usage pattern, general-purpose LLMs are fine-tuned for a specific NLP task based on extensive cross-or domaingeneric textual corpora that are publicly available [17]. \n\nWhile this approach highlights good generalization capabilities and a surprising human-style interaction, the obtained models have major shortcomings in that they lack enterprise knowledge and trivially fail to solve domain-specific NLP tasks. For instance, in the financial domain, state-of-the-art yet generalist models have shown poor performance for different NLP tasks, for which, on the other hand, further fine-tuning with large additional text corpora has been proved to be helpful in improving the results, such as in the case of FinBert [12]. \n\nLimits of domain-specific fine-tuning.",
            "score": 0.417740643873445,
            "section_title": "Introduction: Context and Overview of the Approach",
            "char_start_offset": 53,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 420,
                    "end": 424,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1186,
                    "end": 1190,
                    "matchedPaperCorpusId": "186206211"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "265038146"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "266573578",
            "title": "Machine Learning Approaches for Diagnostics and Prognostics of Industrial Systems Using Industrial Open Source Data",
            "text": "Moreover, the challenge of collecting a sufficiently large, labeled dataset is a significant barrier in practical applications. The development of unsupervised and semisupervised transfer learning techniques may help to address this issue. \n\n(5) Potential Utilization of Large Language Models (LLMs) and Industrial Large Knowledge Models (ILKMs). Recent advancements in large language model technologies have shown remarkable abilities in natural language processing and related tasks, hinting at the potential for general artificial intelligence applications [130]. Leveraging these cutting-edge technologies could lead to new changes in PHM domain. Yang et al. introduced a novel benchmark dataset focused on Question Answering (QA) in the industrial domain and proposed a new model interaction paradigm, aimed at enhancing the performance of LLMs in domain-specific QA tasks [123]. This approach signifies a substantial stride in customizing LLMs for more specialized, industry-oriented applications. Meanwhile, Li's team systematically reviewed the current progress and key components of ChatGPT-like large-scale foundation (LSF) models, and provided a comprehensive guide on adapting these models to meet the specific needs of PHM, underscoring the challenges and opportunities for future development [67]. Moreover, as shown in Figure 6, Lee's team proposed an Industrial Large Knowledge Model (ILKM) framework that aims to solve complex challenges in intelligent manufacturing by combining LLMs and domainspecific knowledge [63]. Therefore, integrating specialized domain knowledge with LLM technology presents a good opportunity to develop more effective ML models, potentially leading to better solutions for challenges in PHM.",
            "score": 0.4174875559357477,
            "section_title": "Prospects",
            "char_start_offset": 61443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1736
                }
            ],
            "ref_mentions": [
                {
                    "start": 1531,
                    "end": 1535,
                    "matchedPaperCorpusId": "271432380"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.100341796875
        },
        {
            "corpus_id": "269930367",
            "title": "Code-mixed Sentiment and Hate-speech Prediction",
            "text": "Initially, large language models (LLMs) were primarily trained on well-resourced languages like English and Chinese due to the availability of ample resources.However, there was a limitation in understanding text containing multiple languages simultaneously.Many widely spoken languages lacked sufficient resources for model training.Multilingual LLMs such as the multilingual BERT [16] and XLM-RoBERTa [14] were developed to address this issue.These models have been trained on balanced datasets comprising about 100 languages, aiming to provide better support for multilingual text understanding and processing.\n\nSimilarly, several few-lingual LLMs have been developed, each trained on a limited number of languages.Examples include CroSloEngual BERT (supporting Croatian, Slovene, and English), FinEst BERT (trained on Finnish, Estonian, and English) [55], and MuRIL [28] (supporting 17 Indian languages)\n\nFor understanding code mixing, models need to comprehend how different languages are interchangeably occurring within a single sentence, typically in informal discourse and often used in an affective context.While multilingual models are pre-trained on general language data (e.g., Wikipedia dumps), code-mixing presents a specialized challenge.An alternative and direct way of improving the code-mixed performance is by pre-training models directly on code-mixed texts.For example, Nayak and Joshi release multiple transformer-based models trained on a carefully curated Hindi-English corpus [41].Such models are aimed at handling the specifics of code-mixing but are not available for many language pairs.In this work, we extend the support for code-mixed language processing by introducing four new specialized LLMs: two for the previously unsupported Slovene-English language pair, and two for Hindi-English.Additionally, we perform a comprehensive code-mixed evaluation using a selection of LLMs, including few-lingual models previously untested in such scenarios.",
            "score": 0.4173818302510495,
            "section_title": "A. Multilingual LLMs",
            "char_start_offset": 5407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 159,
                    "end": 258
                },
                {
                    "start": 258,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 613
                },
                {
                    "start": 615,
                    "end": 718
                },
                {
                    "start": 718,
                    "end": 907
                },
                {
                    "start": 909,
                    "end": 1117
                },
                {
                    "start": 1117,
                    "end": 1254
                },
                {
                    "start": 1254,
                    "end": 1379
                },
                {
                    "start": 1379,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1978
                }
            ],
            "ref_mentions": [
                {
                    "start": 382,
                    "end": 386,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 854,
                    "end": 858,
                    "matchedPaperCorpusId": "219687232"
                },
                {
                    "start": 1502,
                    "end": 1506,
                    "matchedPaperCorpusId": "248227520"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136962890625
        },
        {
            "corpus_id": "268513271",
            "title": "Pre-Trained Language Models Represent Some Geographic Populations Better than Others",
            "text": "Large language models (LLMs) aim to provide a single set of representations that captures both linguistic knowledge and world knowledge across a diverse range of languages.Previous work has focused on developing probing tasks which measure the degree to which such models capture (i) purely linguistic knowledge (e.g., Weissweiler et al. 2023), (ii) reasoning abilities and world knowledge (e.g., Li et al. 2023b), and (iii) the ability to perform tasks across languages (e.g., Pires et al. 2019).A remaining question is whether such models work equally well across diverse populations using the same language.The study described in this paper first uses geography to demarcate different populations around the world and then uses comparable geo-referenced corpora to measure how well two families of LLMs perform across these different populations.Using a single lingua franca, English, controls for potential confounds caused by observing different languages across different populations.\n\nWe focus on the following questions:\n\n\u2022 What populations do pre-trained LLMs best describe, using perplexity scores on comparable corpora to measure goodness-of-fit?\n\n\u2022 What are the geographic and social factors which best predict which populations are better represented by the models?\n\nWe conduct this spatial probing task across models with an increasing number of parameters from two open-source series of LLMs: the BigScience bloom series (with 560m, 1.7 billion, and 3 billion parameters: BigScience Workshop 2023) and the Facebook opt series (with 350 million, 1.7 billion, and 3 billion parameters: Zhang et al. 2022).This approach allows us to determine whether larger models become increasingly more representative of the world's population and whether one family is itself more representative.If these are adequate general-purpose models of language then they would work equally well across all populations that use the same language (English) in the same register (social media).To the degree that a model deviates from this equal distribution of performance, it becomes a model of one population's language use rather than a model of language more broadly.We first collect a corpus of social media that is balanced for key lexical items while representing the written production of 927 local populations.",
            "score": 0.41734763703582034,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 849
                },
                {
                    "start": 849,
                    "end": 990
                },
                {
                    "start": 992,
                    "end": 1028
                },
                {
                    "start": 1030,
                    "end": 1157
                },
                {
                    "start": 1159,
                    "end": 1278
                },
                {
                    "start": 1280,
                    "end": 1618
                },
                {
                    "start": 1618,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1983
                },
                {
                    "start": 1983,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2309
                }
            ],
            "ref_mentions": [
                {
                    "start": 319,
                    "end": 342,
                    "matchedPaperCorpusId": "264077507"
                },
                {
                    "start": 397,
                    "end": 413,
                    "matchedPaperCorpusId": "258947371"
                },
                {
                    "start": 478,
                    "end": 496,
                    "matchedPaperCorpusId": "174798142"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07666015625
        },
        {
            "corpus_id": "248780099",
            "title": "Zero-shot Learning for Grapheme to Phoneme Conversion with Language Ensemble",
            "text": "After obtaining the nearest languages and the monolingual model for each of the training languages, we can use those models to approximate the target model. In particular, we are interested in combining prediction outputs from different models to create a single prediction output. If the models are one of the local prediction models (i.e: for each grapheme, we decide whether to generate a phoneme and which phoneme to generate) (Sejnowski and Rosenberg, 1987;Black et al., 1998), the ensemble task is simple. As we made one phoneme prediction at every grapheme position, we can use the voting to decide the most likely phoneme. \n\n[ \n\nHowever, for the more general sequence-tosequence neural model, it is more complicated. Different models would predict outputs with variable sequences, therefore voting at each position would be meaningless. For example, suppose two phoneme sequences \"/helo/\" and \"/elo/\" are generated from \"hello\" using two different languages. It is difficult to average /h/ and /e/ as they are corresponding to different graphemes. To solve this problem, we use a robust approach to ensemble outputs with variable lengths. Our approach is similar to the ROVER system (Fiscus, 1997), which is a commonly used approach to combine multiple speech outputs into one output. It has been applied to combine phoneme sequence (Schlippe et al., 2014), but only under the monolingual scenario in which they combine different models to improve the performance. This work focus on combining multilingual outputs and modifying the standard word-based network to consider the phonological structure. \n\nOne actual example from our dataset is illustrated in Figure 2. First, we build one confusion network (or lattice) per language in our nearest language set. The raw confusion network represents a single hypothesis using a directed graph whose edge corresponds to a single phoneme from the hypothesis4 . When we compose multiple confusion networks into one confusion network, there would typically be more than one edge connecting two nodes. The set of edges connecting two contiguous nodes is typically referred to as the confusion set (or correspondence set) (Fiscus, 1997;Mangu et al., 2000). For example, the first confusion set from the right network in Figure 2 is {/t/, /s/}.",
            "score": 0.4169519321793126,
            "section_title": "Model Ensemble",
            "char_start_offset": 11923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2051
                },
                {
                    "start": 2052,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 462,
                    "matchedPaperCorpusId": "12926318"
                },
                {
                    "start": 462,
                    "end": 481,
                    "matchedPaperCorpusId": "8571666"
                },
                {
                    "start": 1191,
                    "end": 1205,
                    "matchedPaperCorpusId": "18751160"
                },
                {
                    "start": 1341,
                    "end": 1364,
                    "matchedPaperCorpusId": "9522911"
                },
                {
                    "start": 2171,
                    "end": 2185,
                    "matchedPaperCorpusId": "18751160"
                },
                {
                    "start": 2185,
                    "end": 2204,
                    "matchedPaperCorpusId": "6135726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0830078125
        },
        {
            "corpus_id": "225067077",
            "title": "When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models",
            "text": "Language models are now a new standard to build state-of-the-art Natural Language Processing (NLP) systems. In the past year, monolingual language models have been released for more than 20 languages including Arabic, French, German, Italian, Polish, Russian, Spanish, Swedish, and Vietnamese (Antoun et al., 2020;Martin et al., 2020b;de Vries et al., 2019;Ca\u00f1ete et al., 2020;Kuratov and Arkhipov, 2019;Schweter, 2020, et alia). Additionally, large-scale multilingual models covering more than 100 languages are now available (XLM-R by Conneau et al. (2020) and mBERT by Devlin et al. (2019)). Still, most of the 7000+ spoken languages in the world are not coveredremaining unseen-by those models. Even languages with millions of native speakers like Sorani Kurdish (about 7 million speakers in the Middle East) or Bambara (spoken by around 5 million people in Mali and neighboring countries) are not covered by any available language models. \n\nEven if training multilingual models that cover more languages and language varieties is tempting, the curse of multilinguality described by Conneau et al. (2020) makes it an impractical solution, as it would require to train ever larger models. Furthermore, as shown by Wu and Dredze (2020), large-scale multilingual language models reach sub-optimal performance for languages that only account for a small portion of the pretraining data. \n\nIn this paper, we describe and analyze task and language adaptation experiments to get usable language model-based representations for understudied low resource languages. We run experiments on 16 typologically diverse unseen languages on three NLP tasks with different characteristics: part-of-speech (POS) tagging, dependency parsing (DEP) and named entity recognition (NER). \n\nOur results bring forth a great diversity of behaviors that we classify in three categories reflecting the abilities of pretrained multilingual language models to be used for low-resource languages. Some languages, the \"Easy\" ones, largely behave like high resource languages.",
            "score": 0.416683510359906,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 335,
                    "matchedPaperCorpusId": "207853304"
                },
                {
                    "start": 572,
                    "end": 592,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061309814453125
        },
        {
            "corpus_id": "266644278",
            "title": "Limitations of and Lessons from the Learning of Large Language Models",
            "text": "It is pretty safe to say that nobody really understands how large language models produce their results. Their learning process is quite different from the learning of language and argumentation by humans. Yet, a better understanding of their limits could also shed light on the particular strengths of human thought. \n\nAt least since the introduction of ChatGPT public interest in large language models sparked to new heights. A large strand of scientific analysis of such systems has concentrated on the abilities for logical reasoning (see e.g., Evans et al., 2018;Friedmann, 2023) and this is still investigated in recent research (e.g., Liu, 2023). Findings are that logical reasoning is limited, but also successful in many cases. However, it seems that there is a lack in theoretical understanding of the principal limitations of such systems. The present paper is indented as a small brick to fill this hole. It will argue that the structure of classical logic implies that it cannot be completely learned by large language models. The relevant property of logic that allows this conclusion is, however, only visible when applying the appropriate form of the Curry-Howard isomorphism that maps logic to functional computer programs. \n\nThe results of this paper do, of course, not imply that large language models cannot give answers derived by classical logic if these results were in some form in the training data. The restriction that will be discussed only applies to original logical reasoning. \n\nOne idea to address the restrictions of LLMs in the domain of logical reasoning is to combine a LLM with a classical symbolic inference engine into a hybrid system (e.g., Pan et al., 2023). However, if the limit described here is, in fact, the essential bottleneck, then another approach might be feasible and should avoid the drawbacks that are associated with hybrid systems. \n\nIn the conclusion, some possible implications for philosophy of language and education are briefly sketched.",
            "score": 0.4165260720699676,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 317
                },
                {
                    "start": 320,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 1998
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01395416259765625
        },
        {
            "corpus_id": "271909790",
            "title": "How Well Do Large Language Models Serve as End-to-End Secure Code Producers?",
            "text": "Large language models capable of code processing tasks can be divided into 2 categories: general language models represented by the GPT family and specialized models that are specifically pre-trained on code (i.e., code language models) [40]. For better representation, we choose 2 popular models for our work from each category. \n\n(1) GPT-3.5 is a well-known large language model with 175B parameters proposed by OpenAI in 2022. It is capable of understanding and generating natural language text or code. We use gpt-3.5-turbo-0125 in the study. We choose GPT-3.5 because it is one of OpenAI's most popular models which powered ChatGPT to gain more than 100 million active users in 2023 [25]. \n\n(2) GPT-4 is a multi-modal large language model introduced by OpenAI in 2023, capable of producing more accurate and secure responses [27]. The specific model used in the study is gpt-4-0613. GPT-4 is included in this work because it is one of OpenAI's most advanced models, showing remarkable performance on code generation benchmarks such as HumanEval [9]. \n\n(3) Code Llama is a code language model introduced by Meta in 2023. It was specially trained for code-relevant tasks based on the LLaMA 2 model, with excellent performance in code generation [31]. The specific model used in the study is Code Llama-70B. We include Code Llama in our research because it is an outstanding representation of open-source code language models. \n\n(4) CodeGeeX2 is an open-source code language model with 6B parameters, developed jointly by Tsinghua University and Zhipu AI. It is based on the ChatGLM2 architecture and tailored for coderelevant tasks such as code generation, completion, interpretation, and cross-programming language translation [7,8]. CodeGeeX2 is chosen for our work because it is representative of a range of lightweight code language models. \n\nOur research exclusively considers GPT-based large language models due to their state-of-the-art performance and their widespread use as programming assistants such as GitHub Copilot.",
            "score": 0.41636527639000864,
            "section_title": "Studied Large Language Models",
            "char_start_offset": 6754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 429
                },
                {
                    "start": 430,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 693
                },
                {
                    "start": 696,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1737
                },
                {
                    "start": 1738,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2033
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142578125
        },
        {
            "corpus_id": "52196319",
            "title": "What can linguistics and deep learning contribute to each other? Response to Pater",
            "text": "Systems based on neural networks have become increasingly common in technological applications over the last few years. One of the tasks that neural networks excel in consists in estimating how likely a particular word is to occur given the words that have preceded it; systems that perform this task are referred to as \"language models\". Neural language models, which are typically based on recurrent neural networks (RNNs), are trained using a large corpus in the following way: suppose that the network is processing the sentence the cat is on the mat. After the word cat, the network uses its existing weights to predict how likely each of the words of the language is to follow cat; when it is revealed that that word was in fact is, the network's weights are adjusted such that in the future it will assign a higher probability to is in a similar context. After training completes, the network is considered successful if it assigns a high probability to the words in a new sample of sentences from the same corpus. \n\nAccording to this criterion, contemporary RNN language models are able to predict upcoming words much better than earlier classes of models. At the same time, high probability assigned to a randomly sampled sentence, which may well be syntactically simple, does not necessarily indicate that the model has a robust knowledge of natural language syntax. In language modeling, as well as in tasks such as machine translation, neural networks have now reached a sufficiently high level of performance that accurately measuring their syntactic abilities and improving those abilities further will require more sophisticated evaluation methods. In particular, linguistics provides us with detailed theories of the capacities that make up the knowledge of a language; these theories make it possible to identify sentences that crucially test particular linguistic abilities. Linzen et al.'s 2016 study illustrates this strategy. Linzen et al. argued that given a pair of minimally different sentences, one of which is grammatical and one ungrammatical, a language model should assign a higher probability to the grammatical one. Their study focused on English subject-verb agreement (Elman 1991;Bock & Miller 1991). The model was presented with the first few words of the sentence leading up to the verb, and was expected to assign a higher probability to the correct form of the verb following those words.",
            "score": 0.4160697807649373,
            "section_title": "Linguistics as a normative standard",
            "char_start_offset": 1751,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2233
                },
                {
                    "start": 2234,
                    "end": 2425
                }
            ],
            "ref_mentions": [
                {
                    "start": 2201,
                    "end": 2213,
                    "matchedPaperCorpusId": "7069311"
                },
                {
                    "start": 2213,
                    "end": 2232,
                    "matchedPaperCorpusId": "36387582"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037109375
        },
        {
            "corpus_id": "239049577",
            "title": "Learning Domain Specific Language Models for Automatic Speech Recognition through Machine Translation",
            "text": "Voice interfaces in the form of personal assistants and spoken dialogue systems have gained ubiquitous usage in the last decade. This has been possible due to the advances brought by machine learning and deep learning methods in Automated Speech Recognition (ASR), Text-to-Speech (TTS) synthesis and Natural Language Understanding (NLU) systems. At the same time, lack of sufficient training data, for each of these components, has limited the availability of voice interfaces across several languages. Moreover, the existing interfaces are also restricted to specific domains or tasks in the given language. The work carried out in this internship lies along the research direction of enabling domain-specific or task-specific ASR in different languages.\n\nAn ASR system uses an Acoustic Model (AM) and a Language Model (LM) to perform a speech-to-text conversion. An AM trained on a generic corpus of speech recordings and corresponding text transcriptions can be reused across multiple applications. But an accurate speech transcription for a specific application requires that the LM is well trained or adapted to the target domain, for instance, medical, travel, shopping, etc. Lack of domain specific text corpora limits the development of ASR LMs and hence the deployment of accurate and practical domain specific ASR and voice assistants. This work tries to reuse domain specific corpora from one language to build LMs for other language(s). b. Reliability: Domain-specific LMs can aid to understanding and generating relevant responses for a given task when compared to generic LMs. Such ASR systems can in turn deliver stable services for longer time than their general purpose counterparts.",
            "score": 0.4159163506412361,
            "section_title": "Introduction",
            "char_start_offset": 92,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.285888671875
        },
        {
            "corpus_id": "271039125",
            "title": "Romanization Encoding For Multilingual ASR",
            "text": "Multilingual Automatic Speech Recognition (ASR) systems are designed to recognize and transcribe speech in multiple languages. Code-switching (CS) is a special case of this, dealing with speech that switches between two or more languages within a single utterance or conversation. While emerging cutting-edge web-scale large speech models such as [1,2,3] demonstrate magnificent performance on multilingual ASR, they still fall short in CS scenarios [4], often due to a lack of natural CS data for training. This scarcity hinders the ability of both general large speech models and specialized CS ASR systems to effectively learn and integrate acoustic and linguistic information [5]. \n\nPart of the challenge of multilingual and CS ASR arises from text representations of languages from different language families. Languages like those in the Indo-European family usually use a Latin-based alphabet with relatively * Equal contribution smaller character sets. These can be efficiently represented using methods like byte-pair encoding (BPE), which breaks down words into smaller pieces or sub-words. Research has shown that using sub-words can lead to better performance in language processing tasks [6,7]. However, languages such as Mandarin, Korean, and Japanese have a much larger set of unique characters, making sub-word representation less practical. While there are methods to break these characters into smaller units (like love in Mandarin's \u7231\u60c5 \u2192 \u00e0i q\u00edng with Pinyin and Korean's \u1109 \u1161\u1105 \u1161 \u11bc\u2192 \u3145\u314f\u3139\u314f\u3147 with Jamo) and group these characters into sub-units (i.e. \u00e0i q\u00e0ng \u2192 \u00e0iq\u00e0ng with segmentation), using these phonetic and semantic representations may not always yield the best results [8,9,10]. Despite the effectiveness of character-based approaches for individual languages, their integration into multilingual models is challenging. For instance, [11] documents the use of 8k characters for Mandarin, 4k for Japanese, and 2k for Korean, alongside a standardized set of 512 sub-words per language for other languages. This approach yields 11k unique tokens for these three languages alone, leading to a significantly large and potentially imbalanced vocabulary that inflates the model's output dimension. \n\nEffectively encoding languages with unique scripts is crucial for multilingual and CS ASR models.",
            "score": 0.4158578039466999,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2024
                },
                {
                    "start": 2025,
                    "end": 2211
                },
                {
                    "start": 2214,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 350,
                    "matchedPaperCorpusId": "252923993"
                },
                {
                    "start": 450,
                    "end": 453,
                    "matchedPaperCorpusId": "258762742"
                },
                {
                    "start": 1204,
                    "end": 1206,
                    "matchedPaperCorpusId": "13753208"
                },
                {
                    "start": 1690,
                    "end": 1693,
                    "matchedPaperCorpusId": "204915920"
                },
                {
                    "start": 1693,
                    "end": 1695,
                    "matchedPaperCorpusId": "21713551"
                },
                {
                    "start": 1695,
                    "end": 1698,
                    "matchedPaperCorpusId": "153313438"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "253447230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0472412109375
        },
        {
            "corpus_id": "273404154",
            "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in conversational tasks and general-purpose applications, such as writing emails or answering common questions. However, these general purpose LLMs may have restricted performance on tasks where specific skills and knowledge is required. We primarily focus on skill composition tasks, that necessitate the integration of multiple skills. \n\nMany industrial applications fit in this framework. Consider a company that manufactures ovens and is trying to design a chatbot to answer customer queries about its working and specifics. Directly using a frontier LLM (like gpt-4o) would fail since it lacks knowledge about the company's product. The ideal solution here would be to design an instruction dataset consisting of question-answer pairs about this product and fine-tuning an LLM on it. However, such a data collection and annotation procedure is expensive. Another possible solution is to fine-tune an LLM on a collection of product manuals and then impart it chat abilities by further fine-tuning on an instruction-tuning dataset like  Alpaca [55]. We refer to this method as DATA-MIX. Besides this approach being sequential, it suffers from catastrophic forgetting [28]. Whenever the company creates a new product, they need to redo fine-tuning on this data mixture or create a new question-answer dataset for the former method. \n\nIn this paper, we study model merging as an alternative approach. Given a model that is fine-tuned on the manuals and one that possesses question-answering capabilities, we optimally combine their weights to obtain a model that can answer product-specific questions. This approach is more efficient since we merge skill-specific fine-tuned models without any additional data collection or training from scratch. Among the multiple techniques to perform model merging, our framework specifically builds on LoRA [23] (a review of the LoRA method is in Appendix A), a fine-tuning technique that consists of adding a low-rank update to a few layers in the model. In this context, model merging consists of combining the LoRA weights from different models. \n\nPrior works [19,49,70] have investigated LoRA merging in computer vision where each skill is a visual concept or style and the objective is image generation.",
            "score": 0.4157424550163365,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2158
                },
                {
                    "start": 2161,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "4704285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415283203125
        },
        {
            "corpus_id": "267547973",
            "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
            "text": "The allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;Wei et al., 2021;Mishra et al., 2022), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access. \n\nTo tackle this problem, most past work (Jang et al., 2023;Belofsky, 2023;Durbin, 2024;Maxine, 2023) learns a posthoc routing strategy by comparing an embedding of the input query to the average embedding of examples in each dataset used to train each expert. Such methods implicitly assume there is a single expert well-suited for the query and hope that the retrieval algorithm can accurately identify this best expert. However, Jang et al. (2023) showed that such approaches lag behind an \"oracle\" router that always chooses the best expert for a given query. To explore alternative routing approaches, we first note that many PEFT methods typically insert small trainable modules in many places across the model (e.g. at each weight matrix (Hu et al., 2021)). Meanwhile, many sparsely gated Mixture-of-Experts models make routing decisions separately for each token (Shazeer et al., 2016;Fedus et al., 2022;Du et al., 2022). In this work, we therefore explore the angle of improving zero-shot generalization through adaptive per-token and per-module routing.",
            "score": 0.41533628383991983,
            "section_title": "Introduction",
            "char_start_offset": 2225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 788,
                    "end": 805,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 805,
                    "end": 825,
                    "matchedPaperCorpusId": "237421373"
                },
                {
                    "start": 1825,
                    "end": 1847,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 1866,
                    "end": 1882,
                    "matchedPaperCorpusId": "245124124"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73095703125
        },
        {
            "corpus_id": "278394756",
            "title": "SOAEsV2-7B/72B: Full-Pipeline Optimization for State-Owned Enterprise LLMs via Continual Pre-Training, Domain-Progressive SFT and Distillation-Enhanced Speculative Decoding",
            "text": "The integration of large language models (LLMs) into specialized industrial domains has emerged as a pivotal avenue for advancing digital transformation [1,2]. Within the context of State-Owned Assets and Enterprises (SOAEs), intelligent solutions must satisfy three critical requirements: 1) accurate opendomain knowledge question-answering; 2) generation of industry-compliant professional reports; and 3) data-driven decision recommendations. These capabilities necessitate models that not only possess a deep understanding of industryspecific knowledge but also maintain general language abilities to address diverse demands. Current general-purpose models face a critical challenge in this domain-their overemphasis on generalizability leads to insufficient integration depth of SOAEs-specific expertise. This imbalance between specialization and generalization hampers models' ability to meet practical requirements for precise decision support and complex task processing [3], ultimately limiting the deep integration of AI technologies in industrial intelligence transformation. \n\nPrior research in this field has made strides in domain-specific data curation [4] and domain adaptation and preference alignment training strategies [5]. However, three systemic bottlenecks persist: First, existing SOAEs-specific LLMs are constrained by model capacity limitations, which hinder the integration of SOAEs' complex knowledge systems [5]. Second, current supervised fine-tuning methods typically rely on single-stage training with domain data [4], thereby neglecting the value of progressive knowledge transfer [6], which enables models to gradually adapt to domain-specific tasks and accumulate expertise in a stepwise manner. Third, while current models with relatively smaller scales exhibit minimal inference bottlenecks, the efficiency of inference becomes a critical factor affecting practical usability as models scale up and support longer contexts [7].",
            "score": 0.4152672573238194,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1086
                },
                {
                    "start": 1089,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1964
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 158,
                    "matchedPaperCorpusId": "277486229"
                },
                {
                    "start": 1168,
                    "end": 1171,
                    "matchedPaperCorpusId": "275852537"
                },
                {
                    "start": 1239,
                    "end": 1242,
                    "matchedPaperCorpusId": "276974224"
                },
                {
                    "start": 1437,
                    "end": 1440,
                    "matchedPaperCorpusId": "276974224"
                },
                {
                    "start": 1546,
                    "end": 1549,
                    "matchedPaperCorpusId": "275852537"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1317138671875
        },
        {
            "corpus_id": "644854",
            "title": "A database approach to information retrieval: The remarkable relationship between language models and region models",
            "text": "In this case, an instantiation of Equation 13 would be: \n\nSo, the sum over the whole target language vocabulary will in practice be a sum over the possible translations only (those for which P (S|T ) > 0). The probability function corresponds to the following region expression. \n\n( \n\nEquation 15 can be generated from 14 as shown in the previous sections. \n\n4 Discussion, open issues and future work \n\nIn this report, we presented a unified region model / language model approach and showed its expressiveness for a wide range of applications of language modeling: ad-hoc retrieval, smoothing, video retrieval, web search and cross-language retrieval. In the past, we have developed separate prototype retrieval systems for these approaches. Developing these prototype systems meant we had to reimplement parts of our system: reimplementing APIs, introducing new APIs, introducing new query languages, introducing new indexes, introducing new storage structures, etc. This report shows that such approaches can be supported by a single retrieval system that responds to a simple query language that hides implementation details of information retrieval approaches from the application developer. \n\nThe relationship between the region queries and the language modeling probability functions might seem trivial because we \"hard-wired\" the language modeling probability definition in the CONTAINING operator, but we believe it is remarkable: Note that the language modeling probability functions are arithmetic expressions that define the probability of a single document D. However, the region queries are algebraic expressions for processing sets of documents (regions) instead of single documents. Since the region query language forms a \"bulk algebra\", experiences from relational database system design can be used to develop efficient implementations of such a system, possibly up to a point where applications run as fast as, or possibly even faster than, the dedicated prototypes we developed in the past.",
            "score": 0.4149302178809574,
            "section_title": "Cross-language information retrieval",
            "char_start_offset": 27220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 356
                },
                {
                    "start": 359,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1698
                },
                {
                    "start": 1699,
                    "end": 2011
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05780029296875
        },
        {
            "corpus_id": "248377888",
            "title": "I Do Not Think It Means What You Think It Means: Artificial Intelligence, Cognitive Work & Scale",
            "text": "At the time of its release, GPT-3 was ten times larger than the largest nonsparse language model. There are many benefits to scale, although perhaps the two most important are: when properly trained, larger models tend to have better performance on the same task than smaller models; and larger models tend to be useful in a broader range of tasks, either with fine-tuning or not, than smaller models. Because GPT-3 is useful on a broad range of tasks with little or no additional fine-tuning, it has been possible to offer an application programming interface to developers to allow them to probe the utility of the model for the problems they are interested in solving. \n\nOne of the biggest surprises of the GPT-3 model is that it generalized something about the structure of computer programming languages that allowed it to synthesize code that did not exist in its training data. This realization led to OpenAI fine-tuning a model for computer code called Codex, and in collaboration with GitHub, developing a coding assistant product called Copilot that can write code from natural language prompts. As the Codex model and the Copilot product get better, they will not only assist programmers with their cognitive work, but may also lower the barrier to entry to programming itself. Given that Codex and Copilot work by allowing humans to describe in natural language an effect they would like accomplished through code, the task of coding may become more approachable to many, many more people. 83 Kevin Scott models. 13 Moreover, based on the trends of the past few years, for transfer learning to work better, we will need bigger and more sophisticated models, which in turn will require more training compute. \n\nAI systems designed to assist us with our cognitive work will no doubt continue to surprise us. I have been surprised so many times over the past two decades by what AI scientists and researchers have been able to accomplish that I have learned to heed the second half of Arthur C. Clarke's first law: When a distinguished but elderly scientist states that something is possible, they are almost certainly right. When they state that something is impossible, they are very probably wrong.",
            "score": 0.4141243351397489,
            "section_title": "Kevin Scott",
            "char_start_offset": 21898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 671
                },
                {
                    "start": 674,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1719
                },
                {
                    "start": 1722,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2134
                },
                {
                    "start": 2135,
                    "end": 2210
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048126220703125
        },
        {
            "corpus_id": "265520130",
            "title": "Contextualized word senses: from attention to compositionality",
            "text": "Our work, like all of the aforementioned, make use of empirical distributions learnt from corpora to either validate or not a semantic hypothesis and contribute to the design of computational architectures that better model the semantic interpretation process in a more transparent way. By contrast, other linguisticbased approaches thoroughly explore the results of the neural models to find some glimmer of implicit linguistic knowledge in their black boxes. This task is carried out by defining specific tests to analyze whether neural-based language models can learn syntactic regularities and are provided with compositional abilities (De-Dios-Flores and Garcia, 2022; Kim and Linzen, 2020;Linzen and Leonard, 2018). Linzen (2018) states that the role of linguists would be to clearly delineate the linguistic capabilities that can be expected of large language models, by constructing controlled experimental tests that can determine whether those desiderata have been met. This new paradigm of linguistic research is of great interest and can be seen as very different but complementary to the one we follow. \n\nIn the present work, we will implement a distributional architecture to check whether selectional preferences can dynamically build the semantic meaning of composite expressions.",
            "score": 0.4139960284285442,
            "section_title": "Symbolic and Compositional Approaches",
            "char_start_offset": 18109,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1115
                },
                {
                    "start": 1118,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 695,
                    "matchedPaperCorpusId": "222290851"
                },
                {
                    "start": 695,
                    "end": 720,
                    "matchedPaperCorpusId": "49316999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03448486328125
        },
        {
            "corpus_id": "271349880",
            "title": "Recognition of target domain Japanese speech using language model replacement",
            "text": "However, general domain ASR models and ASR models trained with publicly available datasets may not perform as required in a target domain environment. In fact, we have had great difficulty creating an ASR model for a specialized domain. \n\nAgainst this background, a method to adapt existing large-scale ASR models to a target domain would be very useful. Currently, fine-tuning is the most popular and effective domain adaptation method for ASR tasks [21]. This involves re-training a large-scale, outof-domain ASR model with a small amount of target domain speech and transcription data, in order to create a target domain-adapted ASR model. Many studies have proposed efficient fine-tuning methods which use limited computing resources, such as Adapters [22]. To fine-tune an ASR model, it is generally necessary to prepare several hours of target domain training data, which includes speech and its transcription; thus, there is still the problem of the cost of preparing target domain training data. \n\nAnother effective method for domain adaptation of ASR models is to use ASR models in combination with external language models [23][24][25][26], the most common method of which is Shallow Fusion [27,28]. Other methods for combining ASR models with external language models have also been proposed, including Cold Fusion [29] using gate mechanisms [30], Component Fusion [31], and Deep Fusion [11]. All of these language model integration methods improve ASR performance; however, there are some drawbacks associated with each method. The Shallow Fusion method adds the output probability of the language model for the target domain to that of the existing ASR model, which is dependent on the language information contained in the training data used. This means that Shallow Fusion adds the output probabilities of two models trained with different language information. The Deep, Component, and Cold Fusion methods require retraining of the ASR each time a new language model is integrated, so these methods have not replaced the simple Shallow Fusion method as the go-to method among most of the ASR community, since Shallow Fusion does not require model retraining, as the language model is only applied during decoding.",
            "score": 0.4138300171008473,
            "section_title": "Introduction",
            "char_start_offset": 1786,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 1003
                },
                {
                    "start": 1006,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2229
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "246634891"
                },
                {
                    "start": 1205,
                    "end": 1208,
                    "matchedPaperCorpusId": "41638977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12115478515625
        },
        {
            "corpus_id": "273403604",
            "title": "Fine-Tuning Language Models on Multiple Datasets for Citation Intention Classification",
            "text": "Pretrained language models (PLM), a.k.a. large language models (LLM), are models pretrained on unlabeled text corpus with self-supervised language modeling tasks such as causal language modeling (Radford et al., 2018) and masked language modeling (Taylor, 1953). PLMs achieve state-ofthe-art performance on a wide range of downstream NLP tasks via gradient-based fine-tuning on the corresponding datasets (Devlin et al., 2019;Liu et al., 2019b;Beltagy et al., 2019;Raffel et al., 2020;Radford et al., 2019). \n\nRecently, Brown et al. demonstrate generative PLMs can be specialized for different tasks with incontext learning (ICL), a fine-tuning-free learning paradigm that unifies NLP tasks to the language generation task. Although the increasing scale of PLMs has been improving the performance of ICL on NLP tasks (Wei et al., 2023), we find that ICL with GPT4 (Bubeck et al., 2023) still underperforms our proposed method.",
            "score": 0.4133379346539717,
            "section_title": "Pretrained Language Model",
            "char_start_offset": 8161,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 926
                }
            ],
            "ref_mentions": [
                {
                    "start": 247,
                    "end": 261,
                    "matchedPaperCorpusId": "206666846"
                },
                {
                    "start": 405,
                    "end": 426,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 444,
                    "end": 465,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 465,
                    "end": 485,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 485,
                    "end": 506,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.216064453125
        },
        {
            "corpus_id": "265696189",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models through Additional Training",
            "text": "Large Language Models (LLMs), particularly those based on transformer architecture like the GPT-4, have garnered significant attention for their advanced response capabilities. 1,2  [5][6][7] The potential applications of LLMs in scientific research are vast. They include providing scientific knowledge responses, analyzing experimental results, making predictions and suggestions, controlling robotic systems for automated experiments, and aiding in literature collection and writing tasks. 2, 7 8 wever, several challenges have become apparent with the growing interest in implementing LLM-based AI technologies in research fields. 2 A fundamental issue is the expansion of specialized knowledge. 7 The utility of teaching LLMs specialized information has been proven in computer-related fields, including programming, where many programmers now consider LLMs an essential tool. 2 In contrast, in non-computer science disciplines, LLMs often lack sufficient expertise. 2,7  or instance, while GPT-4 appears to have the knowledge equivalent to a graduatelevel chemistry textbook, it struggles to answer questions about cutting-edge academic papers. 7 This incapability is partly due to the practical difficulties in accessing non-open access papers 7 and, as discussed later, could also stem from the inefficiency of language models in learning from limited data volumes. These observations underscore the need to enhance the specialized knowledge of LLMs for their effective use in various scientific fields. Addressing these challenges will be crucial in fully leveraging the potential of LLMs in scientific research and beyond. \n\nIf users could freely add specialized knowledge to LLMs, it would enable, for instance, the creation of chatbot systems well-versed in specific fields or organizational knowledge. 9 This capability could significantly accelerate research and development. Moreover, advancements in AI technology, such as reducing hallucinations (output of incorrect information), improving long-term memory, and enhancing decision-making abilities, could pave the way for language models capable of supporting a wide range of specialized tasks. 2 This article reports on methods for additional training to introduce new specialized knowledge into pre-built LLMs. \n\nTypically, LLMs acquire knowledge through pre-training when building the model from scratch, implicitly achieved by reading vast amounts of text. 2 Whether knowledge can be acquired in the learning process after the model's construction (fine-tuning) remains an open question. 10",
            "score": 0.41310809518949954,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1632
                },
                {
                    "start": 1635,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2280
                },
                {
                    "start": 2283,
                    "end": 2562
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1336669921875
        },
        {
            "corpus_id": "235458551",
            "title": "Scaling Laws for Acoustic Models",
            "text": "In the field of language modeling, it has been shown that generative models have predictable and favorable scaling properties with respect to training set size, model size, and available compute budget [1,2]. It has also been shown that, as the models become larger and more accurate, less task-specific fine-tuning is necessary to achieve state-of-the-art results [3]. \n\nConsequently, the field of natural language processing is in the middle of an \"arms race\" to build the largest and most powerful models. It started with smaller models like GPT [4] and Bert [5], which had hundreds of millions of parameters. Today, state-of-the-art models like SWITCH-C [6] have more than 1.5 trillion parameters. \n\nMeanwhile, in the field of acoustic modeling of speech, there has been growing interest in improving automatic speech recognition by using bulk untranscribed audio for unsupervised pre-training. Successful techniques use either autoregressive predictive coding (APC) [7,8,9] or contrastive predictive coding (CPC) [10,11,12] to pre-train an acoustic model from a large corpus of unlabeled speech. Once the pre-training is complete, the models are used to generate features for downstream modeling tasks, such as automatic speech recognition. \n\nIf acoustic models trained with predictive coding exhibit similar scaling behavior as the autoregressive generative models studied in [2], then we can use this analysis to discover best practices for scaling and training acoustic models that are much larger and more accurate than today's state-of-the-art. By accurately predicting the amount of data, number of parameters, or length of compute, we eliminate these factors from potentially expensive hyperparameter tuning on the models we build. By understanding whether our model's performance (developmentset loss) is being limited by a lack of data or by the model size, we can make intelligent decisions about how to improve them. This paper tests this premise and develops a generalization of previous work that accounts for the interplay between limited training data, limited model parameters, and the developmentset loss of a converged model.",
            "score": 0.41302846875799437,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1245
                },
                {
                    "start": 1248,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 974,
                    "end": 976,
                    "matchedPaperCorpusId": "204904767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0552978515625
        },
        {
            "corpus_id": "266191532",
            "title": "Large Human Language Models: A Need and the Challenges",
            "text": "Other works (Zhong et al., 2021;Mireshghallah et al., 2022), as discussed earlier, simply prefix the user representation to the word embeddings when processing through the Transformer based architectures. \n\nThe modeling decision questions and existing works spur us to explore many other architectural solutions for large human language models, along with suitable pre-training tasks or loss functions that include human contexts. C3: Model applications. Another key challenge is in effectively applying the pre-trained large human language models on the target downstream tasks and applications. PS3: For instance, (i) the pre-training task may be built similar to downstream task training i.e., we add a classification or regression head on top of the pre-trained language model and fine-tune for target downstream tasks like a traditional large language model, (ii) the pre-trained model can be trained with downstream task-specific objective i.e., in addition to using the pre-training knowledge, we train the model parameters specific to the target downstream task objective alone, (iii) continue the pre-trained model's training in a multi-task learning setup i.e., we train for the pre-training objective as well as a downstream task-specific objective, or (iv) explore different data processing strategies when fine-tuning the pre-trained model for target downstream tasks for example, limiting the historical language context in the fine-tuning stage. C4: Data processing. Processing human context from user's historical language requires effectively handling user-level data: approaches to process user-specific data which can be rather long, and strategies to choose the right amount and relevance of the historical language to be used. First, user information adds another dimension to the data that may require creative ways of processing. Second, the runtime and memory complexity of the selfattention mechanism scales quadratically with the sequence length, which often limits their abilities to directly process long input sequences. And third, answers to questions like: how much historical language is sufficient to capture the human context, whether adding more language will help build a better human context, and whether we need to process even longer documents in a single pass, among other intriguing considerations.",
            "score": 0.4129621808904166,
            "section_title": "Challenges and Possible Solutions",
            "char_start_offset": 13957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 207,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2339
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 32,
                    "matchedPaperCorpusId": "236478014"
                },
                {
                    "start": 32,
                    "end": 59,
                    "matchedPaperCorpusId": "238252929"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0552978515625
        },
        {
            "corpus_id": "248834433",
            "title": "Should attention be all we need? The epistemic and ethical implications of unification in machine learning",
            "text": ": successful performance by one system across different tasks and modalities. Another possible class of explananda that unified models may explain is the mechanisms underlying successful prediction across different tasks and modalities. We suspect that this is part of why the expanded success of transformers across modalities is so exciting to many researchers: they hope that multi-modal, multi-task models are sharing information across different modalities and tasks. Similarly, there is a hope of shared mechanisms: if what we think transformers are doing with sequential information in language is also what they're doing with image patches engineered to be sequential, then we can understand predictive success in multiple domains using a similar mechanism. \n\nIt is as yet unclear if current trends towards unification will bring about this type of unification-through-sharedmechanisms. As Bommasani et al. point out, unified models fall on a spectrum between one model with shared mechanisms for many tasks, or many models that just happen to be \"glued\" together into one large model (Figure 1): \n\nAs one model, behavior can be made interpretable by identifying and characterising the finite number of generalizable model mechanisms that the model uses to produce behaviors across tasks (e.g., mechanisms that assign meaning to words, compare quantities, and perform arithmetic). As many models, explanations of model behavior in one task are not necessarily informative about behavior in other tasks; thus requiring \n\n[sic] to study behavior independently in each task [11]. \n\nGiven this, if unified models are more like the \"one model\" end of the spectrum, then they would be better at explaining the explananda described here. If they are more like the \"many models\" end of the spectrum, then these explananda would not find a unified explanation within the model. Given the newness of these unified models and the difficulties we have in understanding how they make decisions [111], it is unclear at present where on the spectrum they lie.",
            "score": 0.41254728212557606,
            "section_title": "Possible explananda 3",
            "char_start_offset": 25911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0455322265625
        },
        {
            "corpus_id": "264146906",
            "title": "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning",
            "text": "Large language models have shown their remarkable capabilities as a general interface for various language-related applications. Motivated by this, we target to build a unified interface for completing many vision-language tasks including image description, visual question answering, and visual grounding, among others. The challenge is to use a single model for performing diverse vision-language tasks effectively with simple multi-modal instructions. Towards this objective, we introduce MiniGPT-v2, a model that can be treated as a unified interface for better handling various vision-language tasks. We propose using unique identifiers for different tasks when training the model. These identifiers enable our model to better distinguish each task instruction effortlessly and also improve the model learning efficiency for each task. After the three-stage training, the experimental results show that MiniGPT-v2 achieves strong performance on many visual question-answering and visual grounding benchmarks compared to other vision-language generalist models. Our model and codes are available at https://minigpt-v2.github.io/",
            "score": 0.4124828877440676,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11297607421875
        },
        {
            "corpus_id": "265609841",
            "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian",
            "text": "In this subsection, we present observations from the longitudinal comparison of different models in downstream tasks, as detailed in Section 5: 1) While NB-GPT-J-6B did not achieve the highest scores across all tasks, it showed consistent performance and the best perplexity scores compared to our NorGLMs on nearly all tasks. This consistency is likely due to its initial training on large English datasets before being continue-trained on Norwegian data. \n\n2) The 23B model did not show the expected absolute advantage in downstream tasks. We find that with a small-scale pre-training dataset, a larger model cannot demonstrate its ability to better cope with complex problems, which also supports the findings in Hoffmann et al. (2022). \n\n3) The results highlight the promising abilities of smaller language models on specific tasks. However, these models often lack consistency in generating high-quality, meaningful text. 4) A comparison between Table 3 and Table 4 reveals significant differences between summaries written by journalists and those generated by GPT-3.5 or nonprofessionals. However, the model's performance on the latter datasets appears to be proportional to its size. GPT-3.5's performance on NO-Multi-QA-Sum has improved significantly, possibly due to the similarity of frameworks and training data overlap between GPT-3.5 and GPT-4. 5) GPT-3.5's difficulties with specialized Norwegian instructions highlight the unique complexities of the Norwegian language, which are challenging for Englishdominated models. This emphasizes the need to focus on low-resource languages to better understand their cultural nuances. 21",
            "score": 0.41230545160582166,
            "section_title": "Discussion",
            "char_start_offset": 26762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1644
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039337158203125
        },
        {
            "corpus_id": "351666",
            "title": "Natural Language Processing (Almost) from Scratch",
            "text": "Instead we use a single learning system able to discover adequate internal representations. In fact we view the benchmarks as indirect measurements of the relevance of the internal representations discovered by the learning procedure, and we posit that these intermediate representations are more general than any of the benchmarks. Our desire to avoid task-specific engineered features led us to ignore a large body of linguistic knowledge. Instead we reach good performance levels in most of the tasks by transferring intermediate representations discovered on large unlabeled datasets. We call this approach \"almost from scratch\" to emphasize the reduced (but still important) reliance on a priori NLP knowledge. \n\nThe paper is organized as follows. Section 2 describes the benchmark tasks of interest. Section 3 describes the unified model and reports benchmark results obtained with supervised training. Section 4 leverages large unlabeled datasets (\u223c 852 million words) to train the model on a language modeling task. Performance improvements are then demonstrated by transferring the unsupervised internal representations into the supervised benchmark models. Section 5 investigates multitask supervised training. Section 6 then evaluates how much further improvement can be achieved by incorporating standard NLP task-specific engineering into our systems. Drifting away from our initial goals gives us the opportunity to construct an all-purpose tagger that is simultaneously accurate, practical, and fast. We then conclude with a short discussion section.",
            "score": 0.4121249254743571,
            "section_title": "Introduction",
            "char_start_offset": 2363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 715
                },
                {
                    "start": 718,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1565
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07696533203125
        },
        {
            "corpus_id": "258584732",
            "title": "Multilingual text categorization and sentiment analysis: a comparative analysis of the utilization of multilingual approaches for classifying twitter data",
            "text": "The latest indicates that one of the key challenges for the implementation of improved classifications in modern NLP systems is whether to apply and utilize a multilingual classifier in contrary to a monolingual one under the scopes of a research work or task. The emerging utilization of transformers and specifically of BERT led to the deployment of different monolingual language-specific models for many languages based on BERT's architecture and pretraining procedure. In that context, many monolingual BERT variants have been recently implemented, such as for Chinese [26], Danish [27], Greek [28], and Dutch [29] among others. On top of this, several recent research works attempt to compare the performance between monolingual and multilingual models on specific target languages and downstream tasks. The outcomes indicated that certain monolingual models may outperform as compared to multilingual ones as they can leverage more specialized and extensive language models that are tailored to the specific target language [30,31]. The latter is further supported and enhanced specifically in cases that monolingual tokenizers have been trained by native-speaking experts who are aware of relevant linguistic phenomena applying on their target language [32]. Thus, the monolingual models also provide better sentence representations. However, these sentence representations are usually domain-specific and do not generalize well across different tasks and especially on tasks that are based on the utilization of multilingual datasets, where also the utilization of several different monolingual models would be proven an extremely time-consuming approach. The latter highlights the need for better and more generalized sentence embedding models and is highly addressed by the utilization of multilingual models [33]. At the same time multilingual approaches have proven to be more accurate in scenarios that are based on multilingual datasets and that target their outcomes on multiple languages rather than on a specific one [34]. In such cases, using a multilingual model can enable the model to handle the nuances and variations across different languages, and make more accurate predictions leading to better performance. By training a model on multiple languages, it can learn from the similarities and differences between languages and improve its overall performance on all languages leveraging its transfer learning capabilities [35]. This can be especially useful for low-resource languages, where it may be difficult",
            "score": 0.41201290560585124,
            "section_title": "Multilingual versus monolingual classifiers",
            "char_start_offset": 10634,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 574,
                    "end": 578,
                    "matchedPaperCorpusId": "195068911"
                },
                {
                    "start": 587,
                    "end": 591,
                    "matchedPaperCorpusId": "218977395"
                },
                {
                    "start": 599,
                    "end": 603,
                    "matchedPaperCorpusId": "221340533"
                },
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "248240008"
                },
                {
                    "start": 1261,
                    "end": 1265,
                    "matchedPaperCorpusId": "229924220"
                },
                {
                    "start": 2035,
                    "end": 2039,
                    "matchedPaperCorpusId": "256460898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.190673828125
        },
        {
            "corpus_id": "24463589",
            "title": "Enhancing recurrent neural network-based language models by word tokenization",
            "text": "The results that are reported using recurrent neural network-based language models are always better than those of basic neural network models and other traditional model results. The results reported by this work show that recurrent neural network-based language models results outperform traditional models results for two different tasks: English automatic speech recognition and Arabic automatic spelling error correction. The neural network input vector is presented using a binary representation where the current word is set to one in the vector according to its index in the vocabulary, and the other vector values are set to zero. The main problem with this model is the size of both the input and output layer where it needs to be at least as large as the number of words in the language vocabulary. The vocabulary size varies from languages such as English that have simple morphological systems to languages that have richer morphological systems such as the Arabic language [7] (where every Fig. 1 Basic recurrent neural network language model. Basic recurrent neural network language model consists of three layers: input layer, hidden layer and output layer; input word is presented to the network input layer using a 1-of-n encoding. The feedback between hidden and input layer allows the hidden neurons to remember the history of the previously processed word. The hidden layer output is computed using Tanh function [2]. The final network output is computed using the Softmax activation function [3] single root can be converted to several other valid forms using morphological derivation rules). Rich morphological system makes this architecture inefficient if it is used in rich morphological language applications since it needs high computational costs to train and use these models within real-time applications, such as speech recognition and automatic machine translation. \n\nIn this work, we try to build recurrent neural network-based language models that can handle the network training speed problem with languages that have rich morphological systems based on word tokenization. The proposed model input layer size is shown in Fig. 2 for the different training sets extracted from an open source Arabic language corpus [8]. As shown in the figure, the network size remarkably increases as the number of words in the training corpus increases. In real applications, to build a robust language model, a large corpus with several millions of words is needed. Networks with a very large number of neurons have two main problems. The first one is that training time is very long to achieve network learning convergence.",
            "score": 0.41179090002433083,
            "section_title": "Introduction",
            "char_start_offset": 2193,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1897
                },
                {
                    "start": 1900,
                    "end": 2107
                },
                {
                    "start": 2108,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2371
                },
                {
                    "start": 2372,
                    "end": 2484
                },
                {
                    "start": 2485,
                    "end": 2553
                },
                {
                    "start": 2554,
                    "end": 2643
                }
            ],
            "ref_mentions": [
                {
                    "start": 987,
                    "end": 990,
                    "matchedPaperCorpusId": "7705645"
                },
                {
                    "start": 1434,
                    "end": 1437,
                    "matchedPaperCorpusId": "120317426"
                },
                {
                    "start": 2248,
                    "end": 2251,
                    "matchedPaperCorpusId": "57298371"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04083251953125
        },
        {
            "corpus_id": "254974546",
            "title": "Efficient Induction of Language Models Via Probabilistic Concept Formation",
            "text": "The past decade has seen substantial progress in the development of practical language modeling approaches. For example, the influential Word2Vec system (Mikolov et al., 2013a,b) demonstrated that one can extract meaningful semantic information, in the form of low-dimensional word embeddings, by analyzing words and their surrounding contexts. The increased availability of massive training corpora, inexpensive memory storage, and very rapid computing abilities has fueled transformation of this early approach into sophisticated techniques for learning large language models, which are now used widely to extract and store semantic content from text. \n\nAlthough these systems are widely viewed as successful, they have some important drawbacks. First, they require substantial training data and computational resources. For example, the GPT-3 system was trained on a giant corpus that represents a sizable portion of the internet (Brown et al., 2020). Some estimates suggest that its computing costs for training were as high as $12 million (Wiggers, 2020). In addition, systems like GPT utilize batch learning, which means that they train on all available data at once and thus cannot update their model efficiently in light of new cases. Furthermore, French (1999) has shown that neural network approaches, which include Word2Vec and GPT, often experience catastrophic forgetting-where they lose access to the results of early learning if given new cases. As a result, when new instances become available, the systems must be retrained on both old and new cases, which adds to development and maintenance costs. \n\nRecent efforts have tried to offset these expenses by exploring how language models that are trained on large corpora might be subsequently \"fine tuned\" using data from a target domain (Devlin et al., 2018). This approach aims to boost accuracy on the target task by extracting and transferring general knowledge from a large corpus to the target, while lowering overall costs by reusing a previously trained model. Despite the excitement around large pre-trained language models, a recent study by Krishna et al. (2022) suggests that little knowledge actually transfers through fine tuning; their work suggests that the main factor that aids performance is data from the target domain. 1 By only using target domain data, this approach reduces the data needed to build a language model by factors from 10 to 500, but it still requires large training sets to achieve reasonable performance.",
            "score": 0.4116488098033977,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1616
                },
                {
                    "start": 1619,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2307
                },
                {
                    "start": 2308,
                    "end": 2509
                }
            ],
            "ref_mentions": [
                {
                    "start": 933,
                    "end": 953,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1256,
                    "end": 1269,
                    "matchedPaperCorpusId": "2691726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "207924242",
            "title": "From Monolingual to Multilingual FAQ Assistant using Multilingual Co-training",
            "text": "Achievement of scale, agility, and quality in support functions of large enterprises is a key demand. Conversational systems are increasingly being deployed to this effect. Such systems try to classify users' utterances into one of the FAQ (Khurana et al., 2017), usually referred to as intent, and then show an answer that is mapped to the chosen intent. In specific geographies such as Europe, Latin America, and India such FAQ based conversational systems may be required to work in more than one language. Similar requirements are also presented to us by many international consumer oriented businesses such as airlines, shipping companies, and banks. \n\nThe straight-forward approach is to build a different classification model for every language, which is hard to maintain because of manual effort involved in preparing the training data in every language, and training time for every model. We therefore look into cross-lingual transfer learning approaches such as a) Translate-Train (Schuster et al., 2019): here we translate the training data from English1 into all the other languages and train a different model for every language; b) Translate-Test (Artetxe and Schwenk, 2018): here we maintain single model (usually for English), and use machine translation at the inference time before using the classification model; c) Zero-shot (Artetxe and Schwenk, 2018): here we employ multi-lingual pre-trained language representation model (PLRM) such as LASER (Artetxe and Schwenk, 2018), and train the model in high resource language (English) only and use the target language at the inference time only; d) Joint training (Upadhyay et al., 2018a,b): here the same model is trained on all the languages on which it is expected to be used. All these approaches are also shown in Figure 1. Either the accuracy of above mentioned models is low (Zero-Shot, or Translate-Test) or they are too hard to maintain in production system (Translate-Train, or Joint training). We therefore require an approach that performs better than all these approaches and is easier to maintain.",
            "score": 0.4114388316794308,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 655
                },
                {
                    "start": 658,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 262,
                    "matchedPaperCorpusId": "12140097"
                },
                {
                    "start": 991,
                    "end": 1013,
                    "matchedPaperCorpusId": "53110354"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05450439453125
        },
        {
            "corpus_id": "1473",
            "title": "Similarity-Based Models of Word Cooccurrence Probabilities",
            "text": "Similarity-based language models provide an appealing approach for dealing with data sparseness. In this work, we proposed a general method for using similarity-based models to improve the estimates of existing language models, and we evaluated a range of similarity-based models and parameter settings on important language-processing tasks. In the pilot study, we compared the language modeling performance of a similarity-based model with a standard back-off model. While the improvement we achieved over a bigram back-off model is statistically significant, it is relatively modest in its overall effect because of the small proportion of unseen events. In a second, more detailed study we compared several similarity-based models and parameter settings on a smaller, more manageable word-sense disambiguation task. We observed that the similarity-based methods perform much better on unseen word pairs, with the measure based on the Jensen-Shannon divergence being the best overall. \n\nOur experiments were restricted to bigram probability estimation for reasons of simplicity and computational cost. However, the relatively small proportion of unseen bigrams in test data makes the effect of similarity-based methods necessarily modest in the overall tasks. We believe that the benefits of similarity-based methods would be more substantial in tasks with a larger proportion of unseen events, for instance language modeling with longer contexts. There is no obstacle in principle to doing this: in the trigram case, for example, we would still be determining the probability of pairs V 1 \u00d7 V 2 , but V 1 would consist of word pairs instead of single words. However, the number of possible similar events to a given element in V 1 is then much larger than in the bigram case. Direct tabulation of the events most similar to each event would thus not be practical, so more compact or approximate representations would have to be investigated. It would also be worth investigating the benefit of similarity-based methods to improve estimates for low-frequency seen events. However, we would need to replace the back-off model by another one that combines multiple estimates for the same event, for example an interpolated model with context-dependent interpolation parameters. \n\nAnother area for further investigation is the relationship between similarity-based and classbased approaches. As mentioned in the introduction, both rely on a common intuition, namely, that events can be modeled to some extent by similar events.",
            "score": 0.4112102339736181,
            "section_title": "Conclusions",
            "char_start_offset": 52064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2074
                },
                {
                    "start": 2075,
                    "end": 2278
                },
                {
                    "start": 2281,
                    "end": 2391
                },
                {
                    "start": 2392,
                    "end": 2527
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03704833984375
        },
        {
            "corpus_id": "626195",
            "title": "Natural Language Parsing as Statistical Pattern Recognition",
            "text": "unsupervised mode using algorithms from information theory.\n\nThe speech community, in particular the IBM Speech Recognition group and a company called Verbex, recognized HMMs as a solution to a critical problem in speech processing: modeling the intermediate form of speech input.\n\nAt the time, the speech recognition problem had been broken down into two steps. This first step is called the acoustic modeling problem. Here, spoken language waveforms, converted to a sequence of real-valued vectors which mathematically encode the important characteristics of the input, are translated into a sequence of phonemes, the linguistic representation for the building blocks of words. This was accomplished by a variety of rule-based methods. In the second step, the language modeling problem, these phonemes are combined to form word sequences, again using rule systems.\n\nThe critical problem with the early speech systems was the brittleness of their acoustic and language models. Composed of hand-generated rules, these models might have be adequate to a handle a single speaker using a limited-vocabulary language with a low perplexity 3 grammar, but they never scaled up to larger vocabularies and general human speech. There is no theoretical reason why a rule-based system could not be designed to solve the problem; but no system ever approached the level of coverage needed for general large-vocabulary speaker-independent speech recognition.\n\nIf researchers could not adequately encode phonetic representations by hand, HMMs offered an alternative. The speech input and the sentence output are the only givens of the problem. The intermediate representation, the phonemes, can viewed as \"hidden,\" and the whole process can be interpreted as a hidden Markov process. Using the expectation-maximization algorithm from information theory, the classes of \"phonemes\" can be discovered automatically instead of encoded by hand. In other words, information theory provides techniques which, given written and spoken versions of the same text, can generate statistical models for recognizing speech. Porting this technology to new domains and new speakers simply requires retraining the models using text from the domain read by a speaker. Eventually, algorithms were perfected to combine speech from different speakers to allow speaker-independent recognition.\n\nCertainly HMMs are not a panacea. The key issue in applying Markov models to a problem is to determine if they are Markov processes. Even if they are not, as long as the",
            "score": 0.41093367285451743,
            "section_title": "The Speech Recognition Revolution",
            "char_start_offset": 28465,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040313720703125
        },
        {
            "corpus_id": "259252117",
            "title": "Large sequence models for sequential decision-making: a survey",
            "text": "Despite recent advancements, many RL algorithms with sequence models remain domain-specific. Therefore, a unified framework capable of encompassing various RL scenarios is an area of future research that requires attention. Notably, GPT and multi-modal BeITv3 [63] have demonstrated a trend toward unifying upstream and downstream tasks and achieved remarkable results. Although UniMask [116] is trying to unify upstream and downstream tasks in RL, it still falls short in performance. Thus, the development of a unified modeling approach in sequential decision-making do-mains will continue to be a critical issue. \n\nIn the context of large-scale pre-training, effectively incorporating multi-modal knowledge of vision and language into sequential decision-making is of utmost importance. While semantic commonsense information plays a critical role in enhancing the efficiency and effectiveness of sequential decision-making for general purposes, Chat-GPT [4] achieves a remarkable breakthrough as a powerful knowledge base. However, the integration between sequential decision-making and perception modalities still lacks naturalness. For example, LM-Nav and SayCan manually design the fusion mechanism of multiple outputs from large perception models, but fail to perform joint training. While Gato performs joint training of multiple modalities, it lacks alignment between modalities in terms of tasks. It would be interesting to explore the possibility of learning an extra module to splice cross-modal large models together, such as an adapter or an inverse dynamic model. Furthermore, the emergence of in-context learning and chain-of-thought abilities [117] in large language models may be the ingredients for creating general self-improving agents without the need for supervised knowledge from humans.",
            "score": 0.41087005470647775,
            "section_title": "Algorithms",
            "char_start_offset": 58154,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1812
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.041534423828125
        },
        {
            "corpus_id": "1364780",
            "title": "Tied-Mixture Language Modeling in Continuous Space",
            "text": "Despite numerous studies demonstrating the serious short-comings of the n-gram language models, it has been surprisingly difficult to outperform n-gram language models consistently across different domains, tasks and languages. It is well-known that ngram language models are not effective in modeling long range lexical, syntactic and semantic dependencies. Nevertheless, n-gram models have been very appealing due to their simplicity; they require only a plain corpus of data to train the model. The improvements obtained by some more elaborate language models (Chelba & Jelinek, 2000;Erdogan et al., 2005) come from the explicit use of syntactic and semantic knowledge put into the annotated corpus. \n\nIn addition to the mentioned problems above, traditional n-gram language models do not lend themselves easily to rapid and effective adaptation and discriminative training. A typical n-gram model contains millions of parameters and has no structure capturing dependencies and relationships between the words beyond a limited local context. These parameters are estimated from the empirical distributions, and suffer from data sparseness. n-gram language model adaptation (to new domain, speaker, genre and language) is difficult, simply because of the large number of parameters, for which large amount of adaptation data is required. Instead of updating model parameters with an adaptation method, the typical practice is to collect some data in the target domain and build a domain specific language model. The domain specific language model is interpolated with a generic language model trained on a larger domain independent data to achieve robustness. On the other hand, rapid adaptation for acoustic modeling, using such methods as Maximum Likelihood Linear Regression (MLLR) (Legetter & Woodland, 1995), is possible using very small amount of acoustic data, thanks to the inherent structure of acoustic models that allow large degrees of parameter tying across different words (several thousand context dependent states are shared by all the words in the dictionary). Likewise, even though discriminatively trained acoustic models have been widely used, discriminatively trained languages models (Roark et al., 2007) have not widely accepted as a standard practice yet. \n\nIn this study, we present a new perspective to the language modeling. In this perspective, words are not treated as discrete entities but rather vectors of real numbers.",
            "score": 0.4105441619082048,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 702
                },
                {
                    "start": 705,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 2079
                },
                {
                    "start": 2080,
                    "end": 2281
                },
                {
                    "start": 2284,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2453
                }
            ],
            "ref_mentions": [
                {
                    "start": 563,
                    "end": 587,
                    "matchedPaperCorpusId": "14339957"
                },
                {
                    "start": 587,
                    "end": 608,
                    "matchedPaperCorpusId": "2093421"
                },
                {
                    "start": 1787,
                    "end": 1814,
                    "matchedPaperCorpusId": "14708613"
                },
                {
                    "start": 2208,
                    "end": 2228,
                    "matchedPaperCorpusId": "2093421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0279541015625
        },
        {
            "corpus_id": "52115272",
            "title": "A Unified Multilingual Handwriting Recognition System using multigrams sub-lexical units",
            "text": "We show that, whatever the test set (FR or EN), our unified system makes profit of the combination of the languages to outperform the selective approach.System WER (%) on RIMES WER (%) on IAM Our unified system 9.8 (2-multigrams) 11.2 (5-multigrams) Our specialized system 10.8 (2-multigrams) 13.5 (5-multigrams) Voigtlaender et al. (2016) 9.6 9.3 Bluche (2015) 11.8 11.9 Bluche and Messina (2017) 7.9 10.5",
            "score": 0.41045944639441756,
            "section_title": "Discussion",
            "char_start_offset": 34470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 153,
                    "end": 406
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 339,
                    "matchedPaperCorpusId": "8313780"
                },
                {
                    "start": 348,
                    "end": 361,
                    "matchedPaperCorpusId": "4764898"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026763916015625
        },
        {
            "corpus_id": "235313707",
            "title": "A Dataset and Baselines for Multilingual Reply Suggestion",
            "text": "For each language in MRS, we train and compare models in four settings. Future work can experiment with other settings (discussed in Section 6).\n\nMonolingual. Here, we simply train and test models in a single language. This setting simulates the scenario where we have adequate training data for the target language. Previous reply suggestion models were only studied in the English monolingual setting.\n\nZero-Shot. Next, we train models in a zero-shot cross-lingual setting. We train the model on the English training set and use the model on the test set for another language. This setting simulates the scenario where we want to build models for a low-resource language using our large English set.\n\nTo generalize across languages, we initialize the models with pre-trained multilingual models (details in Section 4). These models work well in other tasks (Wu and Dredze, 2019;Liang et al., 2020). We test if they also work for reply suggestion, as different tasks often prefer different multilingual representations (Zhang et al., 2020b).\n\nMachine Translation (MT). Another strategy for cross-lingual generalization is to train on machine-translated data (Banea et al., 2008). We train models on nineteen million English training examples machine-translated to the target language with Microsoft Translator. We compare against the zero-shot setting to compare the two cross-lingual generalization strategies.\n\nMultilingual. Finally, we build a multilingual model by jointly training on the five languages with the most training data: English, Spanish, German, Portuguese, and French. We oversample non-English training data to have the same number of training examples data across all languages (Johnson et al., 2017). We make two comparisons: 1) for the five training languages, we compare against the monolingual setting to test whether fitting multiple languages in a single model hurts performance; and 2) for other languages, we compare against the zero-shot setting to check if adding more training languages helps cross-lingual generalization.",
            "score": 0.41000760575370443,
            "section_title": "Training Settings",
            "char_start_offset": 10651,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 859,
                    "end": 880,
                    "matchedPaperCorpusId": "126167342"
                },
                {
                    "start": 880,
                    "end": 899,
                    "matchedPaperCorpusId": "214794966"
                },
                {
                    "start": 1020,
                    "end": 1041,
                    "matchedPaperCorpusId": "218049673"
                },
                {
                    "start": 1159,
                    "end": 1179,
                    "matchedPaperCorpusId": "2432354"
                },
                {
                    "start": 1699,
                    "end": 1721,
                    "matchedPaperCorpusId": "260464809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046630859375
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "The modules of such models can be trained separately or together, similarly to how humans can flexibly combine different cognitive skills when learning to perform novel complex tasks. \n\nThe Emergent Modularity approach involves training models end-to-end (similarly to contemporary LLMs) but allows modularity to develop naturally within the model. Modular structure has been shown to spontaneously emerge in some end-to-end neural network systems in domains other than language [e.g., Yang et al., 2019, Dobs et al., 2022], suggesting that modularity may constitute an optimal solution to many complex tasks. For this approach to be successful, the model architecture must allow individual, specialized modules to develop within the model. Transformers, the most popular architecture today, satisfy this condition to some extent by allowing different attention heads to attend to different input features [e.g. Manning et al., 2020, Vaswani et al., 2017b, Vig and Belinkov, 2019]; certain approaches promote modularization even more explicitly, e.g., by endowing transformers with a mixture-of-experts architecture [Goyal et al., 2022, Kudugunta et al., 2021, Zhou et al., 2022]. \n\nA modular language model architecture is much better aligned with the fact that real-life language use is a complex capability, requiring both language-specific knowledge (formal competence) and various non-language-specific cognitive abilities (functional competence). Whether built-in or induced to emerge, modularity can lead the models to mirror the functional organization of the human brain and, consequently, make their behavior much more humanlike.",
            "score": 0.409879892219648,
            "section_title": "Modularity",
            "char_start_offset": 78592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1639
                }
            ],
            "ref_mentions": [
                {
                    "start": 486,
                    "end": 503,
                    "matchedPaperCorpusId": "58006968"
                },
                {
                    "start": 503,
                    "end": 523,
                    "matchedPaperCorpusId": "235770756"
                },
                {
                    "start": 955,
                    "end": 980,
                    "matchedPaperCorpusId": "184486755"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127685546875
        },
        {
            "corpus_id": "269981957",
            "title": "Improving Language Models Trained on Translated Data with Continual Pre-Training and Dictionary Learning Analysis",
            "text": "Large Language Models (LLMs) have transformed the landscape of natural language processing (NLP) significantly. With the realization of un-Figure 1: The proposed TinyStories Arabic dataset is formed by translating 2M tiny stories from English to Arabic using NLLB-3B and synthesizing 20K Arabic tiny stories using Command R+ LLM. The former data is used to pre-train small language models (SLMs) with different architectures. The latter is used for continual pre-training. The models are qualitatively and quantitatively evaluated using a GPT judge. Further we train Sparse Auto-Encoder (SAE) on a selected SLM to analyze the model behavior. \n\nprecedented capabilities, the availability of underlying large training corpora and well-established language model training pipelines has shifted the focus in NLP research from defining linguistic inductive biases to the collection and curation of extensive text datasets (Soldaini et al., 2024;Penedo et al., 2024;Computer, 2023;Mehta et al., 2024). The recent trend showed an increase in focus on data curation and augmentation compared to innovation in model architecture or training paradigms (Brown et al., 2020;Touvron et al., 2023). However, emergent capabilities in LLMs are noticeable for models of very large sizes such as 10B parameters or above and most of smaller LLMs of 200M-3B parameters have shown limited abilities in reasoning, fact recall and coherent long generation (Schaeffer et al., 2024). While these models are typically trained with the same data used to train their larger and more capable siblings, the drops in capabilities of these small LLM are usually attributed to their reduced learnability or scale. \n\nHowever, newer small models have been recently shown to compete with 10x bigger models in challenging tasks. Examples include Min-iMA (Zhang et al., 2023) and the Phi family (Abdin et al., 2024;Li et al., 2023). These small language models (SLMs) offer faster training and lower deployment cost at the expense of being more task-specific and less general-use, which is not an issue for application-oriented language models.",
            "score": 0.4096941229951665,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 1142,
                    "end": 1162,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1433,
                    "end": 1457,
                    "matchedPaperCorpusId": "258418299"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "264405937",
            "title": "Enhancing Zero-Shot Crypto Sentiment With Fine-Tuned Language Model and Prompt Engineering",
            "text": "Recently, there has been a significant emphasis on pre-trained language models (PLMs) that utilize self-supervised learning Discard the response r. \n\nfiltered ). 5: end for 6: Select an instruction y \u2208 Y . 7: Let function A : (x, y) \u2192 x which augments each entry \n\nx \u2208 X with an instruction y. 8: for i \u2190 (1 . . . len(X cleaned )) do 9: \n\ncleaned , y). 10: end for 11: Return Augmented dataset X augmented on extensive raw text data [38]. Notable examples of such models include GPT-3 [22], PaLM [39], Chinchilla [40], LLaMA [23], and Falcon 40B [41]. By training on largescale texts using self-learning tasks like masked word prediction, sentence sequence recognition, text completion, and text generation [19], [42] PLMs acquire a comprehensive understanding of language. In addition, these models enhance the semantic representation of words by considering contextual dynamics and provide a unified framework for various NLP tasks. Currently, there are three standard models [43] structures in PLMs: autoregressive language models, autoencoding language models, and hybrid language models. Representative models for each design are GPT [22], BERT [20], and T5 [21], respectively. Autoregressive language models follow a standard approach where language modeling is done decoder-only, predicting words one by one through one-way Algorithm 3 Fine-tuning of a pre-trained language model. Constant random_seed, input sequence size, number of layers, number of hidden layer nodes, number of classifier outputs. \n\nInput Training set (X \n\nend for 7: \n\nloss \u2190 E(Y \n\nCalculate \u2207\u0398 for backpropagation. 9: \n\nAdjust parameters using an optimizer to minimize the loss. \n\n10: \n\nend for 11: end for 12: Return Task-specific Fine-tuned model. language encoding-decoding and token-by-token prediction of words. Autoencoding language models randomly mask words in a sentence, use bidirectional encoding to capture context, and then predict the masked words based on the encoded information. Finally, hybrid language models combine the approaches of the previous two models.",
            "score": 0.4096510814609988,
            "section_title": "C. PRE-TRAINED LANGUAGE MODELS (PLMS)",
            "char_start_offset": 18789,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 150,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1558
                },
                {
                    "start": 1561,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 711,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 978,
                    "end": 982,
                    "matchedPaperCorpusId": "258447166"
                },
                {
                    "start": 1163,
                    "end": 1167,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.093994140625
        },
        {
            "corpus_id": "269005050",
            "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
            "text": "AVIS harnesses a Large Language Model (LLM) to dynamically strategize the use of external tools and scrutinize their outputs, thereby acquiring the essential knowledge needed to respond to the questions posed.Wang et al. [40] offers a comprehensive survey of the methodologies used in the computer vision domain for large vision models and visual prompt engineering.It delves into the latest breakthroughs in visual prompt engineering and showcases influential large models in the visual domain.Zhou et al. [41] explores the applications of Vision-Language Models (VLMs) in Autonomous Driving (AD) and Intelligent Transportation Systems (ITS).It underscores the exceptional performance of these models and their capacity to leverage Large Language Models (LLMs).\n\nLarge Speech Models (LSMs) are a specialized category of Large Language Models (LLMs) that are trained on extensive speech data.Yu Zhang et al. present the Universal Speech Model (USM) [42], a unified model capable of performing automatic speech recognition (ASR) in over 100 languages.The USM is developed by pre-training the model's encoder on a substantial unlabeled multilingual dataset, comprising 12 million hours of data across more than 300 languages, and subsequently fine-tuning it on a smaller labeled dataset.The authors employ multilingual pre-training, random-projection quantization, and speech-text modality matching to achieve leading performance in multilingual ASR and speech-to-text translation tasks.Notably, the USM demonstrates comparable or superior performance to the Whisper model in both in-domain and out-of-domain speech recognition tasks across numerous languages, despite utilizing a labeled training set that is one-seventh the size.The Autonomous agents infused LSM have been a focal point of research in both academia and industry, leading to a surge in studies exploring LLM-based autonomous agents.Large Language and Speech Model (LLaSM) [43] can follow speech-and-language instructions, offering a more user-friendly and natural way for humans to interact with artificial intelligence.Fathullah et al. [44]investigate the augmentation of large language models (LLMs) with speech recognition capabilities.",
            "score": 0.40959486281975777,
            "section_title": "The Evolution of Large Language Models",
            "char_start_offset": 12040,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 495
                },
                {
                    "start": 495,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 762
                },
                {
                    "start": 764,
                    "end": 892
                },
                {
                    "start": 892,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1285
                },
                {
                    "start": 1285,
                    "end": 1485
                },
                {
                    "start": 1485,
                    "end": 1729
                },
                {
                    "start": 1729,
                    "end": 1898
                },
                {
                    "start": 1898,
                    "end": 2086
                },
                {
                    "start": 2086,
                    "end": 2205
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "259211816",
            "title": "Investigating Pre-trained Language Models on Cross-Domain Datasets, a Step Closer to General AI",
            "text": "Transfer learning with transformers started by training models like T5 [4], BART [5], BERT [6], GPT [7], and Roberta [11] in an unsupervised manner on large corpora and fine-tuning them on downstream tasks. It was found that training on large corpora enables the network to learn language representations that are extremely useful in downstream language tasks such as spam detection [13], sentiment analysis [14], translation [4], etc. \n\nSimilarly, in the vision domain, transformers are trained on large datasets such as ImageNet-21k [12] or JFT-300M. Models like ViT [2] and GPT pixels [15] are then fine-tuned on downstream vision tasks and they achieve state-ofthe-art performance. Thus, the standard practice is to pre-train and then fine-tune models on tasks from the same domain. In this paper, we investigate if we can find universal pre-training data that is useful on various tasks. Thus, we investigate whether pre-training on language help the models to develop general knowledge by fine-tuning them on datasets from different domains. \n\nThis work was inspired by [16] where they use T5, a pre-trained language model, to sum two 60-digit numbers. They fine-tune a pre-trained language model on an arithmetic task. However, their goal was to study the importance of the surface representation of the input. They re-represent the input and show that explicitly representing the decimal place of digits in long numbers helps the model to do the sum correctly. They did not compare their results with trained from scratch transformers, but their work inspired us to use T5 on the arithmetic dataset Long Listops and furthermore CIFAR10-LRA and Remote Homology. \n\nOur work is similar to prior work investigating unified models for multi-task learning [17,18]. Multi-task learning is the approach of training a single model that can perform well on multiple tasks at once. Our work focuses on investigating the effect of pre-training on non-language tasks, whereas the previous efforts try to build architectures to solve multiple tasks.",
            "score": 0.4094033472816846,
            "section_title": "Related Work",
            "char_start_offset": 4889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1317
                },
                {
                    "start": 1318,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 71,
                    "end": 74,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "209088210"
                },
                {
                    "start": 426,
                    "end": 429,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "57246310"
                },
                {
                    "start": 588,
                    "end": 592,
                    "matchedPaperCorpusId": "219781060"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "237204499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.173583984375
        },
        {
            "corpus_id": "218971783",
            "title": "Language Models are Few-Shot Learners",
            "text": "Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the training distribution. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions. For instance [HLW + 20] observe that larger models do not necessarily generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm can be poor because the model is overly specific to the training distribution and does not generalize well outside it [YdC + 19, MPL19]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at human-level, may exaggerate actual performance on the underlying task [GSL + 18, NK19].\n\nThird, humans do not require large supervised datasets to learn most language tasks -a brief directive in natural language (e.g. \"please tell me if this sentence describes something happy or something sad\") or at most a tiny number of demonstrations (e.g. \"here are two examples of people acting brave; please give a third example of bravery\") is often Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task. We use the term \"in-context learning\" to describe the inner loop of this process, which occurs within the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded within a single sequence. Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper \"in-context learning curves\" for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a",
            "score": 0.40931789983980316,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.051177978515625
        },
        {
            "corpus_id": "253237346",
            "title": "DuDe: Dual-Decoder Multilingual ASR for Indian Languages using Common Label Set",
            "text": "India is a country with great linguistic diversity having 22 constitutionally recognized languages. So, it becomes important to build Automatic Speech Recognition (ASR) systems for the regional languages in India. Most of the Indian languages are considered to be low resource since the amount of labelled speech data available is less. This makes it challenging to build robust ASR systems for Indian languages. In recent times, there has been increased interest in building multilingual ASR systems [1,2,3,4,5,6] for several reasons. Firstly, a multilingual model is easy to maintain since it replaces multiple monolingual models with a single model. Secondly, the multilingual models benefit from more data obtained by combining various languages. Traditionally, in machine learning and deep learning, more data helps in better parameter estimation leading to better performance. Multilingual models attempt to exploit the same idea and achieve better performance through parameter sharing across languages. *These authors have contributed equally. \n\nUnlike other language families like Latin languages, different Indian languages use different scripts for writing. This makes it more challenging to build multilingual ASR systems for Indian languages. More specifically, this hinders the models from learning the common sounds across the languages. Despite different writing scripts, Indian languages share a rich amount of common sounds. Motivated by this, a common label set for Indian languages was proposed for speech synthesis [7]. A rule-based unified parser for the conversion of native language text to phoneme-based CLS was proposed in [8]. CLS was shown to work well for TTS in [7]. In [3] and [2], CLS was shown to give some improvement for ASR. [2] and [9] used a very similar idea to CLS in which they used transliterated text across languages to train multilingual ASR systems. \n\nThis work explores various approaches including languagedependent and language independent multilingual models. This work mainly focuses on using the Common Label Set (CLS) for building multilingual ASR systems. This work also proposes a novel architecture for building a language independent multilingual ASR system that might pave way for future research in this area. \n\nThe rest of the paper is organized as follows. Section 2 briefs the dataset used in this work.",
            "score": 0.4091411348425499,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 412
                },
                {
                    "start": 413,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1051
                },
                {
                    "start": 1054,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2268
                },
                {
                    "start": 2271,
                    "end": 2317
                },
                {
                    "start": 2318,
                    "end": 2365
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "202565562"
                },
                {
                    "start": 504,
                    "end": 506,
                    "matchedPaperCorpusId": "216036028"
                },
                {
                    "start": 506,
                    "end": 508,
                    "matchedPaperCorpusId": "235313964"
                },
                {
                    "start": 508,
                    "end": 510,
                    "matchedPaperCorpusId": "239705866"
                },
                {
                    "start": 510,
                    "end": 512,
                    "matchedPaperCorpusId": "239652639"
                },
                {
                    "start": 512,
                    "end": 514,
                    "matchedPaperCorpusId": "235780586"
                },
                {
                    "start": 1536,
                    "end": 1539,
                    "matchedPaperCorpusId": "37378685"
                },
                {
                    "start": 1692,
                    "end": 1695,
                    "matchedPaperCorpusId": "37378685"
                },
                {
                    "start": 1700,
                    "end": 1703,
                    "matchedPaperCorpusId": "235313964"
                },
                {
                    "start": 1708,
                    "end": 1711,
                    "matchedPaperCorpusId": "216036028"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "216036028"
                },
                {
                    "start": 1769,
                    "end": 1772,
                    "matchedPaperCorpusId": "226202625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0416259765625
        },
        {
            "corpus_id": "261100979",
            "title": "Sparks of Large Audio Models: A Survey and Outlook",
            "text": "Investigations reveal that the act of scaling pre-trained language models (PLMs), either through enhancing the model size or expanding the data size, typically yields superior model performance on subsequent tasks, adhering to what is known as the scaling law [89]. Numerous investigations have probed the limits of performance by training increasingly larger PLMs, such as the GPT-3 model with 175 billion parameters and the PaLM model with 540 billion parameters. While the majority of scaling endeavours primarily focus on model size (preserving similar architectures and pre-training tasks), these expanded PLMs exhibit distinct characteristics compared to their smaller counterparts, such as BERT with 330 million parameters and GPT-2 with 1.5 billion parameters. They exhibit unexpected proficiency, referred to as emergent abilities, in tackling a variety of intricate tasks. For example, GPT-3 has demonstrated the ability to address few-shot tasks via in-context learning, a feat that GPT-2 struggles with. Hence, the term \"large language models (LLMs)\" has been coined by the research community to describe these enlarged PLMs, and these models have garnered increasing interest. A notable example of an LLM application is ChatGPT, which adapts the GPT series LLMs for dialogue, showcasing exceptional conversational capabilities with humans. A significant surge in arXiv papers pertaining to LLMs can be observed following the launch of ChatGPT. Recently, GPT-4 [90] has been developed, which is a largescale multimodal model that can accept image and text as input and produce text outputs. GPT-4 is capable of achieving human-level performance on some professional and academic benchmarks, including achieving a score around the top 10% of test-takers in a simulated bar exam. Various other multimodal large language models are proposed by utilising multimodal information including visual, audio, and text. These LLMs are considered a crucial step towards Artificial General Intelligence (AGI). Most importantly, Large Audio Models (see Figure 3) attract significant interest from the research community to build LLMs that have intrinsic crossmodal conversational abilities and are capable of perceiving and generating audio or multimodal content. We also show a brief timeline for Large Audio Models in Figure 4.",
            "score": 0.4091234615645377,
            "section_title": "Overview of Large Language Models",
            "char_start_offset": 11000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054901123046875
        },
        {
            "corpus_id": "261076203",
            "title": "From Instructions to Intrinsic Human Values - A Survey of Alignment Goals for Big Models",
            "text": "Big Models, also known as Foundation Models (Bommasani et al., 2021), usually refer to those pre-trained on vast data and containing tens of billions of parameters. The most predominant examples include Large Language Models (LLMs), e.g., GPT-3 (Brown et al., 2020), ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023) and so on (Touvron et al., 2023a,b;Zhang et al., 2022;Scao et al., 2022), as well as Large Multimodal Models (LMMs). Big models possess two features distinguished from general Pretrained Language Models (PLMs): 1) scaling law (Kaplan et al., 2020), where they exhibit significantly better performance with the increase of model sizes; and 2) emergent abilities (Wei et al., 2022a), where special abilities absent in smaller models have emerged, such as incontext learning (Brown et al., 2020) and complex reasoning (Wei et al., 2022a). \n\nCurrent LLMs demonstrate human-like or even human-surpassing capabilities across a variety of tasks (Bubeck et al., 2023). However, 'opportunities and risks always go hand in hand', challenges and risks emerge when applying big models. On the one hand, these models sometimes struggle to understand and follow diverse user instructions (Tamkin et al., 2021;Kenton et al., 2021). On the other hand, big models could generate responses that conflict with human preferences, such as discrimination and harmful messages, eliciting potential social risks (Weidinger et al., 2021;Bommasani et al., 2021). Moerover, these risks exhibit two features accompanying the abilities: 1) emergent risks (Wei et al., 2022a), where unanticipated problems appeared; and 2) inverse scaling law (McKenzie et al., 2023), where some risks do not disappear but become more serious with increased model sizes. This implies that big models could potentially raise greater risks.",
            "score": 0.4089668287246213,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1810
                }
            ],
            "ref_mentions": [
                {
                    "start": 245,
                    "end": 265,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 275,
                    "end": 296,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 791,
                    "end": 811,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05718994140625
        },
        {
            "corpus_id": "56976",
            "title": "Compiling Language Models from a Linguistically Motivated Unification Grammar",
            "text": "Gemini accepts a slightly constrained version of the unification grammar formalism originally used in the Core Language Engine (Alshawi 1992), and compiles it into context-free grammars in the GSL formalism supported by the Nuance Toolkit. The Nuance Toolkit compiles GSL grammars into sets of probabilistic finite state graphs (PFSGs), which form the final language model. \n\nThe relative success of the Gemini system suggests a new question. Unification grammars have been used many times to build substantial general grammars for English and other natural languages, but the language model oriented grammars so far developed for Gemini (including the one for CommandTalk) have all been domain-specific. One naturally wonders how feasible it is to take yet another step in the direction of increased generality; roughly, what we want to do is start with a completely general, linguistically motivated grammar, combine it with a domain-specific lexicon, and compile the result down to a domain-specific contextfree grammar that can be used as a language model. If this programme can be realized, it is easy to believe that the result would be an extremely useful methodology for rapid construction of language models. It is important to note that there are no obvious theoretical obstacles in our way. The claim that English is contextfree has been respectable since at least the early 80s (Pullum and Gazdar 1982) 1 , and the idea of using unification grammar as a compact way of representing an underlying context-free language is one of the main motivations for GPSG (Gazdar et al 1985) and other formalisms based on it. The real question is whether the goal is practically achievable, given the resource limitations of current technology. \n\nIn this paper, we describe work aimed at the target outlined above, in which we used the Gemini system (described in more detail in Section 2) to attempt to compile a variety of linguistically principled unification grammars into language models. Our first experiments (Section 3) were performed on a large pre-existing unification grammar. These were unsuccessful, for reasons that were not entirely obvious; in order to investigate the problem more systematically, we then conducted a second series of experiments (Section 4), in which we incrementally built up a smaller grammar.",
            "score": 0.40890479073446206,
            "section_title": "Introduction",
            "char_start_offset": 2234,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 1390,
                    "end": 1414,
                    "matchedPaperCorpusId": "62240145"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05975341796875
        },
        {
            "corpus_id": "255941592",
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "text": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --\"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.",
            "score": 0.40886609106813737,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0343017578125
        },
        {
            "corpus_id": "249674893",
            "title": "Director: Generator-Classifiers For Supervised Language Modeling",
            "text": "We thus propose DIRECTOR that unifies language modeling and classification into a single model. This allows the model to be efficiently trained on both unlabeled data D LM and supervised data D class . \n\nThen during inference time, we can generate conditioned on the desired attributes (positive class labels). \n\nAs shown in Figure 1, input tokens are first processed by a shared autoregressive core, for which we used a transformer decoder in our experiments. Then those processed token representations are fed to two separate heads. The first is a standard LM head that is comprised of a linear layer followed by a softmax to output a multinomial distribution over the vocabulary V . This LM head is trained by optimizing loss L LM from Equation 1. \n\nThe second head is for classification and it also maps each token representation into a |V | dimensional vector using a linear layer. Then, however, it applies a sigmoid to obtain an independent binomial distribution2 for each word in the vocabulary V . Note that while tokens x 1:t\u22121 are given as inputs and processed by the shared transformer core, the next token candidates for x t are encoded in the row vectors of the linear layer in the classifier head. This classifier head optimizes loss L class from Equation 3 on samples from D class . \n\nThe final joint loss function is \n\nwhere \u03b3 is a hyperparameter weighting the classification loss. In practice, we alternatively sample a batch from D LM or D class and optimize the corresponding loss with backpropagation through the whole model. \n\nTo generate a sequence conditioned on a certain class c according to Equation 2, we combine the outputs from the two heads to compute the probability of the next token \n\nwhere Z normalizes the total probability to be 1. We can also adjust parameter \u03b3 at inference time to alter the weight of the classifier compared to the language model head, where \u03b3 = 0 reverts to standard language modeling. During generation, tokens are produced left-to-right in the same manner as standard language models. The unified architecture of DIRECTOR has three features that make it efficient: \n\n1.",
            "score": 0.40885372889689164,
            "section_title": "DIRECTOR Language Model",
            "char_start_offset": 9397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 201
                },
                {
                    "start": 204,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1398
                },
                {
                    "start": 1399,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1716
                },
                {
                    "start": 1719,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2124
                },
                {
                    "start": 2127,
                    "end": 2129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06475830078125
        },
        {
            "corpus_id": "273963067",
            "title": "Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation",
            "text": "Large language models (LLMs) have revolutionized natural language processing (NLP) by demonstrating exceptional capabilities in a wide range of tasks, from text generation to translation and sentiment analysis 18,[29][30][31] . Models such as GPT-4 and Llama 3 have been termed as 'foundational models' due to their ability to generalize to multiple tasks by the process of fine-tuning on specialized datasets. 3,24 otein sequences are analogous to natural language because both are composed of a set of basic building blocks-amino acids for proteins and words for language-that combine to form meaningful structures. 20,25,28 In proteins, the sequence of amino acids determines the three-dimensional structure and function, whereas in natural language sentences, the sequence of letters and words determine the semantic meaning and local context. These similarities have prompted the application of large language models to various tasks like structure prediction from protein sequence 21 , de-novo protein sequence generation 10 , also termed as uncontrollable generation, and generation of proteins which possess certain properties or are specialized for a certain task, termed as controllable generation. Protein design finds its application in drug development and in the development of efficient artificial enzymes that break down industrial waste or plastics, contributing to carbon neutrality 13 . Figure 1: Training pipeline -Llama 3 and Phi 3 models were trained in two stages. First, they were trained on protein sequences and then on protein-property pairs. LoRA was used to lower the training cost. \n\nRecently there has been a drive towards the development of unified protein foundational models, similar to its counterparts in natural language, which possess an understanding of protein structure, sequence and they way they relate to each other. Such models have demonstrated the creation of novel proteins, effectively speeding up the evolutionary process 11 . However, current protein foundational models are often extremely large in size and are generally closed source, which presents several challenges. Their vast size necessitates substantial computational resources for both training and inference, curbing their accessibility for many researchers and organizations. Additionally, the extensive computational power required for these large models leads to increased energy consumption, contributing to a higher environmental impact. \n\nAs a result, there is a growing need for more efficient protein models that maintain high performance while being computationally economical.",
            "score": 0.4087100506250599,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2289
                },
                {
                    "start": 2290,
                    "end": 2455
                },
                {
                    "start": 2458,
                    "end": 2599
                }
            ],
            "ref_mentions": [
                {
                    "start": 618,
                    "end": 621,
                    "matchedPaperCorpusId": "233380913"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "265416712"
                },
                {
                    "start": 624,
                    "end": 626,
                    "matchedPaperCorpusId": "128342395"
                },
                {
                    "start": 987,
                    "end": 989,
                    "matchedPaperCorpusId": "210221987"
                },
                {
                    "start": 1028,
                    "end": 1030,
                    "matchedPaperCorpusId": "247439606"
                },
                {
                    "start": 1972,
                    "end": 1974,
                    "matchedPaperCorpusId": "270963342"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10858154296875
        },
        {
            "corpus_id": "253420301",
            "title": "Local Structure Matters Most in Most Languages",
            "text": "A recent research trend has explored the sensitivity, or insensitivity, of neural language models to different perturbations of texts (Pham et al., 2021;Sinha et al., 2020Sinha et al., , 2021;;Gupta et al., 2021;O'Connor and Andreas, 2021;Taktasheva et al., 2021;Clouatre et al., 2022). Their findings may be central in directing future NLP research by providing insight into which coding property (Kulmizev and Nivre, 2021) of language are most valuable to performing Natural Language Understanding (NLU) tasks. As research in English NLP tends to be adapted to other languages, such as through single language adaptation of BERT-style models (Devlin et al., 2019;Cui et al., 2019;Le et al., 2019;Martin et al., 2019;Antoun et al., 2020;Carmo et al., 2020;de Vries et al., 2019;Malmsten et al., 2020;Polignano et al., 2019;Nguyen and Tuan Nguyen, 2020) or multilingual adaptations of the same architecture (Lample and Conneau, 2019;Clark et al., 2021;Xue et al., 2020Xue et al., , 2021;;Liu et al., 2020;Devlin et al., 2019), it is vital that we verify how insights derived from the English language generalize to other languages. \n\nOne such coding property, the local structure of text, has recently been shown to be ubiquitously relied upon by both neural language models (Clouatre et al., 2022) and humans (Mollica et al., 2020) to understand text in English. The global structure of text only sometimes being necessary for a model to perform NLU tasks (Clouatre et al., 2022). Such results motivate hierarchical approaches to neural language model development, where one would first build meaning locally and then reason over the global context if necessary. However, we must verify that the importance of that coding property is not merely an artifact of the English language. \n\nIn this short paper, our contributions are as follows:",
            "score": 0.4084401760183454,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 153,
                    "matchedPaperCorpusId": "229923132"
                },
                {
                    "start": 212,
                    "end": 239,
                    "matchedPaperCorpusId": "235446334"
                },
                {
                    "start": 263,
                    "end": 285,
                    "matchedPaperCorpusId": "247839905"
                },
                {
                    "start": 644,
                    "end": 665,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 698,
                    "end": 718,
                    "matchedPaperCorpusId": "207853304"
                },
                {
                    "start": 801,
                    "end": 824,
                    "matchedPaperCorpusId": "229923132"
                },
                {
                    "start": 824,
                    "end": 853,
                    "matchedPaperCorpusId": "211677475"
                },
                {
                    "start": 1005,
                    "end": 1025,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1275,
                    "end": 1298,
                    "matchedPaperCorpusId": "247839905"
                },
                {
                    "start": 1310,
                    "end": 1332,
                    "matchedPaperCorpusId": "210181450"
                },
                {
                    "start": 1457,
                    "end": 1480,
                    "matchedPaperCorpusId": "247839905"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0330810546875
        },
        {
            "corpus_id": "271569176",
            "title": "The changing landscape of text mining: a review of approaches for ecology and evolution",
            "text": "If only a small set of labelled data are available, it may not be possible to train a deep-learning model from scratch. An alternative approach is to fine-tune existing large language models (trained either on general or scientific texts) to a target task via transfer learning. This process reduces the need for labelled data as it uses fewer examples than would be needed to train a full language model. Through transfer learning a variety of NLP tasks could feasibly be performed by a single neural language model. Thus, LLMs allow the flexible development of custom pipelines without the specialized linguistic knowledge required to build traditional NLP tools. While there exist pre-trained language models designed to work with scientific literature, they are either explicitly biomedical focused, such as bioBERT [53] and PubMedBERT [54], or the majority of the training corpus is from the biomedical domain, e.g. scispaCy [14] and sciBERT [51]. The development of language models specifically for ecology and evolution, or disciplines that bridge ecology and biomedical sciences, such as veterinary medicine, could foreseeably be achieved by biomedical or more generalized language models as a foundation. A recent example is TaxoNERD [16], a model for taxonomic named entity recognition trained with transfer learning and updating of the bioBERT biomedical language model. While transfer learning offers a viable avenue for adapting LLMs for tasks in ecology and evolution, it requires the curation of labelled training data specific for each task.",
            "score": 0.40810834489852155,
            "section_title": "(b) Transfer learning: adapting existing language models",
            "char_start_offset": 25588,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1557
                }
            ],
            "ref_mentions": [
                {
                    "start": 820,
                    "end": 824,
                    "matchedPaperCorpusId": "59291975"
                },
                {
                    "start": 840,
                    "end": 844,
                    "matchedPaperCorpusId": "220919723"
                },
                {
                    "start": 930,
                    "end": 934,
                    "matchedPaperCorpusId": "67788603"
                },
                {
                    "start": 947,
                    "end": 951,
                    "matchedPaperCorpusId": "202558505"
                },
                {
                    "start": 1243,
                    "end": 1247,
                    "matchedPaperCorpusId": "235405900"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0872802734375
        },
        {
            "corpus_id": "204509025",
            "title": "Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment",
            "text": "The performance of models for core NLP problems heavily rely on the availability of large amounts of high-quality training data. Word embedding, language modeling or text classification are tasks that all benefit from training at scale. Pretrained models optimized on massive corpora are readily available for most tasks, and are often used in downstream applications. For example, popular pre-trained word vectors like word2vec or fastText are trained for weeks on a large mix of web data and serve as building blocks many NLP applications. In a similarly way text classifiers are trained offline on a fixed and large labeled training set before being shipped to an application. A good example is the fastText language identifier1 that was trained on data from Tatoeba2 and can serve as a component of a larger system. However, such pre-trained models are not without their flaws. First, these general models intended for a broad range of applications suffer from the lack of specialization. Indeed, despite their size, large web data such as Common Crawl lack coverage for highly technical expert fields such as medicine or law. Second, many applications rely on the temporal aspect of training and test data as the language distribution can drastically change over time. New words may appear in the vocabulary, new named entities gain sudden importance and new trends are rapidly emerging. Because of that, in many situations generalpurpose pre-trained models require adaptation to fit the distribution of the task at hand. \n\nThe simplest solution to adapt a model to specialized data is to retrain the model from scratch on the relevant data. However, that is not always possible as it would require: (i) having access to the large dataset that was used for pre-training, (ii) retaining the data history and processing an evergrowing dataset. Another approach to adapt the model is to fine-tune it on new data to fit the new distribution. This solution is technically challenging, as one has to carefully select relevant hyper parameters. Moreoever, even when carried out carefully, it leads to a loss of important statistics gathered on the original large dataset. \n\nIn this work, we propose a simple method allowing to combine a pre-trained model with a model trained on the new data.",
            "score": 0.40781678636550955,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 881
                },
                {
                    "start": 882,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1526
                },
                {
                    "start": 1529,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2169
                },
                {
                    "start": 2172,
                    "end": 2290
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1466064453125
        },
        {
            "corpus_id": "2369736",
            "title": "Acquiring Disambiguation Rules From Text",
            "text": "Thus for example, Klein and Simmons (1963) and Greene and Rubin (1971) report success rates considerably below recent stochastic approaches. \n\nIt is tempting to conclude from this contrast that robust handling of unrestricted text demands general probabilistic methods in preference to deeper linguistic knowledge. The Lancaster (UCREL) group explicitly takes this position, suggesting: \"... if we analyse quantitatively a sufficiently large amount of language data, we will be able to compensate for the computer's lack of sophisticated knowledge and powers of inference, at least to a considerable extent.\" (Garside, Leech and Sampson 1987:3). \n\nIn this paper, I want to emphasize a somewhat different view of the role of large text corpora in building robust models of natural language. In particular, I will show that that large corpora of naturally occurring text can be used together with the rule-based syntactic analyzers we have today -to build more effective linguistic analyzers. As the information derived from text is incorporated into our models, it will help increase the sophistication of our linguistic models. I suggest that in order to move from our current impoverished natural language processing systems to more comprehensive and robust linguistic models we must ask Can we acquire the linguistic information needed on the basis of tezt? \n\nIf we can answer this question aff~matively -and this paper presents evidence that we can -then there is hope that we can make some progress in constructing more adequate natural language processing systems. \n\nIt is important to emphasize that the question whether we can acquire linguistic information from text is independent of whether the model is probabilistic, categorical, or some combination of the two. The issue is not, I believe, symbolic versus probabilistic rules, but rather whether we can acquire the necessary linguistic information instead of building systems completely by hand. No algorithm~ symbolic or otherwise, will succeed in large scale processing of natural text unless it can acquire some of the needed knowledge from sampies of naturally occurring text.",
            "score": 0.40765573270567157,
            "section_title": "Categorial Ambiguity",
            "char_start_offset": 4018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 143,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2143
                }
            ],
            "ref_mentions": [
                {
                    "start": 18,
                    "end": 42,
                    "matchedPaperCorpusId": "826222"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.028594970703125
        },
        {
            "corpus_id": "258865522",
            "title": "An Efficient Multilingual Language Model Compression through Vocabulary Trimming",
            "text": "Multilingual language model (LM) pre-training (Devlin et al., 2019;Conneau et al., 2019;Liu et al., 2020;Xue et al., 2021) has been shown to be an efficient mechanism to store information from many languages into a single model, without the need for training multiple language-specific models. Moreover, it has been proven reliable for cross-lingual tasks (Pires et al., 2019;Conneau and Lample, 2019) and can provide competitive performance in most settings, generally similar to its monolingual counterparts (Goyal et al., 2021), while being generally less affected by culturally-dependant biases (Ahn and Oh, 2021). Similarly to monolingual models, multilingual LMs can be used for zero/fewshot learning (Scao et al., 2022) by increasing the model size and, more frequently, can be specialized to different tasks by fine-tuning to specific data. In practice, however, there are a few practical issues when training multilingual LM such as the curse of multilinguality (Conneau et al., 2019;Pfeiffer et al., 2022), a trade-off between the number of languages and individual performance in a single language, or the multilingual vocabulary construction, which requires a careful design for better generalization (Chung et al., 2020;Zheng et al., 2021;Liang et al., 2023). \n\nBesides such generalization concerns, multilingual LMs usually consist of larger parameters than their monolingual counterparts due to the need for a large vocabulary covering multiple languages. This becomes an important issue in practice when the resources to host models are limited. For instance, while using the same configuration (i.e., same number of layers and hidden units), the parameter size of T5 SMALL (Raffel et al., 2020) and mT5 SMALL (Xue et al., 2021) are 140M and 300M, respectively. This is only due to their difference in vocabulary size, with T5 being 50k and mT5, 250k. In fact, the embedding matrix stemming from the LM vocabulary can occupy a large portion of the parameter space.",
            "score": 0.40744845852121614,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1272
                },
                {
                    "start": 1275,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1980
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 67,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 356,
                    "end": 376,
                    "matchedPaperCorpusId": "174798142"
                },
                {
                    "start": 376,
                    "end": 401,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 510,
                    "end": 530,
                    "matchedPaperCorpusId": "233481097"
                },
                {
                    "start": 599,
                    "end": 617,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 993,
                    "end": 1015,
                    "matchedPaperCorpusId": "248721770"
                },
                {
                    "start": 1213,
                    "end": 1233,
                    "matchedPaperCorpusId": "222327390"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12646484375
        },
        {
            "corpus_id": "248240093",
            "title": "CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex",
            "text": "CodexDB is enabled by recent advances in the domain of natural language processing. Those advances have been fuelled by two key ideas: a novel neural network architecture, the Transformer [23], and new training paradigms, implementing the idea of transfer learning [15]. The Transformer is nowadays the dominant architecture in the domain of language processing [26]. Among other advantages, it lends itself better to parallelization than prior methods. This has, in part, enabled the creation of very large, pre-trained language models. Such models are pre-trained on tasks for which large amounts of training data are easily available, e.g. predicting the next word in text snippets. While pre-training is very expensive, the resulting models can be easily specialized for new tasks via different methods. Fine-tuning describes a process in which pre-trained models are used as a starting point for further training on more specialized tasks (reducing the amount of training samples and computational overheads by orders of magnitude via pre-training [4]). Until recently, fine-tuning has been the primary method of exploiting pre-trained language models. The latest generation of pre-trained models, most notably OpenAI's Generative Pre-Trained Transformer (GPT) version 3, unlocks new possibilities. It turns out that sufficiently large models can oftentimes solve new tasks without specialized training (\"zero-shot learning\"), based on inputs describing the task in natural language alone [1]. Precision increases if the input integrates few (i.e., typically less than ten) examples pairing tasks of the same type with solutions (\"few-shot learning\"). This is the method currently used by CodexDB. The final development that enabled this paper is the emergence of the Codex variant of GPT-3 [2,14]. The primary difference between GPT-3 Codex and the original GPT-3 model lies in the data used for pre-training. GPT-3 Codex is trained on code and technical documentation. This results in a model whose primary use case is the translation of natural language commands into code. \n\nCodexDB connects to prior work on natural language interfaces in the database community [9,16,25].",
            "score": 0.40710422276561176,
            "section_title": "BACKGROUND AND RELATED WORK",
            "char_start_offset": 4292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2081
                },
                {
                    "start": 2084,
                    "end": 2182
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 265,
                    "end": 269,
                    "matchedPaperCorpusId": "186206211"
                },
                {
                    "start": 362,
                    "end": 366,
                    "matchedPaperCorpusId": "269498086"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1527099609375
        },
        {
            "corpus_id": "249674493",
            "title": "A Unified Sequence Interface for Vision Tasks",
            "text": "Training a single neural network model capable of performing myriad tasks is a major step towards artificial general intelligence. In recent years, with the rise of big language models [34,35,2] using Transformers [41], many different language and related tasks are unified under a single modeling framework, where a language model is trained to predict the solution (in text tokens) given a prompt of a task description (also in text tokens). This is only possible because these tasks (both task description and solution) can be expressed in the same, rich language interface. This can be naturally extended to some vision tasks such as image captioning or visual question answering where the solution is given in natural language, but the majority of \"core\" computer vision tasks have diverse outputs that are not readily expressed in terms of natural language. The object detection task produces a set of bounding boxes and their corresponding class labels, often associated with scores for ranking. The output for instance segmentation is a set of segmentation masks corresponding to image regions. The output of keypoint detection is a set of keypoints in an image. As such, existing methods [13,37,15,28,4,15] have developed specialized architectures and sophisticated loss functions for each of these complex tasks. \n\nAn ambitious goal, in the pursuit of artificial general intelligence, is a simple interface that allows one to express seemingly disparate vision tasks in a unified framework. This would simplify the design of architectures and loss functions for new tasks. It would enable greater degrees of feature/representation sharing across many different tasks, thereby avoiding the need for a sophisticated  output head for each task. It would also facilitate adapting of existing models to new tasks, and potentially unlock new capabilities with zero or few demonstrations. \n\nTo this end, we propose an approach to unify four seemingly different vision tasks in a single pixel-to-sequence interface. In effect, this is an extension of Pix2Seq [7] for object detection to a broader set of tasks. As a proof of concept, we focus on four core vision tasks, namely, object detection, instance segmentation, human keypoint detection, and image captioning. We first show how to unify these tasks into a single shared interface, and then train a neural network with a shared architecture and objective function.",
            "score": 0.40698875490350983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2017
                },
                {
                    "start": 2018,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2268
                },
                {
                    "start": 2269,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 189,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 214,
                    "end": 218,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 1197,
                    "end": 1201,
                    "matchedPaperCorpusId": "206770307"
                },
                {
                    "start": 1201,
                    "end": 1204,
                    "matchedPaperCorpusId": "10328909"
                },
                {
                    "start": 1204,
                    "end": 1207,
                    "matchedPaperCorpusId": "54465873"
                },
                {
                    "start": 1207,
                    "end": 1210,
                    "matchedPaperCorpusId": "47252984"
                },
                {
                    "start": 1210,
                    "end": 1212,
                    "matchedPaperCorpusId": "218889832"
                },
                {
                    "start": 1212,
                    "end": 1215,
                    "matchedPaperCorpusId": "54465873"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27783203125
        },
        {
            "corpus_id": "271963310",
            "title": "Platypus: A Generalized Specialist Model for Reading Text in Various Forms",
            "text": "Moreover, the emergence of Multi-modal Large Language Models(MLLMs) has contributed a new dimension to the field by offering models with broad text reading capabilities [6,36,66,83], albeit with the trade-off of computational efficiency and specialized precision [45]. \n\nIn response to these challenges, we present Platypus, a novel generalized specialist model for text reading that offers a comprehensive solution for diverse text reading scenarios. Platypus embodies the amalgamation of high-precision specialized text reading models with the wide-ranging adaptability of multimodal approaches. The name Platypus is inspired by the animal known for its unique combination of traits; similarly, our Platypus model harmonizes the specificity of specialized models with the versatility of generalist approaches, making it adept at tackling the heterogeneity of text reading tasks. Furthermore, Fig. 2 contrasts the integrated approach of Platypus with traditional OCR systems, underscoring its comprehensive and adaptable nature for handling a multitude of text reading tasks. \n\nOur contributions are manifold: \n\n-We introduce Platypus, a single unified architecture adept at various text reading tasks, serving as a versatile and unified text reading solution. -Platypus features a high degree of interactivity, providing users with the ability to specify areas for text recognition and select output granularity, enhancing usability and precision. -A comprehensive text reading dataset, Worms, is curated and presented, supporting the extensive training and evaluation of Platypus. -Platypus surpasses specialized text reading models and MLLMs in multiple text reading scenarios, establishing new state-of-the-art performances.",
            "score": 0.4069454404977587,
            "section_title": "Introduction",
            "char_start_offset": 2662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 271,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 880
                },
                {
                    "start": 881,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1110
                },
                {
                    "start": 1113,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1729
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "248888567",
            "title": "An Analysis of French-Language Tweets About COVID-19 Vaccines: Supervised Learning Approach",
            "text": "The CamemBERT model was released in 2020 and is considered one of the state-of-the-art French-language models [18] (together with its close \"cousin\" flauBERT [19]). It makes use of the Robustly Optimized BERT Pretraining Approach architecture of Liu et al [20], which is an improved variant of the famous Bidirectional Encoder Representations From Transformers (BERT) architecture of Devlin et al [21]. The BERT family of models consists of general, multipurpose, pretrained models that may be used for different NLP tasks, including the following: classification, question answering, and translation. They rely heavily upon transformers, which have radically changed the performance of NLP tasks since their introduction by Google researchers in 2017 [22]. They have been pretrained on a large corpus ranging from gigabits to terabits of data, using considerable computing resources. \n\nAlthough multilingual models are plentiful, they usually lag behind their monolingual counterparts. This is why, in this study, we chose to employ a monolingual model to classify French-language tweets. As far as we are concerned, CamemBERT comes in 6 different \"flavors,\" ranging from small models with 110 million parameters trained on 4 GB of text up to mid-size models with 335 million parameters trained on 135 GB of text. After testing them, we found that better results were obtained with the largest size model that was pretrained on the Criss-Cross Network corpus. \n\nAll these models require fine-tuning on specific data to achieve their full potential. Fine-tuning or transfer learning have been common and successful practices in computer vision for a long time, but it is only in the last 3 years or so that the same approaches have become effective for solving NLP problems on specific data. This approach can be summarized in the following 3 steps: 1. A model language such as BERT is built in an unsupervised manner using a large database, removing the need to label data. 2 . A specific head (such as dense neural network layers) is added to the previous model to make it task-specific. 3 . The new model is trained in its entirety with a small learning rate on specific data.",
            "score": 0.4068726716590531,
            "section_title": "A State-of-the-art French-Language Model",
            "char_start_offset": 2872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 884
                },
                {
                    "start": 887,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2179
                }
            ],
            "ref_mentions": [
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "207853304"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "209202658"
                },
                {
                    "start": 397,
                    "end": 401,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09893798828125
        },
        {
            "corpus_id": "268876017",
            "title": "LM2: A Simple Society of Language Models Solves Complex Reasoning",
            "text": "Recent trends in solving complex reasoning tasks using Large Language Models (LLMs) typically follow two different dominant approaches: (i) well-curated prompting techniques (Zheng et al., 2023;Yao et al., 2024) on LLMs of exorbitant * Equal contribution size like GPT-4 (OpenAI, 2023), or (ii) finetuning a relatively smaller LLM using domain-focused data (Shao et al., 2024;Toshniwal et al., 2024;Dutta et al., 2024).Methods from the former category heavily rely on the proprietary LLM being used and are prone to fail absolutely when employed with less powerful models.The latter category, though cost-effective compared to humongous LLMs, often loses in generalizability due to a narrow training domain.\n\nThe chronicle of decomposed reasoning.A number of recent literature has pointed out that LLMs tend to perform better on complex reasoning tasks when the problem is decomposed into step-by-step subproblems (Zhou et al., 2023;Khattab et al., 2022;Juneja et al., 2023).Earlier techniques demonstrated the superiority by providing the model with examples containing the original problem decomposed into multiple sub-problems along with their answers (Zhou et al., 2023).However, Juneja et al. (2023) illustrated that decoupling the decomposer from the solver by finetuning a separate decomposer language model (LM) to coordinate with a larger solver LM is beneficial to simply prompting a single monolithic LM to decompose and solve.Echoing their findings, Wu et al. (2024) also found that distilling decomposition abilities from a larger LM to a smaller LM is much more generalizable compared to decomposing the solver abilities directly.\n\nOur contributions.However, a major bottleneck in existing methods of decomposer finetuning is the lack of tightness between the decomposersolver interactions.",
            "score": 0.40679731092669275,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 419
                },
                {
                    "start": 419,
                    "end": 572
                },
                {
                    "start": 572,
                    "end": 707
                },
                {
                    "start": 709,
                    "end": 747
                },
                {
                    "start": 747,
                    "end": 975
                },
                {
                    "start": 975,
                    "end": 1175
                },
                {
                    "start": 1175,
                    "end": 1438
                },
                {
                    "start": 1438,
                    "end": 1644
                },
                {
                    "start": 1646,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1804
                }
            ],
            "ref_mentions": [
                {
                    "start": 194,
                    "end": 211,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 399,
                    "end": 418,
                    "matchedPaperCorpusId": "266163180"
                },
                {
                    "start": 914,
                    "end": 933,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 954,
                    "end": 974,
                    "matchedPaperCorpusId": "264590648"
                },
                {
                    "start": 1155,
                    "end": 1174,
                    "matchedPaperCorpusId": "248986239"
                },
                {
                    "start": 1184,
                    "end": 1204,
                    "matchedPaperCorpusId": "264590648"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1300048828125
        },
        {
            "corpus_id": "249848272",
            "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks",
            "text": "Vision and language pre-training has become standard practice for multi-modal models, including unified and non-unified models requiring task-specific heads to train from scratch during finetuning. Many initial pre-training strategies were inspired by BERT (Devlin et al., 2019) and included masked-language-modeling, image-text-matching, or mask-region-modeling objectives, often supplemented with objectives using the predictions of a strong object detector model (e.g, VIL-BERT (Lu et al., 2019), LXMERT (Tan & Bansal, 2019), VisualBERT (Li et al., 2019)). More recently contrastive-image-text losses (Radford et al., 2021;Li et al., 2022b;2021) or auto-regressive generation losses (Wang et al., 2022d;a;Yu et al., 2022a), have become common. Several works have also directly used object detection or segmentation datasets for pre-training Yuan et al. (2021); Wang et al. (2022b); Sun et al. (2022). The generalized masked-data-modeling pretraining objective used in UNIFIED-IO is similar to ones used in several recent works (Wang et al., 2022c;Peng et al., 2022;Singh et al., 2022). \n\nConstructing models that can learn to solve many different tasks has been of long-standing interest to researchers. A traditional approach to this problem is to build models with task-specialized heads on top of shared backbones (He et al., 2017;Liu et al., 2019;Lu et al., 2020). However, this requires manually designing a specialized head for each task, potentially limiting transfer across tasks. An alternative is to build unified models -models that can complete many different tasks without task-specialized components. In NLP, this approach has achieved great success using pretrained generative models (Raffel et al., 2020;Brown et al., 2020;Chowdhery et al., 2022). \n\nInspired by this success, there has been a recent trend to build unified models that can apply to tasks with visual or structured inputs and outputs.",
            "score": 0.4067948101451727,
            "section_title": "RELATED WORK",
            "char_start_offset": 27753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 278,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 481,
                    "end": 498,
                    "matchedPaperCorpusId": "199453025"
                },
                {
                    "start": 507,
                    "end": 527,
                    "matchedPaperCorpusId": "201103729"
                },
                {
                    "start": 604,
                    "end": 626,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 626,
                    "end": 643,
                    "matchedPaperCorpusId": "246411402"
                },
                {
                    "start": 686,
                    "end": 706,
                    "matchedPaperCorpusId": "237291550"
                },
                {
                    "start": 864,
                    "end": 883,
                    "matchedPaperCorpusId": "265040027"
                },
                {
                    "start": 1068,
                    "end": 1087,
                    "matchedPaperCorpusId": "244954250"
                },
                {
                    "start": 1320,
                    "end": 1337,
                    "matchedPaperCorpusId": "54465873"
                },
                {
                    "start": 1337,
                    "end": 1354,
                    "matchedPaperCorpusId": "59523594"
                },
                {
                    "start": 1354,
                    "end": 1370,
                    "matchedPaperCorpusId": "208637516"
                },
                {
                    "start": 1702,
                    "end": 1723,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1723,
                    "end": 1742,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203125
        },
        {
            "corpus_id": "221553599",
            "title": "Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling",
            "text": "The results reported here suggest that for the purpose of modeling reading times, perplexity does not adequately reveal the relationship between language modeling performance and psycholinguistic modeling performance, especially when vocabulary cannot be controlled for. In contrast, PNC has proven to be a much better predictor of the psychometric capabilities of our language models. This finding is consistent with observations by Frisson et al. (2005) and Smith and Levy (2011) that Cloze probabilities predict self-paced reading times better than corpus probabilities do. In addition to allowing models with different vocabularies to be compared with one another, PNC is much more strongly correlated with psycholinguistic modeling performance than perplexity, as demonstrated by our results on \u2206LogLik. More generally, as there is little to no correlation between perplexity and PNC, PNC can serve as a good supplement to perplexity for language model evaluation, providing information about model behavior that is not captured by the latter. \n\nWe have also shown that model architecture, training dataset size, and training dataset composition all contribute substantially to the psycholinguistic modeling capabilities of language models. In particular, the importance of corpus size and composition is reflective of trends in transfer learning, in which advances in downstream NLP tasks are made by using language models to extract general linguistic information from large corpora. Our analysis has shown that large corpora have the potential to provide considerable amounts of linguistic knowledge even through simple model architectures, as in the case of the LM1B n-grams. \n\nAs new pre-trained language models are developed, especially with custom vocabularies that make a direct comparison of perplexities impossible, metrics such as PNC can serve as a valuable tool for assessing the quality of language models. In establishing a strong relationship between PNC and \u2206LogLik, we have demonstrated that PNC scores convey a psychometrically relevant notion of language model quality that directly measures the degree to which language models exhibit humanlike behavior. Reliable metrics like PNC, which are robust to variations in model setup, have the potential to greatly improve our ability to understand the relationship between language models and language.",
            "score": 0.40674226998535773,
            "section_title": "Conclusion",
            "char_start_offset": 27216,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 434,
                    "end": 455,
                    "matchedPaperCorpusId": "15876019"
                },
                {
                    "start": 460,
                    "end": 481,
                    "matchedPaperCorpusId": "6850948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042236328125
        },
        {
            "corpus_id": "258088263",
            "title": "A Survey of Text Representation and Embedding Techniques in NLP",
            "text": "Although BERT significantly improves the performance of a wide range of natural language understanding tasks, its bidirectionality makes it challenging to apply to natural language generation tasks. Reference [58] proposes UNILM that can be applied to both natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is a multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three unsupervised language modeling objectives. The unified LM is jointly pre-trained by multiple language modeling objectives, sharing the same parameters. The shared parameters (or pre-trained unified LM) are fine-tuned and evaluated on various datasets, including language understanding and generation tasks. Like BERT, the pre-trained UNILM can be fine-tuned (with additional task-specific layers if necessary) to adapt to various downstream tasks. \n\nBut unlike BERT, which is used mainly for NLU tasks, UNILM can be configured to aggregate context for different types of language models and thus can be used for both NLU and NLG tasks. One of the advantages of UNILM is that the parameter sharing makes the learned text representations more general because they are jointly optimized for different language modeling objectives where context is utilized in different ways, mitigating overfitting to any single LM task. Furthermore, in addition to its application to NLU tasks, the use of UNILM as a sequence-to-sequence LM makes it a natural choice for NLG. UNILM uses the masking technique to control how much context the token should attend to when computing its contextualized representation. After pretraining (learning the contextualized representation), they are fine-tuned using task-specific data of downstream tasks. Each input token's vector representation is computed by summing the corresponding token embedding, position embedding, and segment embedding. As the Bidirectional LM pretraining task encodes contextual information from both directions, it generates better contextual text representations than its unidirectional counterpart. The results of these dynamic contextual embeddings on GLUE tasks are highlighted in Table 8.",
            "score": 0.406675004488499,
            "section_title": "7) UNIFIED PRE-TRAINED LANGUAGE MODEL (UNILM)",
            "char_start_offset": 84003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 896
                },
                {
                    "start": 899,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 213,
                    "matchedPaperCorpusId": "147704286"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35009765625
        },
        {
            "corpus_id": "1541597",
            "title": "A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes",
            "text": "Probabilistic language models are used extensively in a variety of linguistic applications, including speech recognition, handwriting recognition, optical character recognition, and machine translation. Most language models fall into the class of n-gram models, which approximate the distribution over sentences using the conditional distribution of each word given a context consisting of only the previous n \u2212 1 words,\n\nwith n = 3 (trigram models) being typical. Even for such a modest value of n the number of parameters is still tremendous due to the large vocabulary size. As a result direct maximum-likelihood parameter fitting severely overfits to the training data, and smoothing methods are indispensible for proper training of n-gram models. A large number of smoothing methods have been proposed in the literature (see (Chen and Goodman, 1998;Goodman, 2001;Rosenfeld, 2000) for good overviews). Most methods take a rather ad hoc approach, where n-gram probabilities for various values of n are combined together, using either interpolation or back-off schemes. Though some of these methods are intuitively appealing, the main justification has always been empirical-better perplexities or error rates on test data. Though arguably this should be the only real justification, it only answers the question of whether a method performs better, not how nor why it performs better. This is unavoidable given that most of these methods are not based on internally coherent Bayesian probabilistic models, which have explicitly declared prior assumptions and whose merits can be argued in terms of how closely these fit in with the known properties of natural languages. Bayesian probabilistic models also have additional advantages-it is relatively straightforward to improve these models by incorporating additional knowledge sources and to include them in larger models in a principled manner. Unfortunately the performance of previously proposed Bayesian language models had been dismal compared to other smoothing methods (Nadas, 1984;MacKay and Peto, 1994).\n\nIn this paper, we propose a novel language model based on a hierarchical Bayesian model (Gelman et al., 1995) where each hidden variable is distributed according to a Pitman-Yor process, a nonparametric generalization of the Dirichlet distribution that is widely studied in the statistics and probability theory communities (Pitman and Yor, 1997;Ishwaran and James, 2001;",
            "score": 0.4066349000031555,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 2030,
                    "end": 2043,
                    "matchedPaperCorpusId": "122661322"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0263519287109375
        },
        {
            "corpus_id": "266149771",
            "title": "INSPECT: Intrinsic and Systematic Probing Evaluation for Code Transformers",
            "text": "In this section, we discuss some of the limitations of our study. These may be addressed in future works as we collectively develop the probing paradigm further, ultimately, to better understand the inner workings of large language models (LLMs), particularly in our case, large-scale source code language models. Mono-lingual probes. The majority of the probed models are pre-trained on multiple programming languages, and probing them on a single language (e.g. Java) is just a first step. Further probes in more languages should be designed to understand the code aspects learned by the pre-trained models. Our work presents the probing framework and releases the relevant code with which different models can be probed on other languages. The work to gather and preprocess the necessary data in other languages should still be done, however, and is likely to require some effort. \n\nPossible confounding factors. While we did our best to make sure our tasks are reliable, there can be an influence from confounding factors. In particular, we can think of method size as a possible confounding factors for some of the structural tasks, as it is well known that there is often a relationship between lines of code and source code complexity in general [72]. This is why we chose to formulate the task as a classification task, rather than a regression task. We think that the impact of this compounding factor, if present, is limited, since we observe very different performance for LEN compared to the Complexity-based and Count-based tasks (e.g., PLBART is one of the best performing models for LEN, but one of the weakest on the Complexity-based and Countbased tasks). Moreover, should this confounding factor be more significant, this would only further highlight that the source code models, by overly relying on length, have even weaker abilities to model source code structure than we previously thought.",
            "score": 0.4066069524503439,
            "section_title": "LIMITATIONS",
            "char_start_offset": 82510,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 1253,
                    "end": 1257,
                    "matchedPaperCorpusId": "4363650"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.013275146484375
        },
        {
            "corpus_id": "277634577",
            "title": "Lugha-Llama: Adapting Large Language Models for African Languages",
            "text": "Large language models (LLMs) have achieved impressive results in a wide range of natural language applications. However, they often struggle to recognize low-resource languages, in particular African languages, which are not well represented in large training corpora. In this paper, we consider how to adapt LLMs to low-resource African languages. We find that combining curated data from African languages with high-quality English educational texts results in a training mix that substantially improves the model's performance on these languages. On the challenging IrokoBench dataset, our models consistently achieve the best performance amongst similarly sized baselines, particularly on knowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the cross-lingual question answering benchmark AfriQA, our models outperform the base model by over 10%. To better understand the role of English data during training, we translate a subset of 200M tokens into Swahili language and perform an analysis which reveals that the content of these data is primarily responsible for the strong performance. We release our models and data to encourage future research on African languages.",
            "score": 0.40656224749354464,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "7016337",
            "title": "Having Your Cake and Eating It Too: Autonomy and Interaction in a Model of Sentence Processing",
            "text": "Is the human language understander a collection of modular processes operating with relative autonomy, or is it a single integrated process? This ongoing debate has polarized the language processing community, with two fundamentally different types of model posited, and with each camp concluding that the other is wrong. One camp puts forth a model with separate processors and distinct knowledge sources to explain one body of data, and the other proposes a model with a single processor and a homogeneous, monolithic knowledge source to explain the other body of data. In this paper we argue that a hybrid approach which combines a unified processor with separate knowledge sources provides an explanation of both bodies of data, and we demonstrate the feasibility of this approach with the computational model called COMPERE. We believe that this approach brings the language processing community significantly closer to offering humanlike language processing systems.",
            "score": 0.4064586197702927,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.042236328125
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "In this study, we consider a collection of n existing Large Language Models (LLMs), denoted as {M 1 , . . . , M n }, which have been fine-tuned on diverse corpora. Our objective is to outline a systematic approach towards producing one stronger aggregated model across all knowledge domains. Specifically, the unified LLM incorporates single LLMs mainly through merging and mixture. The concept of Model Merging Model merging is integrating multiple models into one unified model in the weight space, compatible with LLMs of the same initialization [16]. Popular merging methods can be divided into two types: \u2776 Merging entire model weights represented by Model Soup [59] (Linear), SLERP [49], and Model Stock [24]; \u2777 Task-vector based merging represented by Task Arithmetic [22], TIES [63], and DARE [64]. The former method directly interpolates model weights, while the latter subtracts the pre-trained model from the fine-tuned model to obtain task vectors and utilizes sparsity and consistency of parameters for refined merging. The basic Linear interpolation merging is defined as w u = n i=1 s i \u2022 w i , where w i and s i are the corresponding model weights and merging coefficient of M i \u2208 {M 1 , . . . M n }. Selective Merging Pipeline Merging can be easily applied to models with the same architecture, but does not guarantee better results. Therefore, before searching for the merging coefficient, we first pre-process the models by clustering all the models using cosine similarity and then searching for the optimal merging coefficient and method within each cluster. Details are explained in Appendix A. 5.",
            "score": 0.4064586197702927,
            "section_title": "Preliminaries",
            "char_start_offset": 9133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "11290566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305419921875
        },
        {
            "corpus_id": "270703043",
            "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings",
            "text": "Scaling of language models has often resulted in improved model capabilities in various tasks [10,[17][18][19][20][21][22], including those requiring specialized scientific knowledge and reasoning [23]. Research by Kaplan et al. [24] revealed that there is an empirical power-law relationship between the language model performance, in terms of cross-entropy loss, and the model size, dataset size, and amount of compute used for training. It was further found that architectural details, such as network width or depth, had minimal effects on performance. Scaling laws have further been studied by Hoffmann et al. [10] and Bahri et al. [25]. \n\nFollowing these empirical results, several studies have trained progressively larger language models of up to hundreds of billion parameters, such as GPT-3 [18], PaLM [20], Galactica [26], LLaMA [27,28], Claude [29], Gemini 1.5 [30], and Mistral [31]. Among all, GPT-3 and ChatGPT can be considered the precursors of the large language models, the name by which these large-scale language models are known [9,13]. GPT-4, a latter version of GPT-3, stands out for its exceptional performance, often matching or surpassing human performance on a variety of tasks [11,32,33], even in specialized domains [34]. Extensive evaluations have been conducted to GPT-4 [23,[35][36][37], exploring even the path toward Artificial General Intelligence (AGI) [32]. \n\nLLMs can be adapted to different tasks via prompt engineering, which, unlike fine-tuning, does not require retraining the model and updating its weights. These prompting techniques have led to observing unexpected emergent capabilities in LLMs, demonstrating the potential to address a wide range of complex tasks and exhibit apparent reasoning abilities [3,8,14,18,22,[38][39][40][41][42]. In the medical field, for example, Chain of Thought (CoT) has been used for explainability [43] and in-context learning to mitigate the need for costly medical annotations [13].",
            "score": 0.4062897006518954,
            "section_title": "Large language models",
            "char_start_offset": 9878,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 642
                },
                {
                    "start": 645,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1966
                }
            ],
            "ref_mentions": [
                {
                    "start": 94,
                    "end": 98,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 102,
                    "end": 106,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 106,
                    "end": 110,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 110,
                    "end": 114,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 118,
                    "end": 122,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 637,
                    "end": 641,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 801,
                    "end": 805,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 812,
                    "end": 816,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "264425949"
                },
                {
                    "start": 1246,
                    "end": 1250,
                    "matchedPaperCorpusId": "261100760"
                },
                {
                    "start": 1303,
                    "end": 1307,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "258832575"
                },
                {
                    "start": 1311,
                    "end": 1315,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 1753,
                    "end": 1756,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 1756,
                    "end": 1758,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "258299717"
                },
                {
                    "start": 1761,
                    "end": 1764,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1764,
                    "end": 1767,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1767,
                    "end": 1771,
                    "matchedPaperCorpusId": "258418299"
                },
                {
                    "start": 1771,
                    "end": 1775,
                    "matchedPaperCorpusId": "249062918"
                },
                {
                    "start": 1775,
                    "end": 1779,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 1783,
                    "end": 1787,
                    "matchedPaperCorpusId": "249017743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061767578125
        },
        {
            "corpus_id": "256166089",
            "title": "Development of Language Models for Continuous Uzbek Speech Recognition System",
            "text": "There are 27 characters in the Uzbek Latin alphabet (several letters, such as s + h = sh, n + g = ng, are not counted). If we also take into account spaces (\"_\"), characters representing the beginning and end of a line (<s>,</s>), and characters representing an unknown character (<unk>), there will be a total of 31 output neurons. We evaluated the effectiveness of building word-and character-based language models from the LSTM neural network, the results of which are presented in Table 5. In conclusion, it should be stated that character-based language models are more effective than word-based language models. \n\nWithin the framework of the study, the process of integrating the developed language model into an automatic recognition system of Uzbek speech and analyzing the accuracy of the recognition was developed. To achieve a speech recognition system for acoustic modeling, a 105 h speech corpus and several integrated neural network models (E2E-LSTM, RNN-CTC, DNN-HMM, E2E-Transformer, and E2E-Conformer) have been used. For instance, as a result of learning the E2E-Conformer model, the speech recognition system achieved a WER of 18.1% and a CER of 7.8%, whereas for the test set, a WER of 17.4% and a CER of 5.8% were achieved. \n\nBy combining the developed language model with the speech recognition system, we achieve a WER of 15.1% and a CER of 5.5% for the train sample, and a WER of 13.9% and a CER of 5.26% for the test sample. \n\nThe results obtained by combining the language model with the other models are presented in Table 6. When the created language model is applied to the speech recognition system, we can see the results in the table below (examples from the conducted experiments are given). In this case, the first column (Original text) contains the audio text read to the speech recognition system, and the second column (Recognized text) includes the recognized text based on the application of the language model to the speech recognition system.",
            "score": 0.4060631035125136,
            "section_title": "Training and Test Speech Corpus",
            "char_start_offset": 49236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1449
                },
                {
                    "start": 1452,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1984
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055511474609375
        },
        {
            "corpus_id": "259095527",
            "title": "The Two Word Test: A Semantic Benchmark for Large Language Models",
            "text": "Large Language Models (LLMs) have shown remarkable abilities recently, including passing advanced professional exams and demanding benchmark tests. This performance has led many to suggest that they are close to achieving humanlike or 'true' understanding of language, and even Artificial General Intelligence (AGI). Here, we provide a new open-source benchmark that can assess semantic abilities of LLMs using two-word phrases using a task that can be performed relatively easily by humans without advanced training. Combining multiple words into a single concept is a fundamental aspect of human language and intelligence. The test requires meaningfulness judgments of 1768 noun-noun combinations that have been rated as meaningful (e.g., baby boy) or not meaningful (e.g., goat sky). by 150 human raters. We provide versions of the task that probe meaningfulness ratings on a 0-4 scale as well as binary judgments. We conducted a series of experiments using the TWT on GPT-4, GPT-3.5, and Bard, with both versions. Results demonstrated that, compared to humans, all models perform poorly at rating meaningfulness of these phrases. GPT-3.5 and Bard are also unable to make binary discriminations between sensible and nonsense phrases as making sense. GPT-4 makes a substantial improvement in binary discrimination of combinatorial phrases but is still significantly worse than human performance. The TWT can be used to understand the limitations and weaknesses of current LLMs, and potentially improve them. The test also reminds us that caution is warranted in attributing 'true understanding' or AGI to LLMs. TWT is available at: https://github.com/NickRiccardi/two-word-test",
            "score": 0.40602364340109254,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0237274169921875
        },
        {
            "corpus_id": "265466787",
            "title": "Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine",
            "text": "A long-term aspiration in AI research is to develop principles of computational intelligence and to harness these to build learning and reasoning systems that can perform general problem solving across a diversity of tasks [21,22]. In line with this goal, large language models, also referred to as foundation models, such as GPT-3 [3] and GPT-4 [24], have demonstrated surprising competencies on a broad swath of tasks without requiring heavy specialized training [4]. These models build on the text-to-text paradigm [31] with investments in compute and data to learn at scale from indiscriminate consumption of large amounts of public web data. Some of these models are tuned via a learning objective to perform general instruction-following via prompts. A core metric for characterizing the performance of foundation models is the accuracy of next word prediction. Accuracy with next word prediction is found to increase with scale in training data, model parameters, and compute, in accordance with empirically derived \"neural model scaling laws\" [3,12]). However, beyond predictions of scaling laws on basic measures such as next word prediction, foundation models show the sudden emergence of numerous problem-solving capabilities at different thresholds of scale [33,27,24].",
            "score": 0.4058068371630247,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1281
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 227,
                    "matchedPaperCorpusId": "19439915"
                },
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "35199622"
                },
                {
                    "start": 332,
                    "end": 335,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 518,
                    "end": 522,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 1051,
                    "end": 1054,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1270,
                    "end": 1274,
                    "matchedPaperCorpusId": "249674500"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28173828125
        },
        {
            "corpus_id": "7469159",
            "title": "Speech Recognition Grammar Compilation in Grammatical Framework",
            "text": "Compilation of unification grammars to speech recognition grammars is well described in the literature (Moore, 1999;Dowding et al., 2001). Regulus (Rayner et al., 2006) is perhaps the most ambitious such system. Like GF, Regulus uses a general grammar for each language, which is specialized to a domain-specific one. Ljungl\u00f6f (Ljungl\u00f6f, 2007b) relates GF and Regulus by showing how to convert GF grammars to Regulus grammars. We carry compositional semantic interpretation through left-recursion elimination using the same idea as the UNIANCE grammar compiler (Bos, 2002), though our version handles both direct and indirect left-recursion.\n\nThe main difference between our work and the existing compilers is that we work with typetheoretical grammars rather than unification grammars. While the existing work focuses on GSL as the output language, we also support a number of other formats, including finite-state models. By using the GF resource grammars, speech recognition language models can be produced for more languages than with previous systems. One shortcoming of our system is that it does not yet have support for weighted grammars. Jonson (2006) has shown that in addition to generating grammar-based language models, GF can be used to build statistical language models (SLMs). It was found that compared to our grammar-based approach, use of generated SLMs improved the recognition performance for out-of-grammar utterances significantly.",
            "score": 0.4057633893477567,
            "section_title": "Unification Grammar Compilation",
            "char_start_offset": 17855,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 116,
                    "matchedPaperCorpusId": "2795317"
                },
                {
                    "start": 116,
                    "end": 137,
                    "matchedPaperCorpusId": "8308091"
                },
                {
                    "start": 1147,
                    "end": 1160,
                    "matchedPaperCorpusId": "17992480"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.164306640625
        },
        {
            "corpus_id": "14126633",
            "title": "Orthogonal Meta-Modeling",
            "text": "The starting layer L0 of a vertical meta-modeling hierarchy collects models that represent most comprehensive semantic knowledge bases for models on other semantic layers. That means, a model on layer L0 specifies the most general information about a phenomenon that can be semantically specialized on other vertical layers. In general, all modeling languages can be used to specify a semantic knowledge base. However, this section briefly introduces a meta-model that is especially designed for the unification of specification languages for service compositions and therefore suitable to model semantic knowledge bases concerning service compositions. In particular, the unified model is intended to be the engine-internal representation of a service composition that is used for the execution of the particular service composition. Therefore, an engine is not required to support multiple modeling languages and the support of multiple modeling languages in an execution environment does not require the use of multiple engines, respectively. The unified model is based on formal grammars, i.e. a service composition is intended to be executed based on a grammar-based specification (G\u00f6rlach, K. et al. 2013). The same service composition modeled by using a highly developed specification language like BPEL (OASIS 2007), BPMN (OMG 2011), Scufl (Oinn, T. et al. 2006), and ConDec (Pesic, M. 2008) is considered to be a view on the particular service composition grammar (cf. Figure 1). Therefore, a service (composition) grammar is considered to hold the most general information about the logic of a single service composition. A model of the same service composition based on high specification language represent a semantic instance of the service grammar as the semantic information of the service grammar is specialized corresponding to the particular specification language. If the expressiveness of the particular specification language is not sufficient some logic, i.e. semantic information is lost while specialization. For example, alternative execution paths cannot be specified in models exclusively using data flow-based modeling constructs. For enabling alternative paths in data flow-based models the meta-model needs to be extended by particular control flow-based modeling constructs (cf. Oinn, T. et al. (2006)). In general, most languages for service compositions provide mechanisms for extensibility that can be used to avoid the loss of information while specialization (cf.",
            "score": 0.40573935351426454,
            "section_title": "S0: Semantic Knowledge Base",
            "char_start_offset": 5449,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1883
                },
                {
                    "start": 1884,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2158
                },
                {
                    "start": 2159,
                    "end": 2309
                },
                {
                    "start": 2310,
                    "end": 2334
                },
                {
                    "start": 2335,
                    "end": 2499
                }
            ],
            "ref_mentions": [
                {
                    "start": 1383,
                    "end": 1398,
                    "matchedPaperCorpusId": "221023426"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1865234375
        },
        {
            "corpus_id": "257987125",
            "title": "Three-stage training and orthogonality regularization for spoken language recognition",
            "text": "Lately, end-to-end ASR models have achieved outstanding performances and largely simplified multilingual models by learning shared representations directly from data [20,21]. These studies have greatly simplified the process of extracting phonetic features for LID tasks, making it one of the hot-spots in recent years. Watanabe et al. [22] present a single multilingual model with a unique vocabulary that can recognize speeches of 10 languages. Multi-task learning that jointly learns linguistic and phonetic information is studied in [23,24]. Ren et al. [25] build a two-stage language identification system that outperforms the baseline multi-task system. Wang et al. [26] analyze different conformer-based architectures and demonstrate the great improvement of a two-stage system, named transfer learning in their experiments. Duroselle et al. [16,27] study different modeling and training strategies and show that bottleneck features can be greatly improved by using language identification loss during the training of the feature extractor. Alum et al. [28] incorporate a large pre-trained multilingual XLSR-53 wav2vec2.0 model and reveal its excellent modeling abilities, that is, fine-tuning the model with just one utterance per target language already outperforms the baseline model that does not use pre-training but is trained with around 10,000 utterances per language. \n\nAlthough ASR has been applied as an auxiliary task to LID, how to better optimize these two independent LID and ASR neural networks has not yet been well studied. Most of the existing methods use a two-stage training strategy, which is not optimal and faces a trade-off between the recognition accuracy and the generalization ability. \n\nTherefore, in this paper, we introduce a three-stage training strategy with an orthogonality regularization method to overcome the problem of the trade-off between the recognition accuracy and the generalization ability, which we analyze is caused by a trade-off between hybrid-level knowledge, that is, various mixed-level features including phonetic information and prosodies, and noise embedded in a shared encoder.",
            "score": 0.4056112475769048,
            "section_title": "Introduction",
            "char_start_offset": 1968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1720
                },
                {
                    "start": 1723,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 170,
                    "matchedPaperCorpusId": "233476441"
                },
                {
                    "start": 170,
                    "end": 173,
                    "matchedPaperCorpusId": "140221900"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "19343340"
                },
                {
                    "start": 537,
                    "end": 541,
                    "matchedPaperCorpusId": "212648558"
                },
                {
                    "start": 541,
                    "end": 544,
                    "matchedPaperCorpusId": "233299331"
                },
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "239628355"
                },
                {
                    "start": 849,
                    "end": 853,
                    "matchedPaperCorpusId": "236528896"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08197021484375
        },
        {
            "corpus_id": "274233896",
            "title": "Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models",
            "text": "The success of GPT-3.5 demonstrated the immense potential and practical value of large language models, sparking extensive research and further development in the field, leading to the emergence of various LLMs. For instance, LLaMA [24], developed by Meta's research team, showcases excellent text generation and comprehension abilities by training on larger datasets than usual, using only publicly available data, thus promoting the accessibility and research of LLMs. The subsequent LLaMA-2 [24] introduced multiple improvements over LLaMA 1, such as training on a new mixture of publicly available data. \n\nChatGLM [28], proposed by Tsinghua University, is a conversational model developed under the General Language Model (GLM) framework. It performs excellently in natural language understanding (NLU) and text generation tasks through autoregressive blank filling pre-training. ChatGLM can handle multi-label fill-in-the-blank problems and demonstrates outstanding performance in NLU, conditional generation, and unconditional generation tasks. Among smaller-scale LLMs, Microsoft's Phi-1.5 model [29] matches the performance of models five times its size on natural language tasks and outperforms most non-frontier LLMs on more complex reasoning tasks such as elementary mathematics and basic coding. In addition to Transformerbased LLMs, non-Transformer LLMs are also being developed. The RWKV model [30] combines the advantages of RNNs and Transformers while overcoming their key drawbacks. It replaces traditional dot-product attention with linear attention mechanisms, significantly reducing computational and memory complexity. The Mamba model [31] is a new type of selective statespace model (SSMs) [32], designed with a simple and unified architecture that combines previous SSM architectures with Transformer MLP blocks. \n\nDespite the rapid development of LLMs, their computational demands and data requirements remain very high, making them difficult for ordinary developers and researchers to utilize directly. Therefore, methods such as Low-Rank Adaptation (LoRA) [33] or knowledge distillation are needed to achieve efficient training and deployment of models with reduced computational resources. Knowledge distillation transfers the knowledge of LLMs to smaller models, significantly reducing computational resource and storage needs while maintaining high performance.",
            "score": 0.4055833188375017,
            "section_title": "Advancements in LLMs and Knowledge Distillation Techniques",
            "char_start_offset": 15795,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 607
                },
                {
                    "start": 610,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2216
                },
                {
                    "start": 2217,
                    "end": 2390
                }
            ],
            "ref_mentions": [
                {
                    "start": 1712,
                    "end": 1716,
                    "matchedPaperCorpusId": "269362649"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0889892578125
        },
        {
            "corpus_id": "276574769",
            "title": "Audio-FLAN: A Preliminary Release",
            "text": "Recent advances in large language models and multimodal models have highlighted the effectiveness of instruction tuning for broad generalization [Ouyang et al., 2022, Touvron et al., 2023, Achiam et al., 2023]. \n\nInstruction-tuned models can generalize to unseen tasks far better than task-specific counterparts. In the text domain, models like FLAN (Finetuned Language Net) [Wei et al., 2021] demonstrate remarkable zero-shot and few-shot capabilities when fine-tuned on diverse instructions. For example, FLAN (137B parameters) was fine-tuned on 60 NLP tasks and outperformed even larger models, like the 175B GPT-3 [Brown et al., 2020], on many unseen tasks. Similarly, LIMA [Zhou et al., 2024], which used only 1,000 curated examples, achieved results preferred over much larger models, showing that minimal high-quality instruction data can significantly improve a model's ability to follow complex queries. In the vision domain, unified models like Chameleon [Team, 2024] and Janus-Pro 7B [Wu et al., 2024] have demonstrated strong performance by handling both understanding and generation tasks in a single system, outperforming specialized models in image captioning, visual question answering, and image generation. In contrast, the audio domain 3 still lags behind, with audio understanding and generation often treated as separate tasks. \n\nThis gap between modalities highlights a critical limitation: audio-language models still lack the unified modeling and generalization capabilities that are now common in NLP and computer vision. Despite the wide variety of audio tasks (such as speech transcription, speaker identification, emotion recognition, sound event recognition, music understanding, and text-to-speech generation), there is no \"audio GPT\" or \"audio foundation model\" that can seamlessly switch between understanding and generating audio across speech, music, and audio domains. For example, models like Musilingo [Deng et al., 2023] focus on music understanding, while LTU (Listen, Think, Understand) [Gong et al., 2023b] and Audio-Flamingo [Kong et al., 2024] focus on the audio domain.",
            "score": 0.4055128899356544,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 618,
                    "end": 638,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1568603515625
        },
        {
            "corpus_id": "247613500",
            "title": "ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models",
            "text": "Recent years have featured a trend towards Transformer (Vaswani et al., 2017) based pretrained language models (PLMs) in natural language processing (NLP) systems. By being pretrained on massive unlabeled text, PLMs can be directly fine-tuned on downstream tasks, entirely removing the need for task-specific architectures (Radford et al., 2018). This paradigm has led to significant progress on many challenging NLP tasks such as reading comprehension (Devlin et al., 2019) and text generation (Brown et al., 2020).\n\nWith rising new state-of-the-art results that approach or surpass human performance on several tasks, it is a non-trivial research topic about how * Corresponding author to systematically evaluate the language abilities of PLMs from a wide range of perspectives. Given a wide range of publicly released PLMs, it is particularly useful to derive principles or guidelines for selecting suitable PLMs for specific downstream tasks. However, existing works either target some single ability (Talmor et al., 2020;Zhou et al., 2020), or consider a simple mixture of multiple (small-scale) tasks that lack a comprehensive design and test (Wang et al., 2019b;Liang Xu, 2020). There has been no detailed and systematic analysis of PLM's abilities in large-scale NLP tasks. To fill the gap of PLMs evaluation, we introduce the genEral language ability evaluation (ElitePLM) for empirically and systematically assessing the general language abilities of PLMs.\n\nThe ideal goal behind PLMs is to create a humanlike machine learner where it can understand the language and then perform any specific task related to language. In cognitive science, Wechsler Adult Intelligence Scale (WAIS) (Kaufman and Lichtenberger, 2005) is the most commonly used intelligence quotient (IQ) test for measuring the intelligence and cognitive ability of humans. This test would assess the level of individuals on verbal comprehension, perceptual reasoning, working memory, and processing speed. Thus, by imitating the intelligence test on humans, we design four evaluation dimensions in ElitePLM for measuring the abilities of PLMs, including memory, comprehension, reasoning, and composition. Following previous works (Zhou et al., 2020;Wang et al.,",
            "score": 0.4054747296697763,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.145263671875
        },
        {
            "corpus_id": "263134950",
            "title": "LawBench: Benchmarking Legal Knowledge of Large Language Models",
            "text": "We find that the current legal specific LLMs do not necessarily outperform general large language models. We analyze the effect of model size and training approach on large language models to better understand which aspects most influence model performance. \n\nScaling up the model size results in better performance in one-shot case. Prior work shows that larger models perform better in general NLP tasks [31]. We analyze whether this finding still holds in legal domain. Specifically, we select representative models from different categories with varying model sizes and calculate the overall performance and abstention rate of different tasks. The results are shown in Figure 4. We observe that increasing the model size typically helps improve   model performance in one-shot settings. Also, the abstention rate becomes lower, indicating that larger models are better at following instructions. Nevertheless, we find that ChatLaw is an outlier, a larger size results in lower performance. In zero-shot scenarios, mixed results are observed, simply increasing the model size may not automatically lead to better performance. \n\nMost LLMs can not efficiently leverage article content. Retrieval augmentation is a common way to improve the accuracy of generative models [34; 30; 54]. By including the content of the related legal articles in task 3-4 and feeding them into the LLM input, we replicate the retrieval augmentation scenario to form task 3-5. The goal is to see if the model can successfully use this additional knowledge to predict the proper jail sentence when supplied with the relevant article reference that articulates the range of prison terms. We compare 5 models of various types and visualize the comparison between task 3-4 and 3-5 in Figure 5. The results show that the vast majority of models fail to make any progress on the jail sentence prediction challenge by using the provided article content. There is a decrease in performance across the board for most models, including GPT-4. This suggests that simple retrieval enhancement methods may not bring further improvement, and how to obtain LLMs that are able to effectively utilize the retrieval information is still an open problem. \n\nSFT may improve the performance but RLHF may not. We illustrate the performance difference of 9 models before and after SFT in Figure 6. In most cases, models perform better through SFT.",
            "score": 0.4049246794919722,
            "section_title": "Analysis",
            "char_start_offset": 38345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 257
                },
                {
                    "start": 260,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 472
                },
                {
                    "start": 473,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1128
                },
                {
                    "start": 1131,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2214
                },
                {
                    "start": 2217,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2353
                },
                {
                    "start": 2354,
                    "end": 2403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0292510986328125
        },
        {
            "corpus_id": "257666138",
            "title": "A deep hierarchy of predictions enables online meaning extraction in a computational model of human speech comprehension",
            "text": "For the first type, a plausible algorithm of statistical parameter learning of structured contextual and semantic knowledge is the one proposed for the \"topic\" model of semantic representation [9,52]. Griffiths and colleagues [9] also pointed to a possible way to integrate complex syntax and semantic generative models by replacing one component in a syntax model [92] with such a topic model. This would allow the syntax model to determine an appropriate semantic component for the current time point and the semantic model to generate a corresponding word, which is consistent with the way semantic and syntax factors interact in our current model. More recently, Beck and colleagues [93] showed that a formal equivalence of the topic model can be implemented via a probabilistic (neural) population code, providing a plausible path to a neural implementation of the model. The second type of learning can be viewed by updating the relevant parameters within a fixed structure learned from a structure-learning model. Such an updating algorithm has been implemented within the dynamic expectation maximization (DEM) framework that we currently use [94]. To exploit the algorithm, the current generative model needs to be modified to include a relevant task and associated rewards (both external and internal), so that the model can actively adjust its parameters to optimize rewards. This way, top-down predictions can evolve from na\u00efve (e.g., uniform prior as we simulated in Results) to specific. \n\nOverall, this model adopts a different and complementary perspective from the rapidly developing world of large-scale natural language models [19][20][21] in that it puts upfront the gross biological factors that motivate language in the first place [95][96][97][98], rather than those that seek to match human performance via selected measurements in specific tasks. Recent interesting endeavors in merging these two perspectives focus on adding more \"neural features\", such as longer memory span and domain-general knowledge beyond language, to improve natural language models [24,25]. While this strategy is useful from the viewpoint of artificial language processing, it stays relatively removed from the specific biological substrates of language and hence sheds little light on how human language emerged and evolved under evolutionary pressure.",
            "score": 0.4045352119290522,
            "section_title": "Future development towards natural language understanding",
            "char_start_offset": 53289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1501
                },
                {
                    "start": 1504,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2355
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "5715561"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "5715561"
                },
                {
                    "start": 1151,
                    "end": 1155,
                    "matchedPaperCorpusId": "13354308"
                },
                {
                    "start": 1650,
                    "end": 1654,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "7110414"
                },
                {
                    "start": 1758,
                    "end": 1762,
                    "matchedPaperCorpusId": "1150328"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "260641777"
                },
                {
                    "start": 1766,
                    "end": 1770,
                    "matchedPaperCorpusId": "18168432"
                },
                {
                    "start": 2083,
                    "end": 2087,
                    "matchedPaperCorpusId": "211029226"
                },
                {
                    "start": 2087,
                    "end": 2090,
                    "matchedPaperCorpusId": "222144329"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039337158203125
        },
        {
            "corpus_id": "273707069",
            "title": "GPT or BERT: why not both?",
            "text": "Language models have become fundamental tools in natural language processing, with two dominant paradigms: causal language models (CLM) and masked language models (MLM). Six years ago, GPT by Radford et al. (2018) demonstrated the generative abilities of transformer-based causal language models. Just a few months after this publication, BERT by Devlin et al. (2019) heavily outperformed the causal GPT models when finetuned on downstream NLP tasks, showcasing the major advantage of masked language modeling. These two 'historical' models define the main use-cases of the two paradigms up to this date. \n\nThe difference between these paradigms lies in how they process text. CLMs can only look at previous tokens when making predictions, mimicking the left-to-right reading process. This makes them particularly well-suited for efficient text generation. MLMs, on the other hand, can access both previous and following tokens, allowing them to Causal and masked language modeling can be easily unified by shifting both outputs by one token to the right. Then we can train one language model on both paradigms at the same time just by modifying the input tokens, output tokens and attention masks. build richer contextual representations. This bidirectional context has proven especially valuable for tasks requiring deep language understanding. BERTs should not be forgotten A recent paper by Samuel (2024) revealed that BERT-like model are just as capable text generators as GPT-like models. Yet, when these two types of models are evaluated on a shared ground -generative in-context learning (Brown et al., 2020) -they still show radical differences, clearly outperforming each other in different areas. Each paradigm has its own strengths and combining them into a single hybrid might lead to a model with a more general language understanding. strengths of both CLM and MLM approaches. Our key insight is that the two objectives can be unified by reformulating how output tokens are handled in the MLM framework. Instead of predicting masked tokens at their original positions, we shift the predictions one position to the right, aligning them with the CLM's next-token prediction pattern. This simple modification allows us to train a single model that can seamlessly switch between masked and causal modes without any architectural changes or additional parameters.",
            "score": 0.4045266821772465,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 347,
                    "end": 367,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1395,
                    "end": 1408,
                    "matchedPaperCorpusId": "270357544"
                },
                {
                    "start": 1596,
                    "end": 1616,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14794921875
        },
        {
            "corpus_id": "214612869",
            "title": "A Systematic Study of Inner-Attention-Based Sentence Representations in Multilingual Neural Machine Translation",
            "text": "For every language pair, we compare the BLEU scores between models trained and tested on the Multi30k data set: (1) our best model (M \u2194 M plus monolingual data), (2) the bilingual model of that language pair, (3) the zero-shot translation of the many-to-many model trained without that specific language pair, (4) the Johnson et al.(2017) model using language labels (LL) trained in a many-to-many scenario, and (5) the zero-shot of of LL without that specific language pair.\n\nWe observe that these zero-shot translation scores are generally better than the ones from the previous {De,Fr,Cs}\u2194En model with monolingual data (Table 1). We also note that the zero-shot models perform relatively well in comparison with the MANY-TO-MANY model. Furthermore, these zero-shot models almost reach the scores of the bilingual models trained only on the zero-shot language pairs. As a point of comparison, we also implemented the approach of Johnson et al. (2017), using a language label at the beginning of the input sentence to specify the required target language and a single shared model with joint vocabulary. We will refer to this model as the LL approach herein. We used a combined 40k BPE operations model trained on the combined corpora and the same architecture specifications from Section 3.1, without the components of the attention bridge model. The results are shown in Figure 2 in the gray bars next to our attention bridge scores. We can see that the manyto-many LL models perform slightly better than our attention bridge model. This is not very surprising as they are based on a model architecture that also performs better in the bilingual case as we have seen in the comparison between bilingual models with and without attention bridge in Table 1. Section 3.2.2 will also show that this is basically caused by long sentences that are not as well covered by the attention-bridge model.\n\nA similar effect is visible in the zero-shot results that we obtain in the same way as with our attention bridge model (i.e., leaving one language pair out of the training data). The differences to our model are sometimes larger than in the supervised set-up. This can be explained by the positive effect of sharing all",
            "score": 0.40430942180231916,
            "section_title": "Figure 2",
            "char_start_offset": 18840,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 318,
                    "end": 338,
                    "matchedPaperCorpusId": "260464809"
                },
                {
                    "start": 932,
                    "end": 953,
                    "matchedPaperCorpusId": "260464809"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0211639404296875
        },
        {
            "corpus_id": "259257779",
            "title": "Learning Language Structures through Grounding",
            "text": "Building on the initial success reached by my work described above, I am enthusiastic about working on the following directions in the near future:\n\nUsing additional grounding signals.In addition to static images, introducing more forms of grounding signals such as audio or video, may model human behaviors more comprehensively and strengthen the models.Along this line, an ongoing project of mine models syntax acquisition through visually grounded speech and without a supervised speech recognition model.I am also interested in machine language understanding through other grounding signals, such as tactile and eye-tracking information, as well as through humancomputer interaction.Beyond language structures, since these grounding signals provide more information about the context, I would like to explore how they can be used in modeling context-dependent understanding of language, i.e., pragmatics.\n\nImproving efficiency and generalizability with awareness of structures.While pretrained large language models are powerful, they require a large amount of data to train, which is not always available in practice.I am interested in developing models that are similarly powerful but more efficient in terms of both data and computational resources, where structures of natural languages learned or induced from my prior work can be used to guide the learning process.Implicit or explicit awareness of structures may additionally help generalization from shorter sentences to longer ones that share similar structures: for example, I am interested in designing models that understand complicated sentences like \"I heard that Alice said Bob said you are right\" by training with simple examples that have similar syntax, such as \"I said you are right\".\n\nTowards universal natural language understanding.While languages all over the world share many commonalities, they are typologically diverse.Moreover, current models are arguably designed on and biased towards high-resource languages; therefore, I will seek to build systems that can work equally well for lowresource languages through developing cross-lingual transfer techniques.My future work will consider both generic modeling and language-specific parameters such as head directionality.I am particularly interested in analyzing the syntactic and semantic phenomena presented by the pretrained large language models, as well as improving the models based on my findings in a resource-efficient manner.\n\nScientifically, I am also interested in applying computational methods and grounding signals to facilitate linguistics and cognitive science research.",
            "score": 0.4039306548666901,
            "section_title": "Future and Current Work",
            "char_start_offset": 4977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 149,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 687
                },
                {
                    "start": 687,
                    "end": 908
                },
                {
                    "start": 910,
                    "end": 981
                },
                {
                    "start": 981,
                    "end": 1122
                },
                {
                    "start": 1122,
                    "end": 1375
                },
                {
                    "start": 1375,
                    "end": 1757
                },
                {
                    "start": 1759,
                    "end": 1808
                },
                {
                    "start": 1808,
                    "end": 1900
                },
                {
                    "start": 1900,
                    "end": 2140
                },
                {
                    "start": 2140,
                    "end": 2252
                },
                {
                    "start": 2252,
                    "end": 2466
                },
                {
                    "start": 2468,
                    "end": 2618
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07208251953125
        },
        {
            "corpus_id": "259202736",
            "title": "Revealing the structure of language model capabilities",
            "text": "The capabilities of Large language models (LLMs) are growing broader every year. Yet our understanding of the these capabilities remains remarkably narrow. Because these systems are so general, it has proved incredibly challenging to create benchmarks that can provide an informative account of their capabilities and limitations. In an attempt to do so, researchers have developed large benchmarks such as Big-BENCH and HELM [30,19] that test the performance of LLMs on a variety of different tasks. These benchmarks are an important step in the right direction, but they currently lack both explanatory and predictive value-although they might be able to tell us that a model performs poorly on a particular set of tasks, they cannot tell us why the model struggled nor accurately predict how it will behave for a new, untested task. \n\nPart of the problem is that the underlying abilities that enable LLMs to perform well across so many tasks are poorly understood [32,2,3]. We do not yet have a clear idea of how to meaningfully characterize these abilities, how they are structured, or how they are related to one another. We One useful way to build this understanding would be to utilize factor analysis techniques, which can be used to extract latent factors that explain variance across different tasks. The data-driven, \"bottomup\" nature of these techniques makes them an excellent starting point for empirical investigations into the structure of language model capabilities. This approach also fits with research in cognitive science, which has long used factor analysis to build an understanding of the structure of intelligence in humans and animals. \n\nUntil recently, the relatively small number of language models capable of complex behavior had rendered this approach infeasible. However, the recent explosion in the number of LLMs of various scales, architectures, and training regimes now makes it possible to collect data from a varied population of models. In addition, over the past few years, large test benchmarks designed to evaluate the broad capabilities of LLMs have been developed, such as BIG-bench and HELM [30,19]. These benchmarks provide a standardized way of testing LLMs across a variety of tasks, many of which were designed to test specific cognitive abilities.",
            "score": 0.40387463544138336,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1484
                },
                {
                    "start": 1485,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 971,
                    "end": 973,
                    "matchedPaperCorpusId": "250628479"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.110107421875
        },
        {
            "corpus_id": "86439163",
            "title": "Demographic Inference and Representative Population Estimates from Multilingual Social Media Data",
            "text": "Model. The full model comprises two separate pipelines for processing a profile image and each of the three text sources of information, as shown in Figure 2. Here, we use a shared pipeline for all three attributes, using multi-task learning for the output. We refer to this as the M3 model, after its multimodal, multilingual, and multi-attribute abilities. Following, we describe the architectures of each modality and how they are combined. Image Model The image classifier was constructed using DenseNet [38] based on initial performance experiments over current stateof-the-art vision models on our data. We scale all profile images to 224x224 to meet the input size of the model. Text Model Separate character-based neural models were trained for each text input and then a multi-input model was tuned based on the weights from each single model. Embeddings for username and biography were limited to the 3000 most frequent characters in our training corpus, with remaining characters represented with an encoding for their unicode category; screen name are embedded in the ASCII range. We use 2-stack bidirectional character-level Long-short Term Memory (LSTM) architecture [36] to capture necessary information. To incorporate potential different meanings in different languages for the same character, we concatenate a separate trainable language embedding to each character embedding and add a fully connected layer before being fed into an LSTM. The fusion of character-based input with language embeddings provides two critical benefits that let us scale to the linguistic diversity seen in global platforms like twitter. (1) Character-based RNNs are able to capture morphological regularity within a language and substantially reduce the parameter space due to the reduced vocabulary size of the model (i.e., the set of characters) as compared with word-based models with orders-of-magnitude larger vocabularies [15,27,46]. (2) By fusing language embeddings with character embeddings, the joint embedding lets us represent shared linguistic regularity across languages, which lets us better scale to the multilingual environment. Full Multimodal Model To construct the full model, separate models are fully trained for image and text for all attributes.",
            "score": 0.40382462401523067,
            "section_title": "2.2.1",
            "char_start_offset": 7205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2142
                },
                {
                    "start": 2143,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 512,
                    "matchedPaperCorpusId": "9433631"
                },
                {
                    "start": 1181,
                    "end": 1185,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1929,
                    "end": 1932,
                    "matchedPaperCorpusId": "3089175"
                },
                {
                    "start": 1932,
                    "end": 1935,
                    "matchedPaperCorpusId": "686481"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048309326171875
        },
        {
            "corpus_id": "15677874",
            "title": "The Speech-Language Interface in the Spoken Language Translator",
            "text": "In the SLT project, we used the CLE grammar for English (Alshawi, 1992, chapters 4 and 5;Agn\u00e4s et al, 1994, chapter 7), but the techniques do not make any special use of its peculiarities, and would be applicable to any general unification-based phrase-structure grammar. The key point is that the general grammar is unsuitable for the language modelling task because it is over-general; in particular, there is no need in the context of a normal spoken language domain to have a fully recursive grammar. \n\nWe specialize the grammar to the domain by first using it to parse a substantial corpus of examples (in the concrete experiments carried out, we used a set of about 5000 ATIS sentences). We then extract a much simpler grammar from the original one by cutting up the analysis trees from the parsed corpus into sub-trees, where each sub-tree corresponds to a linguistic \"chunk\" or unit; we used only four chunk types (utterance, noun phrase, non-recursive noun-phrase and preposition phrase), compared to about twentyfive different phrase types in the original grammar. The rules contained in each sub-tree are then \"collapsed\" into a single rule for the appropriate chunk-type, using the so-called Explanation-Based Learning algorithm (van Harmelen and Bundy, 1988;Hirsh, 1987). With a suitable choice of chunk-types, we can produce a specialized grammar whose rules correspond to chunk patterns occurring in the training corpus. \n\nBy construction, the specialized grammar has strictly less coverage on the domain than the original one. Our experiments suggest, however, that given a substantial training corpus the loss of coverage is on the order of a few percent at most. This loss of coverage is more than counterbalanced by the greatly simplified structure of the specialized grammar, which can be parsed nearly two orders of magnitude more quickly than the general one, using an LR parsing algorithm (Samuelsson, 1994). The gain in speed is due to the fact that the grammar, after specialization, is nearly finite-state; we have in effect automatically squeezed a general grammar into a finite-state format, after cutting off the few pieces that refuse to fit.",
            "score": 0.4036664678688808,
            "section_title": "GRAMMAR SPECIALIZATION FOR FAST PARSING",
            "char_start_offset": 16372,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1435
                },
                {
                    "start": 1438,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2172
                }
            ],
            "ref_mentions": [
                {
                    "start": 1271,
                    "end": 1283,
                    "matchedPaperCorpusId": "1656825"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127685546875
        },
        {
            "corpus_id": "276408895",
            "title": "Language Complexity Measurement as a Noisy Zero-Shot Proxy for Evaluating LLM Performance",
            "text": "Although our findings demonstrate that language complexity measures (e.g., LIX) can serve as a useful-albeit noisy-zero-shot proxy for general LLM abilities, several limitations merit discussion: \n\nSingle run: Every LIX-computation and dependency-parsing was only done once per sentence and model. \n\nScope of Evaluation: First, our experiments focus exclusively on Swedish texts drawn from university and high-school essays. These sources may not capture the full breadth of linguistic styles and complexities found in other domains, genres, or languages. Consequently, our results may not generalize to languages with different morphological structures or to more specialized text types such as technical documents or colloquial dialogues. \n\nReliance on Proprietary Models: We primarily evaluated closed-source, proprietary models whose internal architectures, tokenization schemes, and training data are neither publicly accessible nor standardized. \n\nTokenization and Computational Fidelity: Our analysis relies on each model's tokenization process, which can differ substantially across architectures and affect the accuracy of LIX or ADD computations. Because we depend on the models to generate the appropriate tokens and numerical outputs, discrepancies in tokenization can introduce systematic errors in measured performance. \n\nRapid Model Evolution: Large Language Models evolve rapidly through fine-tuning and architectural enhancements. Performance measured at a particular snapshot in time may not generalize to newer versions of the same model. Ongoing and systematic evaluations are therefore essential to capture how fast-changing model iterations handle language complexity tasks. \n\nData Sampling: Finally, we used a limited number of paragraphs and sentences for LIX and dependency parsing evaluations. While this was sufficient to identify consistent errors and variance across models, larger datasets-potentially spanning multiple domains-would improve robustness and strengthen the statistical reliability of our conclusions.",
            "score": 0.40365495430838616,
            "section_title": "Limitations",
            "char_start_offset": 12785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 951
                },
                {
                    "start": 954,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1696
                },
                {
                    "start": 1699,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2045
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030731201171875
        },
        {
            "corpus_id": "14784553",
            "title": "Unsupervised morph segmentation and statistical language models for vocabulary expansion",
            "text": "This work concerned the use of unsupervised morphological segmentation and statistical language models for the task of vocabulary expansion. Unsupervised vocabulary expansion has large potential for reducing OOV-rates and improving results in NLP tasks especially in less-resourced settings for morphologically rich languages. \n\nThe suggested method was evaluated on some of the morphologically rich languages of the Babel corpus in the limited language pack condition. The performance of the method was evaluated in terms of the improvement of the OOV-rate on the development set. The suggested combination of segmentation and interpolation of statistical language models provided to our understanding the best results on the task so far. Compared to (Rasooli et al., 2014), our approach differed in that the statistical language models were used directly in the word generation phase. As opposed to (Trmal et al., 2014), our approach operated purely on the grapheme level. \n\nIt is perhaps noteworthy, that the methods are not that different from what one would use in a normal language modelling scenario for automatic speech recognition. Morfessor Baseline (Creutz and Lagus, 2002) has been seen to give good results in morph-based speech recognition (Creutz et al., 2007) when used along with standard n-gram models. If a larger training corpus is available, optimizing unigram likelihood more directly may be a good choice (Varjokallio et al., 2013). \n\nMorph segmentations provided by the Morfessor Flatcat (Gr\u00f6nroos and Virpioja, 2014) -method were also evaluated for this work, but Morfessor Baseline was found to perform better. It is possible, that the tradeoff between the lexicon cost and the corpus encoding cost, as given by the Minimum Description Length -principle, is important for the modelling accuracy in this type of a less-resourced scenario. Morfessor Flatcat will in most cases segment more accurately according to the grammatical morph boundaries. This is likely a more valuable property for statistical machine translation than for the present task. \n\nThe linear interpolation of an n-gram model and a recurrent neural network language model provides at the moment state-of-the-art modelling accuracy in many statistical language modelling tasks.",
            "score": 0.4035495870019017,
            "section_title": "Discussion",
            "char_start_offset": 10257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 326
                },
                {
                    "start": 329,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 974
                },
                {
                    "start": 977,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1971
                },
                {
                    "start": 1972,
                    "end": 2074
                },
                {
                    "start": 2077,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 752,
                    "end": 774,
                    "matchedPaperCorpusId": "7180044"
                },
                {
                    "start": 901,
                    "end": 921,
                    "matchedPaperCorpusId": "10224690"
                },
                {
                    "start": 1160,
                    "end": 1184,
                    "matchedPaperCorpusId": "5133576"
                },
                {
                    "start": 1428,
                    "end": 1454,
                    "matchedPaperCorpusId": "1516153"
                },
                {
                    "start": 1512,
                    "end": 1541,
                    "matchedPaperCorpusId": "18122636"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05224609375
        },
        {
            "corpus_id": "235474507",
            "title": "Pool of Experts: Realtime Querying Specialized Knowledge in Massive Neural Networks",
            "text": ",4,28], and thereby construct smaller versions of oracles. Then, it re-trains only the output layer of these smaller oracles to specialize them for a given target task. In NoScope and its extended version [11,12], multiple smaller architectures are trained as candidate specialized models, and the one with a reasonably good accuracy beyond a predefined threshold is selected. In all these methods, we should carry out either additional training or training from scratch to build a specialized model whenever a new target task is given. However, PoE can synthesize the specialized model for any queried task in realtime because there is no further training process after extracting experts from oracle in the preprocessing phase.\n\nModel unification. The problem of unifying different classifiers has long been studied in the machine learning community. A typically known way is the ensemble method [14,17], where we combine multiple models by voting or averaging over their outputs. However, ensemble methods assume that every model is built for the same task, and therefore are not applicable to merging multiple specialized models like experts of PoE. To the best of our knowledge, in our framework, the method most closely related to the scenario of merging experts is UHC [27]. The UHC method intends to merge heterogeneous neural networks that have been independently trained for different classes by distilling the knowledge from multiple teachers into a single student model. This method can be used to merge experts in our framework as well. Unfortunately, to be experimentally shown in Section 5.3, UHC does not only require a non-trivial time of training, but also generates models even less accurate than the task-specific models built by PoE that needs no training at all. Moreover, we observe that UHC is greatly influenced by how each teacher model is trained. If all the teachers are disjoint in their classes and separately trained from scratch, models combined by UHC turn out to get significantly worse due to the lack of the knowledge across experts. Similar to UHC, DMC [32] extends the standard KD method to combine two disjoint models by using its proposed double distillation method. For the problem of continual learning [22], they focus on how to combine the two models, namely the previously trained old model and the one trained",
            "score": 0.40336204411250487,
            "section_title": "RELATED WORKS",
            "char_start_offset": 9329,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1,
                    "end": 3,
                    "matchedPaperCorpusId": "2238772"
                },
                {
                    "start": 3,
                    "end": 6,
                    "matchedPaperCorpusId": "12971356"
                },
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "208921246"
                },
                {
                    "start": 209,
                    "end": 212,
                    "matchedPaperCorpusId": "20732104"
                },
                {
                    "start": 898,
                    "end": 902,
                    "matchedPaperCorpusId": "1991617"
                },
                {
                    "start": 1276,
                    "end": 1280,
                    "matchedPaperCorpusId": "119185541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135498046875
        },
        {
            "corpus_id": "256312594",
            "title": "K-CTIAA: Automatic Analysis of Cyber Threat Intelligence Based on a Knowledge Graph",
            "text": "Pre-trained language representation models such as BERT [27] can obtain common language representations from large corpora. The BERT model has achieved good performance in various tasks of natural language processing. At the same time, some researchers are applying BERT to professional fields, such as military and agriculture [28,29]. \n\nHowever, due to the lack of specialized knowledge in the general corpus, the model has a poor understanding of the semantic information of proper nouns that rarely appear in the general corpus. To address this issue, some researchers combine knowledge graphs with pre-trained models to annotate these difficult terms. As a result, the pre-trained model can obtain the semantics of these terms more accurately. K-BERT [30] proposes a knowledge-supported language representation model, which is compatible with BERT and can combine related knowledge to solve the situation where the pre-trained model does not perform well in a specific domain. K-BERT inserts knowledge into the input in the form of branches and changes the model structure to solve the knowledge noise problem. K-BERT significantly outperforms BERT not only on domain-specific tasks, but also on open-domain tasks. Some scholars also use other methods to integrate knowledge maps into pre-trained models. These methods can be divided into three types: implicit fusion, explicit fusion, and joint learning of a knowledge graph and a pre-trained model [31].",
            "score": 0.4032395209970701,
            "section_title": "Pre-Trained Model Combined with a Knowledge Graph",
            "char_start_offset": 8538,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 336
                },
                {
                    "start": 339,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1460
                }
            ],
            "ref_mentions": [
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "202583325"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1370849609375
        },
        {
            "corpus_id": "259370612",
            "title": "Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding",
            "text": "In this paper we compare a shared encoder architecture for SLU to a baseline architecture that was chosen based on the specific latency and cost constraints of an industry SLU system. Since encoder model sizes were chosen based on specific constraints the results may not be directly comparable to model sizes more commonly used in the literature such as BERT-large and BERT-base. We expect the general benefit and order of magnitude of accuracy improvements shown in our evaluations to transfer to comparable setups with different parameters. \n\nThe primary focus of this paper is on accuracy improvements and addressing challenges of realworld SLU systems such as distribution drift and feature expansion. We do not elaborate on the details of the computational cost and inference aspects. A detailed analysis of compute cost and benchmarks of CPU and GPU inference would better highlight the infrastructure cost benefits of a shared encoder architecture for SLU. \n\nRegarding the multi-lingual aspect of the encoder we only tested a single grouping of similar European languages (German, French, Italian and Spanish). A more extensive analysis of different language groups would demonstrate that similar trade-offs seen in other works on multi-lingual language models also apply for the shared encoder architecture.",
            "score": 0.40303470492804316,
            "section_title": "Limitations",
            "char_start_offset": 21880,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 543
                },
                {
                    "start": 546,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1316
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08038330078125
        },
        {
            "corpus_id": "264426560",
            "title": "Revisiting Instruction Fine-tuned Model Evaluation to Guide Industrial Applications",
            "text": "Adapting pre-trained language models (LMs) for specific applications is central in industrial NLP to unlock task-specific performance gains and strengthen model alignment with industry requirements. A paradigm gaining traction is the use of instruction fine-tuned (IFT) models, LMs capable of following arbitrary instructions expressed in natural language (Wei et al., 2022a;Sanh et al., 2022;Ouyang et al., 2022). \n\nResearchers primarily concentrate on improving general-purpose IFT models to be used as versatile agents capable of executing instructions expressed in natural language (Li et al., 2023;Zhou et al., 2023;Xu et al., 2023a). In an industrial setting, prompting ChatGPT to improve the wording of an email, or to assist with a code snippet would be instances of this zero-shot utilization scenario, which we define as S 0 . Critical industrial LLM applications may however not always align with S 0 , and often prioritize two other settings. The first scenario, S 1 , requires extending a generalist IFT model's capabilities to new specific tasks not included in the original instruction training set. The second scenario, S 2 , centers around converting IFT models into specialized models proficient exclusively on specific tasks. In S 1 for instance, a large company may want an LLM assistant for internal employee use, and decide to extend an openly available Chat model by training it to write memos with a specific templating scheme, to respond to internal FAQs, and to use internal coding tools, all the while retaining the original chat assistant's general purpose abilities. In S 2 , that same company is only interested in a given specific task; extracting specific information from business documents, and specializes an IFT model for that purpose, aiming to leverage prompting and the generalization capabilities of the model for a more data-efficient training. \n\nIn this paper, we thoroughly examine S 1 and S 2 by investigating the learning dynamics of specializing IFT models through a practical lens. To ensure the reliability of our tooling and the rigor of our conclusions, we first undertake a critical assessment of the current evaluation practices employed for IFT models. Formally, our contributions are: Contribution 1. IFT models are designed to handle tasks of diverse natures and varying difficulties.",
            "score": 0.4027683332668248,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25732421875
        },
        {
            "corpus_id": "266818228",
            "title": "Training and Serving System of Foundation Models: A Comprehensive Survey",
            "text": "Multi-model inference involves utilizing multiple models for serving. These models can be of the same type or different types, often with varying architectures. An important research question in this context is how to effectively combine these diverse models and optimize resource allocation to achieve optimal performance. In the context of multi-task models created through fine-tuning, PetS [94] introduces an innovative framework for multi-task Parameter Efficient Transformers (PET) that processes various tasks in a unified manner. Traditional fine-tuning of the entire model for each task incurs substantial memory overhead. PetS circumvents this by employing parameter-efficient fine-tuning methods such as Adapters [95], [96], splitting the model into a shared core and task-specific small operators. This architecture allows for shared base model usage across tasks, reducing memory demands and streamlining model deployment. In the context of hierarchical models ranging from small to large, one approach is presented by Tabi [97]. Tabi leverages the observation that smaller models often exhibit predictive capabilities similar to larger models by implementing a multi-level inference engine. It employs well-calibrated confidence scores using temperature scaling to determine whether a query can be promptly resolved using the smaller model or if it should be escalated to the larger model. For escalated queries, Tabi reduces system overhead by employing attention-based word pruning and a weighted ensemble approach. Another technique introduced by Google Research is Speculative Decoding [98], which aims to accelerate the inference process for language models. This method involves a smaller model generating tokens sequentially while a larger model simultaneously validates the correctness of each token in parallel. The larger model verifies the sequence of tokens produced by the smaller model, enabling the generation of multiple tokens within a single iteration of the larger model. LLMCad [99] differs from Google's Speculative Decoding by employing a tree-based token generation approach that facilitates the concurrent evaluation of multiple tokens. To accomplish this, LLMCad utilizes a smaller language model to construct a comprehensive vocabulary tree comprising various word paths. The larger LLM then efficiently and concurrently evaluates these paths.",
            "score": 0.40230021738543004,
            "section_title": "E. Multi-Model Inference",
            "char_start_offset": 43333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2311
                },
                {
                    "start": 2312,
                    "end": 2383
                }
            ],
            "ref_mentions": [
                {
                    "start": 394,
                    "end": 398,
                    "matchedPaperCorpusId": "266233970"
                },
                {
                    "start": 724,
                    "end": 728,
                    "matchedPaperCorpusId": "221370525"
                },
                {
                    "start": 1037,
                    "end": 1041,
                    "matchedPaperCorpusId": "258508784"
                },
                {
                    "start": 1604,
                    "end": 1608,
                    "matchedPaperCorpusId": "254096365"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.191650390625
        },
        {
            "corpus_id": "272593221",
            "title": "What is the Role of Small Models in the LLM Era: A Survey",
            "text": "Despite their growing capabilities, generalpurpose large language models (LLMs) still require further customization to achieve optimal performance in specific use cases (e.g. coding) and domains (e.g. medical tasks). While finetuning on specialized data is one approach to adapting LLMs, this process has become increasingly resource-intensive, and in some cases, it is not feasible-especially when access to internal model parameters is restricted, as with models like ChatGPT. Recent research has explored adapting LLMs using smaller models, which can be categorized into two approaches: White-Box Adaptation and Black-Box Adaptation, depending on whether access to the model's internal states is available. \n\nWhite-Box Adaptation typically involves finetuning a small model to adjust the token distributions of frozen LLMs for a specific target domain. For instance, CombLM (Ormazabal et al., 2023) learns a linear function to combine the probability distributions from the large black-box model with those from a smaller domain-specific expert model. IPA (Lu et al., 2023b) introduces a lightweight adapter that tailors a large model toward desired objectives during decoding without requiring fine-tuning. IPA achieves this by optimizing the combined distribution using reinforcement learning. Proxy-tuning (Liu et al., 2024a) fine-tunes a smaller language model, contrasting the probabilities between the tuned model (the expert) and its untuned version (the anti-expert) to guide the larger base model. These approaches only modify the parameters of small domain-specific experts, allowing LLMs to be adapted to specific domain tasks. However, white-box adaptation is not applicable to API-only modeling services, where access to internal model parameters is restricted. \n\nBlack-Box Adaptation involves using a small domain-specific model to guide LLMs toward a target domain by providing textual relevant knowledge. Retrieval Augmented Generation (RAG) can extract query-relevant knowledge from an external document collection or knowledge base, and thus enhance general LLMs by leveraging their incontext learning ability.",
            "score": 0.40227170155077036,
            "section_title": "Domain Adaptation",
            "char_start_offset": 26031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 709
                },
                {
                    "start": 712,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 1059,
                    "end": 1076,
                    "matchedPaperCorpusId": "258865629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1337890625
        },
        {
            "corpus_id": "273229463",
            "title": "Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?",
            "text": "Our study rigorously verified that nearly all models trained on individual programming languages consistently achieve the better logical inference performance than those trained solely on natural language datasets in few-shot ICL settings. Further analysis reveals that an appropriate level of syntax complexity influences logical reasoning performance. Additionally, models trained on programming languages exhibit a greater ability to follow instructions compared to those trained on natural language datasets. Moreover, dependencies expressed in code significantly contribute to logical reasoning in few-shot ICL settings. We hope these findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs. \n\nOwing to the limitation of the computational resources, we could not train the models larger than 1.5B parameters. Especially for FLD tasks, logical inference ability is limited even in models with 10 billion parameters (Morishita et al., 2024). Future work includes investigations into the effect of code-based pre-training with larger models to verify that logical reasoning abilities are more explicitly improved. Each dataset is primarily organized in either natural language or a single programming language, although we did not conduct thorough filtering to ensure complete exclusivity. In Section 4.2, we fixed grammar complexity by selecting a single language and examined the syntax complexity in code data. However, our experiments did not consider semantic complexity or other complexities that might be measureable in both programming and natural languages. Furthermore, it remains unclear whether syntax complexity in pre-training data alone influences logical inference performance. Comparing various complexities between natural and programming language regarding logical reasoning abilities is an important avenue for future research. \n\nIn section 4.4, we assessed the general language understanding of the trained models. The natural language inference tasks in GLUE require commonsense knowledge, which may be difficult to acquire through code-only training. Future experiments could explore whether fine-tuning models pre-trained on code with GLUE datasets enhances natural language reasoning capabilities. Additionally, integrating both code and natural language datasets during the pre-training process may provide a synergistic approach to leverage the strengths of both types of data. \n\nMoreover, a further experiment in Appendix E demonstrates the advantage on FLD tasks between natural language and programming language is reversed when fine-tuning on FLD corpus.",
            "score": 0.4021659369929065,
            "section_title": "Conclusion",
            "char_start_offset": 16262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2472
                },
                {
                    "start": 2475,
                    "end": 2653
                }
            ],
            "ref_mentions": [
                {
                    "start": 985,
                    "end": 1009,
                    "matchedPaperCorpusId": "269804678"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.013275146484375
        },
        {
            "corpus_id": "257078634",
            "title": "Modular Deep Learning",
            "text": "While a full-fledged comprehensive empirical study of the plethora of modular architectures across various application tasks and areas is still lacking, there exist initiatives such as the publicly available AdapterHub platform (Pfeiffer et al., 2020a): it provides (re)implementations of representative modular NLP architectures, within a unified framework tied to HuggingFace Transformers . Among others, AdapterHub includes representatives of each computation method in \u00a7 3: LoRA (Hu et al., 2022) (i.e., low-rank parameter composition), prefix tuning of  (input composition) and a number of bottleneck adapter configurations (function composition). The existence of AdapterHub delineates another crucial advantage of modularity: reusability of existing, already fine-tuned modules which can be (re)combined with the large neural models. In short, any practitioner can share or reuse a module specialised for a particular purpose (e.g., capturing specific task or language knowledge) with the community, facilitating community-wide sharing and thus avoiding time-and energy-costly repetitions of the same fine-tuning procedure. 19 As discussed in \u00a7 4, one can observe initiatives such as AdapterHub as continuously updating community-distributed multi-task models.\n\nThe discussion in this section also points to a more general principle: different end-goals even within the same end-application (e.g., NMT, cross-lingual transfer, domain adaptation) require rethinking the actual modular design, and the desired level and nature of modularity. For instance, if the goal in NMT (or cross-lingual transfer) is to boost performance for a particular translation or transfer direction, it might be useful to trade off some modularity for a better final performance by replacing language-specific monolingual modules with bilingual modules (Bapna & Firat, 2019;Parovi\u0107 et al., 2022). On the other hand, if the goal is to enable zero-shot or few-shot translation or transfer, the design with monolingual modules might be a better choice. In another example, if the focus is on MT or transfer for a particular low-resource language, the model designer should enable positive transfer to that language by 'opening' the flow of information",
            "score": 0.4021109371915476,
            "section_title": "Comparison and Design Principles",
            "char_start_offset": 108207,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 252,
                    "matchedPaperCorpusId": "220525782"
                },
                {
                    "start": 483,
                    "end": 500,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1837,
                    "end": 1858,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 1858,
                    "end": 1879,
                    "matchedPaperCorpusId": "250390710"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.259033203125
        },
        {
            "corpus_id": "244908620",
            "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding",
            "text": "Large-scale models, such as BERT [9], RoBERTa [26], GPT-3 [5], T5 [36], and BART [23], have greatly contributed to the development of the field of natural language processing (NLP), and gradually form the pretrain-then-finetune paradigm. The basic idea of this paradigm is to first pre-train a model on large general-purpose datasets by self-supervised tasks, e.g., masking tokens in training data and asking the model to guess the masked tokens. The trained model is then fine-tuned on smaller and more specialized datasets, each designed to support a specific task. The success of pre-trained models in the natural language domain has also spawned a series of pre-trained models for programming language understanding and generation, including CodeBERT [11], GraphCodeBERT [13], PLBART [2], and the usage of T5 to support code-related tasks [28], improving the performance of a variety of source code understanding and generation tasks. \n\nHowever, pre-training a large-scale model from scratch is costly. Additionally, along with an increasing number of pre-trained models, how to effectively adapt these models for a new task is not fully exploited. In this paper, we try to take the first step to bridge large pre-trained models and code-related downstream tasks. Moreover, despite the success of existing pre-trained models for code-related tasks, these models have two potential issues. First, these models graft NLP pre-training techniques to understand the semantics of source code, however, the semantics of programming language and natural language are essentially different, and semantically equivalent source code may be in various syntactic forms. The second issue is that pre-trained models typically have at least millions of parameters, so when a pre-trained model is applied to downstream tasks with specialized datasets, there is a risk of overfitting because the model is over-parameterized for the target dataset. Many studies have also found that when the test set is different from the actual scene or the test set is slightly perturbed, various models for source code would make mistakes [34,37,53].",
            "score": 0.40205292007827625,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 33,
                    "end": 36,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "232185260"
                },
                {
                    "start": 843,
                    "end": 847,
                    "matchedPaperCorpusId": "231786586"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1495361328125
        },
        {
            "corpus_id": "270371891",
            "title": "LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low-Resource and Extinct Languages",
            "text": "In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",
            "score": 0.40198956371245265,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0255126953125
        },
        {
            "corpus_id": "250590395",
            "title": "An Improved Math Word Problem (MWP) Model Using Unified Pretrained Language Model (UniLM) for Pretraining",
            "text": "Experimental results show that BERT achieved the SOTA results on GLUE, RACE, and SQuAD. Moreover, ERNIE [27] is an exploratory framework for continuous learning and understanding based on knowledge enhancement proposed by Baidu. e framework combines big data presets with multi-source knowledge. \n\nrough learning technology, it continuously absorbs knowledge of the text structure and learns in massive data texts to realize the model. ERNIE has achieved SOTA effects in more than 40 classic NLP missions, and has won more than 10 championships on international celebrities such as GLUE, VCR, XTREME, and SemEval. \n\nUniLM is a BERT-based model, which is a simple but effective multimodal pretraining method of text. Unlike BERT, UniLM can be configured using different self-attention masks to aggregate context for different types of language models. It is made up of Transformer AI models jointly pretrained on large amounts of text and optimized for language modeling. e UniLM model uses three types of language modeling (one-way model, two-way model, and sequence-to-sequence prediction model) for pretraining [28]. Using a shared Transform network, a specific self-attention mask is used to control the context of prediction conditions, thereby achieving unified modeling. For example, in the work of [29], they proposes UniVL: a Unified Video and Language pretraining model for both multimodal understanding and generation. It comprises four components, including two single-modal encoders, a cross encoder, and a decoder with the Transformer backbone. Five objectives, including video-text joint, conditioned masked language model (CMLM), conditioned masked frame model (CMFM), video-text alignment, and language reconstruction, are designed to train each of the components. e train skills in [30][31][32][33] are applied in this paper. \n\nIn this paper, a semisupervised approach based on UniLM is proposed. e model allows unsupervised previewing and supervised tuning for language processing tasks. Experiment results show a maximum accuracy of 79.57% of the proposed model.",
            "score": 0.4018567911872656,
            "section_title": "Related Work",
            "char_start_offset": 10818,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 88,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 295
                },
                {
                    "start": 298,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 613
                },
                {
                    "start": 616,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1842
                },
                {
                    "start": 1845,
                    "end": 1913
                },
                {
                    "start": 1914,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2081
                }
            ],
            "ref_mentions": [
                {
                    "start": 1799,
                    "end": 1803,
                    "matchedPaperCorpusId": "233856216"
                },
                {
                    "start": 1803,
                    "end": 1807,
                    "matchedPaperCorpusId": "196181536"
                },
                {
                    "start": 1807,
                    "end": 1811,
                    "matchedPaperCorpusId": "248689632"
                },
                {
                    "start": 1811,
                    "end": 1815,
                    "matchedPaperCorpusId": "243865347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11163330078125
        },
        {
            "corpus_id": "273963924",
            "title": "Combining Domain and Alignment Vectors to Achieve Better Knowledge-Safety Trade-offs in LLMs",
            "text": "Large language models (LLMs) have demonstrated strong abilities in solving complex tasks such as question answering, summarization, reasoning, and creative writing [Zhao et al., 2024]. However, these abilities are general-purpose, and LLMs can lack deep expertise in tasks requiring domain specialization [Ling et al., 2024]. Naturally, there has been increasing research in developing domain-expert LLMs, either through complete pre-training on domain-specific data [Wu et al., 2023], continued pre-training of existing general-purpose LLMs [Sankarasubbu and Pal, 2024], or instruction-tuning pre-trained LLMs on domain data [Yue et al., 2023]. \n\nWhile powerful, these domain expert models are often significantly less safe compared to their generalist counterparts as they either do not explicitly undergo safety alignment in case of pre-training from scratch and continual pre-training, or their safety alignment gets compromised due to domain-specific fine-tuning or instruction-tuning [Bhardwaj et al., 2024]. Safety alignment of these domain expert models is crucial given their widespread adoption. However, this might be overseen due to a lack of resources, training data, alignment expertise, or concerns about potential degradation in the domain utility of models due to over-alignment -a phenomenon known as the alignment tax [Lin et al., 2024]. \n\nRecently, model merging has emerged as an effective method for combining task-specific models into a single model without additional training [Ilharco et al., 2023]. Model merging interpolates the parameters of multiple models, and has been extended to LLMs by leveraging task vectors. Task vectors capture the adjustments made to the weights of a general-purpose pre-trained model to create a taskspecific one, calculated by subtracting the original model from the task model. Combining them has been shown to yield minimal performance degradation in multi-task evaluations [Yadav et al., 2023]. \n\nDrawing inspiration from these findings, we expand the concept of task vectors to domain and alignment vectors for LLMs, computed by leveraging the corresponding domain adapted and aligned models of a pre-trained model, respectively.",
            "score": 0.4017209630816925,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 645
                },
                {
                    "start": 648,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1356
                },
                {
                    "start": 1359,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1955
                },
                {
                    "start": 1958,
                    "end": 2191
                }
            ],
            "ref_mentions": [
                {
                    "start": 1501,
                    "end": 1523,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1934,
                    "end": 1954,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29443359375
        },
        {
            "corpus_id": "253571549",
            "title": "Dual Language Models for Code Switched Speech Recognition",
            "text": "Code-switching is a commonly occurring phenomenon in multilingual communities, wherein a speaker switches between languages within the span of a single utterance. Code-switched speech presents many challenges for automatic speech recognition (ASR) systems, in the context of both acoustic models and language models. Mixing of dissimilar languages leads to loss of structure, which makes the task of language modeling more difficult. Our focus in this paper is on building robust language models for code-switched speech from bilingual speakers. \n\nA na\u00efve approach towards this problem would be to simply use a bilingual language model. However, the complexity of a full-fledged bilingual language model is significantly higher than that of two monolingual models, and is unsuitable in a limited data setting. More sophisticated approaches relying on translation models have been proposed to overcome this challenge (see Section 2), but they rely on external resources to build the translation model. In this paper, we introduce an alternate -and simpler -approach to address the challenge of limited data in the context of code-switched text without use of any external resources. \n\nAt the heart of our solution is a dual language model (DLM) that has roughly the complexity of two monolingual language models combined. A DLM combines two such models and uses a probabilistic model to switch between them. Its simplicity makes it amenable for generalization in a low-data context. Further there are several other benefits of using DLMs. (1) The DLM construction does not rely on any prior information about the underlying languages. (2) Since the structure of our combined model is derived from monolingual language models, it can be implemented as a finite-state machine and easily incorporated within an ASR system. (3) The monolingual language model for the primary language can be trained further with large amounts of monolingual text data (which is easier to obtain compared to code-switched text). \n\nOur main contributions can be summarized as follows: \u2022 We formalize the framework of DLMs (Section 3). \n\n\u2022 We show significant improvements in perplexity using DLMs when compared against smoothed n-gram language models estimated on code-switched text (Section 4.3). We provide a detailed analysis of DLM improvements (Section 5). \u2022 We evaluate DLMs on the ASR task.",
            "score": 0.4015757843713662,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1181
                },
                {
                    "start": 1184,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2005
                },
                {
                    "start": 2008,
                    "end": 2110
                },
                {
                    "start": 2113,
                    "end": 2273
                },
                {
                    "start": 2274,
                    "end": 2337
                },
                {
                    "start": 2338,
                    "end": 2373
                }
            ],
            "ref_mentions": [
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "15480966"
                },
                {
                    "start": 1634,
                    "end": 1637,
                    "matchedPaperCorpusId": "7111631"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04595947265625
        },
        {
            "corpus_id": "268384837",
            "title": "Revealing the Parallel Multilingual Learning within Large Language Models",
            "text": "Many of the recent large language models (LLMs) are multilingual. Unlike language-specific NLP systems, such as machine translation systems specialized to a given language pair, these models are generally trained on large-scale multilingual datasets, using a unified vocabulary. Because of this training approach, it is possible to learn a universal representation of texts across different lan- guages. Therefore, the resulting models can be directly applied to a variety of multilingual and crosslingual tasks. For example, most commercialized LLMs can respond to user queries in different languages, without needing to specify what languages are used. More recently, the multilingual capabilities of these models have been shown to help crosslingual in-context learning (ICL). By providing simple prompts involving cross-lingual thinking and reasoning, LLMs can understand and generate text in languages that were less represented in the training data (Qin et al., 2023;Huang et al., 2023;Zhang et al., 2023;Nguyen et al., 2023). Despite the apparent usefulness of multilingualism in LLMs, previous work has primarily focused on using English as the pivot language in language understanding and reasoning. It is a natural next step to incorporate more languages and investigate how these languages are simultaneously processed in LLMs. In this paper, we explore methods that make use of parallel multilingual input (PMI) in ICL and explain how neurons are activated in this processing. There are two major findings. \n\n\u2022 LLMs can benefit from receiving parallel input in multiple languages. By transforming single-language input into multi-language input, we build a multi-source LLM that uses contexts from all these languages to make predictions. On the FLORES-200 machine translation benchmark, it achieves improvements of 11.3 BLEU points and 1.52 COMET points over the baseline. \n\n\u2022 Somewhat surprisingly, as more languages are involved in the input, fewer neurons are activated in the LLMs, facilitating more targeted and effective neuron activation patterns.",
            "score": 0.40121901363401147,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1518
                },
                {
                    "start": 1521,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1885
                },
                {
                    "start": 1888,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 955,
                    "end": 973,
                    "matchedPaperCorpusId": "264591444"
                },
                {
                    "start": 973,
                    "end": 992,
                    "matchedPaperCorpusId": "258615377"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10797119140625
        },
        {
            "corpus_id": "15782021",
            "title": "Experience with a Stack Decoder-Based HMM CSR and Back-Off N-Gram Language Models",
            "text": "The class smoothed (less noisy) model, if the classes are appropriately chosen, would produce fewer of these additional errors. \n\nThe use of classes or other language model smoothing techniques is another example of a trade-off that occurs throughout the design of speech recognizers--that of matching the complexity of the model to the amount of available training data. A prespecified-class classed model requires less training data because it has fewer parameters, but because it combines the words into groups, can never model as much detail as the word-based models. As the amount of training data is increased, there will be a point beyond which the word-based models will achieve a better perfor-mance on natural tasks than the classed models. (Of course, the word-based modeling can at best equal the performance of a correctly-classed model if the language is truly classed. In general, one would expect only artificial languages to be truly classed.)",
            "score": 0.40105049904022605,
            "section_title": "Recognition Performance",
            "char_start_offset": 7963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 130,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033843994140625
        },
        {
            "corpus_id": "253661212",
            "title": "Retracted: Hybrid Algorithm for English Translation Speech Recognition Based on Deep Learning Model and Clustering",
            "text": "Although the recognition rate in the outdoor environment is lower than that in the laboratory due to the influence of outdoor noise, the recognition results this time still meet the needs of practical applications. In addition, according to the test results, the larger the test vocabulary, the lower the recognition rate. However, the recognition rate can still reach more than 90%. In general, the recognition rate of the system can meet the actual use requirements. \n\nAccording to the structure of the Transformer model, it belongs to the Seq2Seq model, so the Seq2Seq model is selected as the benchmark model, and the model performance on different modeling units is compared. \n\ne experimental results are shown in Table 1. \n\nTransformer-based language models and Seq2Seq-based language models have the lowest error rates in the task of restoring phonetic symbols to English characters when phonemes are modeling units. e error rates on the test set reach 9.54% and 11.21%, respectively, which are 6.97 and 6.1 percentage points higher than the modeled units of syllables, respectively. \n\nUnder the same experimental conditions, the Transformer-based language model has a slightly lower error rate than the Seq2Seq-based language model in the task of restoring phonemes and syllables to English characters, and its test running speed is faster than that of the Seq2Seq model. \n\nis is because the Transformer model has the advantage of parallel computing. \n\nIn order to test the performance of the speech recognition system combined with the language model and the acoustic model, the acoustic model based on CNN-CTC was combined with the language model of Transfomer and compared with the speech recognition system using only the acoustic model based on CNN-CTC. Table 2 shows the test results of the speech recognition system by selecting different modeling units. \n\ne experimental results show that the English speech recognition performance of the combination of the acoustic model and the language model is better than the speech recognition whose modeling unit is a word. In addition, in a speech recognition system in which the modeling units are syllables and phonemes, it is better to use phonemes as the recognition unit for training.",
            "score": 0.40064723931677104,
            "section_title": "R E T R A C T E D",
            "char_start_offset": 26608,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1090
                },
                {
                    "start": 1093,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1458
                },
                {
                    "start": 1461,
                    "end": 1766
                },
                {
                    "start": 1767,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04443359375
        },
        {
            "corpus_id": "271769503",
            "title": "Morphology Matters: Probing the Cross-linguistic Morphological Generalization Abilities of Large Language Models through a Wug Test",
            "text": "Large language models (LLMs) have been very successful in learning and generating grammaticallycorrect language as humans do (Brown et al., 2020;OpenAI, 2023). This poses the question of whether they actually have linguistic capability that would allow them to generalize beyond the training distribution (Hupkes et al., 2023). In addition, does this capability manifest differently in different languages that LLMs were trained on? Here, we investigate whether LLMs' linguistic knowledge with respect to morphology differs between languages. Specifically, we test the ability of multilingual LLMs to generalize their morphological knowledge to nonce words in six languages. \n\nTesting cross-linguistic differences in the morphosyntactic abilities of LLMs trained on large amounts of human-generated text is particularly interesting given recent findings on the behavioral similarity between humans and language models in a variety of language learning and processing tasks (Galke et al., 2023;Webb et al., 2023;Srikant et al., 2022) and syntactic structure in the models' learned attention patterns (Manning et al., 2020;Chen et al., 2023). One of the key concerns of contemporary efforts in language modeling is to improve the ability to generalize well across the variety of human languages, especially regarding low-resource languages (e.g., Sch\u00e4fer et al., 2024;Zheng et al., 2022;Hedderich et al., 2021;Lauscher et al., 2020;Conneau et al., 2020). \n\nGiven the importance of the training data to LLM's abilities (Kandpal et al., 2023), conventional wisdom would suggest that the amount of exposure to a given language would be the dominant factor in determining the models' ability to learn the language's morphological patterns. Here, we argue that factors beyond the amount of training data play an important role for LLMs' generalization abilities, and in particular suggest that languages' morphological complexity needs to be taken into account.",
            "score": 0.4006239922386458,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 674
                },
                {
                    "start": 677,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1954
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 145,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 993,
                    "end": 1011,
                    "matchedPaperCorpusId": "254854575"
                },
                {
                    "start": 1011,
                    "end": 1032,
                    "matchedPaperCorpusId": "255084569"
                },
                {
                    "start": 1099,
                    "end": 1121,
                    "matchedPaperCorpusId": "219315567"
                },
                {
                    "start": 1366,
                    "end": 1385,
                    "matchedPaperCorpusId": "237940684"
                },
                {
                    "start": 1385,
                    "end": 1408,
                    "matchedPaperCorpusId": "225062337"
                },
                {
                    "start": 1408,
                    "end": 1430,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 1430,
                    "end": 1451,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1516,
                    "end": 1538,
                    "matchedPaperCorpusId": "253522998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050811767578125
        },
        {
            "corpus_id": "269010053",
            "title": "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence",
            "text": "Alignment is an important step in creating an assistant LM, because it helps language models generate relevant and helpful responses, as well as avoiding harmful and biased content. Our Eagle models are tested for Chinese alignment using the AlignBench (Liu et al., 2023b), a benchmark for evaluating the alignment of Chinese LLMs, featuring 683 diverse and challenging queries across eight categories like language abilities, logical reasoning, and professional knowledge. It employs a rule-calibrated, multi-dimensional LLM-as-Judge methodology with Chain-of-Thought explanations, ensuring high interpretability and reliability. \n\nTable 11 showcases a consistent improvement in the performance of Eagle and Finch models on the AlignBench benchmark as model size and generation progresses. This trend is evident across a wide range of categories, highlighting the larger models' enhanced capability to understand and generate contextually relevant responses. Particularly, both the Eagle 7B and Finch 3B model significantly surpasses its smaller and previous generation counterparts, achieving higher overall scores. This progression underscores the critical role of scaling model size as well as improving architecture in aligning with human judgment in complex language understanding tasks. The results affirm the importance of model architecture and capacity in achieving superior alignment and interpretability in language models.",
            "score": 0.4006164497906095,
            "section_title": "F.1 Alignment Benchmark",
            "char_start_offset": 40250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04681396484375
        },
        {
            "corpus_id": "8636445",
            "title": "Robust Probabilistic Predictive Syntactic Processing",
            "text": "We have presented a parser in this thesis that provides fully connected syntactic structure incrementally. It is a model that is consistent with models of human sentence processing (Jurafsky, 1996), yet which handles syntactic complexity without resorting to dynamic programming, while scaling up to handle freely occurring language. In addition to its psycholinguistic motivations, we found a large computational motivation in the model's applicability to language modeling for speech recognition. Our robust parser can cover spontaneous spoken language input without the need for pre-processing, and the language model that it provides yields substantial reductions in perplexity and word error rate on standard test corpora.\n\nThere are several directions to go with this research in the future. One is to attempt to improve the model, by reducing the number of parameters and better modeling the dependencies. The linear order of interpolation, and even the use of interpolation as opposed to other smoothing methods, is one clear area of potential weakness. Maximum entropy modeling is an obvious candidate to replace our current approach.\n\nAnother direction is to modify the model to better fit the needs of statistical speech recognition. In this thesis, we have applied the parser straight \"out of the box\", yet the desideratum is quite different from that which the model was built to deliver. If our goal is language modeling and not parsing, perhaps we can modify the structure of the grammar in ways that do not preserve the original structure, but which capture the dependencies as well or better. Perhaps some of these modifications may also reduce the search problem as well. We demonstrated the efficacy of the selective left-corner transform, in a flattened form that preserved constituent structure. Perhaps other transforms that do not preserve constituent structure would be even more beneficial. If recovery of Penn Treebank style structure is not the goal, then such modifications can be carried out without worry.\n\nGiven the results of this thesis, as well as what has been seen recently in the literature from Chelba and Jelinek, it is clear that syntactic parsing can provide access to dependencies that elude standard language models. We speculate that parsing will play a role in the language models of commonly available speech recognition systems within the next five to ten years. Clearly there is much work to be done to make such models viable under",
            "score": 0.4005537857187768,
            "section_title": "Conclusion",
            "char_start_offset": 301700,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 197,
                    "matchedPaperCorpusId": "16059014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058563232421875
        },
        {
            "corpus_id": "261681742",
            "title": "Embedding Structure Matters: Comparing Methods to Adapt Multilingual Vocabularies to New Languages",
            "text": "For languages other than English and a handful of other very high-resource languages, pre-trained multilingual language models form the backbone of most current NLP systems. These models address the relative data scarcity in most non-English languages by pooling text data across many languages to train a single model that (in theory) covers all training languages (Devlin, 2019;Conneau and Lample, 2019;Conneau et al., 2020;Liu et al., 2020;Scao et al., 2023, i.a.). These models often include language-agnostic tokenization and an increased vocabulary capacity over monolingual models (Conneau et al., 2020). \n\nHowever, Wu and Dredze (2020) show that these massively multilingual models still underperform on lower-resource languages. Recent efforts to cover these languages instead pre-train models that are specialized to specific languages or language families (Ogueji et al., 2021;Ogunremi et al., 2023). These approaches nonetheless require training a new model from scratch and do not leverage transferable information in existing models. \n\nOur study builds on a line of work which instead adapts a pre-trained cross-lingual model (such as XLM-R; Conneau et al., 2020) to a single language, or a smaller set of languages. Language-Adaptive Pre-Training (LAPT)-continuing the MLM or CLM pre-training task on only the target language(s)-is a simple and strong baseline in this regard (Chau et al., 2020). \n\nHowever, LAPT with no change to the crosslingual vocabulary comes with considerable excess computational cost: when adapting to a single language or small subset of languages, only a small fraction of the cross-lingual vocabulary is used. The excess vocabulary still contributes to the computational cost on both the forward and backward pass, and embedding/output matrices often constitute a large fraction of the total trainable model parameters (for XLM-R-base, 192M / 278M \u2248 69% of parameters).",
            "score": 0.40044327615247166,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1411
                },
                {
                    "start": 1414,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1912
                }
            ],
            "ref_mentions": [
                {
                    "start": 380,
                    "end": 405,
                    "matchedPaperCorpusId": "58981712"
                },
                {
                    "start": 405,
                    "end": 426,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 588,
                    "end": 610,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 867,
                    "end": 888,
                    "matchedPaperCorpusId": "240225648"
                },
                {
                    "start": 888,
                    "end": 910,
                    "matchedPaperCorpusId": "258378192"
                },
                {
                    "start": 1156,
                    "end": 1177,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2042236328125
        },
        {
            "corpus_id": "32276",
            "title": "An Overview of the SPHINX-II Speech Recognition System",
            "text": "Acoustic and language models are usually constructed separately, where language models are derived from a large text corpus without consideration for acoustic data, and acoustic models are constructed from the acoustic data without exploiting the existing text corpus used for language training. We recently have developed a unified stochastic engine (USE) that jointly optimizes both acoustic and language models. As the true probability distribution of both the acoustic and language models can not be accurately estimated, they can not be considered as real probabilities but scores from two different sources. Since they are scores instead of probabilities, the straightforward implementation of the Bayes equation will generally not lead to a satisfactory recognition performance.\n\nTo integrate language and acoustic probabilities for decoding, we are forced to weight acoustic and language probabilities with a so called language weight [6]. The constant language weight is usually tuned to balance the acoustic probabilities and the language probabilities such that the recognition error rate can be minimized. Most HMM-based speech recognition systems have one single constant language weight that is independent of any specific acoustic or language information, and that is determined using a hill-climbing procedure on development data. It is often necessary to make many runs with different language weights on the development data in order to determine the best value.\n\nIn the unified stochastic engine (USE), not only can we iteratively adjust language probabilities to fit our given acoustic representations but also acoustic models. Our multi-pass search algorithm generates N-best hypotheses which are used to optimize language weights or implement many discriminative training methods, where recognition errors can be used as the objective function [20,25]. With the progress of new database construction such as DARPA's CSR Phase II, we believe acoustically-driven language modeling will eventually provide us with dramatic performance improvements.\n\nIn the N-best hypothesis list, we can assume that the correct hypothesis is always in the list (we can insert the correct answer if it is not there). Let hypothesis be a sequence of words wl, w2, ...w~ with corresponding language and acoustic probabilities. We denote the correct word sequence as 0, and all the incorrect sentence hypotheses as 0. We can assign a variable weight to each of the n-gram probabilities such that we",
            "score": 0.4002032732121909,
            "section_title": "UNIFIED STOCHASTIC ENGINE",
            "char_start_offset": 15742,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 943,
                    "end": 946,
                    "matchedPaperCorpusId": "222339062"
                },
                {
                    "start": 1866,
                    "end": 1870,
                    "matchedPaperCorpusId": "41867053"
                },
                {
                    "start": 1870,
                    "end": 1873,
                    "matchedPaperCorpusId": "15843983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0305023193359375
        },
        {
            "corpus_id": "272910894",
            "title": "Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE",
            "text": "Driven by the growth of datasets, the increase in model size, and advances in generative language foundation models [Achiam et al., 2023;Touvron et al., 2023], multi-modal large language models (MLLMs) now offer unprecedented abilities as general-purpose interfaces. These advancements are spurring innovation across various visual and linguistic tasks [Chen et al., 2023a;Lyu et al., 2023;Su et al., 2023]. While significant strides have been made in building a unified foundation model for natural scenery [Chen et al., 2022;Lu et al., 2022Lu et al., , 2023]], the development of generalist medical artificial intelligence is still in its early stages [Moor et al., 2023a]. \n\nThe goal of a unified and generalist medical foundation model is to enable joint training on massive medical datasets. This model aims to handle multiple tasks and modalities within a single architecture with shared parameters [Zhang et al., 2023;Li et al., 2024]. It seeks to eliminate the need for task-specific modules and further fine-tuning, thereby revolutionizing the traditional task-specific approach to model development [Wu et al., 2023b;Tu et al., 2024]. However, existing open-source efforts have not yet fully achieved these ambitious goals. \n\nA key challenge in creating a unified medical foundation model is the complexity of multi-modal, multi-task learning, often exacerbated by the tug-of-war problem [Hadsell et al., 2020]. Inherent task conflicts and data imbalances can cause interference during the simultaneous learning of different tasks. This problem is particularly acute in the medical field, where tasks and modalities are highly specialized and diverse. As a result, the performance of each task may degrade compared to taskspecialized models [Yu et al., 2020;Zhu et al., 2022]. \n\nTo mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) [Jacobs et al., 1991] into MLLMs.",
            "score": 0.4001343415804487,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1947
                }
            ],
            "ref_mentions": [
                {
                    "start": 508,
                    "end": 527,
                    "matchedPaperCorpusId": "249674493"
                },
                {
                    "start": 527,
                    "end": 542,
                    "matchedPaperCorpusId": "249848272"
                },
                {
                    "start": 925,
                    "end": 941,
                    "matchedPaperCorpusId": "258999820"
                },
                {
                    "start": 1127,
                    "end": 1143,
                    "matchedPaperCorpusId": "260164663"
                },
                {
                    "start": 1398,
                    "end": 1420,
                    "matchedPaperCorpusId": "226240885"
                },
                {
                    "start": 1751,
                    "end": 1768,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1768,
                    "end": 1785,
                    "matchedPaperCorpusId": "249538647"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.232177734375
        },
        {
            "corpus_id": "14001621",
            "title": "Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm",
            "text": "We have contrasted two approaches to discriminative language model estimation on a difficult large vocabulary task, showing that they can indeed scale effectively to handle this size of a problem. Both algorithms have their benefits. The perceptron algorithm selects a relatively small subset of the total feature set, and requires just a couple of passes over the training data. The CRF algorithm does a better job of parameter estimation for the same feature set, and is parallelizable, so that each pass over the training set can require just a fraction of the real time of the perceptron algorithm. \n\nThe best scenario from among those that we investigated was a combination of both approaches, with the output of the perceptron algorithm taken as the starting point for CRF estimation. \n\nAs a final point, note that the methods we describe do not replace an existing language model, but rather complement it. The existing language model has the benefit that it can be trained on a large amount of text that does not have speech transcriptions. It has the disadvantage of not being a discriminative model. The new language model is trained on the speech transcriptions, meaning that it has less training data, but that it has the advantage of discriminative training -and in particular, the advantage of being able to learn negative evidence in the form of negative weights on n-grams which are rarely or never seen in natural language text (e.g., \"the of\"), but are produced too frequently by the recognizer. The methods we describe combines the two language models, allowing them to complement each other.",
            "score": 0.39986959197886884,
            "section_title": "Conclusion",
            "char_start_offset": 32820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1513
                },
                {
                    "start": 1514,
                    "end": 1611
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0582275390625
        },
        {
            "corpus_id": "211018622",
            "title": "Simplify-Then-Translate: Automatic Preprocessing for Black-Box Translation",
            "text": "-1) by back-translating the ground truth human translations to the source language we obtain a (perhaps noisy) simplified version of the original source, and 2) we can learn a function to map the source sentences to their simplified forms by training a sequence-to-sequence (S2S) model from the aforementioned generated parallel corpus. We term the resulting simplification model an Automated Preprocessing (or APP) model.\n\nWe formalize our APP model as a S2S model from source sentences in one language to back-translations of ground truth sentences i.e., translationese targets in the same lan-guage. The translationese targets can be obtained from multiple high-resource language pairs using black-box MT systems and the trained APP model can be applied to a variety of low resource language pairs. Let (X i , Y i ) be the i th training bitext corpus (X is source, Y is target) with source language s i and target language t i for i \u2208 {1, . . . , M}, where M is the number of training language pairs, and let j \u2208 {M + 1, . . . , M + N } refer to the N test bitext corpora. Note that for the experiments in this paper, s i is fixed to English \u2200i \u2208 {1, . . . , M + N } so we simply refer to it as s. The APP procedure is as follows:\n\n\u2022 Obtain back-translations of target train sets Y i for i = 1 to M to language s i given by T 1 , T 2 , . . . , T M using BBMT models MT ti\u2192s \u2200i.\n\n\u2022 Train an APP simplification model f AP P on the combined parallel corpus\n\n\u2022 At test time, preprocess the source X j s for each test language pair j using the trained APP model to obtain the simplified source X j * , where\n\n\u2022 Translate the simplified source using the BBMT model for the j th test language pair.\n\nAPP provides in-domain simplification bitext at scale and from the same BBMT system that we eventually use to translate into low-resource languages, thus providing a more flexible solution than using precompiled simplification corpora. In the next sections, we compare the performance of BBMT system outputs with and without APP sim",
            "score": 0.39952596426091624,
            "section_title": "APP with Back Translations",
            "char_start_offset": 6579,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0321044921875
        },
        {
            "corpus_id": "270379651",
            "title": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation",
            "text": "Large pre-trained foundation models have become widely available for many real-world applications (Wornow et al., 2023;Thirunavukarasu et al., 2023;Cui et al., 2024). This increasing availability has led to a popular practice of fine-tuning these models to adapt them to a wide range of downstream tasks. Practitioners can independently fine-tune the same pre-trained model, such as CLIP-style models (Radford et al., 2021;Wu et al., 2023;Zhai et al., 2023), large language models (Brown et al., 2020;Rozi\u00e8re et al., 2023;Touvron et al., 2023;Jiang et al., 2024), etc., and then release the fine-tuned models without disclosing the training data. As the deployment of such fine-tuned models increases, combining models with identical architectures and initializations has emerged as a promising approach to combine their respective capabilities. This is useful, especially in scenarios where the training data for each task is private and cannot be shared, such as individual-level patient data in hospitals and behavioral data in social media recommendation systems. \n\nExisting methods for merging models typically involve calculating a weighted average of the parameters from multiple models to enhance performance uniformly across various tasks. However, this approach often overlooks the conflicts among the diverse objectives of these tasks, which can  (Dosovitskiy et al., 2020) obtained from 100 scaling coefficients (sampled uniformly) evaluated on the SUN397 (Xiao et al., 2016) and Cars (Krause et al., 2013) datasets. lead to trade-offs in model performance across different tasks. In real-world applications, it is often more beneficial to obtain a set of Pareto optimal solutions rather than a single model. Such solutions allow practitioners to choose among different trade-offs based on their specific needs. For example, hospitals specializing in certain areas might prefer a model that excels in tasks relevant to their specialty while maintaining adequate performance across a broader spectrum of diseases. \n\nIn this paper, we introduce a novel method that identifies the Pareto front without retraining the models to be merged. Our algorithm utilizes a quadratic approximation of the evaluation metric.",
            "score": 0.3992318697435729,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 2024
                },
                {
                    "start": 2027,
                    "end": 2146
                },
                {
                    "start": 2147,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 148,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 148,
                    "end": 165,
                    "matchedPaperCorpusId": "268028472"
                },
                {
                    "start": 401,
                    "end": 423,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 423,
                    "end": 439,
                    "matchedPaperCorpusId": "253510826"
                },
                {
                    "start": 481,
                    "end": 501,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1468,
                    "end": 1487,
                    "matchedPaperCorpusId": "10224573"
                },
                {
                    "start": 1497,
                    "end": 1518,
                    "matchedPaperCorpusId": "14342571"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056243896484375
        },
        {
            "corpus_id": "248887528",
            "title": "Overcoming Language Disparity in Online Content Classification with Multimodal Learning",
            "text": "Our study demonstrates that in the context of societal tasks -as demonstrated by our focus on three datasets -the performance of large language models on non-English text is subpar when compared to the performance on English text. \n\nIn the subsequent discussion, we highlight how this could have possibly threatening implications on the lives of many individuals who belong to underserved communities. Furthermore, we empirically demonstrate that using images as an additional modality leads to a lesser difference between the performance on English and non-English text, as indicated by decreased RMSD en values. While existing studies have focused on developing advanced monolingual language models that can boost the performance on specific non-English languages to bridge the performance gap, we demonstrate the benefits of including other complementary modalities, especially those that are language-agnostic. Decreased RMSD en values indicate that if images are considered along with the text, the performance on all languages is not only better than when only text is considered, but it is also comparable across English and non-English languages. Implications of language disparity with text-only models: In the context of social computing, disparities between English and non-English languages can lead to inequitable outcomes. For instance, as per our observations, if stateof-the-art NLP techniques that are centered around BERTbased language models are adopted to detect humanitarian information during crises, the detection abilities would be poorer for social media posts in non-English languages than those in English, causing delayed interventions. In coun-  6(a), the relationship between the languagespecific corpus that mBERT is trained on and the classification performance obtained after task-specific fine-tuning, is clear: larger representation in the pre-training corpus is related to better performance on downstream tasks. This trend reinforces the findings of Wu and Dredze (2020) in our context -the performance of large language models drops significantly as the considered languages have less pre-training data. This is concerning because, as Bender et al. ( 2021) argue, \"the large, uncurated, and Internet-based datasets\" that these language models are trained on \"encode the dominant/hegemonic view, which further harms people at the margins.\"",
            "score": 0.39917437542998657,
            "section_title": "Discussion",
            "char_start_offset": 32552,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2376
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0310821533203125
        },
        {
            "corpus_id": "253421169",
            "title": "Multilingualism and AI: The Regimentation of Language in the Age of Digital Capitalism",
            "text": "Indeed, language professionals report that, for example, machine translation is meagre for languages with a high number of grammatical cases like Finnish. 7 In addition, it has been shown that the automatic generation of languages other than English may result in sentences whose grammatical structures are influenced by those of English, particularly where transfer learning is applied (see, e.g., Lauscher et al. 2020;Virtanen et al. 2019, quoted in Bommasani et al. 2021, 25). All in all, \"multilingual models [models for machine translation that entail data sets from two or more languages-not models that would support multilingual practices] show better performance in languages that are similar to the highest-resource languages in their training data\" and researchers from the academic realm warn that there is a danger in \"models that erase language variation and mostly conform to the linguistic majority in their training data\" (Bommasani et al. 2021, 25). \n\nThe tendency to further support what already is dominant becomes even more pronounced if it is considered that at least currently, it is only very few models (among them the aforementioned foundation models BERT and GPT3) that are used in a large number of applications and that define how language technologies are developed, \"Foundation models have led to an unprecedented level of homogenization, Almost all state-of-the-art NLP models are now adapted from one of a few foundation models . . . all AI systems might inherit the same problematic biases of a few foundation models\" (Bommasani et al. 2021, 5). This tendency toward homogenization may be enforced through user practices.",
            "score": 0.39881106421286605,
            "section_title": "Machine Learning and the Amplification of Biases",
            "char_start_offset": 40269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1655
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 420,
                    "matchedPaperCorpusId": "226262344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0144500732421875
        },
        {
            "corpus_id": "224803418",
            "title": "Cross-Lingual Transfer in Zero-Shot Cross-Language Entity Linking",
            "text": "We consider several different training and evaluation settings to explore the abilities of our model. Recent studies suggest that multilingual models can achieve similar or even better performance on cross-language entity linking (Upadhyay et al., 2018). Other work (Mueller et al., 2020) has shown that this is not always the case. Therefore, we begin by asking: does our linker do better when trained on all languages (multilingual crosslanguage) or trained separately on each individual language (monolingual cross-language) ? We train our model on each of the 7 individual languages in the two datasets (noted as Mono). Next, we train a single model for each dataset  (3 languages in TAC, 4 in Wiki, noted as Multi).\n\nNote that Mono and Multi share the exact same architecture -there are no multilingual adjustments made, and the model contains no language-specific features. We note the best performing architecture from  as Prev. Comparing to the previously reported results, our models often have a lower recall rate. This is due to the challenges involved with the Triage for the TAC dataset, which is discussed in Appendix B. Our triage system is tuned to have high recall on the development set and thus we selected a large candidate set; this led to many NIL mentions in the evaluation set being incorrectly assigned an entity. Table 1 shows that for TAC there is a small difference between the Mono and Multi models. For Wiki the difference is often larger. Multi often does better than Mono, suggesting that additional training data is helpful. Overall, these results are encouraging as they suggest that a single trained model for our system can be used for crosslanguage linking for multiple languages. This can reduce the complexity associated with developing, deploying and maintaining multiple models in a multilingual environment.",
            "score": 0.39871807578721286,
            "section_title": "Model Evaluation",
            "char_start_offset": 17128,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 253,
                    "matchedPaperCorpusId": "52307294"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02056884765625
        },
        {
            "corpus_id": "227230468",
            "title": "Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base",
            "text": "Natural language processing (NLP) is increasingly becoming an important tool in medical research. Different NLP applications have been developed for assisting physicians, helping to increase the efficiency of performing a variety of tasks. For example, the clinical concept extraction task (Uzuner et al., 2011) labels clinical findings, treatments, and tests contained in a clinical report, whereas the temporal relations extraction task (Sun et al., 2013) identifies temporal expressions of the clinical concept. The need for these applications calls for better clinical natural language models. \n\nThe recent advances in language model pre-training greatly boosted the performance of NLP models. Compared with the traditional NLP-or word2vec-based techniques, the pre-trained language models capture the meaning of the words based on the context of each sentence. Some pre-trained language models, including BERT (Devlin et al., 2019), ALBERT (Lan et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), achieved state-of-the-art performance on most NLP tasks. These models can be further fine-tuned on a domain-specific corpus for specific tasks. \n\nIn addition to a language model, knowledge base information can help to build a better NLP model. During language model pre-training, the representation of the words is learned from some given corpus. Thus, the language model would not be able to produce a meaningful representation of a concept missing from the corpus. This problem is critical, especially in a low-resource setting like the clinical domain. On the other hand, a knowledge base contains a plethora of concepts and relations. Leveraging such information has the potential to yield models that can generalize better without resorting to extremely large training corpora. For example, the Unified Medical Language System (UMLS) provides 54 types of relations between biomedical concepts of 127 semantic types. It contains 27.7 million relations, which, as we show, can be leveraged to improve clinical NLP models for various downstream tasks. In this paper, we propose a novel method for fusing the knowledge base information into the language model pre-training stage.",
            "score": 0.3987174204921546,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 597
                },
                {
                    "start": 600,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1167
                },
                {
                    "start": 1170,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 311,
                    "matchedPaperCorpusId": "30029552"
                },
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "15493987"
                },
                {
                    "start": 915,
                    "end": 936,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 945,
                    "end": 963,
                    "matchedPaperCorpusId": "202888986"
                },
                {
                    "start": 1003,
                    "end": 1022,
                    "matchedPaperCorpusId": "195069387"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1507568359375
        },
        {
            "corpus_id": "214612871",
            "title": "Abstract Syntax as Interlingua: Scaling Up the Grammatical Framework from Controlled Languages to Robust Pipelines",
            "text": "for deep ones.\n\nExamples of the first kind can be found in Controlled Natural Languages (CNL), where the interlingua can be a domain-specific semantic structure or an \"ontology\"; see Kuhn (2014) for a recent survey of CNL, and Hutchins and Somers (1992) for its potential in translation. A deep interlingua can actually start with a semantic model and generate a fragment of language that matches it, rather than starting with a language and trying to find a semantic model. Shallow interlinguas are representations of some aspects of language that help translation, without going as deep as the full meaning to be rendered. A simple example of a shallow interlingual representation would be a sequence of word senses, such as the ones in multilingual WordNets. The translator would first map source language words to these senses, and then map the senses as target language words in the same order. The words would often appear in wrong senses, in wrong order, or in wrong forms, but the translation could still be helpful.\n\nA bit less shallow interlingua could add syntactic structure to the words, so that it could reorder the words or even enforce agreement in accordance with the rules of the target language. Such a system would still not always select correct word senses, identify multiword constructions, or change the syntactic structure when needed (Dorr 1994). But it could guarantee the grammatical correctness of the output. Figure 3 shows the architecture of a layered interlingua system, where the transferbased shortcuts of the original Vauquois triangle are replaced by interlinguas of varying depth. Such a system enjoys some of the advantages of the original interlingua idea-in particular, the linear scale-up as languages are added.\n\nThe main topic of this article is to show how to build and combine interlinguas of different levels, by using Grammatical Framework (GF; Ranta 2004bRanta , 2011a, a tool designed for this very purpose. More precisely, GF is a special-purpose programming language for defining interlinguas (abstract syntaxes) and reversible mappings from them to individual languages (concrete syntaxes). GF has no single interlingua or fixed set of languages, but a set of tools for building new",
            "score": 0.3986065583368204,
            "section_title": "Figure 1",
            "char_start_offset": 4655,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 194,
                    "matchedPaperCorpusId": "14586568"
                },
                {
                    "start": 1360,
                    "end": 1371,
                    "matchedPaperCorpusId": "2218012"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0260009765625
        },
        {
            "corpus_id": "266521359",
            "title": "Building Your Own Product Copilot: Challenges, Opportunities, and Needs",
            "text": "In its simplest form, a language model is a statistical model that captures the probability distribution over sequences of words in a given language. It aims to understand and generate coherent textual sequences by modeling the relationships and dependencies between words. Language models can predict the next word in a sentence based on the context of the preceding words or generate entirely new text that follows the patterns and characteristics of the training data. These models have gained significant attention in natural language processing (NLP) tasks due to their ability to comprehend and generate text, enabling advancements in machine translation, text summarization, question-answering systems, and sentiment analysis. \n\nLarge-language models (LLMs) such as GPT-42 , Claude3 , and LLaMA4 are a class of language models typically characterized by their large sizes, determined by the number of parameters (typically at least one billion) they contain. Parameters are the learnable elements in a model that allow it to capture and represent complex patterns and relationships within the data. The size of a model directly impacts both the training cost and the computational resources required for inference. Larger models with more parameters generally require more computation and time to train effectively. Similarly, the operational cost of using larger models for inference is higher due to the increased computational requirements during execution. As a result, practitioners have recently been exploring smaller-scale models and fine-tuning existing models to balance performance and resource efficiency, reducing both the training and operational costs associated with larger models. This allows for more practical and cost-effective deployment of language models in software engineering tasks. As more of these models become available to the developers, we see an increasing trend of integrating the output of these models into software applications. That, in turn, will only increase the challenges and frustrations of using these models effectively. Thus, there is a need to have guidelines and lessons learned for the developers to use these models to build usable and reliable experiences.",
            "score": 0.39857759446045876,
            "section_title": "BACKGROUND 2.1 Language Models",
            "char_start_offset": 1970,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 733
                },
                {
                    "start": 736,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06683349609375
        },
        {
            "corpus_id": "268379027",
            "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
            "text": "However, in general, semantics in language, logic and structures in code, and abstract symbol manipulations in math intricately always create a situation of mutual weakening.To elaborate further, comparing highly specialized models (such as those for coding or mathematics) with general-purpose models capable of performing all tasks (like GPT-4) for their expertise is a trap that can easily lead to misinformation.This paper hopes to integrate specialized abilities into a general chat language model with as little loss as possible.\n\nMore specifically, we propose to leverage separate models that are already highly specialized via a fusing structure.In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. 1 To ensure that the fused model benefits from the Figure 1: Performance on three different domains of specialized models and our ULTRAFUSER.The performance for the text domain is computed by the average results on TruthfulQA (Acc) (Lin et al., 2021) and AlpacaEval (Win Rate) (Li et al., 2023b) datasets; the performance for the code domain is Pass@1 of HumanEval (Chen et al., 2021); and the performance for the math domain is the average result of GSM8K (Pass@1) (Cobbe et al., 2021), MATH (Pass@1) (Hendrycks et al., 2021), SAT-Math (Acc) (Zhong et al., 2023), and AQuA-RAT (Acc) (Ling et al., 2017) datasets.All numbers are zero-shot results.specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data.Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model.The key to functioning the model is to train the gating module.For example, when the model conducts code generation, we want the coding specialist to contribute more than the other two.This necessitates a mixed instruction tuning dataset that contains the three domains for the training.",
            "score": 0.398533088984083,
            "section_title": "Introduction",
            "char_start_offset": 1854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 174,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 535
                },
                {
                    "start": 537,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 925
                },
                {
                    "start": 925,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1829
                },
                {
                    "start": 1829,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2014
                },
                {
                    "start": 2014,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 1368,
                    "end": 1387,
                    "matchedPaperCorpusId": "12777818"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "261214450",
            "title": "AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning",
            "text": "AtmoRep builds on a substantial amount of work in the machine learning literature on large scale representation learning. The resulting models are sometimes referred to as foundation [47] or frontier models.\n\nRepresentation learning [62] is a machine learning methodology whose primary objective is not to obtain a model that is effective for a specific task but one that provides an effective encoding, or representation, of the data distribution. Next to being of scientific interest, such an encoding can be used for a variety of applications, e.g. by fine-tuning or appending task-specific tail networks. While representation learning has a long history [62], it recently became central to many efforts in machine learning through the introduction of large language models [8,9,25]. These are domain-specific but task-independent neural networks for natural language that can be specialized, for example, for translation, as chat bots, or for text auto-completion.\n\nLarge language models also popularized the use of self-supervised training protocols because a labelling of the very large training data sets would be impractical. Instead, the pre-training objective, i.e. the one used to learn the task-independent representation, is defined based on the dataset itself. A common approach is to mask part of the information and predict it based on unmasked ones, although alternatives are possible [48]. For transformer-based large language models, masking is most commonly used in the form of masked token models [8,9]. Since transformers are a highly generic architecture once a token has been defined [57], this approach has also been used in computer vision [26] and AtmoRep extended it to space-time neighborhoods.\n\nTraditionally, representation learning required additional computations to make a pre-trained model applicable for a specific task. Brown et al. [25] showed that sufficiently large models have skill for many tasks directly after pre-training and without task-specific refinement. This is referred to as zero-shot abilities in the literature [25]. An extension are few-shot abilities where a few examples are input to the model together with the task, again without update to the model weights, and this typically substantially improves performance. Zero-shot abilities are enabled by pre-training on a large and diverse data set and through the pre-training task. For example",
            "score": 0.39847148004495214,
            "section_title": "Representation learning and generative machine learning",
            "char_start_offset": 49400,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 233,
                    "end": 237,
                    "matchedPaperCorpusId": "393948"
                },
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "393948"
                },
                {
                    "start": 777,
                    "end": 780,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "52967399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07135009765625
        },
        {
            "corpus_id": "4940519",
            "title": "Experiments with Universal CEFR Classification",
            "text": "In this setup, we combined all the language texts and trained a single universal CEFR classifier. Table 3 shows the results. For the non-neural models, we experimented with and without considering language information as a categorical feature. The neural network model is a multitasking model (C \u00b8\u00f6ltekin and Rama, 2016) that consists of character and word embeddings as input. The model learns to predict both the language of the text (language identification) and the CEFR category simultaneously. The model is trained using categorical cross-entropy and Adadelta algorithm. The table shows results with and without language identification for neural models. We observe that the document length baseline seems to perform poorer than monolingual models in this case. Further, we can see that the average result on monolingual model as close to the multilingual model in case of POS n-grams, dependency n-grams, and embeddings. However, domain features clearly perform poorly compared to monolingual case. While one could argue that the better performance multilingual model over some monolingual models is due to more training data, this does not seem to be true for some feature groups (baseline, domain features). One inference we can draw is that some feature groups have similarities in terms of proficiency categories assigned for different languages, which lends support to our hypothesis. Although we did not perform a qualitative language specific evaluation yet, the results so far indicate that efforts to build such a universal scoring model is a worthwhile effort.",
            "score": 0.39845865511850376,
            "section_title": "Multilingual classification",
            "char_start_offset": 11993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1577
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.030670166015625
        },
        {
            "corpus_id": "258865893",
            "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
            "text": "While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MoRE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output. We release all code and data to facilitate future work.",
            "score": 0.39802521138919233,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "paperId": "e2f1f04f648a8863d11439aa4c80ee65d6caccda",
            "corpusId": 261531214,
            "title": "ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 25,
            "citationCount": 21,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2309.00986",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2309.00986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "143971529",
                    "name": "Chenliang Li"
                },
                {
                    "authorId": "123655156",
                    "name": "Hehong Chen"
                },
                {
                    "authorId": "2114009661",
                    "name": "Mingshi Yan"
                },
                {
                    "authorId": "2237809880",
                    "name": "Weizhou Shen"
                },
                {
                    "authorId": "153194420",
                    "name": "Haiyang Xu"
                },
                {
                    "authorId": "2238047773",
                    "name": "Zhikai Wu"
                },
                {
                    "authorId": "2237946970",
                    "name": "Zhicheng Zhang"
                },
                {
                    "authorId": "2237956023",
                    "name": "Wenmeng Zhou"
                },
                {
                    "authorId": "2237827934",
                    "name": "Yingda Chen"
                },
                {
                    "authorId": "2237996616",
                    "name": "Chen Cheng"
                },
                {
                    "authorId": "2238549612",
                    "name": "Hongzhu Shi"
                },
                {
                    "authorId": "2116921824",
                    "name": "Ji Zhang"
                },
                {
                    "authorId": "143857288",
                    "name": "Fei Huang"
                },
                {
                    "authorId": "2237981776",
                    "name": "Jingren Zhou"
                }
            ],
            "abstract": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
            "corpus_id": "261531214",
            "text": "Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\\footnote{https://github.com/modelscope/modelscope-agent} and online demo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now publicly available.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09136962890625
        },
        {
            "paperId": "9d9268b0191891511b09362759ba6a754c28fd9e",
            "corpusId": 273532677,
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 89,
            "citationCount": 11,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17952, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2265148831",
                    "name": "Ran Xu"
                },
                {
                    "authorId": "2326538488",
                    "name": "Hui Liu"
                },
                {
                    "authorId": "7529854",
                    "name": "Sreyashi Nag"
                },
                {
                    "authorId": "2313363021",
                    "name": "Zhenwei Dai"
                },
                {
                    "authorId": "2314869850",
                    "name": "Yaochen Xie"
                },
                {
                    "authorId": "2301317582",
                    "name": "Xianfeng Tang"
                },
                {
                    "authorId": "2305051640",
                    "name": "Chen Luo"
                },
                {
                    "authorId": "2314920176",
                    "name": "Yang Li"
                },
                {
                    "authorId": "2263536473",
                    "name": "Joyce C. Ho"
                },
                {
                    "authorId": "2237940940",
                    "name": "Carl Yang"
                },
                {
                    "authorId": "2315065019",
                    "name": "Qi He"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these self-generated synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
            "corpus_id": "273532677",
            "text": "Retrieval-augmented generation (RAG) enhances the question-answering (QA) abilities of large language models (LLMs) by integrating external knowledge. However, adapting general-purpose RAG systems to specialized fields such as science and medicine poses unique challenges due to distribution shifts and limited access to domain-specific data. To tackle this, we propose SimRAG, a self-training approach that equips the LLM with joint capabilities of question answering and question generation for domain adaptation. Our method first fine-tunes the LLM on instruction-following, question-answering, and search-related data. Then, it prompts the same LLM to generate diverse domain-relevant questions from unlabeled corpora, with an additional filtering strategy to retain high-quality synthetic examples. By leveraging these self-generated synthetic examples, the LLM can improve their performance on domain-specific RAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three domains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.230224609375
        },
        {
            "paperId": "aa6d5d5db5a8d82eb11e77f586e55313453742dd",
            "corpusId": 273502098,
            "title": "SPRIG: Improving Large Language Model Performance by System Prompt Optimization",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 52,
            "citationCount": 9,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.14826, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2267011399",
                    "name": "Lechen Zhang"
                },
                {
                    "authorId": "2326988905",
                    "name": "Tolga Ergen"
                },
                {
                    "authorId": "2876316",
                    "name": "Lajanugen Logeswaran"
                },
                {
                    "authorId": "2327002089",
                    "name": "Moontae Lee"
                },
                {
                    "authorId": "2326992492",
                    "name": "David Jurgens"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities in many scenarios, but their performance depends, in part, on the choice of prompt. Past research has focused on optimizing prompts specific to a task. However, much less attention has been given to optimizing the general instructions included in a prompt, known as a system prompt. To address this gap, we propose SPRIG, an edit-based genetic algorithm that iteratively constructs prompts from prespecified components to maximize the model's performance in general scenarios. We evaluate the performance of system prompts on a collection of 47 different types of tasks to ensure generalizability. Our study finds that a single optimized system prompt performs on par with task prompts optimized for each individual task. Moreover, combining system and task-level optimizations leads to further improvement, which showcases their complementary nature. Experiments also reveal that the optimized system prompts generalize effectively across model families, parameter sizes, and languages. This study provides insights into the role of system-level instructions in maximizing LLM potential.",
            "corpus_id": "273502098",
            "text": "Large Language Models (LLMs) have shown impressive capabilities in many scenarios, but their performance depends, in part, on the choice of prompt. Past research has focused on optimizing prompts specific to a task. However, much less attention has been given to optimizing the general instructions included in a prompt, known as a system prompt. To address this gap, we propose SPRIG, an edit-based genetic algorithm that iteratively constructs prompts from prespecified components to maximize the model's performance in general scenarios. We evaluate the performance of system prompts on a collection of 47 different types of tasks to ensure generalizability. Our study finds that a single optimized system prompt performs on par with task prompts optimized for each individual task. Moreover, combining system and task-level optimizations leads to further improvement, which showcases their complementary nature. Experiments also reveal that the optimized system prompts generalize effectively across model families, parameter sizes, and languages. This study provides insights into the role of system-level instructions in maximizing LLM potential.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.11358642578125
        },
        {
            "paperId": "6b50f585dcaaea31a0ec4e630d652057f3f911c1",
            "corpusId": 269448708,
            "title": "Mixture-of-Instructions: Comprehensive Alignment of a Large Language Model through the Mixture of Diverse System Prompting Instructions",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2404.18410?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2404.18410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2298950112",
                    "name": "Bowen Xu"
                },
                {
                    "authorId": "2298951124",
                    "name": "Shaoyu Wu"
                },
                {
                    "authorId": "2298890930",
                    "name": "Kai Liu"
                },
                {
                    "authorId": "2205727378",
                    "name": "Lulu Hu"
                }
            ],
            "abstract": "With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. However, AI-driven products that leverage language models usually necessitate a fusion of these abilities to function effectively in real-world scenarios. Moreover, the considerable computational resources required for proper alignment of LLMs underscore the need for a more robust, efficient, and encompassing approach to multi-task alignment, ensuring improved generative performance. In response to these challenges, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction concatenation combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks.",
            "corpus_id": "269448708",
            "text": "With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. However, AI-driven products that leverage language models usually necessitate a fusion of these abilities to function effectively in real-world scenarios. Moreover, the considerable computational resources required for proper alignment of LLMs underscore the need for a more robust, efficient, and encompassing approach to multi-task alignment, ensuring improved generative performance. In response to these challenges, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction concatenation combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.145263671875
        },
        {
            "paperId": "630ba7c067ae513bd51d00f624af906cba07d6cb",
            "corpusId": 271958902,
            "title": "Optimal Large Language Model Characteristics to Balance Accuracy and Energy Use for Sustainable Medical Applications.",
            "venue": "Radiology",
            "year": 2024,
            "referenceCount": 15,
            "citationCount": 5,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1148/radiol.240320?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1148/radiol.240320, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2223527019",
                    "name": "Florence X. Doo"
                },
                {
                    "authorId": "2269377418",
                    "name": "Dharmam Savani"
                },
                {
                    "authorId": "2190691974",
                    "name": "Adway U. Kanhere"
                },
                {
                    "authorId": "2251971873",
                    "name": "Ruth C. Carlos"
                },
                {
                    "authorId": "2291281421",
                    "name": "Anupam Joshi"
                },
                {
                    "authorId": "2283934035",
                    "name": "Paul H Yi"
                },
                {
                    "authorId": "2186159349",
                    "name": "Vishwa S. Parekh"
                }
            ],
            "abstract": "Background Large language models (LLMs) for medical applications use unknown amounts of energy, which contribute to the overall carbon footprint of the health care system. Purpose To investigate the tradeoffs between accuracy and energy use when using different LLM types and sizes for medical applications. Materials and Methods This retrospective study evaluated five different billion (B)-parameter sizes of two open-source LLMs (Meta's Llama 2, a general-purpose model, and LMSYS Org's Vicuna 1.5, a specialized fine-tuned model) using chest radiograph reports from the National Library of Medicine's Indiana University Chest X-ray Collection. Reports with missing demographic information and missing or blank files were excluded. Models were run on local compute clusters with visual computing graphic processing units. A single-task prompt explained clinical terminology and instructed each model to confirm the presence or absence of each of the 13 CheXpert disease labels. Energy use (in kilowatt-hours) was measured using an open-source tool. Accuracy was assessed with 13 CheXpert reference standard labels for diagnostic findings on chest radiographs, where overall accuracy was the mean of individual accuracies of all 13 labels. Efficiency ratios (accuracy per kilowatt-hour) were calculated for each model type and size. Results A total of 3665 chest radiograph reports were evaluated. The Vicuna 1.5\u20097B and 13B models had higher efficiency ratios (737.28 and 331.40, respectively) and higher overall labeling accuracy (93.83% [3438.69 of 3665 reports] and 93.65% [3432.38 of 3665 reports], respectively) than that of the Llama 2 models (7B: efficiency ratio of 13.39, accuracy of 7.91% [289.76 of 3665 reports]; 13B: efficiency ratio of 40.90, accuracy of 74.08% [2715.15 of 3665 reports]; 70B: efficiency ratio of 22.30, accuracy of 92.70% [3397.38 of 3665 reports]). Vicuna 1.5\u20097B had the highest efficiency ratio (737.28 vs 13.39 for Llama 2\u20097B). The larger Llama 2\u200970B model used more than seven times the energy of its 7B counterpart (4.16 kWh vs 0.59 kWh) with low overall accuracy, resulting in an efficiency ratio of only 22.30. Conclusion Smaller fine-tuned LLMs were more sustainable than larger general-purpose LLMs, using less energy without compromising accuracy, highlighting the importance of LLM selection for medical applications. \u00a9 RSNA, 2024 Supplemental material is available for this article.",
            "corpus_id": "271958902",
            "text": "Background Large language models (LLMs) for medical applications use unknown amounts of energy, which contribute to the overall carbon footprint of the health care system. Purpose To investigate the tradeoffs between accuracy and energy use when using different LLM types and sizes for medical applications. Materials and Methods This retrospective study evaluated five different billion (B)-parameter sizes of two open-source LLMs (Meta's Llama 2, a general-purpose model, and LMSYS Org's Vicuna 1.5, a specialized fine-tuned model) using chest radiograph reports from the National Library of Medicine's Indiana University Chest X-ray Collection. Reports with missing demographic information and missing or blank files were excluded. Models were run on local compute clusters with visual computing graphic processing units. A single-task prompt explained clinical terminology and instructed each model to confirm the presence or absence of each of the 13 CheXpert disease labels. Energy use (in kilowatt-hours) was measured using an open-source tool. Accuracy was assessed with 13 CheXpert reference standard labels for diagnostic findings on chest radiographs, where overall accuracy was the mean of individual accuracies of all 13 labels. Efficiency ratios (accuracy per kilowatt-hour) were calculated for each model type and size. Results A total of 3665 chest radiograph reports were evaluated. The Vicuna 1.5\u20097B and 13B models had higher efficiency ratios (737.28 and 331.40, respectively) and higher overall labeling accuracy (93.83% [3438.69 of 3665 reports] and 93.65% [3432.38 of 3665 reports], respectively) than that of the Llama 2 models (7B: efficiency ratio of 13.39, accuracy of 7.91% [289.76 of 3665 reports]; 13B: efficiency ratio of 40.90, accuracy of 74.08% [2715.15 of 3665 reports]; 70B: efficiency ratio of 22.30, accuracy of 92.70% [3397.38 of 3665 reports]). Vicuna 1.5\u20097B had the highest efficiency ratio (737.28 vs 13.39 for Llama 2\u20097B). The larger Llama 2\u200970B model used more than seven times the energy of its 7B counterpart (4.16 kWh vs 0.59 kWh) with low overall accuracy, resulting in an efficiency ratio of only 22.30. Conclusion Smaller fine-tuned LLMs were more sustainable than larger general-purpose LLMs, using less energy without compromising accuracy, highlighting the importance of LLM selection for medical applications. \u00a9 RSNA, 2024 Supplemental material is available for this article.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0118255615234375
        },
        {
            "paperId": "866d8d693e941a02f06014a814a0d99f50516a41",
            "corpusId": 276346360,
            "title": "Task Planning for Dual-Arm Robot Empowered by Large Language Model",
            "venue": "ACM Cloud and Autonomic Computing Conference",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CAC63892.2024.10864579?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CAC63892.2024.10864579, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311701686",
                    "name": "Binsheng Feng"
                },
                {
                    "authorId": "2345400245",
                    "name": "Zhigang Wang"
                },
                {
                    "authorId": "2288213729",
                    "name": "Xianzhong Dai"
                }
            ],
            "abstract": "Autonomous decision-making capabilities are a key indicator of a robot's level of intelligence, requiring it to have scene understanding and reasoning abilities. Leveraging the powerful general intelligence of large language models (LLMs) for robot task planning has become an important approach in recent years. However, current research encounters several issues: (1) The output of LLM exhibits inherent randomness, making it challenging to ensure system stability and safety when directly executing LLM-generated code. (2) Existing frameworks are often designed for single-robot applications, and their direct application to multi-robot scenarios can lead to improper task allocation, task conflicts, and other issues. In this study, we propose DRTP-LLM, a framework that leverages LLM for reasoning and decision-making in dual-arm robotic tasks while ensuring system safety. This framework includes the prompt mechanism, a library of primitives, and the primitive sequence extractor, among other components. We deployed this method on a real dual-arm robot system and validated it in tabletop scenarios, evaluating the results with specific metrics. Compared to directly executing LLM-generated code, our method demonstrates a higher success rate. Additionally, the optimizations for dual-arm robot scenarios lead to more reasonable and efficient task allocations.",
            "corpus_id": "276346360",
            "text": "Autonomous decision-making capabilities are a key indicator of a robot's level of intelligence, requiring it to have scene understanding and reasoning abilities. Leveraging the powerful general intelligence of large language models (LLMs) for robot task planning has become an important approach in recent years. However, current research encounters several issues: (1) The output of LLM exhibits inherent randomness, making it challenging to ensure system stability and safety when directly executing LLM-generated code. (2) Existing frameworks are often designed for single-robot applications, and their direct application to multi-robot scenarios can lead to improper task allocation, task conflicts, and other issues. In this study, we propose DRTP-LLM, a framework that leverages LLM for reasoning and decision-making in dual-arm robotic tasks while ensuring system safety. This framework includes the prompt mechanism, a library of primitives, and the primitive sequence extractor, among other components. We deployed this method on a real dual-arm robot system and validated it in tabletop scenarios, evaluating the results with specific metrics. Compared to directly executing LLM-generated code, our method demonstrates a higher success rate. Additionally, the optimizations for dual-arm robot scenarios lead to more reasonable and efficient task allocations.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.04046630859375
        },
        {
            "paperId": "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
            "corpusId": 263609325,
            "title": "Time-LLM: Time Series Forecasting by Reprogramming Large Language Models",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "referenceCount": 63,
            "citationCount": 415,
            "influentialCitationCount": 63,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2310.01728",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.01728, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2254096428",
                    "name": "Ming Jin"
                },
                {
                    "authorId": "2255363760",
                    "name": "Shiyu Wang"
                },
                {
                    "authorId": "2253908414",
                    "name": "Lintao Ma"
                },
                {
                    "authorId": "2237992280",
                    "name": "Zhixuan Chu"
                },
                {
                    "authorId": "2253786576",
                    "name": "James Y. Zhang"
                },
                {
                    "authorId": "2119204984",
                    "name": "X. Shi"
                },
                {
                    "authorId": "2254173316",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "2253824408",
                    "name": "Yuxuan Liang"
                },
                {
                    "authorId": "2256011160",
                    "name": "Yuan-Fang Li"
                },
                {
                    "authorId": "2254047333",
                    "name": "Shirui Pan"
                },
                {
                    "authorId": "2253561592",
                    "name": "Qingsong Wen"
                }
            ],
            "abstract": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
            "corpus_id": "263609325",
            "text": "Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.19873046875
        },
        {
            "paperId": "bee9abd40678b3564102df6bc6bb7757faa49d67",
            "corpusId": 274141558,
            "title": "GeoGround: A Unified Large Vision-Language Model. for Remote Sensing Visual Grounding",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 7,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.11904, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2306083274",
                    "name": "Yue Zhou"
                },
                {
                    "authorId": "2262217751",
                    "name": "Mengcheng Lan"
                },
                {
                    "authorId": "2341907483",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2238953821",
                    "name": "Yiping Ke"
                },
                {
                    "authorId": "2331680587",
                    "name": "Xue Jiang"
                },
                {
                    "authorId": "2262404878",
                    "name": "Litong Feng"
                },
                {
                    "authorId": "2262401622",
                    "name": "Wayne Zhang"
                }
            ],
            "abstract": "Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround",
            "corpus_id": "274141558",
            "text": "Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.07147216796875
        },
        {
            "paperId": "be1453580d595121e97855632088ce1231c28814",
            "corpusId": 272604166,
            "title": "Are Large Language Models General-Purpose Solvers for Dialogue Breakdown Detection? An Empirical Investigation",
            "venue": "Canadian Conference on Electrical and Computer Engineering",
            "year": 2024,
            "referenceCount": 31,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/CCECE59415.2024.10667232?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/CCECE59415.2024.10667232, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320858269",
                    "name": "Abdellah Ghassel"
                },
                {
                    "authorId": "2284259400",
                    "name": "Xiaodan Zhu"
                },
                {
                    "authorId": "2284191074",
                    "name": "Stephen W. Thomas"
                }
            ],
            "abstract": "This study addresses the challenge of dialogue breakdown\u2014characterized as incoherence, irrelevance, or any disruption that significantly hampers the flow of the conversation. The impact of dialogue breakdowns has become critical with the adoption of large language models in various industries for companies with dialogue-based systems, such as Salesforce, Amazon, and Microsoft. Leveraging the Dialogue Breakdown Detection Challenge Dataset, we investigate the performance of generalist large language models, including ChatGPT, GPT-4, and Mistral-Medium, in identifying instances of dialogue breakdown without domain-specific fine-tuning. Through a series of experiments employing zero-shot and few-shot prompting techniques combined with chain-of-thought reasoning, this research sets a new benchmark in the field. Our findings reveal that GPT-4 outperforms both specialized models previously considered state-of-the-art and other generalist models in detecting dialogue breakdowns by over a 3% margin, achieving an accuracy of 82.0%. To our knowledge, this study is the first to demonstrate the enhanced capability of generalist large language models in this domain. Our experiments found that when detecting dialogue breakdowns, larger models like GPT-4 are less sensitive to how they are prompted. In contrast, smaller models like ChatGPT and Mistral-Medium can improve their performance by using prompting techniques that combine few-shot learning with the chain-of-thought method. This work proposes future research directions, including enhanced error analysis and developing an Ensemble RAG approach for improved generalization in dialogue breakdown detection.",
            "corpus_id": "272604166",
            "text": "This study addresses the challenge of dialogue breakdown\u2014characterized as incoherence, irrelevance, or any disruption that significantly hampers the flow of the conversation. The impact of dialogue breakdowns has become critical with the adoption of large language models in various industries for companies with dialogue-based systems, such as Salesforce, Amazon, and Microsoft. Leveraging the Dialogue Breakdown Detection Challenge Dataset, we investigate the performance of generalist large language models, including ChatGPT, GPT-4, and Mistral-Medium, in identifying instances of dialogue breakdown without domain-specific fine-tuning. Through a series of experiments employing zero-shot and few-shot prompting techniques combined with chain-of-thought reasoning, this research sets a new benchmark in the field. Our findings reveal that GPT-4 outperforms both specialized models previously considered state-of-the-art and other generalist models in detecting dialogue breakdowns by over a 3% margin, achieving an accuracy of 82.0%. To our knowledge, this study is the first to demonstrate the enhanced capability of generalist large language models in this domain. Our experiments found that when detecting dialogue breakdowns, larger models like GPT-4 are less sensitive to how they are prompted. In contrast, smaller models like ChatGPT and Mistral-Medium can improve their performance by using prompting techniques that combine few-shot learning with the chain-of-thought method. This work proposes future research directions, including enhanced error analysis and developing an Ensemble RAG approach for improved generalization in dialogue breakdown detection.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.12841796875
        },
        {
            "paperId": "4d539a10651128f0cf32de013bd93b380f196273",
            "corpusId": 274023105,
            "title": "LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.09301, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2059291",
                    "name": "Zhenshi Li"
                },
                {
                    "authorId": "2166353997",
                    "name": "Dilxat Muhtar"
                },
                {
                    "authorId": "2267812963",
                    "name": "Feng Gu"
                },
                {
                    "authorId": "2330665114",
                    "name": "Xueliang Zhang"
                },
                {
                    "authorId": "2976074",
                    "name": "P. Xiao"
                },
                {
                    "authorId": "2240822976",
                    "name": "Guangjun He"
                },
                {
                    "authorId": "2330705795",
                    "name": "Xiaoxiang Zhu"
                }
            ],
            "abstract": "Automatically and rapidly understanding Earth's surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth's surface to address a wide range of human needs. The emergence of multimodal large language models (MLLMs) has great potential in boosting the efficiency and convenience of intelligent Earth observation. These models can engage in human-like conversations, serve as unified platforms for understanding images, follow diverse instructions, and provide insightful feedbacks. In this study, we introduce LHRS-Bot-Nova, an MLLM specialized in understanding remote sensing (RS) images, designed to expertly perform a wide range of RS understanding tasks aligned with human instructions. LHRS-Bot-Nova features an enhanced vision encoder and a novel bridge layer, enabling efficient visual compression and better language-vision alignment. To further enhance RS-oriented vision-language alignment, we propose a large-scale RS image-caption dataset, generated through feature-guided image recaptioning. Additionally, we introduce an instruction dataset specifically designed to improve spatial recognition abilities. Extensive experiments demonstrate superior performance of LHRS-Bot-Nova across various RS image understanding tasks. We also evaluate different MLLM performances in complex RS perception and instruction following using a complicated multi-choice question evaluation benchmark, providing a reliable guide for future model selection and improvement. Data, code, and models will be available at https://github.com/NJU-LHRS/LHRS-Bot.",
            "corpus_id": "274023105",
            "text": "Automatically and rapidly understanding Earth's surface is fundamental to our grasp of the living environment and informed decision-making. This underscores the need for a unified system with comprehensive capabilities in analyzing Earth's surface to address a wide range of human needs. The emergence of multimodal large language models (MLLMs) has great potential in boosting the efficiency and convenience of intelligent Earth observation. These models can engage in human-like conversations, serve as unified platforms for understanding images, follow diverse instructions, and provide insightful feedbacks. In this study, we introduce LHRS-Bot-Nova, an MLLM specialized in understanding remote sensing (RS) images, designed to expertly perform a wide range of RS understanding tasks aligned with human instructions. LHRS-Bot-Nova features an enhanced vision encoder and a novel bridge layer, enabling efficient visual compression and better language-vision alignment. To further enhance RS-oriented vision-language alignment, we propose a large-scale RS image-caption dataset, generated through feature-guided image recaptioning. Additionally, we introduce an instruction dataset specifically designed to improve spatial recognition abilities. Extensive experiments demonstrate superior performance of LHRS-Bot-Nova across various RS image understanding tasks. We also evaluate different MLLM performances in complex RS perception and instruction following using a complicated multi-choice question evaluation benchmark, providing a reliable guide for future model selection and improvement. Data, code, and models will be available at https://github.com/NJU-LHRS/LHRS-Bot.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0836181640625
        },
        {
            "paperId": "17a32c825bd746a2625eddc2728092171a9ef72a",
            "corpusId": 266362126,
            "title": "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model",
            "venue": "Computer Vision and Pattern Recognition",
            "year": 2023,
            "referenceCount": 132,
            "citationCount": 36,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2312.12423",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.12423, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1564558163",
                    "name": "Shraman Pramanick"
                },
                {
                    "authorId": "2275814516",
                    "name": "Guangxing Han"
                },
                {
                    "authorId": "2266467782",
                    "name": "Rui Hou"
                },
                {
                    "authorId": "2269464026",
                    "name": "Sayan Nag"
                },
                {
                    "authorId": "2237082163",
                    "name": "Ser-Nam Lim"
                },
                {
                    "authorId": "2275249198",
                    "name": "Nicolas Ballas"
                },
                {
                    "authorId": "2266712798",
                    "name": "Qifan Wang"
                },
                {
                    "authorId": "2257181610",
                    "name": "Rama Chellappa"
                },
                {
                    "authorId": "2634674",
                    "name": "Amjad Almahairi"
                }
            ],
            "abstract": "The ability of large language models (LLMs) to process visual inputs has given rise to general-purpose vision systems, unifying various vision-language (VL) tasks by instruction tuning. However, due to the enormous diversity in input-output formats in the vision domain, existing general-purpose models fail to successfully integrate segmentation and multi-image inputs with coarse-level tasks into a single framework. In this work, we introduce VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework. VistaLLM utilizes an instruction-guided image tokenizer that filters global embeddings using task descriptions to extract compressed and refined features from numerous images. Moreover, VistaLLM employs a gradient-aware adaptive sampling technique to represent binary segmentation masks as sequences, significantly improving over previously used uniform sampling. To bolster the desired capability of VistaLLM, we curate CoinIt, a comprehensive coarse-to-fine instruction tuning dataset with 6.8M samples. We also address the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images. Extensive experiments on a wide range of V- and VL tasks demonstrate the effectiveness of VistaLLM by achieving consistent state-of-the-art performance over strong baselines across many downstream tasks. Our project page can be found at https://shramanpramanick.github.io/VistaLLM/.",
            "corpus_id": "266362126",
            "text": "The ability of large language models (LLMs) to process visual inputs has given rise to general-purpose vision systems, unifying various vision-language (VL) tasks by instruction tuning. However, due to the enormous diversity in input-output formats in the vision domain, existing general-purpose models fail to successfully integrate segmentation and multi-image inputs with coarse-level tasks into a single framework. In this work, we introduce VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework. VistaLLM utilizes an instruction-guided image tokenizer that filters global embeddings using task descriptions to extract compressed and refined features from numerous images. Moreover, VistaLLM employs a gradient-aware adaptive sampling technique to represent binary segmentation masks as sequences, significantly improving over previously used uniform sampling. To bolster the desired capability of VistaLLM, we curate CoinIt, a comprehensive coarse-to-fine instruction tuning dataset with 6.8M samples. We also address the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images. Extensive experiments on a wide range of V- and VL tasks demonstrate the effectiveness of VistaLLM by achieving consistent state-of-the-art performance over strong baselines across many downstream tasks. Our project page can be found at https://shramanpramanick.github.io/VistaLLM/.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.153564453125
        },
        {
            "paperId": "11bc70d51aaef6cf547a25b453b4c2706cc70181",
            "corpusId": 278543524,
            "title": "Medical Text and Image Reasoning for Radiation Oncology via Large Language Model",
            "venue": "IEEE International Symposium on Biomedical Imaging",
            "year": 2025,
            "referenceCount": 15,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ISBI60581.2025.10981179?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ISBI60581.2025.10981179, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2360729828",
                    "name": "Ashish Beary"
                },
                {
                    "authorId": "153164178",
                    "name": "Likun Xia"
                },
                {
                    "authorId": "2360846051",
                    "name": "Yading Yuan"
                },
                {
                    "authorId": "2360874111",
                    "name": "Honggang Wang"
                },
                {
                    "authorId": "2113429387",
                    "name": "Ming Ma"
                }
            ],
            "abstract": "Evaluating text based logical reasoning and medical image reasoning abilities in large language models (LLMs) within the domain of radiation oncology is often challenging. Testing LLMs without fine-tuning in this context often fails to achieve high accuracy, particularly when dealing with domain-specific questions, limiting their utility in medical decision-making. In this paper, five prominent LLMs - ChatGPT-4o, Claude-3.5 Sonnet, Llama 3.2, Gemini 1.5 Flash, and o1-preview are evaluated to assess their text based logical reasoning and medical image reasoning performances on two publicly available datasets. We propose a new challenging benchmark dataset featuring a mixture of single-choice questions as well as multiple choice questions in radiation oncology for the reasoning task. To overcome the limitation of LLMs in handling complex and specialized tasks, we also propose a new hybrid fine-tuning method, where Llama 3.2 is fine-tuned using QLoRA, combined with prompt engineering technique to enhance its reasoning performance. Extensive experiments demonstrate that o1-preview outperforms the other models in overall accuracy, while the fine-tuned Llama 3.2 using our proposed method achieves improved performance, showcasing the effectiveness of hybrid fine-tuning for enhancing LLMs reasoning performance.",
            "corpus_id": "278543524",
            "text": "Evaluating text based logical reasoning and medical image reasoning abilities in large language models (LLMs) within the domain of radiation oncology is often challenging. Testing LLMs without fine-tuning in this context often fails to achieve high accuracy, particularly when dealing with domain-specific questions, limiting their utility in medical decision-making. In this paper, five prominent LLMs - ChatGPT-4o, Claude-3.5 Sonnet, Llama 3.2, Gemini 1.5 Flash, and o1-preview are evaluated to assess their text based logical reasoning and medical image reasoning performances on two publicly available datasets. We propose a new challenging benchmark dataset featuring a mixture of single-choice questions as well as multiple choice questions in radiation oncology for the reasoning task. To overcome the limitation of LLMs in handling complex and specialized tasks, we also propose a new hybrid fine-tuning method, where Llama 3.2 is fine-tuned using QLoRA, combined with prompt engineering technique to enhance its reasoning performance. Extensive experiments demonstrate that o1-preview outperforms the other models in overall accuracy, while the fine-tuned Llama 3.2 using our proposed method achieves improved performance, showcasing the effectiveness of hybrid fine-tuning for enhancing LLMs reasoning performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.041229248046875
        },
        {
            "paperId": "b36cd724332d4cd0963ba24460c16e2a9f055fda",
            "corpusId": 278623452,
            "title": "A Multimodal Large Language Model Framework for Intelligent Perception and Decision-Making in Smart Manufacturing",
            "venue": "Italian National Conference on Sensors",
            "year": 2025,
            "referenceCount": 54,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12114979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2360463083",
                    "name": "Tianyu Wang"
                },
                {
                    "authorId": "2361578566",
                    "name": "Bowen Zhang"
                },
                {
                    "authorId": "2362327897",
                    "name": "Daqi Jiang"
                },
                {
                    "authorId": "2288000948",
                    "name": "Dong Li"
                }
            ],
            "abstract": "In modern manufacturing, making accurate and timely decisions requires the ability to effectively handle multiple types of data. This paper presents a multimodal system designed specifically for smart manufacturing applications. The system combines various data sources including images, sensor data, and production records, using advanced multimodal large language models. This approach addresses common limitations of traditional single-modal methods, such as isolated data analysis and poor integration between different data types. Key contributions include a unified method for representing different data types, dynamic semantic tokenization for better data processing, strong alignment strategies across modalities, and a practical two-stage training method involving initial large-scale pretraining and later fine-tuning for specific tasks. Additionally, a novel Transformer-based model is introduced for generating both images and text, significantly improving real-time decision-making capabilities. Experiments on relevant industrial datasets show that this method consistently performs better than current state-of-the-art approaches in tasks like image\u2013text retrieval and visual question answering. The results demonstrate the effectiveness and versatility of the proposed methods, offering important insights and practical solutions to enhance intelligent manufacturing, predictive maintenance, and anomaly detection, thus supporting the development of more efficient and reliable industrial systems.",
            "corpus_id": "278623452",
            "text": "In modern manufacturing, making accurate and timely decisions requires the ability to effectively handle multiple types of data. This paper presents a multimodal system designed specifically for smart manufacturing applications. The system combines various data sources including images, sensor data, and production records, using advanced multimodal large language models. This approach addresses common limitations of traditional single-modal methods, such as isolated data analysis and poor integration between different data types. Key contributions include a unified method for representing different data types, dynamic semantic tokenization for better data processing, strong alignment strategies across modalities, and a practical two-stage training method involving initial large-scale pretraining and later fine-tuning for specific tasks. Additionally, a novel Transformer-based model is introduced for generating both images and text, significantly improving real-time decision-making capabilities. Experiments on relevant industrial datasets show that this method consistently performs better than current state-of-the-art approaches in tasks like image\u2013text retrieval and visual question answering. The results demonstrate the effectiveness and versatility of the proposed methods, offering important insights and practical solutions to enhance intelligent manufacturing, predictive maintenance, and anomaly detection, thus supporting the development of more efficient and reliable industrial systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1539306640625
        },
        {
            "paperId": "9ada8fa11b1cdece31f253acae50b62df8d5f823",
            "corpusId": 258685677,
            "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2023,
            "referenceCount": 71,
            "citationCount": 491,
            "influentialCitationCount": 61,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2305.07922",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2305.07922, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "49416727",
                    "name": "Yue Wang"
                },
                {
                    "authorId": "2064728738",
                    "name": "Hung Le"
                },
                {
                    "authorId": "144049726",
                    "name": "Akhilesh Deepak Gotmare"
                },
                {
                    "authorId": "26910508",
                    "name": "Nghi D. Q. Bui"
                },
                {
                    "authorId": "49299019",
                    "name": "Junnan Li"
                },
                {
                    "authorId": "2184854289",
                    "name": "Steven C. H. Hoi"
                }
            ],
            "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
            "corpus_id": "258685677",
            "text": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.171630859375
        },
        {
            "paperId": "d35079d10bfc7652b0fff972b1262839f6c76131",
            "corpusId": 259137871,
            "title": "GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "referenceCount": 42,
            "citationCount": 13,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.06629",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.06629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "152502434",
                    "name": "Shicheng Tan"
                },
                {
                    "authorId": "1403621152",
                    "name": "W. Tam"
                },
                {
                    "authorId": "2303359307",
                    "name": "Yuanchun Wang"
                },
                {
                    "authorId": "2219922267",
                    "name": "Wenwen Gong"
                },
                {
                    "authorId": "2152915671",
                    "name": "Yang Yang"
                },
                {
                    "authorId": "3449368",
                    "name": "Hongyin Tang"
                },
                {
                    "authorId": "2058349088",
                    "name": "Keqing He"
                },
                {
                    "authorId": "2108421184",
                    "name": "Jiahao Liu"
                },
                {
                    "authorId": "2109593338",
                    "name": "Jingang Wang"
                },
                {
                    "authorId": "47601233",
                    "name": "Shuo Zhao"
                },
                {
                    "authorId": "47243067",
                    "name": "P. Zhang"
                },
                {
                    "authorId": "46199760",
                    "name": "Jie Tang"
                }
            ],
            "abstract": "Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs.",
            "corpus_id": "259137871",
            "text": "Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.05877685546875
        },
        {
            "paperId": "3076cfba160fc4d64eec459e2f99b307b72cb12a",
            "corpusId": 276317419,
            "title": "Can a Single Model Master Both Multi-turn Conversations and Tool Use? CoALM: A Unified Conversational Agentic Language Model",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 41,
            "citationCount": 3,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08820, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189544804",
                    "name": "Emre Can Acikgoz"
                },
                {
                    "authorId": "2275188530",
                    "name": "Jeremy Greer"
                },
                {
                    "authorId": "2345187428",
                    "name": "Akul Datta"
                },
                {
                    "authorId": "2345183631",
                    "name": "Ze Yang"
                },
                {
                    "authorId": "2345875846",
                    "name": "William Zeng"
                },
                {
                    "authorId": "147733481",
                    "name": "Oussama Elachqar"
                },
                {
                    "authorId": "2889843",
                    "name": "Emmanouil Koukoumidis"
                },
                {
                    "authorId": "2365041900",
                    "name": "Dilek Hakkani-Tur"
                },
                {
                    "authorId": "5108268",
                    "name": "Gokhan Tur"
                }
            ],
            "abstract": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.",
            "corpus_id": "276317419",
            "text": "Large Language Models (LLMs) with API-calling capabilities enabled building effective Language Agents (LA), while also revolutionizing the conventional task-oriented dialogue (TOD) paradigm. However, current approaches face a critical dilemma: TOD systems are often trained on a limited set of target APIs, requiring new data to maintain their quality when interfacing with new services, while LAs are not trained to maintain user intent over multi-turn conversations. Because both robust multi-turn management and advanced function calling are crucial for effective conversational agents, we evaluate these skills on three popular benchmarks: MultiWOZ 2.4 (TOD), BFCL V3 (LA), and API-Bank (LA), and our analyses reveal that specialized approaches excel in one domain but underperform in the other. To bridge this chasm, we introduce CoALM (Conversational Agentic Language Model), a unified approach that integrates both conversational and agentic capabilities. We created CoALM-IT, a carefully constructed multi-task dataset that interleave multi-turn ReAct reasoning with complex API usage. Using CoALM-IT, we train three models CoALM 8B, CoALM 70B, and CoALM 405B, which outperform top domain-specific models, including GPT-4o, across all three benchmarks. This demonstrates the feasibility of a single model approach for both TOD and LA, setting a new standard for conversational agents.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.30859375
        },
        {
            "paperId": "ea78d3f609a08e2d8882fc296ebeb2700bbcfee2",
            "corpusId": 278936196,
            "title": "Research on Multi-Agent Question Answering Based on Large Language Models",
            "venue": "CNML",
            "year": 2025,
            "referenceCount": 14,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3728199.3728217?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3728199.3728217, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2364696884",
                    "name": "Xinpeng Lei"
                },
                {
                    "authorId": "2248572518",
                    "name": "Fucheng Wan"
                }
            ],
            "abstract": "The continuous development of large language models has led to tremendous performance improvement and has made them increasingly more important. More and more researchers are starting to explore the use of these large language models in developing artificial intelligence agents. The goal of this research is to develop a question-answering system based on a multi-agent setup with the open-source large language model Qwen2.5. In this multi-agent setup, the two agents include a student and a teacher where the student solves issues with guidance from the teacher. We evaluate accuracy difference between a single-agent and a multi-agent setup with the Chinese evaluation data, C-Eval. This is a departure from other approaches like fine-tuning large models or utilizing methods like chain of thought (CoT) and n-shot learning that are tailor-made to improve model performance. Rather, it is a more general approach aimed at further testing the abilities of large language models and thereby improve their logical reasoning capabilities. The experimental results demonstrate a significant improvement in accuracy in multi-agent systems compared to single-agent setup. In particular, improvement in accuracy over various subjects is around 2.5%. In addition, it has been found that improvement is directly proportional to model parameter size; for example, a model with a parameter size of 14 billion has a 22.18% improvement in accuracy for hard cases and reflects a significant improvement in performance.",
            "corpus_id": "278936196",
            "text": "The continuous development of large language models has led to tremendous performance improvement and has made them increasingly more important. More and more researchers are starting to explore the use of these large language models in developing artificial intelligence agents. The goal of this research is to develop a question-answering system based on a multi-agent setup with the open-source large language model Qwen2.5. In this multi-agent setup, the two agents include a student and a teacher where the student solves issues with guidance from the teacher. We evaluate accuracy difference between a single-agent and a multi-agent setup with the Chinese evaluation data, C-Eval. This is a departure from other approaches like fine-tuning large models or utilizing methods like chain of thought (CoT) and n-shot learning that are tailor-made to improve model performance. Rather, it is a more general approach aimed at further testing the abilities of large language models and thereby improve their logical reasoning capabilities. The experimental results demonstrate a significant improvement in accuracy in multi-agent systems compared to single-agent setup. In particular, improvement in accuracy over various subjects is around 2.5%. In addition, it has been found that improvement is directly proportional to model parameter size; for example, a model with a parameter size of 14 billion has a 22.18% improvement in accuracy for hard cases and reflects a significant improvement in performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.09466552734375
        },
        {
            "paperId": "9ceba58be8abd8b79865bf0d87cfb0f0f8a41631",
            "corpusId": 277685847,
            "title": "From large language models to artificial general intelligence: Evolution pathways in clinical healthcare",
            "venue": "World Journal of Advanced Research and Reviews",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.30574/wjarr.2025.26.1.1086?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.30574/wjarr.2025.26.1.1086, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354696277",
                    "name": "Indraneel Borgohain"
                }
            ],
            "abstract": "This article examines the trajectory and challenges of evolving current large language models (LLMs) toward artificial general intelligence (AGI) capabilities within clinical healthcare environments. The text analyzes the gaps between contemporary LLMs' pattern recognition abilities and the robust reasoning, causal understanding, and contextual adaptation required for true medical AGI. Through a systematic review of current clinical applications and limitations of LLMs, the article identifies three critical areas requiring advancement: dynamic integration of multi-modal medical data streams, consistent medical reasoning across novel scenarios, and autonomous learning from clinical interactions while maintaining safety constraints. A novel architectural framework is proposed that combines LLM capabilities with symbolic reasoning, causal inference, and continual learning mechanisms specifically designed for clinical environments. The article suggests that while LLMs provide a promising foundation, achieving AGI in clinical systems requires fundamental breakthroughs in areas including knowledge representation, uncertainty quantification, and ethical decision-making. The article concludes by outlining a roadmap for research priorities and safety considerations essential for progressing toward clinical AGI while maintaining patient safety and care quality.",
            "corpus_id": "277685847",
            "text": "This article examines the trajectory and challenges of evolving current large language models (LLMs) toward artificial general intelligence (AGI) capabilities within clinical healthcare environments. The text analyzes the gaps between contemporary LLMs' pattern recognition abilities and the robust reasoning, causal understanding, and contextual adaptation required for true medical AGI. Through a systematic review of current clinical applications and limitations of LLMs, the article identifies three critical areas requiring advancement: dynamic integration of multi-modal medical data streams, consistent medical reasoning across novel scenarios, and autonomous learning from clinical interactions while maintaining safety constraints. A novel architectural framework is proposed that combines LLM capabilities with symbolic reasoning, causal inference, and continual learning mechanisms specifically designed for clinical environments. The article suggests that while LLMs provide a promising foundation, achieving AGI in clinical systems requires fundamental breakthroughs in areas including knowledge representation, uncertainty quantification, and ethical decision-making. The article concludes by outlining a roadmap for research priorities and safety considerations essential for progressing toward clinical AGI while maintaining patient safety and care quality.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.05633544921875
        },
        {
            "paperId": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f",
            "corpusId": 253180684,
            "title": "Collaborating with language models for embodied reasoning",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 11,
            "citationCount": 69,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2302.00763",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2302.00763, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46745316",
                    "name": "Ishita Dasgupta"
                },
                {
                    "authorId": "1403585268",
                    "name": "Christine Kaeser-Chen"
                },
                {
                    "authorId": "35789996",
                    "name": "Kenneth Marino"
                },
                {
                    "authorId": "37968006",
                    "name": "Arun Ahuja"
                },
                {
                    "authorId": "2189420976",
                    "name": "Sheila Babayan"
                },
                {
                    "authorId": "145783676",
                    "name": "Felix Hill"
                },
                {
                    "authorId": "2276554",
                    "name": "R. Fergus"
                }
            ],
            "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.",
            "corpus_id": "253180684",
            "text": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1412353515625
        }
    ],
    "quotes": {
        "cost": 0.060693000000000004,
        "quotes": [
            {
                "idx": 0,
                "key": "[249538647 | Zhu et al. | 2022 | Citations: 69]",
                "snippets": "To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[254877510 | Jin et al. | 2022 | Citations: 250]",
                "snippets": "In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[258865893 | Si et al. | 2023 | Citations: 29]",
                "snippets": "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively...Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 703,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 221
                            },
                            {
                                "start": 222,
                                "end": 603
                            },
                            {
                                "start": 604,
                                "end": 704
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1756,
                        "end": 1931,
                        "sentence_offsets": [
                            {
                                "start": 1756,
                                "end": 1930
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[267547973 | Muqeeth et al. | 2024 | Citations: 38]",
                "snippets": "The allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;(Wei et al., 2021)(Mishra et al., 2021), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237416585 | Wei et al. | 2021 | Citations: 3788]": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
                    "[237421373 | Mishra et al. | 2021 | Citations: 752]": "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 953,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 247
                            },
                            {
                                "start": 248,
                                "end": 496
                            },
                            {
                                "start": 497,
                                "end": 630
                            },
                            {
                                "start": 631,
                                "end": 953
                            }
                        ],
                        "ref_mentions": [
                            "237416585",
                            "237421373"
                        ],
                        "quote": "The allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;(Wei et al., 2021)(Mishra et al., 2021), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[268379027 | Ding et al. | 2024 | Citations: 7]",
                "snippets": "In this paper, we propose to leverage separate models that are already highly specialized via a fusing structure. In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. To ensure that the fused model benefits from the specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data. Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we propose to leverage separate models that are already highly specialized via a fusing structure. In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. To ensure that the fused model benefits from the specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data. Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[270702345 | Lu et al. | 2024 | Citations: 63]",
                "snippets": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[271533631 | Al-Maamari et al. | 2024 | Citations: 2]",
                "snippets": "Specialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts (Jiang et al., 2023). Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[256808267 | Jiang et al. | 2023 | Citations: 188]": "Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164 % more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1175,
                        "end": 2304,
                        "sentence_offsets": [
                            {
                                "start": 1175,
                                "end": 1388
                            },
                            {
                                "start": 1389,
                                "end": 1587
                            },
                            {
                                "start": 1588,
                                "end": 1740
                            },
                            {
                                "start": 1741,
                                "end": 1881
                            },
                            {
                                "start": 1882,
                                "end": 1990
                            },
                            {
                                "start": 1991,
                                "end": 2144
                            },
                            {
                                "start": 2145,
                                "end": 2304
                            }
                        ],
                        "ref_mentions": [
                            "256808267"
                        ],
                        "quote": "Specialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts (Jiang et al., 2023). Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[273162841 | Yadav et al. | 2024 | Citations: 22]",
                "snippets": "This work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "BACKGROUND",
                        "pdf_hash": "",
                        "start": 566,
                        "end": 1289,
                        "sentence_offsets": [
                            {
                                "start": 566,
                                "end": 689
                            },
                            {
                                "start": 690,
                                "end": 795
                            },
                            {
                                "start": 796,
                                "end": 935
                            },
                            {
                                "start": 936,
                                "end": 1105
                            },
                            {
                                "start": 1106,
                                "end": 1289
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[273185866 | Grangier et al. | 2024 | Citations: 5]",
                "snippets": "Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[278310893 | Sengupta et al. | 2025 | Citations: 0]",
                "snippets": "The final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020)(Yadav et al., 2023), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[220496384 | He et al. | 2020 | Citations: 121]": "We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks.",
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                },
                "metadata": [
                    {
                        "section_title": "Proposed Downscaling Pipeline",
                        "pdf_hash": "",
                        "start": 198,
                        "end": 1134,
                        "sentence_offsets": [
                            {
                                "start": 198,
                                "end": 329
                            },
                            {
                                "start": 330,
                                "end": 455
                            },
                            {
                                "start": 456,
                                "end": 706
                            },
                            {
                                "start": 707,
                                "end": 861
                            },
                            {
                                "start": 862,
                                "end": 995
                            },
                            {
                                "start": 996,
                                "end": 1134
                            }
                        ],
                        "ref_mentions": [
                            "220496384",
                            "259064039"
                        ],
                        "quote": "The final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020)(Yadav et al., 2023), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.018732,
        "cot": "For the query \"Plot specialized language models to build a general system combine abilities different models this unified model be good the single large LM\", I'll break down the dimensions needed to provide a comprehensive answer:\n\n1. \"Background on Specialized vs. General Language Models\" - This should be a synthesis paragraph that introduces the concept of specialized language models versus general models, setting up the foundation for understanding the query. This will help users understand the key differences and trade-offs.\n\n2. \"Approaches to Combining Specialized Models\" - This should be a synthesis format since there are various methodologies for combining specialized models that need to be explained coherently. Quotes [1], [2], [4], [5], [6], and [7] discuss different approaches like dataless knowledge fusion, mixture-of-reasoning-experts, dynamic gating mechanisms, twin-merging, and knowledge distillation with mixture of experts.\n\n3. \"Benefits of Combining Specialized Models\" - This should be a synthesis format to explain the advantages of combining specialized models instead of using a single large LM. Quotes [0], [2], [3], [4], [7], and [9] mention benefits like reducing interference, leveraging complementary strengths, decentralized development, and improved performance.\n\n4. \"Challenges and Solutions in Model Combination\" - This should be a synthesis format discussing the challenges faced when combining models and the proposed solutions. Quotes [0], [5], and possibly others touch on interference issues and how to address them.\n\n5. \"Performance Comparisons\" - This should be a synthesis format summarizing what the papers say about the performance of combined specialized models compared to single large LMs. Quotes [2], [9], and potentially others provide information about performance outcomes.\n\nThe ordering of these dimensions follows a logical flow: starting with background information, then explaining approaches to combination, followed by benefits, challenges/solutions, and finally performance comparisons. This structure will provide a comprehensive and coherent answer to the query.",
        "plan": {
            "Background on Specialized vs. General Language Models (synthesis)": [],
            "Approaches to Combining Specialized Models (synthesis)": [
                1,
                2,
                4,
                5,
                6,
                7
            ],
            "Benefits of Combining Specialized Models (synthesis)": [
                0,
                2,
                3,
                4,
                7,
                9
            ],
            "Challenges and Solutions in Model Combination (synthesis)": [
                0,
                5,
                8
            ],
            "Performance Comparisons (synthesis)": [
                2,
                9
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Background on Specialized vs. General Language Models",
                "tldr": "Specialized language models are designed to excel at specific tasks or domains while general language models aim to perform well across a wide range of applications. Combining specialized models into a unified system can potentially leverage the strengths of each component to create a more capable overall system than a single large general model. (LLM Memory)",
                "text": "\nIn the landscape of language models, we can distinguish between two primary approaches: specialized and general models. Specialized language models are trained or fine-tuned for specific domains or tasks, such as medical diagnosis, legal document analysis, code generation, or mathematical reasoning. These models develop deep expertise in their targeted areas but may underperform when faced with tasks outside their specialization. Their focused nature often allows them to achieve superior performance with fewer parameters compared to general models when operating within their intended domain. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nGeneral language models, in contrast, are designed to handle a broad spectrum of tasks across multiple domains. Models like GPT-4, PaLM, and Claude aim to be versatile by training on diverse datasets spanning numerous subjects and applications. While these models demonstrate impressive breadth, they may not match the depth of specialized models in particular domains. This limitation stems from the inherent tradeoff between breadth and depth - as a single model attempts to cover more ground, it may sacrifice some performance in specific areas. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe tension between specialized and general approaches has led researchers to explore whether combining multiple specialized models might create systems that benefit from both the depth of specialized expertise and the breadth of general capabilities. Rather than viewing this as an either/or choice, the field is increasingly investigating how to integrate specialized components into unified systems that can seamlessly access the right capabilities for each task. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Approaches to Combining Specialized Models",
                "tldr": "Researchers have developed several strategies for combining specialized language models, including parameter-space merging, mixture-of-experts architectures, and dynamic gating mechanisms. These approaches aim to preserve the specialized knowledge of individual models while creating unified systems that can effectively handle diverse tasks. (7 sources)",
                "text": "\nSeveral promising approaches have emerged for integrating specialized language models into unified systems. One key strategy is parameter-space merging, where models trained on different datasets are combined at the parameter level rather than simply ensembling their outputs. Jin et al. propose a \"dataless knowledge fusion\" method that merges models by minimizing prediction differences between the merged model and individual specialized models, outperforming techniques like Fisher-weighted averaging and traditional ensembling <Paper corpusId=\"254877510\" paperTitle=\"(Jin et al., 2022)\" isShortName></Paper>.\n\nMixture-of-Experts (MoE) architectures represent another prominent approach. Si et al. developed the Mixture-of-Reasoning-Experts (MORE) framework, which maintains a pool of specialized models for different reasoning types and employs a classifier to select the most appropriate expert for each question <Paper corpusId=\"258865893\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. This method leverages the complementary strengths of specialized models while allowing the system to abstain from answering when confidence is low.\n\nDynamic routing mechanisms further enhance the flexibility of combined model systems. Ding et al. introduced ULTRAFUSER, which implements a dynamic gating mechanism that sits atop specialized models for text, code, and math, adaptively controlling each specialist's contribution to the final output at the token level <Paper corpusId=\"268379027\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. This approach enables both specialization of individual components and generalization of the fused model.\n\nMore sophisticated merging approaches recognize the importance of distinguishing between shared and task-specific knowledge. Lu et al. propose Twin-Merging, which first modularizes knowledge into shared and exclusive components, then dynamically merges them based on input characteristics <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This method addresses interference issues that can arise when directly combining models with exclusive knowledge domains.\n\nThe integration of knowledge distillation (KD) with MoE architectures represents another promising direction. Al-Maamari et al. explore various strategies for combining these techniques to create specialized, efficient, and modular language models <Paper corpusId=\"271533631\" paperTitle=\"(Al-Maamari et al., 2024)\" isShortName></Paper>. This approach is particularly relevant for domains like programming languages, where specialized models have demonstrated superior performance in tasks like code completion and bug detection compared to general-purpose alternatives <Paper corpusId=\"271533631\" paperTitle=\"(Al-Maamari et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256808267\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper>.\n\nA final approach focuses on merging multiple fine-tuned versions of a single base model. Yadav et al. propose a flexible, modular method for post-training large language models by combining expert models trained on distinct datasets covering different tasks and domains <Paper corpusId=\"273162841\" paperTitle=\"(Yadav et al., 2024)\" isShortName></Paper>. This strategy aims to preserve the capabilities of individual experts on their specialized tasks while improving zero-shot generalization to unseen tasks.",
                "citations": [
                    {
                        "id": "(Jin et al., 2022)",
                        "snippets": [
                            "In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data."
                        ],
                        "paper": {
                            "corpus_id": 254877510,
                            "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
                            "authors": [
                                {
                                    "authorId": "2148654757",
                                    "name": "Xisen Jin"
                                },
                                {
                                    "authorId": "1384550891",
                                    "name": "Xiang Ren"
                                },
                                {
                                    "authorId": "1398830377",
                                    "name": "Daniel Preotiuc-Pietro"
                                },
                                {
                                    "authorId": "2904366",
                                    "name": "Pengxiang Cheng"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 250
                        },
                        "score": 0.57373046875
                    },
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively",
                            "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."
                        ],
                        "paper": {
                            "corpus_id": 258865893,
                            "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
                            "authors": [
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "145756130",
                                    "name": "Chen Zhao"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1389036863",
                                    "name": "Jordan L. Boyd-Graber"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 29
                        },
                        "score": 0.75390625
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "In this paper, we propose to leverage separate models that are already highly specialized via a fusing structure. In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. To ensure that the fused model benefits from the specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data. Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model."
                        ],
                        "paper": {
                            "corpus_id": 268379027,
                            "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
                            "authors": [
                                {
                                    "authorId": "46649145",
                                    "name": "Ning Ding"
                                },
                                {
                                    "authorId": "2135835258",
                                    "name": "Yulin Chen"
                                },
                                {
                                    "authorId": "52297757",
                                    "name": "Ganqu Cui"
                                },
                                {
                                    "authorId": "2221271501",
                                    "name": "Xingtai Lv"
                                },
                                {
                                    "authorId": "2257007994",
                                    "name": "Ruobing Xie"
                                },
                                {
                                    "authorId": "2218723159",
                                    "name": "Bowen Zhou"
                                },
                                {
                                    "authorId": "2273470196",
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "2273551430",
                                    "name": "Maosong Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.61328125
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input."
                        ],
                        "paper": {
                            "corpus_id": 270702345,
                            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                            "authors": [
                                {
                                    "authorId": "2262512474",
                                    "name": "Zhenyi Lu"
                                },
                                {
                                    "authorId": "2277238906",
                                    "name": "Chenghao Fan"
                                },
                                {
                                    "authorId": "2284721764",
                                    "name": "Wei Wei"
                                },
                                {
                                    "authorId": "2262446609",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2182623368",
                                    "name": "Dangyang Chen"
                                },
                                {
                                    "authorId": "2284687448",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 63
                        },
                        "score": 0.623046875
                    },
                    {
                        "id": "(Al-Maamari et al., 2024)",
                        "snippets": [
                            "Specialized models, when trained on narrow domains such as programming languages, have demonstrated superiority in specific tasks like code completion and bug detection over their general-purpose counterparts (Jiang et al., 2023). Introducing modularity into neural network design enhances flexibility, scalability, and maintainability, enabling updates to individual network segments without necessitating a complete retraining. This research primarily focuses on exploring various integration strategies of KD and MoE to create specialized, efficient, and modular language models. While we employed straightforward knowledge distillation techniques, reaching state-of-the-art knowledge distillation was not our objective. Instead, our primary goal was to investigate the feasibility of different integration methods of KD and MoE. KD is the process where smaller student models learn to mimic the behavior of a larger, more capable teacher model using their probabilistic outputs [9]. MoE architectures, on the other hand, dynamically delegate tasks to specialized models, thereby enhancing performance across varied domains and languages [10]."
                        ],
                        "paper": {
                            "corpus_id": 271533631,
                            "title": "Mixture of Modular Experts: Distilling Knowledge from a Multilingual Teacher into Specialized Modular Language Models",
                            "authors": [
                                {
                                    "authorId": "2313634509",
                                    "name": "Mohammed Al-Maamari"
                                },
                                {
                                    "authorId": "2074055134",
                                    "name": "Mehdi Ben Amor"
                                },
                                {
                                    "authorId": "2259357506",
                                    "name": "Michael Granitzer"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.630859375
                    },
                    {
                        "id": "(Jiang et al., 2023)",
                        "snippets": [
                            "Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164 % more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem."
                        ],
                        "paper": {
                            "corpus_id": 256808267,
                            "title": "Impact of Code Language Models on Automated Program Repair",
                            "authors": [
                                {
                                    "authorId": "2057958465",
                                    "name": "Nan Jiang"
                                },
                                {
                                    "authorId": "2152352414",
                                    "name": "Kevin Liu"
                                },
                                {
                                    "authorId": "2492099",
                                    "name": "Thibaud Lutellier"
                                },
                                {
                                    "authorId": "2106349652",
                                    "name": "Lin Tan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Software Engineering",
                            "n_citations": 188
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yadav et al., 2024)",
                        "snippets": [
                            "This work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models."
                        ],
                        "paper": {
                            "corpus_id": 273162841,
                            "title": "What Matters for Model Merging at Scale?",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "144244743",
                                    "name": "Tu Vu"
                                },
                                {
                                    "authorId": "2325489176",
                                    "name": "Jonathan Lai"
                                },
                                {
                                    "authorId": "2324583448",
                                    "name": "Alexandra Chronopoulou"
                                },
                                {
                                    "authorId": "1779225",
                                    "name": "Manaal Faruqui"
                                },
                                {
                                    "authorId": "2253396640",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "2227827",
                                    "name": "Tsendsuren Munkhdalai"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.640625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Benefits of Combining Specialized Models",
                "tldr": "Combining specialized language models offers numerous advantages including enhanced performance across diverse tasks, more efficient resource utilization, and a path to decentralized AI development. This approach allows systems to leverage the complementary strengths of specialized experts while avoiding the task interference that can occur in monolithic general models. (10 sources)",
                "text": "\nThe integration of specialized language models into unified systems provides several significant benefits over relying on a single general-purpose model. Perhaps the most compelling advantage is the ability to achieve superior performance across a broader range of tasks by leveraging the complementary strengths of different experts. Si et al. demonstrated that their Mixture-of-Reasoning-Experts (MORE) framework \"significantly outperforms any single specialized model across all four diverse reasoning types\" <Paper corpusId=\"258865893\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. This performance improvement stems from the system's ability to route each task to the most appropriate specialized model, rather than forcing a single model to handle all tasks with varying degrees of competence.\n\nAnother key benefit is the mitigation of task interference that can plague general models. Zhu et al. identified \"interference among different tasks and modalities\" as the primary factor causing performance degradation in generalist models compared to task-specialized alternatives <Paper corpusId=\"249538647\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>. By maintaining separation between specialized components while intelligently combining their outputs, approaches like ULTRAFUSER's dynamic gating mechanism enable \"both the specialization of individual specialists and the generalization of the fused model\" <Paper corpusId=\"268379027\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. This architecture preserves the depth of expertise in each domain while creating a unified interface for users.\n\nThe combined model approach also offers a more resource-efficient path to building comprehensive AI systems. Sengupta et al. note that their ensemble methodology for combining domain-adapted specialized language models creates \"a more versatile final model\" while \"maintaining a smaller computational footprint compared to traditional large-scale models\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This efficiency advantage is particularly important as model sizes continue to grow, potentially making the development of massive general models increasingly concentrated among organizations with substantial computing resources.\n\nPerhaps most importantly, combining specialized models enables a more decentralized and collaborative approach to advancing language model capabilities. Muqeeth et al. highlight that such an approach \"would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute\" and \"provide a way to recycle the widespread effort and compute already being expended to create specialized models\" <Paper corpusId=\"267547973\" paperTitle=\"(Muqeeth et al., 2024)\" isShortName></Paper>. This aligns with extensive evidence that multitask training improves zero-shot generalization <Paper corpusId=\"267547973\" paperTitle=\"(Muqeeth et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237416585\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237421373\" paperTitle=\"(Mishra et al., 2021)\" isShortName></Paper>.\n\nThe modular nature of combined approaches also facilitates ongoing enhancement of system capabilities. Yadav et al. describe their merging approach as providing \"a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models\" <Paper corpusId=\"273162841\" paperTitle=\"(Yadav et al., 2024)\" isShortName></Paper>. This allows for incremental improvement as new specialized models are developed, rather than requiring complete retraining of massive general models to incorporate new capabilities.\n\nAdvanced ensemble techniques further enhance the collaborative potential of specialized model integration. Methods developed by He et al. and Yadav et al. enable effective aggregation of domain expertise from component models in \"a more collaborative manner, ensuring better combined performance\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper> <Paper corpusId=\"220496384\" paperTitle=\"(He et al., 2020)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This collaborative approach stands in contrast to traditional mixture-of-experts architectures that primarily encourage sparsity within experts, instead focusing on truly leveraging the complementary strengths of each specialized component.",
                "citations": [
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively",
                            "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."
                        ],
                        "paper": {
                            "corpus_id": 258865893,
                            "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
                            "authors": [
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "145756130",
                                    "name": "Chen Zhao"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1389036863",
                                    "name": "Jordan L. Boyd-Graber"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 29
                        },
                        "score": 0.75390625
                    },
                    {
                        "id": "(Zhu et al., 2022)",
                        "snippets": [
                            "To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon."
                        ],
                        "paper": {
                            "corpus_id": 249538647,
                            "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
                            "authors": [
                                {
                                    "authorId": "150167730",
                                    "name": "Jinguo Zhu"
                                },
                                {
                                    "authorId": "2578924",
                                    "name": "Xizhou Zhu"
                                },
                                {
                                    "authorId": "71074736",
                                    "name": "Wenhai Wang"
                                },
                                {
                                    "authorId": "2155455424",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "49404547",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "31843833",
                                    "name": "Xiaogang Wang"
                                },
                                {
                                    "authorId": "3304536",
                                    "name": "Jifeng Dai"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 69
                        },
                        "score": 0.5703125
                    },
                    {
                        "id": "(Ding et al., 2024)",
                        "snippets": [
                            "In this paper, we propose to leverage separate models that are already highly specialized via a fusing structure. In this fusing framework, namely ULTRAFUSER, we use three well-trained LLMs as initial specialist models in text, code, and math. To ensure that the fused model benefits from the specialized knowledge of each specialist model, a dynamic gating mechanism is implemented, which sits on top of the three specialists and adaptively controls the contribution of each specialist to the final output logits based on the input data. Such a mechanism is adopted at the token level, which allows both the specialization of individual specialists and the generalization of the fused model."
                        ],
                        "paper": {
                            "corpus_id": 268379027,
                            "title": "Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models",
                            "authors": [
                                {
                                    "authorId": "46649145",
                                    "name": "Ning Ding"
                                },
                                {
                                    "authorId": "2135835258",
                                    "name": "Yulin Chen"
                                },
                                {
                                    "authorId": "52297757",
                                    "name": "Ganqu Cui"
                                },
                                {
                                    "authorId": "2221271501",
                                    "name": "Xingtai Lv"
                                },
                                {
                                    "authorId": "2257007994",
                                    "name": "Ruobing Xie"
                                },
                                {
                                    "authorId": "2218723159",
                                    "name": "Bowen Zhou"
                                },
                                {
                                    "authorId": "2273470196",
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "2273551430",
                                    "name": "Maosong Sun"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 7
                        },
                        "score": 0.61328125
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "The final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020)(Yadav et al., 2023), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost."
                        ],
                        "paper": {
                            "corpus_id": 278310893,
                            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6923828125
                    },
                    {
                        "id": "(Muqeeth et al., 2024)",
                        "snippets": [
                            "The allure of general-purpose language models and the proliferation of specialized PEFT-based models raises a natural question: Can we leverage a large collection of specialized modules to improve zero-shot generalization of a base language model? Such an approach is attractive for various reasons: First, it would provide a path to decentralized development of generalist language models, which otherwise require a huge amount of centralized compute (Kaplan et al., 2020;Hoffmann et al., 2022). In addition, it would provide a way to recycle the widespread effort and compute already being expended to create specialized models. We might hope such an approach might be successful given the extensive evidence that multitask training improves zero-shot generalization (Sanh et al., 2021;(Wei et al., 2021)(Mishra et al., 2021), and combining specialized models could be seen as a form of multitask learning that does not require simultaneous data access."
                        ],
                        "paper": {
                            "corpus_id": 267547973,
                            "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
                            "authors": [
                                {
                                    "authorId": "1582888954",
                                    "name": "Mohammed Muqeeth"
                                },
                                {
                                    "authorId": "48447436",
                                    "name": "Haokun Liu"
                                },
                                {
                                    "authorId": "2283449435",
                                    "name": "Yufan Liu"
                                },
                                {
                                    "authorId": "2269733851",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 38
                        },
                        "score": 0.73095703125
                    },
                    {
                        "id": "(Wei et al., 2021)",
                        "snippets": [
                            "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."
                        ],
                        "paper": {
                            "corpus_id": 237416585,
                            "title": "Finetuned Language Models Are Zero-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "144026731",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2664737",
                                    "name": "Vincent Zhao"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "40625240",
                                    "name": "Adams Wei Yu"
                                },
                                {
                                    "authorId": "144104130",
                                    "name": "Brian Lester"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3788
                        },
                        "score": 0
                    },
                    {
                        "id": "(Mishra et al., 2021)",
                        "snippets": [
                            "Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction."
                        ],
                        "paper": {
                            "corpus_id": 237421373,
                            "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions",
                            "authors": [
                                {
                                    "authorId": "1817207",
                                    "name": "Swaroop Mishra"
                                },
                                {
                                    "authorId": "1783281",
                                    "name": "Daniel Khashabi"
                                },
                                {
                                    "authorId": "2064619864",
                                    "name": "Chitta Baral"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2021,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 752
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yadav et al., 2024)",
                        "snippets": [
                            "This work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models."
                        ],
                        "paper": {
                            "corpus_id": 273162841,
                            "title": "What Matters for Model Merging at Scale?",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "144244743",
                                    "name": "Tu Vu"
                                },
                                {
                                    "authorId": "2325489176",
                                    "name": "Jonathan Lai"
                                },
                                {
                                    "authorId": "2324583448",
                                    "name": "Alexandra Chronopoulou"
                                },
                                {
                                    "authorId": "1779225",
                                    "name": "Manaal Faruqui"
                                },
                                {
                                    "authorId": "2253396640",
                                    "name": "Mohit Bansal"
                                },
                                {
                                    "authorId": "2227827",
                                    "name": "Tsendsuren Munkhdalai"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.640625
                    },
                    {
                        "id": "(He et al., 2020)",
                        "snippets": [
                            "We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks."
                        ],
                        "paper": {
                            "corpus_id": 220496384,
                            "title": "Bayesian Deep Ensembles via the Neural Tangent Kernel",
                            "authors": [
                                {
                                    "authorId": "1810714948",
                                    "name": "Bobby He"
                                },
                                {
                                    "authorId": "40627523",
                                    "name": "Balaji Lakshminarayanan"
                                },
                                {
                                    "authorId": "1725303",
                                    "name": "Y. Teh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 121
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Challenges and Solutions in Model Combination",
                "tldr": "Combining specialized language models faces significant challenges including task interference, knowledge conflicts, and the limited availability of specialized training data. Researchers have developed solutions such as knowledge modularization, dynamic merging based on input characteristics, and creating specialists from generalist data when domain-specific corpora are scarce. (3 sources)",
                "text": "\nDespite the promising benefits of combining specialized language models, several significant challenges must be addressed to create effective unified systems. Task interference emerges as one of the primary obstacles in model combination. Zhu et al. identified that \"interference among different tasks and modalities is the main factor\" causing performance degradation in generalist models compared to task-specialized alternatives <Paper corpusId=\"249538647\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>. This interference occurs when knowledge required for different tasks conflicts or competes for the same model capacity, potentially degrading performance across multiple domains.\n\nKnowledge conflicts present another substantial challenge, particularly when merging models with exclusive domain-specific knowledge. Lu et al. found that \"directly merging exclusive knowledge hinders overall performance\" even though both shared and exclusive task-specific knowledge are crucial for effective model merging <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. Traditional merging approaches often produce significant performance gaps compared to individually fine-tuned models due to these knowledge conflicts and their inability to adapt to heterogeneous test data.\n\nTo address these challenges, researchers have developed increasingly sophisticated merging techniques. Lu et al. proposed Twin-Merging, which implements a two-stage approach: first \"modularizing knowledge into shared and exclusive components\" with compression to reduce redundancy, and then \"dynamically merging shared and task-specific knowledge based on the input\" <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This method explicitly addresses the tension between shared and exclusive knowledge, allowing the system to adaptively emphasize different aspects of its knowledge depending on the specific input characteristics.\n\nAnother practical challenge is the limited availability of domain-specific training data for creating specialized models in many fields. Grangier et al. tackled this constraint by developing an approach to \"build specialist models from large generalist training sets\" when specialist data is available only in limited amounts <Paper corpusId=\"273185866\" paperTitle=\"(Grangier et al., 2024)\" isShortName></Paper>. This technique enables the creation of effective specialized models even for domains where collecting substantial amounts of targeted training data would be prohibitively expensive or impractical.\n\nThese solutions represent important progress in addressing the core challenges of combining specialized models. By explicitly managing knowledge conflicts, dynamically adjusting model contributions based on input characteristics, and developing methods to create specialists even with limited domain data, researchers are steadily improving the feasibility and effectiveness of unified multi-specialist systems <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.",
                "citations": [
                    {
                        "id": "(Zhu et al., 2022)",
                        "snippets": [
                            "To build an artificial neural network like the biological intelligence system, recent works have unified numerous tasks into a generalist model, which can process various tasks with shared parameters and do not have any task-specific modules. While generalist models achieve promising results on various benchmarks, they have performance degradation on some tasks compared with task-specialized models. In this work, we find that interference among different tasks and modalities is the main factor to this phenomenon."
                        ],
                        "paper": {
                            "corpus_id": 249538647,
                            "title": "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs",
                            "authors": [
                                {
                                    "authorId": "150167730",
                                    "name": "Jinguo Zhu"
                                },
                                {
                                    "authorId": "2578924",
                                    "name": "Xizhou Zhu"
                                },
                                {
                                    "authorId": "71074736",
                                    "name": "Wenhai Wang"
                                },
                                {
                                    "authorId": "2155455424",
                                    "name": "Xiaohua Wang"
                                },
                                {
                                    "authorId": "49404547",
                                    "name": "Hongsheng Li"
                                },
                                {
                                    "authorId": "31843833",
                                    "name": "Xiaogang Wang"
                                },
                                {
                                    "authorId": "3304536",
                                    "name": "Jifeng Dai"
                                }
                            ],
                            "year": 2022,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 69
                        },
                        "score": 0.5703125
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input."
                        ],
                        "paper": {
                            "corpus_id": 270702345,
                            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                            "authors": [
                                {
                                    "authorId": "2262512474",
                                    "name": "Zhenyi Lu"
                                },
                                {
                                    "authorId": "2277238906",
                                    "name": "Chenghao Fan"
                                },
                                {
                                    "authorId": "2284721764",
                                    "name": "Wei Wei"
                                },
                                {
                                    "authorId": "2262446609",
                                    "name": "Xiaoye Qu"
                                },
                                {
                                    "authorId": "2182623368",
                                    "name": "Dangyang Chen"
                                },
                                {
                                    "authorId": "2284687448",
                                    "name": "Yu Cheng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 63
                        },
                        "score": 0.623046875
                    },
                    {
                        "id": "(Grangier et al., 2024)",
                        "snippets": [
                            "Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead."
                        ],
                        "paper": {
                            "corpus_id": 273185866,
                            "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling",
                            "authors": [
                                {
                                    "authorId": "2529182",
                                    "name": "David Grangier"
                                },
                                {
                                    "authorId": "2324421747",
                                    "name": "Simin Fan"
                                },
                                {
                                    "authorId": "2324783252",
                                    "name": "Skyler Seto"
                                },
                                {
                                    "authorId": "1763708",
                                    "name": "Pierre Ablin"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 5
                        },
                        "score": 0.587890625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparisons",
                "tldr": "Studies comparing specialized model combinations with general models consistently show that well-designed multi-specialist systems can outperform single general models across diverse tasks. These performance advantages are achieved while maintaining smaller computational footprints than traditional large-scale models. (4 sources)",
                "text": "\nEmpirical evaluations of systems that combine specialized language models reveal significant performance advantages over both individual specialized models and general-purpose alternatives. Si et al. demonstrated that their Mixture-of-Reasoning-Experts (MORE) framework \"significantly outperforms any single specialized model across all four diverse reasoning types\" evaluated in their study <Paper corpusId=\"258865893\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. This performance improvement stems from MORE's ability to leverage the complementary strengths of different specialized models by routing each question to the most appropriate expert or abstaining when confidence is low.\n\nThe computational efficiency of combined specialist approaches provides another quantifiable advantage. Sengupta et al. found that their ensemble methodology for combining domain-adapted specialized language models creates \"a more versatile final model\" while \"maintaining a smaller computational footprint compared to traditional large-scale models\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This efficiency advantage allows combined specialist systems to achieve better performance than original general models at equivalent computational costs, as demonstrated in their research.\n\nAdvanced ensemble techniques further enhance performance outcomes when combining specialized models. By leveraging methods developed by He et al. and Yadav et al., researchers can \"effectively aggregate the domain expertise of each component model\" in \"a more collaborative manner, ensuring better combined performance\" <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper> <Paper corpusId=\"220496384\" paperTitle=\"(He et al., 2020)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. These approaches differ fundamentally from traditional mixture-of-experts architectures by focusing on true collaboration between specialists rather than merely encouraging sparsity within different experts.\n\nUnlike simply increasing the size of a single general model, which often yields diminishing returns in performance improvements relative to computational costs, combining specialized models offers a more efficient scaling path. The performance benefits of multi-specialist systems can be particularly pronounced in domains requiring deep expertise, where even the largest general models may struggle to match the capabilities of targeted specialists working in concert.",
                "citations": [
                    {
                        "id": "(Si et al., 2023)",
                        "snippets": [
                            "Therefore, we go against this trend of building a single generalist language model, but rather design a more interpretable system that consists of a pool of specialized models and each question is answered by one of them. Crucially, to best use complementary strengths of multiple QA models, we implement a pool of diverse and capable specialized models (e.g., by equipping LLMs with corresponding prompting strategies) for each specific reasoning type; then we train a classifier to select the best candidate answer from the specialized models for each question or to abstain from answering (Figure 1). This framework, Mixture-of-Reasoning-Experts (MORE), aims to both generalize and answer selectively",
                            "Experiments validate that by ensembling the specialized experts this way, MORE significantly outperforms any single specialized model across all four diverse reasoning types."
                        ],
                        "paper": {
                            "corpus_id": 258865893,
                            "title": "Getting MoRE out of Mixture of Language Model Reasoning Experts",
                            "authors": [
                                {
                                    "authorId": "152358188",
                                    "name": "Chenglei Si"
                                },
                                {
                                    "authorId": "3040379",
                                    "name": "Weijia Shi"
                                },
                                {
                                    "authorId": "145756130",
                                    "name": "Chen Zhao"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "1389036863",
                                    "name": "Jordan L. Boyd-Graber"
                                }
                            ],
                            "year": 2023,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 29
                        },
                        "score": 0.75390625
                    },
                    {
                        "id": "(Sengupta et al., 2025)",
                        "snippets": [
                            "The final stage of our pipeline implements a model ensemble methodology to combine these domain-adapted SLMs into a unified system. This ensemble approach preserves the specialized capabilities of individual SLMs while creating a more versatile final model. By leveraging advanced ensemble techniques (He et al., 2020)(Yadav et al., 2023), we can effectively aggregate the domain expertise of each component model while maintaining a smaller computational footprint compared to traditional large-scale models. This approach is fundamentally different from mixture of experts, where the primary motivation is usually to encour-age sparsity within different experts. Rather, we focus on combining multiple smaller experts together in a more collaborative manner, ensuring better combined performance. As shown earlier in Proposition 4.1, this ensemble would give a better performance than the original model at the same computational cost."
                        ],
                        "paper": {
                            "corpus_id": 278310893,
                            "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling",
                            "authors": [
                                {
                                    "authorId": "34920835",
                                    "name": "Ayan Sengupta"
                                },
                                {
                                    "authorId": "2345922770",
                                    "name": "Yash Goel"
                                },
                                {
                                    "authorId": "2249914540",
                                    "name": "Tanmoy Chakraborty"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.6923828125
                    },
                    {
                        "id": "(He et al., 2020)",
                        "snippets": [
                            "We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks."
                        ],
                        "paper": {
                            "corpus_id": 220496384,
                            "title": "Bayesian Deep Ensembles via the Neural Tangent Kernel",
                            "authors": [
                                {
                                    "authorId": "1810714948",
                                    "name": "Bobby He"
                                },
                                {
                                    "authorId": "40627523",
                                    "name": "Balaji Lakshminarayanan"
                                },
                                {
                                    "authorId": "1725303",
                                    "name": "Y. Teh"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 121
                        },
                        "score": 0
                    },
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.09778200000000001
    }
}