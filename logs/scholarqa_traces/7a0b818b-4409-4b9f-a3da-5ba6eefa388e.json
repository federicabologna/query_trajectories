{
    "query": "What are the most effective parameter-space merging techniques for combining specialized language models, and how do they address knowledge conflict and task interference?",
    "user_id": "lib_user",
    "task_id": "7a0b818b-4409-4b9f-a3da-5ba6eefa388e",
    "timestamp": "2025-06-23T22:41:33.949785",
    "n_retrieval": 256,
    "n_retrieved": 264,
    "n_candidates": 38,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.45768,
    "decomposed_query": {
        "rewritten_query": "Most effective parameter-space merging techniques for combining specialized language models, and how they address knowledge conflict and task interference.",
        "keyword_query": "parameter-space merging techniques specialized language models knowledge conflict task interference",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.00978,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12420, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305720492",
                    "name": "Shuqi Liu"
                },
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2276605422",
                    "name": "Bowei He"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "2257556686",
                    "name": "Linqi Song"
                }
            ],
            "abstract": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.",
            "corpus_id": 276422064,
            "sentences": [
                {
                    "corpus_id": "276422064",
                    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                    "text": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.",
                    "score": 0.6118955703255962,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.98046875
                },
                {
                    "corpus_id": "276422064",
                    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                    "text": "The rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023;Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024;Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022;Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023;Yu et al., 2024). \n\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process. \n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers.",
                    "score": 0.5480574796487947,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 159
                        },
                        {
                            "start": 160,
                            "end": 371
                        },
                        {
                            "start": 372,
                            "end": 573
                        },
                        {
                            "start": 574,
                            "end": 804
                        },
                        {
                            "start": 807,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1344
                        },
                        {
                            "start": 1345,
                            "end": 1465
                        },
                        {
                            "start": 1466,
                            "end": 1682
                        },
                        {
                            "start": 1685,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2040
                        },
                        {
                            "start": 2041,
                            "end": 2183
                        },
                        {
                            "start": 2184,
                            "end": 2319
                        },
                        {
                            "start": 2320,
                            "end": 2451
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 349,
                            "end": 370,
                            "matchedPaperCorpusId": "268537132"
                        },
                        {
                            "start": 553,
                            "end": 572,
                            "matchedPaperCorpusId": "237485414"
                        },
                        {
                            "start": 767,
                            "end": 787,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 787,
                            "end": 803,
                            "matchedPaperCorpusId": "265034087"
                        },
                        {
                            "start": 1173,
                            "end": 1193,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1193,
                            "end": 1209,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97021484375
                },
                {
                    "corpus_id": "276422064",
                    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                    "text": "By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing taskvector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. \n\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted finetuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)-derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to codespecific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models. \n\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling ap-proach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance.",
                    "score": 0.4379216848691117,
                    "section_title": "Introduction",
                    "char_start_offset": 2335,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 131
                        },
                        {
                            "start": 132,
                            "end": 281
                        },
                        {
                            "start": 282,
                            "end": 441
                        },
                        {
                            "start": 444,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 798
                        },
                        {
                            "start": 799,
                            "end": 948
                        },
                        {
                            "start": 949,
                            "end": 1120
                        },
                        {
                            "start": 1121,
                            "end": 1321
                        },
                        {
                            "start": 1324,
                            "end": 1496
                        },
                        {
                            "start": 1497,
                            "end": 1631
                        },
                        {
                            "start": 1632,
                            "end": 1812
                        },
                        {
                            "start": 1813,
                            "end": 2102
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 230,
                            "end": 253,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 263,
                            "end": 280,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96337890625
                },
                {
                    "corpus_id": "276422064",
                    "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                    "text": "Merging Models with Sense-Merging. We first evaluate the effectiveness of our Sens-Merging method by utilizing it as a plug-and-play module to enhance existing task-vector-based baselines. Table 1 presents the performance of the baseline methods alongside their Sens-Merging enhanced counterparts across seven datasets. Specifically, when merging fine-tuned models specialized in general knowledge (Chat1 ), mathematical reasoning (Math2 ), and code generation (Code3 ), all derived from LLaMA2-7B4 , Sens-Merging demonstrates a consistent improvement in the average performance across all domains. Specifically, when comparing the average scores of each method with and without Sens-Merging, we find that: \n\n(1) Superior Improvement in Task Arithmetic: Task Arithmetic exhibits a particularly notable increase from an average score of 29.03 without Sens-Merging to 34.78 with Sens-Merging, achieving a 19.22% relative improvement of 5.58 points. As both Ties-Merging and DARE have implemented drop strategies to mitigate parameter interference, the integration of scaling coefficient adjustments through Sens-Merging does not achieve as substantial an enhancement as seen with Task Arithmetic. Nevertheless, Sens-Merging still contributes to performance improvements in these methods, with Ties-Merging increasing from an average score  (47.69) and MATH (7.80), surpassing their respective baselines. In code generation, Task Arithmetic shows substantial improvements, increasing from 13.5 to 33.1 on MBPP and from 7.3 to 18.9 on HumanEval. \n\n(3) Enhanced Performance than Individual Fine-tuned Models: Sens-Merging enables the combined models to achieve higher performance on general knowledge and code generation tasks, even surpassing the original code fine-tuned model. For example, when integrating the Chat, Math, and Code models using Sens-Merging, performance on the MBPP and HumanEval datasets increases significantly. Specifically, accuracy improves from 26.8 to 32.3 on the MBPP dataset and from 12.8 to 19.5 on the HumanEval dataset. This demonstrates that model merging can overcome the challenges associated with training a single model for complex tasks by effectively integrating capabilities from other specialized fine-tuned models.",
                    "score": 0.4051437908555608,
                    "section_title": "Main Results",
                    "char_start_offset": 16128,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 34
                        },
                        {
                            "start": 35,
                            "end": 188
                        },
                        {
                            "start": 189,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 598
                        },
                        {
                            "start": 599,
                            "end": 706
                        },
                        {
                            "start": 709,
                            "end": 946
                        },
                        {
                            "start": 947,
                            "end": 1194
                        },
                        {
                            "start": 1195,
                            "end": 1401
                        },
                        {
                            "start": 1402,
                            "end": 1541
                        },
                        {
                            "start": 1544,
                            "end": 1774
                        },
                        {
                            "start": 1775,
                            "end": 1928
                        },
                        {
                            "start": 1929,
                            "end": 2046
                        },
                        {
                            "start": 2047,
                            "end": 2251
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91357421875
                }
            ],
            "relevance_judgement": 0.98046875,
            "relevance_judgment_input_expanded": "# Title: Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models\n# Venue: arXiv.org\n# Authors: Shuqi Liu, Han Wu, Bowei He, Xiongwei Han, Mingxuan Yuan, Linqi Song\n## Abstract\nRecent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.\n## Introduction\nThe rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023;Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024;Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022;Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023;Yu et al., 2024). \n\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process. \n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers.\n...\nBy combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing taskvector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. \n\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted finetuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)-derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to codespecific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models. \n\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling ap-proach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance.\n\n## Main Results\nMerging Models with Sense-Merging. We first evaluate the effectiveness of our Sens-Merging method by utilizing it as a plug-and-play module to enhance existing task-vector-based baselines. Table 1 presents the performance of the baseline methods alongside their Sens-Merging enhanced counterparts across seven datasets. Specifically, when merging fine-tuned models specialized in general knowledge (Chat1 ), mathematical reasoning (Math2 ), and code generation (Code3 ), all derived from LLaMA2-7B4 , Sens-Merging demonstrates a consistent improvement in the average performance across all domains. Specifically, when comparing the average scores of each method with and without Sens-Merging, we find that: \n\n(1) Superior Improvement in Task Arithmetic: Task Arithmetic exhibits a particularly notable increase from an average score of 29.03 without Sens-Merging to 34.78 with Sens-Merging, achieving a 19.22% relative improvement of 5.58 points. As both Ties-Merging and DARE have implemented drop strategies to mitigate parameter interference, the integration of scaling coefficient adjustments through Sens-Merging does not achieve as substantial an enhancement as seen with Task Arithmetic. Nevertheless, Sens-Merging still contributes to performance improvements in these methods, with Ties-Merging increasing from an average score  (47.69) and MATH (7.80), surpassing their respective baselines. In code generation, Task Arithmetic shows substantial improvements, increasing from 13.5 to 33.1 on MBPP and from 7.3 to 18.9 on HumanEval. \n\n(3) Enhanced Performance than Individual Fine-tuned Models: Sens-Merging enables the combined models to achieve higher performance on general knowledge and code generation tasks, even surpassing the original code fine-tuned model. For example, when integrating the Chat, Math, and Code models using Sens-Merging, performance on the MBPP and HumanEval datasets increases significantly. Specifically, accuracy improves from 26.8 to 32.3 on the MBPP dataset and from 12.8 to 19.5 on the HumanEval dataset. This demonstrates that model merging can overcome the challenges associated with training a single model for complex tasks by effectively integrating capabilities from other specialized fine-tuned models.",
            "reference_string": "[276422064 | Liu et al. | 2025 | Citations: 3]"
        },
        {
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.15283, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257363937",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "2302773646",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2336739354",
                    "name": "Ganggui Ding"
                },
                {
                    "authorId": "3263719",
                    "name": "Xinyi Yu"
                },
                {
                    "authorId": "2062580704",
                    "name": "Linlin Ou"
                },
                {
                    "authorId": "3194022",
                    "name": "Bohan Zhuang"
                }
            ],
            "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "corpus_id": 274965385,
            "sentences": [
                {
                    "corpus_id": "274965385",
                    "title": "Channel Merging: Preserving Specialization for Merged Experts",
                    "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
                    "score": 0.6005676053947996,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96875
                }
            ],
            "relevance_judgement": 0.96875,
            "relevance_judgment_input_expanded": "# Title: Channel Merging: Preserving Specialization for Merged Experts\n# Venue: arXiv.org\n# Authors: Mingyang Zhang, Jing Liu, Ganggui Ding, Xinyi Yu, Linlin Ou, Bohan Zhuang\n## Abstract\nLately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.\n",
            "reference_string": "[274965385 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1609/aaai.v39i21.34405",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v39i21.34405?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v39i21.34405, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257363937",
                    "name": "Mingyang Zhang"
                },
                {
                    "authorId": "2302773646",
                    "name": "Jing Liu"
                },
                {
                    "authorId": "2336739354",
                    "name": "Ganggui Ding"
                },
                {
                    "authorId": "2062580704",
                    "name": "Linlin Ou"
                },
                {
                    "authorId": "3263719",
                    "name": "Xinyi Yu"
                },
                {
                    "authorId": "3194022",
                    "name": "Bohan Zhuang"
                }
            ],
            "abstract": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "corpus_id": 277762515,
            "sentences": [
                {
                    "corpus_id": "277762515",
                    "title": "Channel Merging: Preserving Specialization for Merged Experts",
                    "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
                    "score": 0.5830828687008799,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96826171875
                }
            ],
            "relevance_judgement": 0.96826171875,
            "relevance_judgment_input_expanded": "# Title: Channel Merging: Preserving Specialization for Merged Experts\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Mingyang Zhang, Jing Liu, Ganggui Ding, Linlin Ou, Xinyi Yu, Bohan Zhuang\n## Abstract\nLately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.\n",
            "reference_string": "[277762515 | Zhang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.00997, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2266789873",
                    "name": "Yuhang Zhou"
                },
                {
                    "authorId": "8458211",
                    "name": "Giannis Karamanolakis"
                },
                {
                    "authorId": "2302332301",
                    "name": "Victor Soto"
                },
                {
                    "authorId": "1681193",
                    "name": "Anna Rumshisky"
                },
                {
                    "authorId": "2302332615",
                    "name": "Mayank Kulkarni"
                },
                {
                    "authorId": "2257407889",
                    "name": "Furong Huang"
                },
                {
                    "authorId": "2218202090",
                    "name": "Wei Ai"
                },
                {
                    "authorId": "2302633316",
                    "name": "Jianhua Lu"
                }
            ],
            "abstract": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.",
            "corpus_id": 276095183,
            "sentences": [
                {
                    "corpus_id": "276095183",
                    "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
                    "text": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.",
                    "score": 0.5533059982096883,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96484375
                }
            ],
            "relevance_judgement": 0.96484375,
            "relevance_judgment_input_expanded": "# Title: MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Yuhang Zhou, Giannis Karamanolakis, Victor Soto, Anna Rumshisky, Mayank Kulkarni, Furong Huang, Wei Ai, Jianhua Lu\n## Abstract\nThe recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.\n",
            "reference_string": "[276095183 | Zhou et al. | 2025 | Citations: 3]"
        },
        {
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 38,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.00487, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2185835781",
                    "name": "Bingdong Li"
                },
                {
                    "authorId": "2301202811",
                    "name": "Zixiang Di"
                },
                {
                    "authorId": "2304289936",
                    "name": "Yanting Yang"
                },
                {
                    "authorId": "2259965088",
                    "name": "Hong Qian"
                },
                {
                    "authorId": "2289681804",
                    "name": "Peng Yang"
                },
                {
                    "authorId": "2065509138",
                    "name": "Hao Hao"
                },
                {
                    "authorId": "2301205560",
                    "name": "Ke Tang"
                },
                {
                    "authorId": "2258140188",
                    "name": "Aimin Zhou"
                }
            ],
            "abstract": "In this paper, we introduce a novel approach for addressing the multi-objective optimization problem in large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, model merging faces two significant challenges: First, existing methods rely heavily on human knowledge or intuition. Second, it's difficult to obtain the great model merging configuration in limited evaluations. To address these challenges, we formalize model merging as a multi-objective optimization problem and propose an automated optimization approach named MM-MO. This method leverages multi-objective optimization algorithms to autonomously search for optimal merging configurations across various tasks, alleviating the need for human intervention. In MM-MO, a weak-to-strong method is employed to enhance the acquisition function, allowing previously evaluated superior configurations to guide the search for new ones. Meanwhile, Fisher information is applied to screen these configurations, increasing the possibility of identifying high-quality merging configuration. Additionally, we designed a sparsity metric as an additional optimization objective to enhance the model's generalization performance across different tasks. We conducted comprehensive experiments with other mainstream model merging methods, demonstrating that the proposed MM-MO algorithm is competitive and effective in achieving high-quality model merging.",
            "corpus_id": 270869770,
            "sentences": [
                {
                    "corpus_id": "270869770",
                    "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
                    "text": "We experiment with five model merging methods: \n\n\u2022 Linear (Model Soups) [12]. Model Soups improves model accuracy by averaging the weights of multiple models fine-tuned with different hyperparameters. This method enhances robustness without additional inference or memory costs, making it effective for large pre-trained models in tasks like image classification and natural language processing. \n\n\u2022 Task Arithmetic [13]. Task Arithmetic uses task vectors, which represent directions in the weight space of a pre-trained model, to modify and combine model behaviors. By performing arithmetic operations on these vectors, the model's performance can be steered across multiple tasks efficiently and effectively, without requiring additional training data for each task. \n\n\u2022 TIES [17]. TIES (TRIM, ELECT SIGN & MERGE) addresses parameter interference in model merging by resetting minimally changed parameters, resolving sign conflicts, and merging only aligned parameters. This method outperforms existing techniques in various settings, maintaining valuable information and ensuring better performance in multi-task models. \n\n\u2022 DARE [21]. DARE (Drop And Rescale) sparsifies delta parameters by setting most to zero and rescaling the rest. This approach mitigates parameter interference when merging multiple fine-tuned models into one, enhancing capabilities without retraining. DARE proves particularly effective for large-scale language models, often surpassing the performance of individual source models. \n\n\u2022 Model Breadcrumbs [41]. Model Breadcrumbs enhance task performance by defining a sparse set of weights that create a trajectory within the weight space of a pre-trained model. This method improves efficiency and performance across multiple tasks without needing hyperparameter tuning.",
                    "score": 0.5953066158079785,
                    "section_title": "B.4 Descriptions of Model Merging Methods",
                    "char_start_offset": 36923,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 46
                        },
                        {
                            "start": 49,
                            "end": 77
                        },
                        {
                            "start": 78,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 395
                        },
                        {
                            "start": 398,
                            "end": 421
                        },
                        {
                            "start": 422,
                            "end": 566
                        },
                        {
                            "start": 567,
                            "end": 768
                        },
                        {
                            "start": 771,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 971
                        },
                        {
                            "start": 972,
                            "end": 1123
                        },
                        {
                            "start": 1126,
                            "end": 1138
                        },
                        {
                            "start": 1139,
                            "end": 1238
                        },
                        {
                            "start": 1239,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1508
                        },
                        {
                            "start": 1511,
                            "end": 1536
                        },
                        {
                            "start": 1537,
                            "end": 1688
                        },
                        {
                            "start": 1689,
                            "end": 1797
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 72,
                            "end": 76,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 778,
                            "end": 782,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1133,
                            "end": 1137,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95556640625
                }
            ],
            "relevance_judgement": 0.95556640625,
            "relevance_judgment_input_expanded": "# Title: It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization\n# Venue: arXiv.org\n# Authors: Bingdong Li, Zixiang Di, Yanting Yang, Hong Qian, Peng Yang, Hao Hao, Ke Tang, Aimin Zhou\n## Abstract\nIn this paper, we introduce a novel approach for addressing the multi-objective optimization problem in large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, model merging faces two significant challenges: First, existing methods rely heavily on human knowledge or intuition. Second, it's difficult to obtain the great model merging configuration in limited evaluations. To address these challenges, we formalize model merging as a multi-objective optimization problem and propose an automated optimization approach named MM-MO. This method leverages multi-objective optimization algorithms to autonomously search for optimal merging configurations across various tasks, alleviating the need for human intervention. In MM-MO, a weak-to-strong method is employed to enhance the acquisition function, allowing previously evaluated superior configurations to guide the search for new ones. Meanwhile, Fisher information is applied to screen these configurations, increasing the possibility of identifying high-quality merging configuration. Additionally, we designed a sparsity metric as an additional optimization objective to enhance the model's generalization performance across different tasks. We conducted comprehensive experiments with other mainstream model merging methods, demonstrating that the proposed MM-MO algorithm is competitive and effective in achieving high-quality model merging.\n## B.4 Descriptions of Model Merging Methods\nWe experiment with five model merging methods: \n\n\u2022 Linear (Model Soups) [12]. Model Soups improves model accuracy by averaging the weights of multiple models fine-tuned with different hyperparameters. This method enhances robustness without additional inference or memory costs, making it effective for large pre-trained models in tasks like image classification and natural language processing. \n\n\u2022 Task Arithmetic [13]. Task Arithmetic uses task vectors, which represent directions in the weight space of a pre-trained model, to modify and combine model behaviors. By performing arithmetic operations on these vectors, the model's performance can be steered across multiple tasks efficiently and effectively, without requiring additional training data for each task. \n\n\u2022 TIES [17]. TIES (TRIM, ELECT SIGN & MERGE) addresses parameter interference in model merging by resetting minimally changed parameters, resolving sign conflicts, and merging only aligned parameters. This method outperforms existing techniques in various settings, maintaining valuable information and ensuring better performance in multi-task models. \n\n\u2022 DARE [21]. DARE (Drop And Rescale) sparsifies delta parameters by setting most to zero and rescaling the rest. This approach mitigates parameter interference when merging multiple fine-tuned models into one, enhancing capabilities without retraining. DARE proves particularly effective for large-scale language models, often surpassing the performance of individual source models. \n\n\u2022 Model Breadcrumbs [41]. Model Breadcrumbs enhance task performance by defining a sparse set of weights that create a trajectory within the weight space of a pre-trained model. This method improves efficiency and performance across multiple tasks without needing hyperparameter tuning.",
            "reference_string": "[270869770 | Li et al. | 2024 | Citations: 6]"
        },
        {
            "title": "Training-free LLM Merging for Multi-task Learning",
            "venue": "",
            "year": 2025,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.12379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275537250",
                    "name": "Zichuan Fu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2367227017",
                    "name": "Shanshan Ye"
                },
                {
                    "authorId": "2282271789",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2316790244",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.",
            "corpus_id": 279402179,
            "sentences": [],
            "relevance_judgement": 0.953125,
            "relevance_judgment_input_expanded": "# Title: Training-free LLM Merging for Multi-task Learning\n# Venue: \n# Authors: Zichuan Fu, Xian Wu, Yejing Wang, Wanyu Wang, Shanshan Ye, Hongzhi Yin, Yi Chang, Yefeng Zheng, Xiangyu Zhao\n## Abstract\nLarge Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.\n",
            "reference_string": "[279402179 | Fu et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Parameter Competition Balancing for Model Merging",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 98,
            "citation_count": 24,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.02396, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2304551899",
                    "name": "Guodong Du"
                },
                {
                    "authorId": "2304707602",
                    "name": "Junlin Lee"
                },
                {
                    "authorId": "2307592281",
                    "name": "Jing Li"
                },
                {
                    "authorId": "2277489109",
                    "name": "Runhua Jiang"
                },
                {
                    "authorId": "2304744632",
                    "name": "Yifei Guo"
                },
                {
                    "authorId": "2304776697",
                    "name": "Shuyang Yu"
                },
                {
                    "authorId": "2307331727",
                    "name": "Hanting Liu"
                },
                {
                    "authorId": "2678283",
                    "name": "Sim Kuan Goh"
                },
                {
                    "authorId": "2277702989",
                    "name": "Ho-Kin Tang"
                },
                {
                    "authorId": "2325816146",
                    "name": "Daojing He"
                },
                {
                    "authorId": "2324304769",
                    "name": "Min Zhang"
                }
            ],
            "abstract": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.",
            "corpus_id": 273098230,
            "sentences": [
                {
                    "corpus_id": "273098230",
                    "title": "Parameter Competition Balancing for Model Merging",
                    "text": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.",
                    "score": 0.46632156761599663,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94921875
                }
            ],
            "relevance_judgement": 0.94921875,
            "relevance_judgment_input_expanded": "# Title: Parameter Competition Balancing for Model Merging\n# Venue: Neural Information Processing Systems\n# Authors: Guodong Du, Junlin Lee, Jing Li, Runhua Jiang, Yifei Guo, Shuyang Yu, Hanting Liu, Sim Kuan Goh, Ho-Kin Tang, Daojing He, Min Zhang\n## Abstract\nWhile fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.\n",
            "reference_string": "[273098230 | Du et al. | 2024 | Citations: 24]"
        },
        {
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.00406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315296298",
                    "name": "Quy-Anh Dang"
                },
                {
                    "authorId": "2328975967",
                    "name": "Chris Ngo"
                }
            ],
            "abstract": "Large language models (LLMs) have enabled the development of numerous specialized, task-specific variants. However, the maintenance and deployment of these individual models present substantial challenges in terms of resource utilization and operational efficiency. In this work, we propose the \\textit{Mixture of Distributions (MoD)} framework, a novel approach for merging LLMs that operates directly on their output probability distributions, rather than on model weights. Unlike traditional weight-averaging methods, MoD effectively preserves the specialized capabilities of individual models while enabling efficient knowledge sharing across tasks. Through extensive experimentation on mathematical reasoning benchmarks using Qwen2.5 models, we demonstrate that MoD significantly outperforms existing model merging techniques across multiple benchmarks. All code, data, and experimental materials are published at https://github.com/knovel-eng/mod.",
            "corpus_id": 273798488,
            "sentences": [
                {
                    "corpus_id": "273798488",
                    "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
                    "text": "In this paper, we introduced Mixture of Distributions (MoD), a novel approach for merging Large Language Models that preserves and leverages the strengths of constituent models through probabilistic distribution combination. Our method demonstrates significant advantages over existing parameter-merging techniques by maintaining critical density characteristics while enabling selective integration of model capabilities. The experimental results across diverse mathematical benchmarks validate MoD's effectiveness, achieving state-ofthe-art performance on both fundamental and advanced tasks. Our findings suggest that distributionbased merging approaches offer a promising direc-tion for developing more capable and adaptable language models, particularly in specialized domains requiring precise knowledge integration.",
                    "score": 0.5013858510612836,
                    "section_title": "Conclusions",
                    "char_start_offset": 15057,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 224
                        },
                        {
                            "start": 225,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 822
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9462890625
                }
            ],
            "relevance_judgement": 0.9462890625,
            "relevance_judgment_input_expanded": "# Title: MoD: A Distribution-Based Approach for Merging Large Language Models\n# Venue: arXiv.org\n# Authors: Quy-Anh Dang, Chris Ngo\n## Abstract\nLarge language models (LLMs) have enabled the development of numerous specialized, task-specific variants. However, the maintenance and deployment of these individual models present substantial challenges in terms of resource utilization and operational efficiency. In this work, we propose the \\textit{Mixture of Distributions (MoD)} framework, a novel approach for merging LLMs that operates directly on their output probability distributions, rather than on model weights. Unlike traditional weight-averaging methods, MoD effectively preserves the specialized capabilities of individual models while enabling efficient knowledge sharing across tasks. Through extensive experimentation on mathematical reasoning benchmarks using Qwen2.5 models, we demonstrate that MoD significantly outperforms existing model merging techniques across multiple benchmarks. All code, data, and experimental materials are published at https://github.com/knovel-eng/mod.\n## Conclusions\nIn this paper, we introduced Mixture of Distributions (MoD), a novel approach for merging Large Language Models that preserves and leverages the strengths of constituent models through probabilistic distribution combination. Our method demonstrates significant advantages over existing parameter-merging techniques by maintaining critical density characteristics while enabling selective integration of model capabilities. The experimental results across diverse mathematical benchmarks validate MoD's effectiveness, achieving state-ofthe-art performance on both fundamental and advanced tasks. Our findings suggest that distributionbased merging approaches offer a promising direc-tion for developing more capable and adaptable language models, particularly in specialized domains requiring precise knowledge integration.",
            "reference_string": "[273798488 | Dang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 39,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.09774, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2263268557",
                    "name": "Luyang Fang"
                },
                {
                    "authorId": "2258714804",
                    "name": "Ehsan Latif"
                },
                {
                    "authorId": "2298332870",
                    "name": "Haoran Lu"
                },
                {
                    "authorId": "2325891087",
                    "name": "Yifan Zhou"
                },
                {
                    "authorId": "2276478007",
                    "name": "Ping Ma"
                },
                {
                    "authorId": "2262445470",
                    "name": "Xiaoming Zhai"
                }
            ],
            "abstract": "Automatic scoring of student responses enhances efficiency in education, but deploying a separate neural network for each task increases storage demands, maintenance efforts, and redundant computations. To address these challenges, this paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM) method, which merges models based on feature distribution similarities measured via the Gromov-Wasserstein distance. Our approach begins by extracting features from student responses using individual models, capturing both item-specific context and unique learned representations. The Gromov-Wasserstein distance then quantifies the similarity between these feature distributions, identifying the most compatible models for merging. Models exhibiting the smallest pairwise distances, typically in pairs or trios, are merged by combining only the shared layers preceding the classification head. This strategy results in a unified feature extractor while preserving separate classification heads for item-specific scoring. We validated our approach against human expert knowledge and a GPT-o1-based merging method. GW-SMM consistently outperformed both, achieving a higher micro F1 score, macro F1 score, exact match accuracy, and per-label accuracy. The improvements in micro F1 and per-label accuracy were statistically significant compared to GPT-o1-based merging (p=0.04, p=0.01). Additionally, GW-SMM reduced storage requirements by half without compromising much accuracy, demonstrating its computational efficiency alongside reliable scoring performance.",
            "corpus_id": 276961298,
            "sentences": [
                {
                    "corpus_id": "276961298",
                    "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
                    "text": "In our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference. The final merged parameters are obtained by \n\nwhere the coefficients \u03bb t are determined by the alignment and pruning process. By combining representation alignment with targeted pruning, TIES-MERGING effectively leverages shared knowledge across tasks while maintaining nuanced task-specific distinctions. This makes our method particularly well-suited for applications such as automated scoring systems, where both accuracy and efficiency are critical.",
                    "score": 0.47961550425292243,
                    "section_title": "Model Merging",
                    "char_start_offset": 8245,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 399
                        },
                        {
                            "start": 402,
                            "end": 622
                        },
                        {
                            "start": 623,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 794
                        },
                        {
                            "start": 797,
                            "end": 932
                        },
                        {
                            "start": 933,
                            "end": 1095
                        },
                        {
                            "start": 1098,
                            "end": 1195
                        },
                        {
                            "start": 1196,
                            "end": 1333
                        },
                        {
                            "start": 1334,
                            "end": 1377
                        },
                        {
                            "start": 1380,
                            "end": 1459
                        },
                        {
                            "start": 1460,
                            "end": 1639
                        },
                        {
                            "start": 1640,
                            "end": 1787
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94189453125
                }
            ],
            "relevance_judgement": 0.94189453125,
            "relevance_judgment_input_expanded": "# Title: Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment\n# Venue: arXiv.org\n# Authors: Luyang Fang, Ehsan Latif, Haoran Lu, Yifan Zhou, Ping Ma, Xiaoming Zhai\n## Abstract\nAutomatic scoring of student responses enhances efficiency in education, but deploying a separate neural network for each task increases storage demands, maintenance efforts, and redundant computations. To address these challenges, this paper introduces the Gromov-Wasserstein Scoring Model Merging (GW-SMM) method, which merges models based on feature distribution similarities measured via the Gromov-Wasserstein distance. Our approach begins by extracting features from student responses using individual models, capturing both item-specific context and unique learned representations. The Gromov-Wasserstein distance then quantifies the similarity between these feature distributions, identifying the most compatible models for merging. Models exhibiting the smallest pairwise distances, typically in pairs or trios, are merged by combining only the shared layers preceding the classification head. This strategy results in a unified feature extractor while preserving separate classification heads for item-specific scoring. We validated our approach against human expert knowledge and a GPT-o1-based merging method. GW-SMM consistently outperformed both, achieving a higher micro F1 score, macro F1 score, exact match accuracy, and per-label accuracy. The improvements in micro F1 and per-label accuracy were statistically significant compared to GPT-o1-based merging (p=0.04, p=0.01). Additionally, GW-SMM reduced storage requirements by half without compromising much accuracy, demonstrating its computational efficiency alongside reliable scoring performance.\n## Model Merging\nIn our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference. The final merged parameters are obtained by \n\nwhere the coefficients \u03bb t are determined by the alignment and pruning process. By combining representation alignment with targeted pruning, TIES-MERGING effectively leverages shared knowledge across tasks while maintaining nuanced task-specific distinctions. This makes our method particularly well-suited for applications such as automated scoring systems, where both accuracy and efficiency are critical.",
            "reference_string": "[276961298 | Fang et al. | 2025 | Citations: 0]"
        },
        {
            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 28,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10743, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305720492",
                    "name": "Shuqi Liu"
                },
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2276605422",
                    "name": "Bowei He"
                },
                {
                    "authorId": "2333317068",
                    "name": "Zehua Liu"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                },
                {
                    "authorId": "2257556686",
                    "name": "Linqi Song"
                }
            ],
            "abstract": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that 1bit-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.",
            "corpus_id": 276409347,
            "sentences": [
                {
                    "corpus_id": "276409347",
                    "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
                    "text": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers-enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \\texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.",
                    "score": 0.4593163932843843,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9404296875
                },
                {
                    "corpus_id": "276409347",
                    "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
                    "text": "We propose 1bit-Merging, a novel framework that effectively combines specialized language models while addressing the fundamental trade-off between performance and storage efficiency. By incorporating dynamic routing with binary quantization, our approach maintains task-specific exper-tise while significantly reducing storage overhead. \n\nExtensive experiments across general knowledge, mathematical reasoning, and code generation tasks demonstrate that 1bit-Merging not only preserves the specialized capabilities of individual models but often enhances their performance.",
                    "score": 0.4824178195175888,
                    "section_title": "Conclusion",
                    "char_start_offset": 21957,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 183
                        },
                        {
                            "start": 184,
                            "end": 337
                        },
                        {
                            "start": 340,
                            "end": 574
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88037109375
                }
            ],
            "relevance_judgement": 0.9404296875,
            "relevance_judgment_input_expanded": "# Title: 1bit-Merging: Dynamic Quantized Merging for Large Language Models\n# Venue: arXiv.org\n# Authors: Shuqi Liu, Han Wu, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Linqi Song\n## Abstract\nRecent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that 1bit-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.\n## Conclusion\nWe propose 1bit-Merging, a novel framework that effectively combines specialized language models while addressing the fundamental trade-off between performance and storage efficiency. By incorporating dynamic routing with binary quantization, our approach maintains task-specific exper-tise while significantly reducing storage overhead. \n\nExtensive experiments across general knowledge, mathematical reasoning, and code generation tasks demonstrate that 1bit-Merging not only preserves the specialized capabilities of individual models but often enhances their performance.",
            "reference_string": "[276409347 | Liu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "TIES-Merging: Resolving Interference When Merging Models",
            "venue": "Neural Information Processing Systems",
            "year": 2023,
            "reference_count": 90,
            "citation_count": 317,
            "influential_citation_count": 94,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.01708, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "46841632",
                    "name": "Prateek Yadav"
                },
                {
                    "authorId": "1390031652",
                    "name": "Derek Tam"
                },
                {
                    "authorId": "41019330",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                },
                {
                    "authorId": "143977268",
                    "name": "Mohit Bansal"
                }
            ],
            "abstract": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
            "corpus_id": 259064039,
            "sentences": [
                {
                    "corpus_id": "259064039",
                    "title": "TIES-Merging: Resolving Interference When Merging Models",
                    "text": "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models. \n\nWe demonstrate the effectiveness of our proposed TIES-MERGING method in various setups with: (1) different modalities, including language and vision benchmarks, (2) distinct model sizes and families, such as T5-base and T5-large [58] as well as ViT-B/32 and ViT-L/14 [14], (3) in-domain and out-ofdomain tasks, (4) full finetuning or parameter-efficient finetuning, and (5) in the presence or absence of a validation set for setting merging hyperparameters. We show that TIES-MERGING outperforms other merging methods, such as Task Arithmetic [29], RegMean [31], Fisher Merging [45], and weight averaging [9,82] across all these experimental settings. Notably, for in-domain evaluation, TIES-MERGING outperforms the strongest baseline by an average of 2.3% and 1.7% absolute in NLP and vision settings (Table 1), respectively. For out-of-domain generalization (Table 2), TIES-MERGING outperforms the strongest baseline by 1.0% and 4.4% absolute for T5-base and T5-large models respectively. In Section 7, we perform ablations on our method components and demonstrate the impact of interference on parameter values. Additionally, we showcase the increased advantage of TIES-MERGING over task arithmetic [29] as the number of tasks increases. Finally, we examine the importance of obtaining the correct sign vector.",
                    "score": 0.5859714530570411,
                    "section_title": "Original Values",
                    "char_start_offset": 3768,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 446
                        },
                        {
                            "start": 447,
                            "end": 761
                        },
                        {
                            "start": 764,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1590
                        },
                        {
                            "start": 1591,
                            "end": 1754
                        },
                        {
                            "start": 1755,
                            "end": 1878
                        },
                        {
                            "start": 1879,
                            "end": 2004
                        },
                        {
                            "start": 2005,
                            "end": 2077
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1031,
                            "end": 1035,
                            "matchedPaperCorpusId": "225039882"
                        },
                        {
                            "start": 1307,
                            "end": 1311,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1321,
                            "end": 1325,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 1342,
                            "end": 1346,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 1372,
                            "end": 1375,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1966,
                            "end": 1970,
                            "matchedPaperCorpusId": "254408495"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.939453125,
            "relevance_judgment_input_expanded": "# Title: TIES-Merging: Resolving Interference When Merging Models\n# Venue: Neural Information Processing Systems\n# Authors: Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, Mohit Bansal\n## Abstract\nTransfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging\n## Original Values\nTo address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models. \n\nWe demonstrate the effectiveness of our proposed TIES-MERGING method in various setups with: (1) different modalities, including language and vision benchmarks, (2) distinct model sizes and families, such as T5-base and T5-large [58] as well as ViT-B/32 and ViT-L/14 [14], (3) in-domain and out-ofdomain tasks, (4) full finetuning or parameter-efficient finetuning, and (5) in the presence or absence of a validation set for setting merging hyperparameters. We show that TIES-MERGING outperforms other merging methods, such as Task Arithmetic [29], RegMean [31], Fisher Merging [45], and weight averaging [9,82] across all these experimental settings. Notably, for in-domain evaluation, TIES-MERGING outperforms the strongest baseline by an average of 2.3% and 1.7% absolute in NLP and vision settings (Table 1), respectively. For out-of-domain generalization (Table 2), TIES-MERGING outperforms the strongest baseline by 1.0% and 4.4% absolute for T5-base and T5-large models respectively. In Section 7, we perform ablations on our method components and demonstrate the impact of interference on parameter values. Additionally, we showcase the increased advantage of TIES-MERGING over task arithmetic [29] as the number of tasks increases. Finally, we examine the importance of obtaining the correct sign vector.",
            "reference_string": "[259064039 | Yadav et al. | 2023 | Citations: 317]"
        },
        {
            "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2353385023",
                    "name": "Nikhil Shivakumar Nayak"
                },
                {
                    "authorId": "2273669923",
                    "name": "Krishnateja Killamsetty"
                },
                {
                    "authorId": "2249472971",
                    "name": "Ligong Han"
                },
                {
                    "authorId": "150894502",
                    "name": "Abhishek Bhandwaldar"
                },
                {
                    "authorId": "2290484410",
                    "name": "Prateek Chanda"
                },
                {
                    "authorId": "2261102120",
                    "name": "Kai Xu"
                },
                {
                    "authorId": "2336343895",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2288530668",
                    "name": "Aldo Pareja"
                },
                {
                    "authorId": "2354335460",
                    "name": "Oleg Silkin"
                },
                {
                    "authorId": "2166051120",
                    "name": "Mustafa Eyceoz"
                },
                {
                    "authorId": "2265502573",
                    "name": "Akash Srivastava"
                }
            ],
            "abstract": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.",
            "corpus_id": 277634330,
            "sentences": [
                {
                    "corpus_id": "277634330",
                    "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
                    "text": "Standard full-model fine-tuning methods update all parameters when learning each new task, fully exploiting the model's expressive power but risking severe catastrophic forgetting due to conflicting updates (Luo et al., 2025). On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging. \n\nPositioning Our Work: Our approach introduces a novel constrained full-parameter update method that differs fundamentally from existing categories. Unlike parameter-efficient approaches, we leverage the entire parameter space, maximizing expressive capacity. Unlike isolation approaches, we don't partition parameters or require additional task-specific modules. Unlike constrained full fine-tuning, we explicitly mitigate interference through geometric constraints. Specifically, we dynamically identify low-rank subspaces via Singular Value Decomposition (SVD) and constrain updates to be orthogonal to previously learned task representations. This geometric approach to interference minimization ensures knowledge preservation while maintaining update flexibility. By operating in the full parameter space while enforcing orthogonality constraints, our method achieves a unique balance between knowledge retention and model plasticity, providing a theoretically grounded and practically scalable solution for continual learning in large language models.",
                    "score": 0.499691950164191,
                    "section_title": "Full-Model Fine-Tuning and Model Merging:",
                    "char_start_offset": 8495,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 226
                        },
                        {
                            "start": 227,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 637
                        },
                        {
                            "start": 640,
                            "end": 787
                        },
                        {
                            "start": 788,
                            "end": 898
                        },
                        {
                            "start": 899,
                            "end": 1002
                        },
                        {
                            "start": 1003,
                            "end": 1106
                        },
                        {
                            "start": 1107,
                            "end": 1285
                        },
                        {
                            "start": 1286,
                            "end": 1407
                        },
                        {
                            "start": 1408,
                            "end": 1696
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 289,
                            "end": 311,
                            "matchedPaperCorpusId": "251493208"
                        },
                        {
                            "start": 318,
                            "end": 338,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93896484375
                }
            ],
            "relevance_judgement": 0.93896484375,
            "relevance_judgment_input_expanded": "# Title: Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning\n# Venue: arXiv.org\n# Authors: Nikhil Shivakumar Nayak, Krishnateja Killamsetty, Ligong Han, Abhishek Bhandwaldar, Prateek Chanda, Kai Xu, Hao Wang, Aldo Pareja, Oleg Silkin, Mustafa Eyceoz, Akash Srivastava\n## Abstract\nContinual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.\n## Full-Model Fine-Tuning and Model Merging:\nStandard full-model fine-tuning methods update all parameters when learning each new task, fully exploiting the model's expressive power but risking severe catastrophic forgetting due to conflicting updates (Luo et al., 2025). On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging. \n\nPositioning Our Work: Our approach introduces a novel constrained full-parameter update method that differs fundamentally from existing categories. Unlike parameter-efficient approaches, we leverage the entire parameter space, maximizing expressive capacity. Unlike isolation approaches, we don't partition parameters or require additional task-specific modules. Unlike constrained full fine-tuning, we explicitly mitigate interference through geometric constraints. Specifically, we dynamically identify low-rank subspaces via Singular Value Decomposition (SVD) and constrain updates to be orthogonal to previously learned task representations. This geometric approach to interference minimization ensures knowledge preservation while maintaining update flexibility. By operating in the full parameter space while enforcing orthogonality constraints, our method achieves a unique balance between knowledge retention and model plasticity, providing a theoretically grounded and practically scalable solution for continual learning in large language models.",
            "reference_string": "[277634330 | Nayak et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17380, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2241702793",
                    "name": "Qiuming Zhao"
                },
                {
                    "authorId": "2107310187",
                    "name": "Guangzhi Sun"
                },
                {
                    "authorId": "2256775692",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2241950375",
                    "name": "Mingxing Xu"
                },
                {
                    "authorId": "2241350908",
                    "name": "Thomas Fang Zheng"
                }
            ],
            "abstract": "Language diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging reduces the word error rate by 10% and improves BLEU scores by 4% compared to conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications.",
            "corpus_id": 276575632,
            "sentences": [
                {
                    "corpus_id": "276575632",
                    "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
                    "text": "Model merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues (Wang et al., 2019;Zhang et al., 2023b;Wang et al., 2020b) commonly observed in multi-lingual training. This training independence also enables the use of optimal training configurations for each language or task instead of the unified settings required in multi-lingual training. \n\nMoreover, we propose Low-Rank and Sparse model Merging (LoRS-Merging), which uses a low-rank component to capture the compact structure and a sparse component to capture the scattered details in the weights. LoRS-Merging retains effective parts of structure and details while reducing redundant parts to reduce task interference. Specifically, coarse-grained singular value pruning is used to retain the low-rank structure, while fine-grained magnitude pruning is used to remove redundant details. The main contribution of this paper can be summarised as follows. \n\n\u2022 We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models. \n\n\u2022 LoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model. 2 Related Work",
                    "score": 0.4225654466923844,
                    "section_title": "Introduction",
                    "char_start_offset": 1979,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 119
                        },
                        {
                            "start": 120,
                            "end": 271
                        },
                        {
                            "start": 272,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 716
                        },
                        {
                            "start": 719,
                            "end": 926
                        },
                        {
                            "start": 927,
                            "end": 1048
                        },
                        {
                            "start": 1049,
                            "end": 1216
                        },
                        {
                            "start": 1217,
                            "end": 1282
                        },
                        {
                            "start": 1285,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1508
                        },
                        {
                            "start": 1511,
                            "end": 1823
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 436,
                            "end": 455,
                            "matchedPaperCorpusId": "53748459"
                        },
                        {
                            "start": 455,
                            "end": 475,
                            "matchedPaperCorpusId": "235790783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation\n# Venue: arXiv.org\n# Authors: Qiuming Zhao, Guangzhi Sun, Chao Zhang, Mingxing Xu, Thomas Fang Zheng\n## Abstract\nLanguage diversity presents a significant challenge in speech-to-text (S2T) tasks, such as automatic speech recognition and translation. Traditional multi-task training approaches aim to address this by jointly optimizing multiple speech recognition and translation tasks across various languages. While models like Whisper, built on these strategies, demonstrate strong performance, they still face issues of high computational cost, language interference, suboptimal training configurations, and limited extensibility. To overcome these challenges, we introduce LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility. Experimental results across a range of languages demonstrate that LoRS-Merging reduces the word error rate by 10% and improves BLEU scores by 4% compared to conventional multi-lingual multi-task training baselines. Our findings suggest that model merging, particularly LoRS-Merging, is a scalable and effective complement to traditional multi-lingual training strategies for S2T applications.\n## Introduction\nModel merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues (Wang et al., 2019;Zhang et al., 2023b;Wang et al., 2020b) commonly observed in multi-lingual training. This training independence also enables the use of optimal training configurations for each language or task instead of the unified settings required in multi-lingual training. \n\nMoreover, we propose Low-Rank and Sparse model Merging (LoRS-Merging), which uses a low-rank component to capture the compact structure and a sparse component to capture the scattered details in the weights. LoRS-Merging retains effective parts of structure and details while reducing redundant parts to reduce task interference. Specifically, coarse-grained singular value pruning is used to retain the low-rank structure, while fine-grained magnitude pruning is used to remove redundant details. The main contribution of this paper can be summarised as follows. \n\n\u2022 We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models. \n\n\u2022 LoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model. 2 Related Work",
            "reference_string": "[276575632 | Zhao et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 75,
            "citation_count": 63,
            "influential_citation_count": 12,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.15479, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2262512474",
                    "name": "Zhenyi Lu"
                },
                {
                    "authorId": "2277238906",
                    "name": "Chenghao Fan"
                },
                {
                    "authorId": "2284721764",
                    "name": "Wei Wei"
                },
                {
                    "authorId": "2262446609",
                    "name": "Xiaoye Qu"
                },
                {
                    "authorId": "2182623368",
                    "name": "Dangyang Chen"
                },
                {
                    "authorId": "2284687448",
                    "name": "Yu Cheng"
                }
            ],
            "abstract": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}",
            "corpus_id": 270702345,
            "sentences": [
                {
                    "corpus_id": "270702345",
                    "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                    "text": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}",
                    "score": 0.5400923140816184,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9345703125
                },
                {
                    "corpus_id": "270702345",
                    "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
                    "text": "In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
                    "score": 0.5931306816463314,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 417
                        },
                        {
                            "start": 418,
                            "end": 555
                        },
                        {
                            "start": 558,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 784
                        },
                        {
                            "start": 785,
                            "end": 877
                        },
                        {
                            "start": 878,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1337
                        },
                        {
                            "start": 1338,
                            "end": 1402
                        },
                        {
                            "start": 1403,
                            "end": 1577
                        },
                        {
                            "start": 1578,
                            "end": 1699
                        },
                        {
                            "start": 1700,
                            "end": 1865
                        },
                        {
                            "start": 1868,
                            "end": 2036
                        },
                        {
                            "start": 2037,
                            "end": 2200
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 340,
                            "end": 343,
                            "matchedPaperCorpusId": "249674500"
                        },
                        {
                            "start": 550,
                            "end": 554,
                            "matchedPaperCorpusId": "232110907"
                        },
                        {
                            "start": 677,
                            "end": 681,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 681,
                            "end": 684,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 684,
                            "end": 687,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 687,
                            "end": 690,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 929,
                            "end": 932,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1592,
                            "end": 1596,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 1596,
                            "end": 1599,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 1646,
                            "end": 1650,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1650,
                            "end": 1653,
                            "matchedPaperCorpusId": "258832777"
                        },
                        {
                            "start": 1653,
                            "end": 1656,
                            "matchedPaperCorpusId": "263831551"
                        },
                        {
                            "start": 1656,
                            "end": 1659,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 2028,
                            "end": 2032,
                            "matchedPaperCorpusId": "258865647"
                        },
                        {
                            "start": 2032,
                            "end": 2035,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.9345703125,
            "relevance_judgment_input_expanded": "# Title: Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging\n# Venue: Neural Information Processing Systems\n# Authors: Zhenyi Lu, Chenghao Fan, Wei Wei, Xiaoye Qu, Dangyang Chen, Yu Cheng\n## Abstract\nIn the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}\n## Introduction\nIn recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
            "reference_string": "[270702345 | Lu et al. | 2024 | Citations: 63]"
        },
        {
            "title": "ProFuser: Progressive Fusion of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257014771",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2217614543",
                    "name": "Fanqi Wan"
                },
                {
                    "authorId": "2258677979",
                    "name": "Canbin Huang"
                },
                {
                    "authorId": "2261363209",
                    "name": "Xiaojun Quan"
                },
                {
                    "authorId": "143971529",
                    "name": "Chenliang Li"
                },
                {
                    "authorId": "2114009661",
                    "name": "Mingshi Yan"
                },
                {
                    "authorId": "2281901929",
                    "name": "Ji Zhang"
                }
            ],
            "abstract": "While fusing the capacities and advantages of various large language models (LLMs) offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during the training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including vicuna-7b-v1.5, Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.",
            "corpus_id": 271843401,
            "sentences": [
                {
                    "corpus_id": "271843401",
                    "title": "ProFuser: Progressive Fusion of Large Language Models",
                    "text": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. \n\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths.",
                    "score": 0.48783784968028454,
                    "section_title": "Model Merging",
                    "char_start_offset": 5998,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 113
                        },
                        {
                            "start": 114,
                            "end": 298
                        },
                        {
                            "start": 299,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 597
                        },
                        {
                            "start": 598,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 895
                        },
                        {
                            "start": 896,
                            "end": 1084
                        },
                        {
                            "start": 1085,
                            "end": 1224
                        },
                        {
                            "start": 1225,
                            "end": 1325
                        },
                        {
                            "start": 1326,
                            "end": 1534
                        },
                        {
                            "start": 1537,
                            "end": 1614
                        },
                        {
                            "start": 1615,
                            "end": 1765
                        },
                        {
                            "start": 1766,
                            "end": 1863
                        },
                        {
                            "start": 1864,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 311,
                            "end": 334,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1339,
                            "end": 1359,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: ProFuser: Progressive Fusion of Large Language Models\n# Venue: arXiv.org\n# Authors: Tianyuan Shi, Fanqi Wan, Canbin Huang, Xiaojun Quan, Chenliang Li, Mingshi Yan, Ji Zhang\n## Abstract\nWhile fusing the capacities and advantages of various large language models (LLMs) offers a pathway to construct more powerful and versatile models, a fundamental challenge is to properly select advantageous model during the training. Existing fusion methods primarily focus on the training mode that uses cross entropy on ground truth in a teacher-forcing setup to measure a model's advantage, which may provide limited insight towards model advantage. In this paper, we introduce a novel approach that enhances the fusion process by incorporating both the training and inference modes. Our method evaluates model advantage not only through cross entropy during training but also by considering inference outputs, providing a more comprehensive assessment. To combine the two modes effectively, we introduce ProFuser to progressively transition from inference mode to training mode. To validate ProFuser's effectiveness, we fused three models, including vicuna-7b-v1.5, Llama-2-7b-chat, and mpt-7b-8k-chat, and demonstrated the improved performance in knowledge, reasoning, and safety compared to baseline methods.\n## Model Merging\nModel merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. \n\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths.",
            "reference_string": "[271843401 | Shi et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 47,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2407.03994",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.03994, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2053641126",
                    "name": "Mingxu Tao"
                },
                {
                    "authorId": "2111574159",
                    "name": "Chen Zhang"
                },
                {
                    "authorId": "2007771781",
                    "name": "Quzhe Huang"
                },
                {
                    "authorId": "2310297716",
                    "name": "Tianyao Ma"
                },
                {
                    "authorId": "2290972467",
                    "name": "Songfang Huang"
                },
                {
                    "authorId": "2258097237",
                    "name": "Dongyan Zhao"
                },
                {
                    "authorId": "2115387922",
                    "name": "Yansong Feng"
                }
            ],
            "abstract": "Adapting large language models (LLMs) to new languages typically involves continual pre-training (CT) followed by supervised fine-tuning (SFT). However, this CT-then-SFT approach struggles with limited data in the context of low-resource languages, failing to balance language modeling and task-solving capabilities. We thus propose model merging as an alternative for low-resource languages, combining models with distinct capabilities into a single model without additional training. We use model merging to develop task-solving LLMs for low-resource languages without SFT data in the target languages. Our experiments based on Llama-2-7B demonstrate that model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data. Observing performance saturation in model merging with more training tokens, we further analyze the merging process and introduce a slack variable to the model merging algorithm to mitigate the loss of important parameters, thereby enhancing performance. We hope that model merging can benefit more human languages suffering from data scarcity with its higher data efficiency.",
            "corpus_id": 271039884,
            "sentences": [
                {
                    "corpus_id": "271039884",
                    "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
                    "text": "Model merging is a technique for combining multiple models possessing different capabilities into a single versatile model without additional training. For example, we can merge a model specialized for Japanese and a model specialized for math to obtain a model that excels at solving mathematical problems in Japanese (Akiba et al., 2024). In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}.",
                    "score": 0.4139667360141592,
                    "section_title": "Preliminary: Model Merging",
                    "char_start_offset": 7352,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 559
                        },
                        {
                            "start": 562,
                            "end": 672
                        },
                        {
                            "start": 675,
                            "end": 760
                        },
                        {
                            "start": 761,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 995
                        },
                        {
                            "start": 996,
                            "end": 1085
                        },
                        {
                            "start": 1088,
                            "end": 1283
                        },
                        {
                            "start": 1286,
                            "end": 1434
                        },
                        {
                            "start": 1437,
                            "end": 1609
                        },
                        {
                            "start": 1610,
                            "end": 1750
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 455,
                            "end": 477,
                            "matchedPaperCorpusId": "247362886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93359375
                }
            ],
            "relevance_judgement": 0.93359375,
            "relevance_judgment_input_expanded": "# Title: Unlocking the Potential of Model Merging for Low-Resource Languages\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mingxu Tao, Chen Zhang, Quzhe Huang, Tianyao Ma, Songfang Huang, Dongyan Zhao, Yansong Feng\n## Abstract\nAdapting large language models (LLMs) to new languages typically involves continual pre-training (CT) followed by supervised fine-tuning (SFT). However, this CT-then-SFT approach struggles with limited data in the context of low-resource languages, failing to balance language modeling and task-solving capabilities. We thus propose model merging as an alternative for low-resource languages, combining models with distinct capabilities into a single model without additional training. We use model merging to develop task-solving LLMs for low-resource languages without SFT data in the target languages. Our experiments based on Llama-2-7B demonstrate that model merging effectively endows LLMs for low-resource languages with task-solving abilities, outperforming CT-then-SFT in scenarios with extremely scarce data. Observing performance saturation in model merging with more training tokens, we further analyze the merging process and introduce a slack variable to the model merging algorithm to mitigate the loss of important parameters, thereby enhancing performance. We hope that model merging can benefit more human languages suffering from data scarcity with its higher data efficiency.\n## Preliminary: Model Merging\nModel merging is a technique for combining multiple models possessing different capabilities into a single versatile model without additional training. For example, we can merge a model specialized for Japanese and a model specialized for math to obtain a model that excels at solving mathematical problems in Japanese (Akiba et al., 2024). In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}.",
            "reference_string": "[271039884 | Tao et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 193,
            "citation_count": 27,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.06089, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "81758928",
                    "name": "Jinliang Lu"
                },
                {
                    "authorId": "2310334594",
                    "name": "Ziliang Pang"
                },
                {
                    "authorId": "2310342808",
                    "name": "Min Xiao"
                },
                {
                    "authorId": "2290018211",
                    "name": "Yaochen Zhu"
                },
                {
                    "authorId": "2258711381",
                    "name": "Rui Xia"
                },
                {
                    "authorId": "2290006077",
                    "name": "Jiajun Zhang"
                }
            ],
            "abstract": "The remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging, Ensemble, and Cooperation. Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.",
            "corpus_id": 271050386,
            "sentences": [
                {
                    "corpus_id": "271050386",
                    "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
                    "text": "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.In addition, we introduce a tool that includes some methods in Toolkit.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.",
                    "score": 0.5609793422255078,
                    "section_title": "Methods based on Task Property",
                    "char_start_offset": 20934,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 190
                        },
                        {
                            "start": 190,
                            "end": 277
                        },
                        {
                            "start": 277,
                            "end": 365
                        },
                        {
                            "start": 365,
                            "end": 595
                        },
                        {
                            "start": 597,
                            "end": 750
                        },
                        {
                            "start": 752,
                            "end": 838
                        },
                        {
                            "start": 838,
                            "end": 1041
                        },
                        {
                            "start": 1041,
                            "end": 1193
                        },
                        {
                            "start": 1193,
                            "end": 1369
                        },
                        {
                            "start": 1369,
                            "end": 1440
                        },
                        {
                            "start": 1442,
                            "end": 1644
                        },
                        {
                            "start": 1644,
                            "end": 1743
                        },
                        {
                            "start": 1743,
                            "end": 1960
                        },
                        {
                            "start": 1960,
                            "end": 2114
                        },
                        {
                            "start": 2114,
                            "end": 2210
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 190,
                            "end": 211,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 365,
                            "end": 386,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1489,
                            "end": 1511,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1826,
                            "end": 1846,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models\n# Venue: arXiv.org\n# Authors: Jinliang Lu, Ziliang Pang, Min Xiao, Yaochen Zhu, Rui Xia, Jiajun Zhang\n## Abstract\nThe remarkable success of Large Language Models (LLMs) has ushered natural language processing (NLP) research into a new era. Despite their diverse capabilities, LLMs trained on different corpora exhibit varying strengths and weaknesses, leading to challenges in maximizing their overall efficiency and versatility. To address these challenges, recent studies have explored collaborative strategies for LLMs. This paper provides a comprehensive overview of this emerging research area, highlighting the motivation behind such collaborations. Specifically, we categorize collaborative strategies into three primary approaches: Merging, Ensemble, and Cooperation. Merging involves integrating multiple LLMs in the parameter space. Ensemble combines the outputs of various LLMs. Cooperation} leverages different LLMs to allow full play to their diverse capabilities for specific tasks. We provide in-depth introductions to these methods from different perspectives and discuss their potential applications. Additionally, we outline future research directions, hoping this work will catalyze further studies on LLM collaborations and paving the way for advanced NLP applications.\n## Methods based on Task Property\nMerging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.In addition, we introduce a tool that includes some methods in Toolkit.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.",
            "reference_string": "[271050386 | Lu et al. | 2024 | Citations: 27]"
        },
        {
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "venue": "IEEE Transactions on Artificial Intelligence",
            "year": 2024,
            "reference_count": 181,
            "citation_count": 26,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.14962, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1930733",
                    "name": "D. Hagos"
                },
                {
                    "authorId": "2312327093",
                    "name": "Rick Battle"
                },
                {
                    "authorId": "2260694752",
                    "name": "Danda B. Rawat"
                }
            ],
            "abstract": "The emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.",
            "corpus_id": 271329267,
            "sentences": [
                {
                    "corpus_id": "271329267",
                    "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
                    "text": "Model merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging [45] or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
                    "score": 0.6091474672680863,
                    "section_title": "E. Model Merging",
                    "char_start_offset": 24254,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 152
                        },
                        {
                            "start": 153,
                            "end": 287
                        },
                        {
                            "start": 288,
                            "end": 432
                        },
                        {
                            "start": 433,
                            "end": 708
                        },
                        {
                            "start": 709,
                            "end": 868
                        },
                        {
                            "start": 869,
                            "end": 1157
                        },
                        {
                            "start": 1158,
                            "end": 1326
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 327,
                            "end": 331,
                            "matchedPaperCorpusId": "244345933"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9287109375
                }
            ],
            "relevance_judgement": 0.9287109375,
            "relevance_judgment_input_expanded": "# Title: Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives\n# Venue: IEEE Transactions on Artificial Intelligence\n# Authors: D. Hagos, Rick Battle, Danda B. Rawat\n## Abstract\nThe emergence of generative artificial intelligence (AI) and large language models (LLMs) has marked a new era of natural language processing (NLP), introducing unprecedented capabilities that are revolutionizing various domains. This article explores the current state of these cutting-edge technologies, demonstrating their remarkable advancements and wide-ranging applications. Our article contributes to providing a holistic perspective on the technical foundations, practical applications, and emerging challenges within the evolving landscape of generative AI and LLMs. We believe that understanding the generative capabilities of AI systems and the specific context of LLMs is crucial for researchers, practitioners, and policymakers to collaboratively shape the responsible and ethical integration of these technologies into various domains. Furthermore, we identify and address main research gaps, providing valuable insights to guide future research endeavors within the AI research community.\n## E. Model Merging\nModel merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging [45] or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
            "reference_string": "[271329267 | Hagos et al. | 2024 | Citations: 26]"
        },
        {
            "title": "Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 53,
            "citation_count": 22,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.06173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "2248152216",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2150649639",
                    "name": "Yong Luo"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2247556302",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2212029373",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Merging models fine-tuned from a common, extensively pre-trained large model but specialized for different tasks has been demonstrated as a cheap and scalable strategy to construct a multi-task model that performs well across diverse tasks. Recent research, exemplified by task arithmetic, highlights that this multi-task model can be derived through arithmetic operations on task vectors. Nevertheless, current merging techniques frequently resolve potential conflicts among parameters from task-specific models by evaluating individual attributes, such as the parameters' magnitude or sign, overlooking their collective impact on the overall functionality of the model. In this work, we propose the CONtinuous relaxation of disCRETE (Concrete) subspace learning method to identify a common low-dimensional subspace and utilize its shared information to track the interference problem without sacrificing much performance. Specifically, we model the problem as a bi-level optimization problem and introduce a meta-learning framework to find the Concrete subspace mask through gradient-based techniques. At the upper level, we focus on learning a shared Concrete mask to identify the subspace, while at the inner level, model merging is performed to maximize the performance of the merged model. We conduct extensive experiments on both vision domain and language domain, and the results demonstrate the effectiveness of our method. The code is available at https://github.com/tanganke/subspace_fusion",
            "corpus_id": 266163240,
            "sentences": [],
            "relevance_judgement": 0.92822265625,
            "relevance_judgment_input_expanded": "# Title: Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion\n# Venue: arXiv.org\n# Authors: A. Tang, Li Shen, Yong Luo, Liang Ding, Han Hu, Bo Du, D. Tao\n## Abstract\nMerging models fine-tuned from a common, extensively pre-trained large model but specialized for different tasks has been demonstrated as a cheap and scalable strategy to construct a multi-task model that performs well across diverse tasks. Recent research, exemplified by task arithmetic, highlights that this multi-task model can be derived through arithmetic operations on task vectors. Nevertheless, current merging techniques frequently resolve potential conflicts among parameters from task-specific models by evaluating individual attributes, such as the parameters' magnitude or sign, overlooking their collective impact on the overall functionality of the model. In this work, we propose the CONtinuous relaxation of disCRETE (Concrete) subspace learning method to identify a common low-dimensional subspace and utilize its shared information to track the interference problem without sacrificing much performance. Specifically, we model the problem as a bi-level optimization problem and introduce a meta-learning framework to find the Concrete subspace mask through gradient-based techniques. At the upper level, we focus on learning a shared Concrete mask to identify the subspace, while at the inner level, model merging is performed to maximize the performance of the merged model. We conduct extensive experiments on both vision domain and language domain, and the results demonstrate the effectiveness of our method. The code is available at https://github.com/tanganke/subspace_fusion\n",
            "reference_string": "[266163240 | Tang et al. | 2023 | Citations: 22]"
        },
        {
            "title": "Evolutionary Optimization of Model Merging Recipes",
            "venue": "Nat. Mac. Intell.",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 125,
            "influential_citation_count": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1038/s42256-024-00975-8",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.13187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2292261829",
                    "name": "Takuya Akiba"
                },
                {
                    "authorId": "2205654861",
                    "name": "Makoto Shing"
                },
                {
                    "authorId": "2244138850",
                    "name": "Yujin Tang"
                },
                {
                    "authorId": "2326487319",
                    "name": "Qi Sun"
                },
                {
                    "authorId": "2292260083",
                    "name": "David Ha"
                }
            ],
            "abstract": "\n Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. Although model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models such as a Japanese LLM with math reasoning capabilities. Surprisingly, our Japanese math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with substantially more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally aware Japanese vision\u2013language model generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese vision\u2013language models. This work not only contributes new state-of-the-art models back to the open-source community but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.",
            "corpus_id": 268537290,
            "sentences": [
                {
                    "corpus_id": "268537290",
                    "title": "Evolutionary Optimization of Model Merging Recipes",
                    "text": "Simple weight interpolation works well for merging image generation models that operate in a real valued latent and output space, but to our surprise, they also still work for merging language models under the right conditions, but have performance issues [49].Recently, new methods have been proposed to address merging language models specifically.One method, Task Arithmetic [21], involves building task vectors by subtracting pre-trained from fine-tuned model weights, enabling manipulation through arithmetic operations to steer the merged model's behavior.\n\nA key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation.A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance.The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences.In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49].\n\nImage generation model merging really took off in the community only after such methods were implemented in open-source toolkits [4].Similarly, language model merging really gained popularity after the implementation of mergekit [15,28].This toolkit provides all of the popular recipes for merging language models.In addition to simple linear and spherical interpolation, advanced recipes such as Task Arithmetic, TIES-Merging, DARE are also implemented, offering users the ability to experiment with combining these methods to work with fine-tunes of popular base models such as Mistral [22].With this toolkit, a large number of capable merged models are developed by the community, and most of the top models on the Open LLM Leaderboard [20] are gradually dominated by merged models produced by the community of language model enthusiasts.\n\nMergekit also introduced an additional method, called Frankenmerging which is not based on weight merging, but for the user to experiment with trying to stack different layers from multiple models to sequentially create a new model.",
                    "score": 0.5306972868444395,
                    "section_title": "Merging Language Models",
                    "char_start_offset": 6677,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 261
                        },
                        {
                            "start": 261,
                            "end": 350
                        },
                        {
                            "start": 350,
                            "end": 562
                        },
                        {
                            "start": 564,
                            "end": 696
                        },
                        {
                            "start": 696,
                            "end": 923
                        },
                        {
                            "start": 923,
                            "end": 1140
                        },
                        {
                            "start": 1142,
                            "end": 1329
                        },
                        {
                            "start": 1329,
                            "end": 1422
                        },
                        {
                            "start": 1424,
                            "end": 1557
                        },
                        {
                            "start": 1557,
                            "end": 1661
                        },
                        {
                            "start": 1661,
                            "end": 1738
                        },
                        {
                            "start": 1738,
                            "end": 2017
                        },
                        {
                            "start": 2017,
                            "end": 2265
                        },
                        {
                            "start": 2267,
                            "end": 2499
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 256,
                            "end": 260,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 710,
                            "end": 714,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1417,
                            "end": 1421,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: Evolutionary Optimization of Model Merging Recipes\n# Venue: Nat. Mac. Intell.\n# Authors: Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, David Ha\n## Abstract\n\n Large language models (LLMs) have become increasingly capable, but their development often requires substantial computational resources. Although model merging has emerged as a cost-effective promising approach for creating new models by combining existing ones, it currently relies on human intuition and domain knowledge, limiting its potential. Here we propose an evolutionary approach that overcomes this limitation by automatically discovering effective combinations of diverse open-source models, harnessing their collective intelligence without requiring extensive additional training data or compute. Our approach operates in both parameter space and data flow space, allowing optimization beyond just the weights of the individual models. This approach even facilitates cross-domain merging, generating models such as a Japanese LLM with math reasoning capabilities. Surprisingly, our Japanese math LLM achieved state-of-the-art performance on a variety of established Japanese LLM benchmarks, even surpassing models with substantially more parameters, despite not being explicitly trained for such tasks. Furthermore, a culturally aware Japanese vision\u2013language model generated through our approach demonstrates its effectiveness in describing Japanese culture-specific content, outperforming previous Japanese vision\u2013language models. This work not only contributes new state-of-the-art models back to the open-source community but also introduces a new paradigm for automated model composition, paving the way for exploring alternative, efficient approaches to foundation model development.\n## Merging Language Models\nSimple weight interpolation works well for merging image generation models that operate in a real valued latent and output space, but to our surprise, they also still work for merging language models under the right conditions, but have performance issues [49].Recently, new methods have been proposed to address merging language models specifically.One method, Task Arithmetic [21], involves building task vectors by subtracting pre-trained from fine-tuned model weights, enabling manipulation through arithmetic operations to steer the merged model's behavior.\n\nA key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation.A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance.The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences.In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49].\n\nImage generation model merging really took off in the community only after such methods were implemented in open-source toolkits [4].Similarly, language model merging really gained popularity after the implementation of mergekit [15,28].This toolkit provides all of the popular recipes for merging language models.In addition to simple linear and spherical interpolation, advanced recipes such as Task Arithmetic, TIES-Merging, DARE are also implemented, offering users the ability to experiment with combining these methods to work with fine-tunes of popular base models such as Mistral [22].With this toolkit, a large number of capable merged models are developed by the community, and most of the top models on the Open LLM Leaderboard [20] are gradually dominated by merged models produced by the community of language model enthusiasts.\n\nMergekit also introduced an additional method, called Frankenmerging which is not based on weight merging, but for the user to experiment with trying to stack different layers from multiple models to sequentially create a new model.",
            "reference_string": "[268537290 | Akiba et al. | 2024 | Citations: 125]"
        },
        {
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2024,
            "reference_count": 90,
            "citation_count": 25,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.13656, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2182670937",
                    "name": "Yifei He"
                },
                {
                    "authorId": "2317078449",
                    "name": "Yuzheng Hu"
                },
                {
                    "authorId": "2292270783",
                    "name": "Yong Lin"
                },
                {
                    "authorId": "2306841244",
                    "name": "Tong Zhang"
                },
                {
                    "authorId": "2283183420",
                    "name": "Han Zhao"
                }
            ],
            "abstract": "Model merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.",
            "corpus_id": 271957310,
            "sentences": [
                {
                    "corpus_id": "271957310",
                    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                    "text": "Model merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.",
                    "score": 0.39286560501247025,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                },
                {
                    "corpus_id": "271957310",
                    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                    "text": "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model. Empirical evaluation on various vision and language benchmarks validate the effectiveness of our approach. Beyond model merging, our approach performs effective model compression, which compresses the model size to be 1% of the original without sacrificing performance. Additionally, Localize-and-Stitch also excels at retaining the pretrained knowledge. Overall, our approach offers a novel pathway for flexible and continual skills composition from finetuned models with minimal storage and computational overhead. \n\nPublished in Transactions on Machine Learning Research (12/2024) (SST-2, CR, MR, MPQA). ii) Conceptually dissimilar subset: Including tasks from different categories (SST-2 for sentiment classification, TREC for question classification, SUBJ for subjectivity, and MNLI for entailment). \n\nIn Table 6, we report the average performance for each subset. In the similar subset, where tasks share similar skills, all merging methods perform equally well. However, in the dissimilar subset, where task skills differ and may even conflict, Localize-and-Stitch shows a significant advantage, demonstrating its ability to effectively resolve task interference. Where are the localized regions? We analyze the distribution of the localized regions for both language and vision tasks in Figure 10, both in terms of the layer index and the transformer components. For the layers, different tasks seem to occupy different layers, although the earlier layers in the network seldomly appear in the localized regions. Interestingly, most of the localized regions concentrate in the LayerNorm parameters. This pattern can possibly be attributed to a distribution shift observed in the finetuning data compared to the pretraining data, necessitating adjustments to the LayerNorm parameters to accommodate this shift. Full grafted performance. We evaluate the quality of the localized regions by the grafted performance.",
                    "score": 0.4094483211987704,
                    "section_title": "Conclusion",
                    "char_start_offset": 35729,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 508
                        },
                        {
                            "start": 509,
                            "end": 615
                        },
                        {
                            "start": 616,
                            "end": 778
                        },
                        {
                            "start": 779,
                            "end": 863
                        },
                        {
                            "start": 864,
                            "end": 1025
                        },
                        {
                            "start": 1028,
                            "end": 1115
                        },
                        {
                            "start": 1116,
                            "end": 1313
                        },
                        {
                            "start": 1316,
                            "end": 1378
                        },
                        {
                            "start": 1379,
                            "end": 1477
                        },
                        {
                            "start": 1478,
                            "end": 1679
                        },
                        {
                            "start": 1680,
                            "end": 1712
                        },
                        {
                            "start": 1713,
                            "end": 1879
                        },
                        {
                            "start": 1880,
                            "end": 2029
                        },
                        {
                            "start": 2030,
                            "end": 2115
                        },
                        {
                            "start": 2116,
                            "end": 2326
                        },
                        {
                            "start": 2327,
                            "end": 2352
                        },
                        {
                            "start": 2353,
                            "end": 2429
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9033203125
                },
                {
                    "corpus_id": "271957310",
                    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                    "text": "This has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation. We demonstrate this by a case study (Figure 3) on merging models finetuned on two conflicting language tasks: QNLI (Wang et al., 2018) and MNLI (Bowman et al., 2015). QNLI involves predicting whether a context contains the answer to the given question, and MNLI involves predicting text entailment given a sentence pair. These tasks are conflicting, manifested by a noticeable performance decline for both tasks when using simple averaging to merge the corresponding finetuned models. However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference.",
                    "score": 0.5865346144091742,
                    "section_title": "Motivation and objectives",
                    "char_start_offset": 9244,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 416
                        },
                        {
                            "start": 417,
                            "end": 570
                        },
                        {
                            "start": 571,
                            "end": 734
                        },
                        {
                            "start": 735,
                            "end": 952
                        },
                        {
                            "start": 953,
                            "end": 1140
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.869140625
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Yifei He, Yuzheng Hu, Yong Lin, Tong Zhang, Han Zhao\n## Abstract\nModel merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.\n## Motivation and objectives\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation. We demonstrate this by a case study (Figure 3) on merging models finetuned on two conflicting language tasks: QNLI (Wang et al., 2018) and MNLI (Bowman et al., 2015). QNLI involves predicting whether a context contains the answer to the given question, and MNLI involves predicting text entailment given a sentence pair. These tasks are conflicting, manifested by a noticeable performance decline for both tasks when using simple averaging to merge the corresponding finetuned models. However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference.\n\n## Conclusion\nIn this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model. Empirical evaluation on various vision and language benchmarks validate the effectiveness of our approach. Beyond model merging, our approach performs effective model compression, which compresses the model size to be 1% of the original without sacrificing performance. Additionally, Localize-and-Stitch also excels at retaining the pretrained knowledge. Overall, our approach offers a novel pathway for flexible and continual skills composition from finetuned models with minimal storage and computational overhead. \n\nPublished in Transactions on Machine Learning Research (12/2024) (SST-2, CR, MR, MPQA). ii) Conceptually dissimilar subset: Including tasks from different categories (SST-2 for sentiment classification, TREC for question classification, SUBJ for subjectivity, and MNLI for entailment). \n\nIn Table 6, we report the average performance for each subset. In the similar subset, where tasks share similar skills, all merging methods perform equally well. However, in the dissimilar subset, where task skills differ and may even conflict, Localize-and-Stitch shows a significant advantage, demonstrating its ability to effectively resolve task interference. Where are the localized regions? We analyze the distribution of the localized regions for both language and vision tasks in Figure 10, both in terms of the layer index and the transformer components. For the layers, different tasks seem to occupy different layers, although the earlier layers in the network seldomly appear in the localized regions. Interestingly, most of the localized regions concentrate in the LayerNorm parameters. This pattern can possibly be attributed to a distribution shift observed in the finetuning data compared to the pretraining data, necessitating adjustments to the LayerNorm parameters to accommodate this shift. Full grafted performance. We evaluate the quality of the localized regions by the grafted performance.",
            "reference_string": "[271957310 | He et al. | 2024 | Citations: 25]"
        },
        {
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 58,
            "citation_count": 5,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.15065, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35640834",
                    "name": "Wenju Sun"
                },
                {
                    "authorId": "2262408434",
                    "name": "Qingyong Li"
                },
                {
                    "authorId": "2119293226",
                    "name": "Wen Wang"
                },
                {
                    "authorId": "8010931",
                    "name": "Yangli-ao Geng"
                },
                {
                    "authorId": "2342563128",
                    "name": "Boyang Li"
                }
            ],
            "abstract": "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations. In this paper, we formally define this issue as knowledge conflicts, characterized by the performance degradation of one task after merging with a model fine-tuned for another task. Through in-depth analysis, we show that these conflicts stem primarily from the components of task vectors that align with the gradient of task-specific losses at $\\theta_{\\text{pre}}$. To address this, we propose Task Arithmetic in Trust Region (TATR), which defines the trust region as dimensions in the model parameter space that cause only small changes (corresponding to the task vector components with gradient orthogonal direction) in the task-specific losses. Restricting parameter merging within this trust region, TATR can effectively alleviate knowledge conflicts. Moreover, TATR serves as both an independent approach and a plug-and-play module compatible with a wide range of TA-based methods. Extensive empirical evaluations on eight distinct datasets robustly demonstrate that TATR improves the multi-task performance of several TA-based model merging methods by an observable margin.",
            "corpus_id": 275921352,
            "sentences": [
                {
                    "corpus_id": "275921352",
                    "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
                    "text": "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations. In this paper, we formally define this issue as knowledge conflicts, characterized by the performance degradation of one task after merging with a model fine-tuned for another task. Through in-depth analysis, we show that these conflicts stem primarily from the components of task vectors that align with the gradient of task-specific losses at $\\theta_{\\text{pre}}$. To address this, we propose Task Arithmetic in Trust Region (TATR), which defines the trust region as dimensions in the model parameter space that cause only small changes (corresponding to the task vector components with gradient orthogonal direction) in the task-specific losses. Restricting parameter merging within this trust region, TATR can effectively alleviate knowledge conflicts. Moreover, TATR serves as both an independent approach and a plug-and-play module compatible with a wide range of TA-based methods. Extensive empirical evaluations on eight distinct datasets robustly demonstrate that TATR improves the multi-task performance of several TA-based model merging methods by an observable margin.",
                    "score": 0.49759107952096454,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                },
                {
                    "corpus_id": "275921352",
                    "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
                    "text": "Model merging techniques, which aim to integrate knowledge across models, have attracted increasing attention in recent years. As a precursor, Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) averages model weights near the end of training. This concept was further advanced by approaches like SWAD (Cha et al., 2021) and Ensemble of Averages (EoA) (Arpit et al., 2022). Empirical evidence from Ilharco et al. (2023a) demonstrates that parameter averaging effectively integrates knowledge from models trained on diverse tasks. DLCPA (Sun et al., 2023) proposes to apply cumulative parameter averaging (CPA) to continually assimilate knowledge across distinct tasks. Fisher-Merging (Matena & Raffel, 2022) leverages the Fisher information matrix Fisher (1925) to measure the importance of model parameters and merge models using weighted averaging. Additionally, RegMean (Jin et al., 2023) formulates an optimal merging model by minimizing the distance to each model in the parameter space. \n\nRecently, Task Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors. Furthermore, several approaches combine test-time adaptation techniques with TA, yielding superior MTL performance. These test-time adaptation-based methods typically allocate merging weights and fine-tune them during testing using unsupervised test data.",
                    "score": 0.49189524074728636,
                    "section_title": "MULTI-TASK LEARNING THROUGH MODEL MERGING",
                    "char_start_offset": 4020,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 126
                        },
                        {
                            "start": 127,
                            "end": 249
                        },
                        {
                            "start": 250,
                            "end": 379
                        },
                        {
                            "start": 380,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 674
                        },
                        {
                            "start": 675,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 998
                        },
                        {
                            "start": 1001,
                            "end": 1208
                        },
                        {
                            "start": 1209,
                            "end": 1427
                        },
                        {
                            "start": 1428,
                            "end": 1579
                        },
                        {
                            "start": 1580,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1859
                        },
                        {
                            "start": 1860,
                            "end": 1975
                        },
                        {
                            "start": 1976,
                            "end": 2115
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 177,
                            "end": 200,
                            "matchedPaperCorpusId": "3833416"
                        },
                        {
                            "start": 308,
                            "end": 326,
                            "matchedPaperCorpusId": "235367622"
                        },
                        {
                            "start": 358,
                            "end": 378,
                            "matchedPaperCorpusId": "239049452"
                        },
                        {
                            "start": 404,
                            "end": 426,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 879,
                            "end": 897,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 1032,
                            "end": 1055,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 1465,
                            "end": 1485,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1585,
                            "end": 1606,
                            "matchedPaperCorpusId": "259262373"
                        },
                        {
                            "start": 1616,
                            "end": 1633,
                            "matchedPaperCorpusId": "269293160"
                        },
                        {
                            "start": 1680,
                            "end": 1697,
                            "matchedPaperCorpusId": "235458009"
                        },
                        {
                            "start": 1720,
                            "end": 1747,
                            "matchedPaperCorpusId": "258832777"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88427734375
                },
                {
                    "corpus_id": "275921352",
                    "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
                    "text": "The entire merging process does not rely on any additional training process. \n\nMoreover, the techniques introduced in TATR selectively limit the merging process to a subset of model parameters, allowing it to function as a plug-and-play module that seamlessly integrates with a wide range of TA-based approaches, such as: \n\n\u2022 Ties-Merging & TATR: Ties-Merging (Yadav et al., 2023) partially reduces knowledge conflicts by pruning low-magnitude parameters and aligning the signs of task vectors. However, this approach overlooks conflicts that may arise from high-magnitude parameters. This bias can lead to knowledge conflicts, where some tasks dominate the model's behavior. The combination of TATR with Ties-Merging refines the process, as shown in the following formula: \n\nwhere \u03a6(.) indicates the TrIm, Elect Sign, and Merge operation of Ties-Merging. \n\n\u2022 AdaMerging & TATR: AdaMerging (Yang et al., 2024b) adaptively learns merging coefficients but does not inherently resolve knowledge conflicts between task vectors. This can lead to interference during coefficient learning, especially when tasks require opposing parameter adaptations. TATR addresses this by pre-filtering task vectors to retain only Published as a conference paper at ICLR 2025 those components within the trust region, ensuring that AdaMerging operates in a conflictreduced parameter space: \n\nwhere \u03bb 1 , . . . , \u03bb K represent the learnable coefficients for AdaMerging. \u2022 Surgery & TATR: Similarly, Surgery (Yang et al., 2024a) introduces additional modules to align task-specific features during merging. TATR complements Surgery by pre-selecting components of task vectors that reside in the trust region. The integrated approach is formalized as: \n\nwhere \u03b8 surgery denotes the additional parameters introduce by the Surgery module.",
                    "score": 0.39161031555185843,
                    "section_title": "TASK ARITHMETIC IN THE TRUST REGION",
                    "char_start_offset": 17481,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 76
                        },
                        {
                            "start": 79,
                            "end": 321
                        },
                        {
                            "start": 324,
                            "end": 494
                        },
                        {
                            "start": 495,
                            "end": 584
                        },
                        {
                            "start": 585,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 855
                        },
                        {
                            "start": 858,
                            "end": 1023
                        },
                        {
                            "start": 1024,
                            "end": 1144
                        },
                        {
                            "start": 1145,
                            "end": 1368
                        },
                        {
                            "start": 1371,
                            "end": 1388
                        },
                        {
                            "start": 1389,
                            "end": 1447
                        },
                        {
                            "start": 1448,
                            "end": 1583
                        },
                        {
                            "start": 1584,
                            "end": 1685
                        },
                        {
                            "start": 1686,
                            "end": 1727
                        },
                        {
                            "start": 1730,
                            "end": 1812
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 360,
                            "end": 380,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 890,
                            "end": 910,
                            "matchedPaperCorpusId": "263620126"
                        },
                        {
                            "start": 1485,
                            "end": 1505,
                            "matchedPaperCorpusId": "267412030"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8671875
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts\n# Venue: arXiv.org\n# Authors: Wenju Sun, Qingyong Li, Wen Wang, Yangli-ao Geng, Boyang Li\n## Abstract\nMulti-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations. In this paper, we formally define this issue as knowledge conflicts, characterized by the performance degradation of one task after merging with a model fine-tuned for another task. Through in-depth analysis, we show that these conflicts stem primarily from the components of task vectors that align with the gradient of task-specific losses at $\\theta_{\\text{pre}}$. To address this, we propose Task Arithmetic in Trust Region (TATR), which defines the trust region as dimensions in the model parameter space that cause only small changes (corresponding to the task vector components with gradient orthogonal direction) in the task-specific losses. Restricting parameter merging within this trust region, TATR can effectively alleviate knowledge conflicts. Moreover, TATR serves as both an independent approach and a plug-and-play module compatible with a wide range of TA-based methods. Extensive empirical evaluations on eight distinct datasets robustly demonstrate that TATR improves the multi-task performance of several TA-based model merging methods by an observable margin.\n## MULTI-TASK LEARNING THROUGH MODEL MERGING\nModel merging techniques, which aim to integrate knowledge across models, have attracted increasing attention in recent years. As a precursor, Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) averages model weights near the end of training. This concept was further advanced by approaches like SWAD (Cha et al., 2021) and Ensemble of Averages (EoA) (Arpit et al., 2022). Empirical evidence from Ilharco et al. (2023a) demonstrates that parameter averaging effectively integrates knowledge from models trained on diverse tasks. DLCPA (Sun et al., 2023) proposes to apply cumulative parameter averaging (CPA) to continually assimilate knowledge across distinct tasks. Fisher-Merging (Matena & Raffel, 2022) leverages the Fisher information matrix Fisher (1925) to measure the importance of model parameters and merge models using weighted averaging. Additionally, RegMean (Jin et al., 2023) formulates an optimal merging model by minimizing the distance to each model in the parameter space. \n\nRecently, Task Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors. Furthermore, several approaches combine test-time adaptation techniques with TA, yielding superior MTL performance. These test-time adaptation-based methods typically allocate merging weights and fine-tune them during testing using unsupervised test data.\n\n## TASK ARITHMETIC IN THE TRUST REGION\nThe entire merging process does not rely on any additional training process. \n\nMoreover, the techniques introduced in TATR selectively limit the merging process to a subset of model parameters, allowing it to function as a plug-and-play module that seamlessly integrates with a wide range of TA-based approaches, such as: \n\n\u2022 Ties-Merging & TATR: Ties-Merging (Yadav et al., 2023) partially reduces knowledge conflicts by pruning low-magnitude parameters and aligning the signs of task vectors. However, this approach overlooks conflicts that may arise from high-magnitude parameters. This bias can lead to knowledge conflicts, where some tasks dominate the model's behavior. The combination of TATR with Ties-Merging refines the process, as shown in the following formula: \n\nwhere \u03a6(.) indicates the TrIm, Elect Sign, and Merge operation of Ties-Merging. \n\n\u2022 AdaMerging & TATR: AdaMerging (Yang et al., 2024b) adaptively learns merging coefficients but does not inherently resolve knowledge conflicts between task vectors. This can lead to interference during coefficient learning, especially when tasks require opposing parameter adaptations. TATR addresses this by pre-filtering task vectors to retain only Published as a conference paper at ICLR 2025 those components within the trust region, ensuring that AdaMerging operates in a conflictreduced parameter space: \n\nwhere \u03bb 1 , . . . , \u03bb K represent the learnable coefficients for AdaMerging. \u2022 Surgery & TATR: Similarly, Surgery (Yang et al., 2024a) introduces additional modules to align task-specific features during merging. TATR complements Surgery by pre-selecting components of task vectors that reside in the trust region. The integrated approach is formalized as: \n\nwhere \u03b8 surgery denotes the additional parameters introduce by the Surgery module.",
            "reference_string": "[275921352 | Sun et al. | 2025 | Citations: 5]"
        },
        {
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.08998, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2321405806",
                    "name": "Wei Ruan"
                },
                {
                    "authorId": "2263682353",
                    "name": "Tianze Yang"
                },
                {
                    "authorId": "2325891087",
                    "name": "Yifan Zhou"
                },
                {
                    "authorId": "2349736445",
                    "name": "Tianming Liu"
                },
                {
                    "authorId": "2331910055",
                    "name": "Jin Lu"
                }
            ],
            "abstract": "Model merging has achieved significant success, with numerous innovative methods proposed to enhance capabilities by combining multiple models. However, challenges persist due to the lack of a unified framework for classification and systematic comparative analysis, leading to inconsistencies in terminologies and categorizations. Meanwhile, as an increasing number of fine-tuned models are publicly available, their original training data often remain inaccessible due to privacy concerns or intellectual property restrictions. This makes traditional multi-task learning based on shared training data impractical. In scenarios where direct access to training data is infeasible, merging model parameters to create a unified model with broad generalization across multiple domains becomes crucial, further underscoring the importance of model merging techniques. Despite the rapid progress in this field, a comprehensive taxonomy and survey summarizing recent advances and predicting future directions are still lacking. This paper addresses these gaps by establishing a new taxonomy of model merging methods, systematically comparing different approaches, and providing an overview of key developments. By offering a structured perspective on this evolving area, we aim to help newcomers quickly grasp the field's landscape and inspire further innovations.",
            "corpus_id": 276937513,
            "sentences": [
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "Relying solely on weight magnitude for pruning is insufficient, as it does not fully capture a model's functional contributions. A common alternative is pruning based on model activations or output responses, which helps reduce conflicts and improve merging effectiveness. By selectively retaining important neurons and representations, this approach enhances model performance. Representative methods include: \n\nZipIt!: This method merges models trained on disjoint tasks by identifying shared feature activations and selectively \"zipping\" them together. It introduces a partial merging strategy, allowing models to retain distinct task-specific heads while merging up to a certain layer [Stoica et al., 2023]. \n\nSurgeryV2: Addresses representation bias in model merging by performing layer-wise representation surgery. Instead of directly averaging weights, it aligns activations between expert models and the merged model, progressively refining feature representations to reduce task interference [Yang et al., 2024b]. \n\nLocalize-and-Stitch: A localized merging approach that first identifies sparse task-specific activation regions and stitches only these into the final model. By selecting highly activated neurons, it minimizes task conflicts and retains essential task-specific knowledge [He et al., 2024]. \n\nThese methods improve model merging efficiency by pruning low-importance activations, ensuring better generalization across tasks while reducing parameter redundancy. \n\nAdaMerging: It assigns different sparsity rates to different layers, pruning less important layers more aggressively while preserving critical layers [Yang et al., 2023]. This ensures that merging does not indiscriminately combine task-specific features, reducing task interference. Instead of relying solely on weight magnitudes, AdaMerging minimizes entropy to ensure more deterministic outputs during merging, making it an activation-aware approach.",
                    "score": 0.4281094919367957,
                    "section_title": "Activation-based Pruning Merging type",
                    "char_start_offset": 22679,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 128
                        },
                        {
                            "start": 129,
                            "end": 272
                        },
                        {
                            "start": 273,
                            "end": 378
                        },
                        {
                            "start": 379,
                            "end": 410
                        },
                        {
                            "start": 413,
                            "end": 555
                        },
                        {
                            "start": 556,
                            "end": 711
                        },
                        {
                            "start": 714,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 1022
                        },
                        {
                            "start": 1025,
                            "end": 1182
                        },
                        {
                            "start": 1183,
                            "end": 1314
                        },
                        {
                            "start": 1317,
                            "end": 1483
                        },
                        {
                            "start": 1486,
                            "end": 1656
                        },
                        {
                            "start": 1657,
                            "end": 1768
                        },
                        {
                            "start": 1769,
                            "end": 1938
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 689,
                            "end": 710,
                            "matchedPaperCorpusId": "258480011"
                        },
                        {
                            "start": 1001,
                            "end": 1021,
                            "matchedPaperCorpusId": "267499590"
                        },
                        {
                            "start": 1296,
                            "end": 1313,
                            "matchedPaperCorpusId": "231740691"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91650390625
                },
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "This type of model merging not only considers weight magnitudes to address conflicts and interference but also decomposes the weights to analyze both their magnitude and direction. This approach further mitigates conflicts between models. Below, we introduce representative methods. \n\nPCB-Masks: This method introduces task-specific binary masks to improve model merging and compression. Binary masks are generated for each task using task vectors from fine-tuned models, highlighting parameters important for that task while ignoring irrelevant ones. During merging, the method eliminates \"selfish weights\" (important to one task only) and \"catastrophic weights\" (irrelevant to all tasks), preserving only shared parameters that benefit multiple tasks [Wang et al., 2024]. \n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024]. \n\nWIDEN(Weight Disentanglement ): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]. Additionally, it employs a Softmax-based score calibration to adaptively balance the contributions of different models, ensuring that the merged model retains and optimally integrates their abili-ties (Table1). \n\nFREE-Merging: It is a novel model merging approach that leverages Fourier transform-based filtering and lightweight expert modules [Zheng and Wang, 2024]. It mitigates task conflicts by applying high-pass filtering in the frequency domain, removing low-frequency signals that reduce generalization while preserving essential model structures.",
                    "score": 0.45832550820554707,
                    "section_title": "Decomposing weight type",
                    "char_start_offset": 19865,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 238
                        },
                        {
                            "start": 239,
                            "end": 282
                        },
                        {
                            "start": 285,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 773
                        },
                        {
                            "start": 776,
                            "end": 972
                        },
                        {
                            "start": 973,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1276
                        },
                        {
                            "start": 1279,
                            "end": 1436
                        },
                        {
                            "start": 1437,
                            "end": 1540
                        },
                        {
                            "start": 1541,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1992
                        },
                        {
                            "start": 1995,
                            "end": 2149
                        },
                        {
                            "start": 2150,
                            "end": 2337
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 753,
                            "end": 772,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1255,
                            "end": 1275,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1762,
                            "end": 1780,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 2126,
                            "end": 2148,
                            "matchedPaperCorpusId": "247362886"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                },
                {
                    "corpus_id": "276937513",
                    "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                    "text": "As the name suggests, this method assumes that parameters with larger magnitudes have a greater impact on model performance and should be preserved. The approach involves sorting parameters by their absolute values, retaining those with larger magnitudes while dropping those with smaller absolute values. This helps reduce parameter interference during model merging, ensuring that essential information is maintained. Model merging methods of this type include the following. \n\nTIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024]. Model Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024]. In contrast to DARE, which randomly drops delta parameters with a fixed probability, Model Breadcrumbs applies a deterministic, layerwise masking process that accounts for task vector distributions, allowing for finer control over parameter adjustments. Compared to DELLA-Merging's stochastic pruning strategy, MAGPRUNE, which relies on magnitude-based probability sampling, Model Breadcrumbs adopts a more direct filtering mechanism that eliminates extreme values and maintains task relevance through robust aggregation. Furthermore, Model Breadcrumbs emphasizes layer-specific processing over global operations, enhancing its adaptability to diverse tasks. These distinctions make Model Breadcrumbs particularly effective in reducing noise accumulation and achieving consistent performance across multi-task scenarios, while requiring less hyperparameter tuning than prior methods.",
                    "score": 0.44766946630237203,
                    "section_title": "Weight sorting type",
                    "char_start_offset": 17365,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 419
                        },
                        {
                            "start": 420,
                            "end": 477
                        },
                        {
                            "start": 480,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 768
                        },
                        {
                            "start": 769,
                            "end": 896
                        },
                        {
                            "start": 897,
                            "end": 993
                        },
                        {
                            "start": 994,
                            "end": 1078
                        },
                        {
                            "start": 1079,
                            "end": 1213
                        },
                        {
                            "start": 1214,
                            "end": 1488
                        },
                        {
                            "start": 1489,
                            "end": 1588
                        },
                        {
                            "start": 1589,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 2110
                        },
                        {
                            "start": 2111,
                            "end": 2247
                        },
                        {
                            "start": 2248,
                            "end": 2472
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1558,
                            "end": 1587,
                            "matchedPaperCorpusId": "273903113"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.91650390625,
            "relevance_judgment_input_expanded": "# Title: From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches\n# Venue: arXiv.org\n# Authors: Wei Ruan, Tianze Yang, Yifan Zhou, Tianming Liu, Jin Lu\n## Abstract\nModel merging has achieved significant success, with numerous innovative methods proposed to enhance capabilities by combining multiple models. However, challenges persist due to the lack of a unified framework for classification and systematic comparative analysis, leading to inconsistencies in terminologies and categorizations. Meanwhile, as an increasing number of fine-tuned models are publicly available, their original training data often remain inaccessible due to privacy concerns or intellectual property restrictions. This makes traditional multi-task learning based on shared training data impractical. In scenarios where direct access to training data is infeasible, merging model parameters to create a unified model with broad generalization across multiple domains becomes crucial, further underscoring the importance of model merging techniques. Despite the rapid progress in this field, a comprehensive taxonomy and survey summarizing recent advances and predicting future directions are still lacking. This paper addresses these gaps by establishing a new taxonomy of model merging methods, systematically comparing different approaches, and providing an overview of key developments. By offering a structured perspective on this evolving area, we aim to help newcomers quickly grasp the field's landscape and inspire further innovations.\n## Weight sorting type\nAs the name suggests, this method assumes that parameters with larger magnitudes have a greater impact on model performance and should be preserved. The approach involves sorting parameters by their absolute values, retaining those with larger magnitudes while dropping those with smaller absolute values. This helps reduce parameter interference during model merging, ensuring that essential information is maintained. Model merging methods of this type include the following. \n\nTIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024]. Model Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024]. In contrast to DARE, which randomly drops delta parameters with a fixed probability, Model Breadcrumbs applies a deterministic, layerwise masking process that accounts for task vector distributions, allowing for finer control over parameter adjustments. Compared to DELLA-Merging's stochastic pruning strategy, MAGPRUNE, which relies on magnitude-based probability sampling, Model Breadcrumbs adopts a more direct filtering mechanism that eliminates extreme values and maintains task relevance through robust aggregation. Furthermore, Model Breadcrumbs emphasizes layer-specific processing over global operations, enhancing its adaptability to diverse tasks. These distinctions make Model Breadcrumbs particularly effective in reducing noise accumulation and achieving consistent performance across multi-task scenarios, while requiring less hyperparameter tuning than prior methods.\n\n## Decomposing weight type\nThis type of model merging not only considers weight magnitudes to address conflicts and interference but also decomposes the weights to analyze both their magnitude and direction. This approach further mitigates conflicts between models. Below, we introduce representative methods. \n\nPCB-Masks: This method introduces task-specific binary masks to improve model merging and compression. Binary masks are generated for each task using task vectors from fine-tuned models, highlighting parameters important for that task while ignoring irrelevant ones. During merging, the method eliminates \"selfish weights\" (important to one task only) and \"catastrophic weights\" (irrelevant to all tasks), preserving only shared parameters that benefit multiple tasks [Wang et al., 2024]. \n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024]. \n\nWIDEN(Weight Disentanglement ): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]. Additionally, it employs a Softmax-based score calibration to adaptively balance the contributions of different models, ensuring that the merged model retains and optimally integrates their abili-ties (Table1). \n\nFREE-Merging: It is a novel model merging approach that leverages Fourier transform-based filtering and lightweight expert modules [Zheng and Wang, 2024]. It mitigates task conflicts by applying high-pass filtering in the frequency domain, removing low-frequency signals that reduce generalization while preserving essential model structures.\n\n## Activation-based Pruning Merging type\nRelying solely on weight magnitude for pruning is insufficient, as it does not fully capture a model's functional contributions. A common alternative is pruning based on model activations or output responses, which helps reduce conflicts and improve merging effectiveness. By selectively retaining important neurons and representations, this approach enhances model performance. Representative methods include: \n\nZipIt!: This method merges models trained on disjoint tasks by identifying shared feature activations and selectively \"zipping\" them together. It introduces a partial merging strategy, allowing models to retain distinct task-specific heads while merging up to a certain layer [Stoica et al., 2023]. \n\nSurgeryV2: Addresses representation bias in model merging by performing layer-wise representation surgery. Instead of directly averaging weights, it aligns activations between expert models and the merged model, progressively refining feature representations to reduce task interference [Yang et al., 2024b]. \n\nLocalize-and-Stitch: A localized merging approach that first identifies sparse task-specific activation regions and stitches only these into the final model. By selecting highly activated neurons, it minimizes task conflicts and retains essential task-specific knowledge [He et al., 2024]. \n\nThese methods improve model merging efficiency by pruning low-importance activations, ensuring better generalization across tasks while reducing parameter redundancy. \n\nAdaMerging: It assigns different sparsity rates to different layers, pruning less important layers more aggressively while preserving critical layers [Yang et al., 2023]. This ensures that merging does not indiscriminately combine task-specific features, reducing task interference. Instead of relying solely on weight magnitudes, AdaMerging minimizes entropy to ensure more deterministic outputs during merging, making it an activation-aware approach.",
            "reference_string": "[276937513 | Ruan et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 7,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "151497321",
                    "name": "Enneng Yang"
                },
                {
                    "authorId": "2327007623",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2279402395",
                    "name": "Yong Luo"
                },
                {
                    "authorId": "2247556302",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2338825263",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2135519749",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "Deep model merging represents an emerging research direction that combines multiple fine-tuned models to harness their specialized capabilities across different tasks and domains. Current model merging techniques focus on merging all available models simultaneously, with weight interpolation-based methods being the predominant approaches. However, these conventional approaches are not well-suited for scenarios where models become available sequentially, and they often suffer from high memory requirements and potential interference between tasks. In this study, we propose a training-free projection-based continual merging method that processes models sequentially through orthogonal projections of weight matrices and adaptive scaling mechanisms. Our method operates by projecting new parameter updates onto subspaces orthogonal to existing merged parameter updates while using an adaptive scaling mechanism to maintain stable parameter distances, enabling efficient sequential integration of task-specific knowledge. Our approach maintains constant memory complexity to the number of models, minimizes interference between tasks through orthogonal projections, and retains the performance of previously merged models through adaptive task vector scaling. Extensive experiments on CLIP-ViT models demonstrate that our method achieves a 5-8% average accuracy improvement while maintaining robust performance in different task orderings.",
            "corpus_id": 275570290,
            "sentences": [],
            "relevance_judgement": 0.91650390625,
            "relevance_judgment_input_expanded": "# Title: Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging\n# Venue: arXiv.org\n# Authors: A. Tang, Enneng Yang, Li Shen, Yong Luo, Han Hu, Bo Du, Dacheng Tao\n## Abstract\nDeep model merging represents an emerging research direction that combines multiple fine-tuned models to harness their specialized capabilities across different tasks and domains. Current model merging techniques focus on merging all available models simultaneously, with weight interpolation-based methods being the predominant approaches. However, these conventional approaches are not well-suited for scenarios where models become available sequentially, and they often suffer from high memory requirements and potential interference between tasks. In this study, we propose a training-free projection-based continual merging method that processes models sequentially through orthogonal projections of weight matrices and adaptive scaling mechanisms. Our method operates by projecting new parameter updates onto subspaces orthogonal to existing merged parameter updates while using an adaptive scaling mechanism to maintain stable parameter distances, enabling efficient sequential integration of task-specific knowledge. Our approach maintains constant memory complexity to the number of models, minimizes interference between tasks through orthogonal projections, and retains the performance of previously merged models through adaptive task vector scaling. Extensive experiments on CLIP-ViT models demonstrate that our method achieves a 5-8% average accuracy improvement while maintaining robust performance in different task orderings.\n",
            "reference_string": "[275570290 | Tang et al. | 2025 | Citations: 7]"
        },
        {
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "venue": "",
            "year": 2025,
            "reference_count": 64,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.06977, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35640834",
                    "name": "Wenju Sun"
                },
                {
                    "authorId": "2262408434",
                    "name": "Qingyong Li"
                },
                {
                    "authorId": "8010931",
                    "name": "Yangli-ao Geng"
                },
                {
                    "authorId": "2342563128",
                    "name": "Boyang Li"
                }
            ],
            "abstract": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
            "corpus_id": 278501405,
            "sentences": [
                {
                    "corpus_id": "278501405",
                    "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                    "text": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
                    "score": 0.5555760159129728,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91552734375
                },
                {
                    "corpus_id": "278501405",
                    "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                    "text": "Despite the significant progress made by magnitude-based methods, they often overlook the risk of merging highmagnitude components that may overwrite task-specific knowledge during integration. As illustrated in Figure 1, methods like Ties-Merging prioritize dimensions with larger magnitudes (e.g., the vertical axis in Task 1's vector), which can unintentionally overwrite critical information from Task 2. This imbalance skews the merged model towards Task 1, substantially degrading performance on Task 2 and undermining the overall multi-task capability. In contrast, the lowmagnitude horizontal axis associated with Task 2 carries valuable information while exerting minimal negative impact on Task 1's performance. Trimming these components is therefore counterproductive. This example highlights the dual objectives of effective model merging: (1) suppressing conflicts where dominant knowledge from one task undermines the performance of others, and (2) preserving the unique and essential knowledge required by each task. \n\nMotivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars. We evaluate CAT Merging on diverse visual, language, and visual-language datasets, demonstrating its superiority over state-of-the-art methods while maintaining robustness with limited exemplars.",
                    "score": 0.5308672955454024,
                    "section_title": "Introduction",
                    "char_start_offset": 2119,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 559
                        },
                        {
                            "start": 560,
                            "end": 721
                        },
                        {
                            "start": 722,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 1031
                        },
                        {
                            "start": 1034,
                            "end": 1255
                        },
                        {
                            "start": 1256,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1564
                        },
                        {
                            "start": 1565,
                            "end": 1749
                        },
                        {
                            "start": 1750,
                            "end": 1945
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87255859375
                }
            ],
            "relevance_judgement": 0.91552734375,
            "relevance_judgment_input_expanded": "# Title: CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging\n# Venue: \n# Authors: Wenju Sun, Qingyong Li, Yangli-ao Geng, Boyang Li\n## Abstract\nMulti-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.\n## Introduction\nDespite the significant progress made by magnitude-based methods, they often overlook the risk of merging highmagnitude components that may overwrite task-specific knowledge during integration. As illustrated in Figure 1, methods like Ties-Merging prioritize dimensions with larger magnitudes (e.g., the vertical axis in Task 1's vector), which can unintentionally overwrite critical information from Task 2. This imbalance skews the merged model towards Task 1, substantially degrading performance on Task 2 and undermining the overall multi-task capability. In contrast, the lowmagnitude horizontal axis associated with Task 2 carries valuable information while exerting minimal negative impact on Task 1's performance. Trimming these components is therefore counterproductive. This example highlights the dual objectives of effective model merging: (1) suppressing conflicts where dominant knowledge from one task undermines the performance of others, and (2) preserving the unique and essential knowledge required by each task. \n\nMotivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars. We evaluate CAT Merging on diverse visual, language, and visual-language datasets, demonstrating its superiority over state-of-the-art methods while maintaining robustness with limited exemplars.",
            "reference_string": "[278501405 | Sun et al. | 2025 | Citations: 2]"
        },
        {
            "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 35,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10749, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2333317068",
                    "name": "Zehua Liu"
                },
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2345985527",
                    "name": "Yuxuan Yao"
                },
                {
                    "authorId": "2320312026",
                    "name": "Ruifeng She"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2332348570",
                    "name": "Tao Zhong"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                }
            ],
            "abstract": "While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.",
            "corpus_id": 276408756,
            "sentences": [
                {
                    "corpus_id": "276408756",
                    "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
                    "text": "Large Language Models (LLMs) have become ubiquitous in numerous real-world applications (Bommasani et al., 2021;Zhuang et al., 2020). The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging (Garipov et al., 2018;Wortsman et al., 2022;Yu et al., 2024b), which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. \n\nBuilding on recent studies (Ilharco et al., 2022;Yadav et al., 2024;Yu et al., 2024b), task vectorbased merging approaches have demonstrated significant effectiveness, where task vectors are de-fined as the parameter differences between finetuned models and the base LLM. Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, Yu et al. (2024b) applied random dropping with probability p to obtain a sparse representation of task vectors, while Yadav et al. (2024) retained only the top-k elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference. \n\nNevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations (Yu et al., 2024b). As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness (Yu et al., 2024a). Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques.",
                    "score": 0.5371706571405862,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 682
                        },
                        {
                            "start": 685,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1092
                        },
                        {
                            "start": 1093,
                            "end": 1189
                        },
                        {
                            "start": 1190,
                            "end": 1454
                        },
                        {
                            "start": 1455,
                            "end": 1570
                        },
                        {
                            "start": 1573,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1853
                        },
                        {
                            "start": 1854,
                            "end": 2069
                        },
                        {
                            "start": 2070,
                            "end": 2235
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 112,
                            "end": 132,
                            "matchedPaperCorpusId": "207847753"
                        },
                        {
                            "start": 358,
                            "end": 380,
                            "matchedPaperCorpusId": "4055784"
                        },
                        {
                            "start": 380,
                            "end": 402,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 734,
                            "end": 753,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1322,
                            "end": 1341,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 2050,
                            "end": 2068,
                            "matchedPaperCorpusId": "265034087"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging\n# Venue: arXiv.org\n# Authors: Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan\n## Abstract\nWhile most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \\textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.\n## Introduction\nLarge Language Models (LLMs) have become ubiquitous in numerous real-world applications (Bommasani et al., 2021;Zhuang et al., 2020). The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging (Garipov et al., 2018;Wortsman et al., 2022;Yu et al., 2024b), which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. \n\nBuilding on recent studies (Ilharco et al., 2022;Yadav et al., 2024;Yu et al., 2024b), task vectorbased merging approaches have demonstrated significant effectiveness, where task vectors are de-fined as the parameter differences between finetuned models and the base LLM. Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, Yu et al. (2024b) applied random dropping with probability p to obtain a sparse representation of task vectors, while Yadav et al. (2024) retained only the top-k elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference. \n\nNevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations (Yu et al., 2024b). As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness (Yu et al., 2024a). Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques.",
            "reference_string": "[276408756 | Liu et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Merging by Matching Models in Task Parameter Subspaces",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023,
            "reference_count": 99,
            "citation_count": 12,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.04339, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1390031652",
                    "name": "Derek Tam"
                },
                {
                    "authorId": "2253762115",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2402716",
                    "name": "Colin Raffel"
                }
            ],
            "abstract": "Model merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task parameter subspace'' in which models are matched before being merged. We connect the task parameter subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task parameter subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Parameter Subspace'' (MaTS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work at https://github.com/r-three/mats.",
            "corpus_id": 266053657,
            "sentences": [
                {
                    "corpus_id": "266053657",
                    "title": "Merging by Matching Models in Task Parameter Subspaces",
                    "text": "We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task. To match models in their task parameter subspace, merging methods upweight each model in its task parameter subspace, which aims to ensure that the task-relevant components of a given model will not be washed out after the models are combined. In particular, we show how Fisher merging (Matena & Raffel, 2022), RegMean (Jin et al., 2022), and simple parameter averaging (McMahan et al., 2017;Stich, 2018) all perform merging in this way and differ only in their choice of task parameter subspace. Concurrently, other works have focused on inaccuracies in model merging stemming from gradient mismatches in different models, and use these insights to connect diagonal Fisher merging and Task Arithmetic (Daheim et al., 2023). \n\nMatching models in their task parameter subspace requires solving a linear system of equations. This linear system implicitly defines a merging objective that relates to a given merging method's choice of task parameter subspace. While previous merging methods used merging objectives with a tractable closed-form solution, we instead develop a merging framework that uses the conjugate gradient method (Hestenes & Stiefel, 1952) to solve a given linear system. We refer to our merging framework as MaTS (Matching Models in their Task Subspace). By using the conjugate gradient method, MaTS flexibly supports different merging objectives and initializations (which can impact convergence speed). MaTS also enables the use of merging objectives for linear systems that don't have a tractable closed-form solution. To explore this possibility, we leverage insights from K-FAC (Grosse & Martens, 2016;Martens & Grosse, 2015) and introduce a merging method where a model's task parameter subspace is based on a block-diagonal approximation of the Fisher information matrix. \n\nTo explore the effectiveness of MaTS, we comprehensively compare it to existing merging methods on multitask and intermediate-task merging of language models and vision models trained via parameter-efficient or fullmodel fine-tuning.",
                    "score": 0.4396506628741437,
                    "section_title": "Introduction",
                    "char_start_offset": 1882,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 422
                        },
                        {
                            "start": 423,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 903
                        },
                        {
                            "start": 906,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1135
                        },
                        {
                            "start": 1136,
                            "end": 1367
                        },
                        {
                            "start": 1368,
                            "end": 1451
                        },
                        {
                            "start": 1452,
                            "end": 1601
                        },
                        {
                            "start": 1602,
                            "end": 1718
                        },
                        {
                            "start": 1719,
                            "end": 1975
                        },
                        {
                            "start": 1978,
                            "end": 2211
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 465,
                            "end": 488,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 549,
                            "end": 571,
                            "matchedPaperCorpusId": "14955348"
                        },
                        {
                            "start": 1309,
                            "end": 1335,
                            "matchedPaperCorpusId": "2207234"
                        },
                        {
                            "start": 1780,
                            "end": 1804,
                            "matchedPaperCorpusId": "11724661"
                        },
                        {
                            "start": 1804,
                            "end": 1827,
                            "matchedPaperCorpusId": "11480464"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90234375
                }
            ],
            "relevance_judgement": 0.90234375,
            "relevance_judgment_input_expanded": "# Title: Merging by Matching Models in Task Parameter Subspaces\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Derek Tam, Mohit Bansal, Colin Raffel\n## Abstract\nModel merging aims to cheaply combine individual task-specific models into a single multitask model. In this work, we view past merging methods as leveraging different notions of a ''task parameter subspace'' in which models are matched before being merged. We connect the task parameter subspace of a given model to its loss landscape and formalize how this approach to model merging can be seen as solving a linear system of equations. While past work has generally been limited to linear systems that have a closed-form solution, we consider using the conjugate gradient method to find a solution. We show that using the conjugate gradient method can outperform closed-form solutions, enables merging via linear systems that are otherwise intractable to solve, and flexibly allows choosing from a wide variety of initializations and estimates for the ''task parameter subspace''. We ultimately demonstrate that our merging framework called ''Matching Models in their Task Parameter Subspace'' (MaTS) achieves state-of-the-art results in multitask and intermediate-task model merging. We release all of the code and checkpoints used in our work at https://github.com/r-three/mats.\n## Introduction\nWe define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task. To match models in their task parameter subspace, merging methods upweight each model in its task parameter subspace, which aims to ensure that the task-relevant components of a given model will not be washed out after the models are combined. In particular, we show how Fisher merging (Matena & Raffel, 2022), RegMean (Jin et al., 2022), and simple parameter averaging (McMahan et al., 2017;Stich, 2018) all perform merging in this way and differ only in their choice of task parameter subspace. Concurrently, other works have focused on inaccuracies in model merging stemming from gradient mismatches in different models, and use these insights to connect diagonal Fisher merging and Task Arithmetic (Daheim et al., 2023). \n\nMatching models in their task parameter subspace requires solving a linear system of equations. This linear system implicitly defines a merging objective that relates to a given merging method's choice of task parameter subspace. While previous merging methods used merging objectives with a tractable closed-form solution, we instead develop a merging framework that uses the conjugate gradient method (Hestenes & Stiefel, 1952) to solve a given linear system. We refer to our merging framework as MaTS (Matching Models in their Task Subspace). By using the conjugate gradient method, MaTS flexibly supports different merging objectives and initializations (which can impact convergence speed). MaTS also enables the use of merging objectives for linear systems that don't have a tractable closed-form solution. To explore this possibility, we leverage insights from K-FAC (Grosse & Martens, 2016;Martens & Grosse, 2015) and introduce a merging method where a model's task parameter subspace is based on a block-diagonal approximation of the Fisher information matrix. \n\nTo explore the effectiveness of MaTS, we comprehensively compare it to existing merging methods on multitask and intermediate-task merging of language models and vision models trained via parameter-efficient or fullmodel fine-tuning.",
            "reference_string": "[266053657 | Tam et al. | 2023 | Citations: 12]"
        },
        {
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12001, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2345924598",
                    "name": "Thibault Rousset"
                },
                {
                    "authorId": "2326795128",
                    "name": "Taisei Kakibuchi"
                },
                {
                    "authorId": "2346085934",
                    "name": "Yusuke Sasaki"
                },
                {
                    "authorId": "2345925100",
                    "name": "Yoshihide Nomura"
                }
            ],
            "abstract": "Advancements in Natural Language Processing have enabled specialized language models, but integrating domain-specific knowledge into general-purpose models in multilingual settings remains challenging, particularly for technical vocabulary. This paper investigates the integration of technical vocabulary in merged language models and explores the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.",
            "corpus_id": 276422131,
            "sentences": [
                {
                    "corpus_id": "276422131",
                    "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
                    "text": "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance. \n\nModel merging offers advantages over other domain adaptation techniques like finetuning [4] and continual learning [5,6], particularly regarding data requirements and computational costs. Fine-tuning often necessitates substantial labeled domain-specific data, which may be scarce and can be computationally expensive, while continual learning can be susceptible to catastrophic forgetting [7]. Model merging, in contrast, leverages pretrained models, reducing the need for extensive retraining and minimizing computational overhead. However, the choice of merging method can introduce constraints; some methods may require models of similar size and architecture, potentially limiting the flexibility of model selection and hindering the benefits of combining models with complementary strengths. Current research aims to overcome these limitations by developing more flexible and efficient model merging strategies for optimal LLM domain adaptation.",
                    "score": 0.5005448962142872,
                    "section_title": "Model Merging for LLM Domain Adaptation",
                    "char_start_offset": 4872,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 528
                        },
                        {
                            "start": 529,
                            "end": 680
                        },
                        {
                            "start": 681,
                            "end": 867
                        },
                        {
                            "start": 870,
                            "end": 1057
                        },
                        {
                            "start": 1058,
                            "end": 1264
                        },
                        {
                            "start": 1265,
                            "end": 1403
                        },
                        {
                            "start": 1404,
                            "end": 1667
                        },
                        {
                            "start": 1668,
                            "end": 1821
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90185546875
                }
            ],
            "relevance_judgement": 0.90185546875,
            "relevance_judgment_input_expanded": "# Title: Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition\n# Venue: arXiv.org\n# Authors: Thibault Rousset, Taisei Kakibuchi, Yusuke Sasaki, Yoshihide Nomura\n## Abstract\nAdvancements in Natural Language Processing have enabled specialized language models, but integrating domain-specific knowledge into general-purpose models in multilingual settings remains challenging, particularly for technical vocabulary. This paper investigates the integration of technical vocabulary in merged language models and explores the knowledge transfer mechanisms involved when combining a general-purpose language-specific model with a domain-specific model, focusing on the resulting model's comprehension of technical jargon. Our experiments analyze the impact of this merging process on the target model's proficiency in handling specialized terminology. We present a quantitative evaluation of the performance of the merged model, comparing it with that of the individual constituent models. The findings offer insights into the effectiveness of different model merging methods for enhancing domain-specific knowledge and highlight potential challenges and future directions in leveraging these methods for cross-lingual knowledge transfer in Natural Language Processing.\n## Model Merging for LLM Domain Adaptation\nModel merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance. \n\nModel merging offers advantages over other domain adaptation techniques like finetuning [4] and continual learning [5,6], particularly regarding data requirements and computational costs. Fine-tuning often necessitates substantial labeled domain-specific data, which may be scarce and can be computationally expensive, while continual learning can be susceptible to catastrophic forgetting [7]. Model merging, in contrast, leverages pretrained models, reducing the need for extensive retraining and minimizing computational overhead. However, the choice of merging method can introduce constraints; some methods may require models of similar size and architecture, potentially limiting the flexibility of model selection and hindering the benefits of combining models with complementary strengths. Current research aims to overcome these limitations by developing more flexible and efficient model merging strategies for optimal LLM domain adaptation.",
            "reference_string": "[276422131 | Rousset et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Revisiting Weight Averaging for Model Merging",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.12153, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2336394540",
                    "name": "Jiho Choi"
                },
                {
                    "authorId": "122204266",
                    "name": "Donggyun Kim"
                },
                {
                    "authorId": "2335812409",
                    "name": "Chanhyuk Lee"
                },
                {
                    "authorId": "2335816260",
                    "name": "Seunghoon Hong"
                }
            ],
            "abstract": "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors. Our method demonstrates robust and scalable performance on vision benchmarks across varying numbers of tasks and model sizes. Furthermore, we observe that our approach is applicable to natural language processing tasks with competitive performance.",
            "corpus_id": 274788575,
            "sentences": [
                {
                    "corpus_id": "274788575",
                    "title": "Revisiting Weight Averaging for Model Merging",
                    "text": "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors. Our method demonstrates robust and scalable performance on vision benchmarks across varying numbers of tasks and model sizes. Furthermore, we observe that our approach is applicable to natural language processing tasks with competitive performance.",
                    "score": 0.4563258671421564,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: Revisiting Weight Averaging for Model Merging\n# Venue: arXiv.org\n# Authors: Jiho Choi, Donggyun Kim, Chanhyuk Lee, Seunghoon Hong\n## Abstract\nModel merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors. Our method demonstrates robust and scalable performance on vision benchmarks across varying numbers of tasks and model sizes. Furthermore, we observe that our approach is applicable to natural language processing tasks with competitive performance.\n",
            "reference_string": "[274788575 | Choi et al. | 2024 | Citations: 5]"
        },
        {
            "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging",
            "venue": "",
            "year": 2025,
            "reference_count": 33,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.02478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239036328",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "2674998",
                    "name": "Xiaocheng Feng"
                },
                {
                    "authorId": "2365043614",
                    "name": "Huixin Liu"
                },
                {
                    "authorId": "2118138548",
                    "name": "Yi-Chong Huang"
                },
                {
                    "authorId": "2274093523",
                    "name": "Ting Liu"
                },
                {
                    "authorId": "2257004102",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "With the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.",
            "corpus_id": 279118713,
            "sentences": [],
            "relevance_judgement": 0.888671875,
            "relevance_judgment_input_expanded": "# Title: FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging\n# Venue: \n# Authors: Zijian Li, Xiaocheng Feng, Huixin Liu, Yi-Chong Huang, Ting Liu, Bing Qin\n## Abstract\nWith the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.\n",
            "reference_string": "[279118713 | Li et al. | 2025 | Citations: 0]"
        },
        {
            "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 67,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12649, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2242179319",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "145498976",
                    "name": "S. Hu"
                },
                {
                    "authorId": "2303652170",
                    "name": "Wayne Luk"
                },
                {
                    "authorId": "2253592926",
                    "name": "Timothy M. Hospedales"
                },
                {
                    "authorId": "10001427",
                    "name": "Hongxiang Fan"
                }
            ],
            "abstract": "Model merging has emerged as a promising approach for multi-task learning (MTL), offering a data-efficient alternative to conventional fine-tuning. However, with the rapid development of the open-source AI ecosystem and the increasing availability of fine-tuned foundation models, existing model merging methods face two key limitations: (i) They are primarily designed for in-house fine-tuned models, making them less adaptable to diverse model sources with partially unknown model and task information, (ii) They struggle to scale effectively when merging numerous model checkpoints. To address these challenges, we formulate model merging as a constrained optimization problem and introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by Frank-Wolfe optimization, our approach iteratively selects the most relevant model in the pool to minimize a linear approximation of the objective function and then executes a local merging similar to the Frank-Wolfe update. The objective function is designed to capture the desired behavior of the target-merged model, while the fine-tuned candidate models define the constraint set. More importantly, FW-Merging serves as an orthogonal technique for existing merging methods, seamlessly integrating with them to further enhance accuracy performance. Our experiments show that FW-Merging scales across diverse model sources, remaining stable with 16 irrelevant models and improving by 15.3% with 16 relevant models on 20 CV tasks, while maintaining constant memory overhead, unlike the linear overhead of data-informed merging methods. Compared with the state-of-the-art approaches, FW-Merging surpasses the data-free merging method by 32.8% and outperforms the data-informed Adamerging by 8.39% when merging 20 ViT models. Our code is open-sourced at github.com/hmarkc/FW-Merging.",
            "corpus_id": 277065877,
            "sentences": [
                {
                    "corpus_id": "277065877",
                    "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
                    "text": "Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration. In contrast to the data-free methods mentioned above, data-informed methods [56,69,70] optimize merging coefficients using additional data. Model merging is impactful for LLMs, enabling efficient knowledge integration without full retraining, facilitating distributed finetuning [62], multi-task learning [49], and cost-effective model adaptation.",
                    "score": 0.4185848365485682,
                    "section_title": "Related Work",
                    "char_start_offset": 7319,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 223
                        },
                        {
                            "start": 224,
                            "end": 363
                        },
                        {
                            "start": 364,
                            "end": 571
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87841796875
                }
            ],
            "relevance_judgement": 0.87841796875,
            "relevance_judgment_input_expanded": "# Title: FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization\n# Venue: arXiv.org\n# Authors: Hao Chen, S. Hu, Wayne Luk, Timothy M. Hospedales, Hongxiang Fan\n## Abstract\nModel merging has emerged as a promising approach for multi-task learning (MTL), offering a data-efficient alternative to conventional fine-tuning. However, with the rapid development of the open-source AI ecosystem and the increasing availability of fine-tuned foundation models, existing model merging methods face two key limitations: (i) They are primarily designed for in-house fine-tuned models, making them less adaptable to diverse model sources with partially unknown model and task information, (ii) They struggle to scale effectively when merging numerous model checkpoints. To address these challenges, we formulate model merging as a constrained optimization problem and introduce a novel approach: Frank-Wolfe Merging (FW-Merging). Inspired by Frank-Wolfe optimization, our approach iteratively selects the most relevant model in the pool to minimize a linear approximation of the objective function and then executes a local merging similar to the Frank-Wolfe update. The objective function is designed to capture the desired behavior of the target-merged model, while the fine-tuned candidate models define the constraint set. More importantly, FW-Merging serves as an orthogonal technique for existing merging methods, seamlessly integrating with them to further enhance accuracy performance. Our experiments show that FW-Merging scales across diverse model sources, remaining stable with 16 irrelevant models and improving by 15.3% with 16 relevant models on 20 CV tasks, while maintaining constant memory overhead, unlike the linear overhead of data-informed merging methods. Compared with the state-of-the-art approaches, FW-Merging surpasses the data-free merging method by 32.8% and outperforms the data-informed Adamerging by 8.39% when merging 20 ViT models. Our code is open-sourced at github.com/hmarkc/FW-Merging.\n## Related Work\nTask Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration. In contrast to the data-free methods mentioned above, data-informed methods [56,69,70] optimize merging coefficients using additional data. Model merging is impactful for LLMs, enabling efficient knowledge integration without full retraining, facilitating distributed finetuning [62], multi-task learning [49], and cost-effective model adaptation.",
            "reference_string": "[277065877 | Chen et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.18729, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310819377",
                    "name": "Feng Xiong"
                },
                {
                    "authorId": "2315308131",
                    "name": "Runxi Cheng"
                },
                {
                    "authorId": "2333239207",
                    "name": "Wang Chen"
                },
                {
                    "authorId": "2316516702",
                    "name": "Zhanqiu Zhang"
                },
                {
                    "authorId": "2326995130",
                    "name": "Yiwen Guo"
                },
                {
                    "authorId": "2333235284",
                    "name": "Chun Yuan"
                },
                {
                    "authorId": "2333328707",
                    "name": "Ruifeng Xu"
                }
            ],
            "abstract": "Model merging has recently gained attention as an economical and scalable approach to incorporate task-specific weights from various tasks into a unified multi-task model. For example, in Task Arithmetic (TA), adding the fine-tuned weights of different tasks can enhance the model's performance on those tasks, while subtracting them leads to task forgetting. Although TA is highly effective, interference among task still hampers the performance of the merged model. Existing methods for handling conflicts between task generally rely on empirical selection, resulting in suboptimal performance. In this paper, we introduce an Adaptive Weight Disentanglement method. We begin by theoretically proving that task vectors employed in model merging should be orthogonal to minimize interference among tasks. Guided by this insight, we initialize redundant vectors such that, when subtracted from the original task vectors, the resulting vectors exhibit increased orthogonality. Additionally, we impose an norm constraint on the redundant vectors to preserve the performance of the task-specific models. Experimental results demonstrate the effectiveness of our proposed technique: it successfully extracts redundant vectors, and after their subtraction, the task vectors not only retain robust performance but also achieve superior fusion outcomes. Our code is available at \\href{https://github.com/FarisXiong/AWD.git}{https://github.com/FarisXiong/AWD.git}.",
            "corpus_id": 274423281,
            "sentences": [
                {
                    "corpus_id": "274423281",
                    "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
                    "text": "This approach leverages the advantages of linearized fine-tuning for model merging while efficiently performing both fine-tuning and inference. For post-hoc methods: Early merging methods primarily focused on integrating individual models. Simple Averaging [36] constructs the merged weights by independently computing the arithmetic mean of each corresponding parameter across all models. Fisher Merging [19] performs weighted parameter fusion by utilizing the fisher information matrix to assess the importance of individually fine-tuned model parameters. RegMean [11] addresses model merging by minimizing the predictive discrepancies between the merged model and the task-specific models. \n\nRecently, Ilharco et al. [10] demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging [39] resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE [44] mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging [33] eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality.",
                    "score": 0.5030678068477844,
                    "section_title": "Related Work",
                    "char_start_offset": 7837,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 389
                        },
                        {
                            "start": 390,
                            "end": 557
                        },
                        {
                            "start": 558,
                            "end": 692
                        },
                        {
                            "start": 695,
                            "end": 845
                        },
                        {
                            "start": 846,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1221
                        },
                        {
                            "start": 1224,
                            "end": 1411
                        },
                        {
                            "start": 1412,
                            "end": 1614
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 257,
                            "end": 261,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 405,
                            "end": 409,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 566,
                            "end": 570,
                            "matchedPaperCorpusId": "254877510"
                        },
                        {
                            "start": 720,
                            "end": 724,
                            "matchedPaperCorpusId": "254408495"
                        },
                        {
                            "start": 919,
                            "end": 923,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1085,
                            "end": 1089,
                            "matchedPaperCorpusId": "265034087"
                        },
                        {
                            "start": 1242,
                            "end": 1246,
                            "matchedPaperCorpusId": "269757600"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87353515625
                }
            ],
            "relevance_judgement": 0.87353515625,
            "relevance_judgment_input_expanded": "# Title: Multi-Task Model Merging via Adaptive Weight Disentanglement\n# Venue: arXiv.org\n# Authors: Feng Xiong, Runxi Cheng, Wang Chen, Zhanqiu Zhang, Yiwen Guo, Chun Yuan, Ruifeng Xu\n## Abstract\nModel merging has recently gained attention as an economical and scalable approach to incorporate task-specific weights from various tasks into a unified multi-task model. For example, in Task Arithmetic (TA), adding the fine-tuned weights of different tasks can enhance the model's performance on those tasks, while subtracting them leads to task forgetting. Although TA is highly effective, interference among task still hampers the performance of the merged model. Existing methods for handling conflicts between task generally rely on empirical selection, resulting in suboptimal performance. In this paper, we introduce an Adaptive Weight Disentanglement method. We begin by theoretically proving that task vectors employed in model merging should be orthogonal to minimize interference among tasks. Guided by this insight, we initialize redundant vectors such that, when subtracted from the original task vectors, the resulting vectors exhibit increased orthogonality. Additionally, we impose an norm constraint on the redundant vectors to preserve the performance of the task-specific models. Experimental results demonstrate the effectiveness of our proposed technique: it successfully extracts redundant vectors, and after their subtraction, the task vectors not only retain robust performance but also achieve superior fusion outcomes. Our code is available at \\href{https://github.com/FarisXiong/AWD.git}{https://github.com/FarisXiong/AWD.git}.\n## Related Work\nThis approach leverages the advantages of linearized fine-tuning for model merging while efficiently performing both fine-tuning and inference. For post-hoc methods: Early merging methods primarily focused on integrating individual models. Simple Averaging [36] constructs the merged weights by independently computing the arithmetic mean of each corresponding parameter across all models. Fisher Merging [19] performs weighted parameter fusion by utilizing the fisher information matrix to assess the importance of individually fine-tuned model parameters. RegMean [11] addresses model merging by minimizing the predictive discrepancies between the merged model and the task-specific models. \n\nRecently, Ilharco et al. [10] demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging [39] resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE [44] mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging [33] eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality.",
            "reference_string": "[274423281 | Xiong et al. | 2024 | Citations: 8]"
        },
        {
            "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 82,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.09568, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203935492",
                    "name": "Meghdad Dehghan"
                },
                {
                    "authorId": "2304517862",
                    "name": "Jie Jw Wu"
                },
                {
                    "authorId": "2304472005",
                    "name": "Fatemeh H. Fard"
                },
                {
                    "authorId": "1994554",
                    "name": "Ali Ouni"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown high capabilities in several software development-related tasks such as program repair, documentation, code refactoring, debugging, and testing. However, training these models requires massive amount of data and significant computational resources. Adapters are specialized, small modules designed for parameter efficient fine-tuning of LLMs for specific tasks, domains, or applications without requiring extensive retraining of the entire model. These adapters offer a more efficient way to customize LLMs for particular needs, leveraging the pre-existing capabilities of the large model. Model (and adapter) merging have emerged as a technique to develop one model capable of multiple tasks, with minimal or no training required. Although model and adapter merging has shown promising performance in domains such as natural language processing and computer vision, its applicability to software engineering tasks remains underexplored. In this paper, we investigate the effectiveness of merged adapters within the context of software engineering, with a particular focus on the Automated Program Repair (APR) task, through our approach, MergeRepair. In particular, we merge multiple task-specific adapters using three different merging methods, including weight-averaging, ties, and dare-ties, and evaluate the performance of the merged adapter on the APR task. We introduce a continual merging approach, a novel method in which we sequentially merge the task-specific adapters where the order and weight of the merged adapters play a significant role. We further compare the performance of our approach with a baseline method consisting of equal-weight merging applied on parameters of different adapters, where all adapters are of equal importance.",
            "corpus_id": 271903078,
            "sentences": [
                {
                    "corpus_id": "271903078",
                    "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
                    "text": "Merging models involves combining the parameters of two or more specialized models to create a unified multi-task model [20]. Recent work has demonstrated that merging Llama2-7b-chat, a general-purpose chat model, with Meditron-7b, specialized for the medical domain, resulted in a merged model that outperformed its constituent models across both general and medical benchmarks [9]. The increasing number of merged models on the Open LLM leaderboard [38] further proves the success of applying this approach to various benchmarks. Several studies have proposed different merging techniques, such as weight-space averaging [27], Fisher-Weighted averaging [20], Git Re-Basin [21], TIES-Merging [28], and DARE [22], to efficiently merge models trained on distinct domains. Considering the trend of scaling models to billions of parameters and the cost of fully fine-tuning these models, there is an increasing need to explore the feasibility of merging multiple adapters within a single language model when adapters are used to fine-tune the models. Works such as Adaptersoup [23] have investigated this approach for both in-domain and out-of-domain evaluation of mixed adapters. Another study has conducted a comprehensive analysis of the effectiveness of different adapter architectures for the indomain generalizability of merged adapters [24]. In this work, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods.",
                    "score": 0.4757357502733871,
                    "section_title": "C. Merging",
                    "char_start_offset": 7852,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 383
                        },
                        {
                            "start": 384,
                            "end": 531
                        },
                        {
                            "start": 532,
                            "end": 770
                        },
                        {
                            "start": 771,
                            "end": 1047
                        },
                        {
                            "start": 1048,
                            "end": 1177
                        },
                        {
                            "start": 1178,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1506
                        },
                        {
                            "start": 1507,
                            "end": 1714
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 120,
                            "end": 124,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 623,
                            "end": 627,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 655,
                            "end": 659,
                            "matchedPaperCorpusId": "244345933"
                        },
                        {
                            "start": 674,
                            "end": 678,
                            "matchedPaperCorpusId": "252199400"
                        },
                        {
                            "start": 693,
                            "end": 697,
                            "matchedPaperCorpusId": "259064039"
                        },
                        {
                            "start": 1074,
                            "end": 1078,
                            "matchedPaperCorpusId": "256846453"
                        },
                        {
                            "start": 1425,
                            "end": 1429,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 1444,
                            "end": 1448,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.873046875
                }
            ],
            "relevance_judgement": 0.873046875,
            "relevance_judgment_input_expanded": "# Title: MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair\n# Venue: arXiv.org\n# Authors: Meghdad Dehghan, Jie Jw Wu, Fatemeh H. Fard, Ali Ouni\n## Abstract\nLarge Language Models (LLMs) have shown high capabilities in several software development-related tasks such as program repair, documentation, code refactoring, debugging, and testing. However, training these models requires massive amount of data and significant computational resources. Adapters are specialized, small modules designed for parameter efficient fine-tuning of LLMs for specific tasks, domains, or applications without requiring extensive retraining of the entire model. These adapters offer a more efficient way to customize LLMs for particular needs, leveraging the pre-existing capabilities of the large model. Model (and adapter) merging have emerged as a technique to develop one model capable of multiple tasks, with minimal or no training required. Although model and adapter merging has shown promising performance in domains such as natural language processing and computer vision, its applicability to software engineering tasks remains underexplored. In this paper, we investigate the effectiveness of merged adapters within the context of software engineering, with a particular focus on the Automated Program Repair (APR) task, through our approach, MergeRepair. In particular, we merge multiple task-specific adapters using three different merging methods, including weight-averaging, ties, and dare-ties, and evaluate the performance of the merged adapter on the APR task. We introduce a continual merging approach, a novel method in which we sequentially merge the task-specific adapters where the order and weight of the merged adapters play a significant role. We further compare the performance of our approach with a baseline method consisting of equal-weight merging applied on parameters of different adapters, where all adapters are of equal importance.\n## C. Merging\nMerging models involves combining the parameters of two or more specialized models to create a unified multi-task model [20]. Recent work has demonstrated that merging Llama2-7b-chat, a general-purpose chat model, with Meditron-7b, specialized for the medical domain, resulted in a merged model that outperformed its constituent models across both general and medical benchmarks [9]. The increasing number of merged models on the Open LLM leaderboard [38] further proves the success of applying this approach to various benchmarks. Several studies have proposed different merging techniques, such as weight-space averaging [27], Fisher-Weighted averaging [20], Git Re-Basin [21], TIES-Merging [28], and DARE [22], to efficiently merge models trained on distinct domains. Considering the trend of scaling models to billions of parameters and the cost of fully fine-tuning these models, there is an increasing need to explore the feasibility of merging multiple adapters within a single language model when adapters are used to fine-tune the models. Works such as Adaptersoup [23] have investigated this approach for both in-domain and out-of-domain evaluation of mixed adapters. Another study has conducted a comprehensive analysis of the effectiveness of different adapter architectures for the indomain generalizability of merged adapters [24]. In this work, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods.",
            "reference_string": "[271903078 | Dehghan et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.20641, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2346255376",
                    "name": "Han Wu"
                },
                {
                    "authorId": "2345985527",
                    "name": "Yuxuan Yao"
                },
                {
                    "authorId": "2305720492",
                    "name": "Shuqi Liu"
                },
                {
                    "authorId": "2333317068",
                    "name": "Zehua Liu"
                },
                {
                    "authorId": "2221337060",
                    "name": "Xiaojin Fu"
                },
                {
                    "authorId": "2148635550",
                    "name": "Xiongwei Han"
                },
                {
                    "authorId": "2344902525",
                    "name": "Xing Li"
                },
                {
                    "authorId": "2267558779",
                    "name": "Hui-Ling Zhen"
                },
                {
                    "authorId": "2332348570",
                    "name": "Tao Zhong"
                },
                {
                    "authorId": "2347282055",
                    "name": "Mingxuan Yuan"
                }
            ],
            "abstract": "The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.",
            "corpus_id": 277322544,
            "sentences": [
                {
                    "corpus_id": "277322544",
                    "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                    "text": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models. \n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential. \n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization. \n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks. DARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: \n\n(1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.",
                    "score": 0.4901107837504234,
                    "section_title": "MODEL MERGING",
                    "char_start_offset": 9525,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 181,
                            "end": 235
                        },
                        {
                            "start": 236,
                            "end": 342
                        },
                        {
                            "start": 345,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 632
                        },
                        {
                            "start": 633,
                            "end": 774
                        },
                        {
                            "start": 777,
                            "end": 998
                        },
                        {
                            "start": 999,
                            "end": 1131
                        },
                        {
                            "start": 1132,
                            "end": 1225
                        },
                        {
                            "start": 1228,
                            "end": 1357
                        },
                        {
                            "start": 1360,
                            "end": 1518
                        },
                        {
                            "start": 1519,
                            "end": 1762
                        },
                        {
                            "start": 1763,
                            "end": 1910
                        },
                        {
                            "start": 1911,
                            "end": 2000
                        },
                        {
                            "start": 2003,
                            "end": 2285
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 377,
                            "end": 400,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 904,
                            "end": 926,
                            "matchedPaperCorpusId": "254408495"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8720703125
                }
            ],
            "relevance_judgement": 0.8720703125,
            "relevance_judgment_input_expanded": "# Title: Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging\n# Venue: arXiv.org\n# Authors: Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan\n## Abstract\nThe transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.\n## MODEL MERGING\nModel merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models. \n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential. \n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization. \n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks. DARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: \n\n(1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.",
            "reference_string": "[277322544 | Wu et al. | 2025 | Citations: 14]"
        },
        {
            "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.06876, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275639981",
                    "name": "Jinluan Yang"
                },
                {
                    "authorId": "2344837635",
                    "name": "Dingnan Jin"
                },
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "2327007623",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2148404332",
                    "name": "Didi Zhu"
                },
                {
                    "authorId": "2272001249",
                    "name": "Zhengyu Chen"
                },
                {
                    "authorId": "2057764",
                    "name": "Daixin Wang"
                },
                {
                    "authorId": "2279851906",
                    "name": "Qing Cui"
                },
                {
                    "authorId": "2344807347",
                    "name": "Zhiqiang Zhang"
                },
                {
                    "authorId": "2344948641",
                    "name": "Jun Zhou"
                },
                {
                    "authorId": "2327084756",
                    "name": "Fei Wu"
                },
                {
                    "authorId": "2272718198",
                    "name": "Kun Kuang"
                }
            ],
            "abstract": "Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular \\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model merging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release our models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for further investigations.",
            "corpus_id": 276259373,
            "sentences": [
                {
                    "corpus_id": "276259373",
                    "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
                    "text": "Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular \\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model merging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release our models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for further investigations.",
                    "score": 0.47363097710665125,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8720703125
                }
            ],
            "relevance_judgement": 0.8720703125,
            "relevance_judgment_input_expanded": "# Title: Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging\n# Venue: arXiv.org\n# Authors: Jinluan Yang, Dingnan Jin, A. Tang, Li Shen, Didi Zhu, Zhengyu Chen, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang\n## Abstract\nAchieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular \\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model merging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release our models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for further investigations.\n",
            "reference_string": "[276259373 | Yang et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Bielik 11B v2 Technical Report",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02410, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327339379",
                    "name": "Krzysztof Ociepa"
                },
                {
                    "authorId": "2327338025",
                    "name": "Lukasz Flis"
                },
                {
                    "authorId": "2327337845",
                    "name": "Krzysztof Wr'obel"
                },
                {
                    "authorId": "2327337329",
                    "name": "Adrian Gwo'zdziej"
                },
                {
                    "authorId": "2327339098",
                    "name": "Remigiusz Kinas"
                }
            ],
            "abstract": "We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.",
            "corpus_id": 278328055,
            "sentences": [
                {
                    "corpus_id": "278328055",
                    "title": "Bielik 11B v2 Technical Report",
                    "text": "To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) Wortsman et al. [2022], TIES (TIES-Merging: Resolving Interference When Merging Models) Yadav et al. [2023], Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training. \n\nTo further explore the potential of model merging, we conducted an experiment by introducing the original base model into the mixture of the three DPO-P fine-tuned models. This addition aimed to assess whether incorporating the foundational model could enhance the merged model's performance or stability. \n\nBeyond post-alignment merging, we also applied merging techniques during earlier stages of model development, specifically between different Supervised Fine-Tuning (SFT) runs. This strategy aimed to consolidate improvements from various SFT iterations, thereby enhancing the base model's quality prior to alignment through reinforcement learning from human feedback (RLHF).",
                    "score": 0.5259122972250128,
                    "section_title": "Model Merging",
                    "char_start_offset": 25160,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 333
                        },
                        {
                            "start": 336,
                            "end": 521
                        },
                        {
                            "start": 524,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 767
                        },
                        {
                            "start": 768,
                            "end": 972
                        },
                        {
                            "start": 975,
                            "end": 1146
                        },
                        {
                            "start": 1147,
                            "end": 1280
                        },
                        {
                            "start": 1283,
                            "end": 1458
                        },
                        {
                            "start": 1459,
                            "end": 1656
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 193,
                            "end": 215,
                            "matchedPaperCorpusId": "247362886"
                        },
                        {
                            "start": 281,
                            "end": 300,
                            "matchedPaperCorpusId": "259064039"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.87060546875
                }
            ],
            "relevance_judgement": 0.87060546875,
            "relevance_judgment_input_expanded": "# Title: Bielik 11B v2 Technical Report\n# Venue: arXiv.org\n# Authors: Krzysztof Ociepa, Lukasz Flis, Krzysztof Wr'obel, Adrian Gwo'zdziej, Remigiusz Kinas\n## Abstract\nWe present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.\n## Model Merging\nTo leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) Wortsman et al. [2022], TIES (TIES-Merging: Resolving Interference When Merging Models) Yadav et al. [2023], Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training. \n\nTo further explore the potential of model merging, we conducted an experiment by introducing the original base model into the mixture of the three DPO-P fine-tuned models. This addition aimed to assess whether incorporating the foundational model could enhance the merged model's performance or stability. \n\nBeyond post-alignment merging, we also applied merging techniques during earlier stages of model development, specifically between different Supervised Fine-Tuning (SFT) runs. This strategy aimed to consolidate improvements from various SFT iterations, thereby enhancing the base model's quality prior to alignment through reinforcement learning from human feedback (RLHF).",
            "reference_string": "[278328055 | Ociepa et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Reinforced Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 30,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.21272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2352931775",
                    "name": "Jiaqi Han"
                },
                {
                    "authorId": "3764313",
                    "name": "Jingwen Ye"
                },
                {
                    "authorId": "2128786021",
                    "name": "Shunyu Liu"
                },
                {
                    "authorId": "1739347431",
                    "name": "Haofei Zhang"
                },
                {
                    "authorId": "2278400946",
                    "name": "Jie Song"
                },
                {
                    "authorId": "2219055368",
                    "name": "Zunlei Feng"
                },
                {
                    "authorId": "2152127912",
                    "name": "Mingli Song"
                }
            ],
            "abstract": "The success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at https://github.com/WuDiHJQ/Reinforced-Model-Merging.",
            "corpus_id": 277349207,
            "sentences": [
                {
                    "corpus_id": "277349207",
                    "title": "Reinforced Model Merging",
                    "text": "The success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at https://github.com/WuDiHJQ/Reinforced-Model-Merging.",
                    "score": 0.44002835580286354,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86767578125
                }
            ],
            "relevance_judgement": 0.86767578125,
            "relevance_judgment_input_expanded": "# Title: Reinforced Model Merging\n# Venue: arXiv.org\n# Authors: Jiaqi Han, Jingwen Ye, Shunyu Liu, Haofei Zhang, Jie Song, Zunlei Feng, Mingli Song\n## Abstract\nThe success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at https://github.com/WuDiHJQ/Reinforced-Model-Merging.\n",
            "reference_string": "[277349207 | Han et al. | 2025 | Citations: 0]"
        },
        {
            "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 77,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.16770, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326912401",
                    "name": "Qianli Ma"
                },
                {
                    "authorId": "2289139082",
                    "name": "Dongrui Liu"
                },
                {
                    "authorId": "2288537472",
                    "name": "Chen Qian"
                },
                {
                    "authorId": "2359205673",
                    "name": "Linfeng Zhang"
                },
                {
                    "authorId": "2326115911",
                    "name": "Jing Shao"
                }
            ],
            "abstract": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.",
            "corpus_id": 276574910,
            "sentences": [
                {
                    "corpus_id": "276574910",
                    "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
                    "text": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.",
                    "score": 0.47184878095542715,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8662109375
                }
            ],
            "relevance_judgement": 0.8662109375,
            "relevance_judgment_input_expanded": "# Title: LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint\n# Venue: arXiv.org\n# Authors: Qianli Ma, Dongrui Liu, Chen Qian, Linfeng Zhang, Jing Shao\n## Abstract\nFine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.\n",
            "reference_string": "[276574910 | Ma et al. | 2025 | Citations: 2]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "271064761",
            "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic",
            "text": "Existing model merging techniques can be broadly categorized into two main types (Yang et al., 2024): (i) Pre-Merging Methods: These methods focus on enhancing the conditions necessary for effective model merging by optimizing the fine-tuning process of individual models. (ii) During Merging Methods: These approaches address task conflicts and interference through various strategies before executing the parameter merging operations. \n\nPre-Merging Methods. To establish better conditions for model merging, a significant body of work has focused on the fine-tuning processes of independent models. For instance, Ortiz-Jimenez et al. ( 2024) propose fine-tuning within the tangent space of the pre-trained model, leveraging the NTK framework to enhance performance during the fine-tuning stage. However, fine-tuning all parameters in the linearized model can be computationally intensive compared to nonlinear finetuning. To mitigate this issue, some studies advocate for selectively linearizing only certain layers; for example, Tang et al. (2023) suggest partially linearizing Adapter modules prior to merging them. Additionally, TAFT (Liu et al., 2024) introduces an efficient linearization method specifically for Transformer architectures, deriving closed-form linearized solutions that facilitate smoother integration of models. Overall, fine-tuning in the tangent space aids in disentangling both input and weight spaces, thereby reducing potential interference during subsequent model merging. \n\nDuring Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018;Ilharco et al., 2023;Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023).",
            "score": 0.637809796899746,
            "section_title": "RELATED WORK",
            "char_start_offset": 23891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2308
                }
            ],
            "ref_mentions": [
                {
                    "start": 1032,
                    "end": 1050,
                    "matchedPaperCorpusId": "263831551"
                },
                {
                    "start": 1862,
                    "end": 1884,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1884,
                    "end": 1905,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1905,
                    "end": 1927,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In this paper, following the settings of model merging [29,76,79], we consider the case of T tasks, where training for each task t starts from pre-trained model weight \u03b8 0 and fine-tunes on D \n\nHere F represents an arbitrary merging function. For example, in Task Arithmetic [28],  Although existing merging methods, like Task Arithmetic, can combine multiple task-specific models efficiently, they often exhibit significant performance gaps compared to single-task models. Prior research, such as Ties Merging [76], attributes this phenomenon to parameter interference. This term refers to the redundancy or sign discrepancies found in parameters located at the same position (e.g., self-attention weights) across different task models, which in turn result in information conflicting and performance loss. Additionally, task interference, as noted in multi-task learning literature [13,31], arises from the inherent differences between tasks. For instance, tasks such as summarization, mathematical reasoning, and code generation require the model to process information in distinct ways. These differences worsen interference when models trained on different tasks are merged. \n\nTo understand these performance drops, we conducted two experiments using Task Arithmetic. First, we fine-tuned Qwen-14B with LoRA, assigning non-overlapping modules to avoid parameter interference. Despite this, a 3.21% drop in performance occurred, indicating persistent interference. Second, using two similar summarization tasks (XSUM and DailyMail), we observed an 8.42% drop compared to individually fine-tuned models, confirming that interference persists even between similar tasks. These results suggest that interference in model merging is not limited to parameter-wise and task-wise issues.",
            "score": 0.6327875105536127,
            "section_title": "Analysis of the Performance Gap in Model Merging",
            "char_start_offset": 7535,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 59,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 59,
                    "end": 62,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 275,
                    "end": 279,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 511,
                    "end": 515,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 884,
                    "end": 888,
                    "matchedPaperCorpusId": "260611732"
                },
                {
                    "start": 888,
                    "end": 891,
                    "matchedPaperCorpusId": "258865647"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. We present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients. Extensive experiments on Mistral 7B and LLaMA2-7B/13B models demonstrate that Sens-Merging significantly improves performance across general knowledge, mathematical reasoning, and code generation tasks. Notably, when combined with existing merging techniques, our method enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. Our findings reveal important trade-offs between task-specific and cross-task scalings, providing insights for future model merging strategies.",
            "score": 0.6118955703255962,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.98046875
        },
        {
            "corpus_id": "271329267",
            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
            "text": "Model merging is a technique used to combine the parameters of multiple task-specific pre-trained LLMs to create a new and improved language model [44]. Initially, this involves the process of selecting base models and aligning the architectures of chosen models to ensure compatibility. Techniques such as parameter averaging [45] or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks. Following integration, the merged model undergoes fine-tuning on task-specific data to refine its representations and potentially optimize overall performance. The resulting merged model retains the knowledge and capabilities of its constituent models, leading to enhanced performance and capabilities across tasks compared to traditional methods of training a single model from scratch, as well as improved robustness and resource efficiency [50]. However, challenges such as ensuring compatibility between models, managing computational complexity, and avoiding performance degradation must be addressed [50], [51].",
            "score": 0.6091474672680863,
            "section_title": "E. Model Merging",
            "char_start_offset": 24254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1326
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 331,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9287109375
        },
        {
            "corpus_id": "274965385",
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "score": 0.6005676053947996,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96875
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "We experiment with five model merging methods: \n\n\u2022 Linear (Model Soups) [12]. Model Soups improves model accuracy by averaging the weights of multiple models fine-tuned with different hyperparameters. This method enhances robustness without additional inference or memory costs, making it effective for large pre-trained models in tasks like image classification and natural language processing. \n\n\u2022 Task Arithmetic [13]. Task Arithmetic uses task vectors, which represent directions in the weight space of a pre-trained model, to modify and combine model behaviors. By performing arithmetic operations on these vectors, the model's performance can be steered across multiple tasks efficiently and effectively, without requiring additional training data for each task. \n\n\u2022 TIES [17]. TIES (TRIM, ELECT SIGN & MERGE) addresses parameter interference in model merging by resetting minimally changed parameters, resolving sign conflicts, and merging only aligned parameters. This method outperforms existing techniques in various settings, maintaining valuable information and ensuring better performance in multi-task models. \n\n\u2022 DARE [21]. DARE (Drop And Rescale) sparsifies delta parameters by setting most to zero and rescaling the rest. This approach mitigates parameter interference when merging multiple fine-tuned models into one, enhancing capabilities without retraining. DARE proves particularly effective for large-scale language models, often surpassing the performance of individual source models. \n\n\u2022 Model Breadcrumbs [41]. Model Breadcrumbs enhance task performance by defining a sparse set of weights that create a trajectory within the weight space of a pre-trained model. This method improves efficiency and performance across multiple tasks without needing hyperparameter tuning.",
            "score": 0.5953066158079785,
            "section_title": "B.4 Descriptions of Model Merging Methods",
            "char_start_offset": 36923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 49,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 395
                },
                {
                    "start": 398,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1797
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 76,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1133,
                    "end": 1137,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95556640625
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In recent years, Large Language Models (LLMs) have demonstrated notable success across various Natural Language Processing (NLP) tasks [12, 16, 43, 61-63, 65, 68], including code generation [22,56], solving math problems [2,44], multilingualism [47], etc. These models, with billions of parameters, excel in various downstream tasks [25,34,72] but require extensive training on large datasets using thousands of GPUs. The considerable computational and energy costs [53] limit their specialization and deployment in resource-constrained environments [38]. \n\nTo tackle this challenge, model fusion has emerged as a promising solution [37]. One notable paradigm is model merging [29,33,76,78], where multiple task-specific models, or \"experts\", are combined into a single unified model. This unified model can quickly adapt to new tasks without the need to retrain a large model. Various techniques, such as parameter averaging [6,74], weight Figure 1: Subfigure (I) shows that in conventional merging methods, parameters from different task-specific models and a pre-trained model are weighted-summed into a single multitask model for inference. Subfigure (II) illustrates that our Twin-Merging method first isolates shared knowledge, then extracts exclusive knowledge by identifying differences between task experts and the shared model. This exclusive knowledge is then compressed into sparse vectors. Subfigure (III) shows that during testing, Twin-Merging dynamically merges shared and compressed specialized knowledge based on test inputs to form the final inference model. interpolation [33,46], and advanced strategies like task arithmetic [29,51,67,78], have been developed for model merging. These techniques have been proven effective, enabling the integration of fine-tuned knowledge from diverse tasks into a multi-task model without additional training. \n\nHowever, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert [31,76]. Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models.",
            "score": 0.5931306816463314,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1577
                },
                {
                    "start": 1578,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2200
                }
            ],
            "ref_mentions": [
                {
                    "start": 340,
                    "end": 343,
                    "matchedPaperCorpusId": "249674500"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "232110907"
                },
                {
                    "start": 677,
                    "end": 681,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 681,
                    "end": 684,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 684,
                    "end": 687,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 687,
                    "end": 690,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1592,
                    "end": 1596,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1596,
                    "end": 1599,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1650,
                    "end": 1653,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1653,
                    "end": 1656,
                    "matchedPaperCorpusId": "263831551"
                },
                {
                    "start": 1656,
                    "end": 1659,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 2028,
                    "end": 2032,
                    "matchedPaperCorpusId": "258865647"
                },
                {
                    "start": 2032,
                    "end": 2035,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "This has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation. We demonstrate this by a case study (Figure 3) on merging models finetuned on two conflicting language tasks: QNLI (Wang et al., 2018) and MNLI (Bowman et al., 2015). QNLI involves predicting whether a context contains the answer to the given question, and MNLI involves predicting text entailment given a sentence pair. These tasks are conflicting, manifested by a noticeable performance decline for both tasks when using simple averaging to merge the corresponding finetuned models. However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference.",
            "score": 0.5865346144091742,
            "section_title": "Motivation and objectives",
            "char_start_offset": 9244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1140
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.869140625
        },
        {
            "corpus_id": "259064039",
            "title": "TIES-Merging: Resolving Interference When Merging Models",
            "text": "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models. \n\nWe demonstrate the effectiveness of our proposed TIES-MERGING method in various setups with: (1) different modalities, including language and vision benchmarks, (2) distinct model sizes and families, such as T5-base and T5-large [58] as well as ViT-B/32 and ViT-L/14 [14], (3) in-domain and out-ofdomain tasks, (4) full finetuning or parameter-efficient finetuning, and (5) in the presence or absence of a validation set for setting merging hyperparameters. We show that TIES-MERGING outperforms other merging methods, such as Task Arithmetic [29], RegMean [31], Fisher Merging [45], and weight averaging [9,82] across all these experimental settings. Notably, for in-domain evaluation, TIES-MERGING outperforms the strongest baseline by an average of 2.3% and 1.7% absolute in NLP and vision settings (Table 1), respectively. For out-of-domain generalization (Table 2), TIES-MERGING outperforms the strongest baseline by 1.0% and 4.4% absolute for T5-base and T5-large models respectively. In Section 7, we perform ablations on our method components and demonstrate the impact of interference on parameter values. Additionally, we showcase the increased advantage of TIES-MERGING over task arithmetic [29] as the number of tasks increases. Finally, we examine the importance of obtaining the correct sign vector.",
            "score": 0.5859714530570411,
            "section_title": "Original Values",
            "char_start_offset": 3768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 761
                },
                {
                    "start": 764,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2077
                }
            ],
            "ref_mentions": [
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 1307,
                    "end": 1311,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1321,
                    "end": 1325,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1342,
                    "end": 1346,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1372,
                    "end": 1375,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1966,
                    "end": 1970,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.Akiba et al. (2024) propose a merging method for LLMs, which not only employ TIE-MERGING for merging in parameter space, but also adopt evolutionary algorithms to optimize the data inference path inside the merge model.The above methods have been successfully adapted to LLMs.Kim et al. (2024) apply the above methods to fuse the LLMs obtain a stronger LLM evaluator.Hammoud et al. (2024) investigate the effects of above methods on LLM safety alignment.\n\nUnlike the aforementioned task vector based methods to resolve conflicting parameters, Stoica et al. (2024) propose ZIPIT that retains similar parameters from another perspective.ZIPIT first identifies highly correlated parameters between different models.It then merges these parameters while retaining significantly different layers, thus improving the merging flexibility.\n\nPruning Redundant Parameters Given that conflicts may exist in the parameters of different models, another solution is to employ pruning tech-niques to reduce these conflicts before merging.Such methods further enhances the relevance of parameters to the task, and we introduce these methods separately.DARE (Yu et al., 2023), a technique that efficiently reduces redundancy in finetuned language models by dropping and rescaling parameters.DELLA-MERGING (Deep et al., 2024) further selects important parameters for fusion on the basis of DARE.As domain-specific data and training techniques grow, the distinctions between fine-tuned models and their base models become more significant.However, DARE experiences significant performance drops, resulting in insufficient capability to process multiple domains effectively.DPPA (Zhu et al., 2024) presents a dual-stage pruning approach as Dynamic Pruning Partition Amplification (DPPA) for effectively merging divergent fine-tuned large language models across different domains.\n\nToolkit Recently, Goddard et al. (2024) have developed Arcee's MergeKit, an open-source toolkit that integrates various model merging methods, including Model Soups, DARE, and TIES-MERGING.This toolkit significantly advances the application of model merging strategies in LLMs3 .",
            "score": 0.5852865742312965,
            "section_title": "Methods based on Task Property",
            "char_start_offset": 23048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 96,
                    "end": 315
                },
                {
                    "start": 315,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 463
                },
                {
                    "start": 463,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 927
                },
                {
                    "start": 929,
                    "end": 1119
                },
                {
                    "start": 1119,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1616
                },
                {
                    "start": 1616,
                    "end": 1750
                },
                {
                    "start": 1750,
                    "end": 1955
                },
                {
                    "start": 1957,
                    "end": 2146
                },
                {
                    "start": 2146,
                    "end": 2236
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79052734375
        },
        {
            "corpus_id": "277065877",
            "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
            "text": "Efficient Multi-Task Learning. In traditional Multi-Task Learning (MTL), a single model is trained on a dataset containing multiple tasks to enable the model to acquire diverse capabilities [3]. However, a significant challenge in traditional MTL is the issue of negative transfer [24]. To mitigate this, architecture-based approaches have been developed, such as parameter sparsification [36,55] and shared structure modularization [39,40]. On the optimization side, methods to resolve gradient conflicts [7,73] and domination of gradient or learning rate [6,34] have been proposed. With the rise of Large Language Models (LLMs), MTL faces additional challenges, particularly the high computational costs. To address these challenges, strategies like parameter-efficient fine-tuning [19,30,31] and memoryefficient fine-tuning [14,32,41] have been introduced to minimize both memory and computational resource usage. More recently, model merging has emerged as a promising approach to make MTL more compute-and data-efficient. \n\nModel Merging. While pre-merging methods prepare favorable conditions for merging, during-merging techniques combine multiple neural networks into a single model while retaining or enhancing their capabilities [68]. In this work, we focus on during-merging methods. Early insights into neural network landscapes [17] revealed that linear interpolation between models exposes useful loss surface properties, laying the foundation for weight averaging-a core merging technique. Simple averaging widens optima and improves generalization [23], evolving into advanced methods like model soups [64] and heterogeneous model merging. Recent advances introduce more structured approaches, such as Fisher-Weighted Averaging [52], which incorporates Fisher information to weight parameters more effectively, and Permutation Alignment methods like Git Re-Basin [1], which address weight permutation symmetries. Interference Resolution techniques, including TIES [35] and DOGE [63], mitigate parameter conflicts either through explicit alignment or projective gradient descent. Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration.",
            "score": 0.5851980715471433,
            "section_title": "Related Work",
            "char_start_offset": 5224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 30
                },
                {
                    "start": 31,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1026
                },
                {
                    "start": 1029,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 193,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "258865647"
                },
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 393,
                    "end": 396,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 433,
                    "end": 437,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 437,
                    "end": 440,
                    "matchedPaperCorpusId": "58145688"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "222341884"
                },
                {
                    "start": 509,
                    "end": 512,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "258959274"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "277762515",
            "title": "Channel Merging: Preserving Specialization for Merged Experts",
            "text": "Lately, the practice of utilizing task-specific fine-tuning has been implemented to improve the performance of large language models (LLM) in subsequent tasks. Through the integration of diverse LLMs, the overall competency of LLMs is significantly boosted. Nevertheless, traditional ensemble methods are notably memory-intensive, necessitating the simultaneous loading of all specialized models into GPU memory. To address the inefficiency, model merging strategies have emerged, merging all LLMs into one model to reduce the memory footprint during inference. Despite these advances, model merging often leads to parameter conflicts and performance decline as the number of experts increases. Previous methods to mitigate these conflicts include post-pruning and partial merging. However, both approaches have limitations, particularly in terms of performance and storage efficiency when merged experts increase. To address these challenges, we introduce Channel Merging, a novel strategy designed to minimize parameter conflicts while enhancing storage efficiency. This method initially clusters and merges channel parameters based on their similarity to form several groups offline. By ensuring that only highly similar parameters are merged within each group, it significantly reduces parameter conflicts. During inference, we can instantly look up the expert parameters from the merged groups, preserving specialized knowledge. Our experiments demonstrate that Channel Merging consistently delivers high performance, matching unmerged models in tasks like English and Chinese reasoning, mathematical reasoning, and code generation. Moreover, it obtains results comparable to model ensemble with just 53% parameters when used with a task-specific router.",
            "score": 0.5830828687008799,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96826171875
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "Knowledge conflict frequently arises when merging MTL models, as the expert models encapsulate diverse, sometimes conflicting, knowledge. We formally define knowledge conflict as follows: Definition 1 (Knowledge Conflict). Given a pre-trained model \u03b8 pre and a set of fine-tuned, taskspecific models {\u03b8 k } K k=1 , where \u03b8 k represents the parameters optimized for task k, the knowledge conflict on task j caused by task i can be quantified by the change in performance of task j when task i is included in the model merging process. Formally, the knowledge conflict is defined as \n\nwhere L j (\u03b8) denotes the loss for task j with model parameters \u03b8, and \u03b8 MTL ({\u03b8 k } k\u0338 =i ) represents the merged model parameters excluding the model fine-tuned for task i. The overall knowledge conflict, C, is computed as the sum of C j|i across all task pairs (i, j): \n\nA higher value of C indicates a greater degree of conflict, as it reflects a larger negative impact on task j's performance when task i is incorporated into the merging process. \n\nKnowledge conflict can be regarded as a special case of negative transfer, although these concepts emphasize different aspects. In traditional MTL, negative transfer typically refers to the dynamic interference between tasks during joint training, where conflicting gradients impede the model from learning effective representations (Zhang et al., 2023b). In contrast, the knowledge conflict defined here highlights a static nature among the fine-tuned model parameters, where further training to resolve task interference is prohibited. Each fine-tuned model has already encoded task-specific knowledge, which may be inherently incompatible with that of other tasks. As a result, knowledge conflict in model merging presents a unique challenge, necessitating methods that can align or reconcile parameters without resorting to retraining. \n\nIn the context of TA, knowledge conflict can be further articulated through task vectors: Figure 2: (a) Performance comparison across eight datasets (Cf. Section 6.1) when merging negative components (e.g., components aligned with the loss descent direction) and orthogonal components (e.g., components orthogonal to the gradient) of task vectors, corresponding to \u03b8 neg TATR and \u03b8 orth TATR , respectively.",
            "score": 0.5703950080948343,
            "section_title": "ANALYZING KNOWLEDGE CONFLICT",
            "char_start_offset": 8262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1392
                },
                {
                    "start": 1393,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1876
                },
                {
                    "start": 1879,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 1370,
                    "end": 1391,
                    "matchedPaperCorpusId": "235790783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6455078125
        },
        {
            "corpus_id": "273403685",
            "title": "Deep Model Merging: The Sister of Neural Network Interpretability -- A Survey",
            "text": "Model merging explores procedures for altering and combining the weights of neural network models to produce more performant and generalizable predictors. The model merging problem is as follows: Given the parameters of several source models  1 ,  2 , . . . ,   , we seek to develop a merging procedure  ( 1 ,  2 , . . . ,   ) : \u0398  \u21a6 \u2192 \u0398 to produce a new merged model  \u2605 that minimizes some loss L ( ). That is, model merging methods produce a single model by combining the parameters of several previously trained models. \n\nMerging models is inherently challenging: we must account for symmetries in neural network architectures [29] (which lead to the existence of many behaviorally equivalent but mathematically distinct solutions to the same learning problem), understand how task-specific knowledge is stored in learnable parameters [1,5], and comprehend how changes in a model's training setup affect its final learned representations [18]. One of the core challenges of model merging is that, barring certain exceptions, naively averaging model parameters leads to destructive interference-the tendency for merged parameters to overwrite or destroy information present in the original source models. As a result, model merging research explores techniques for weighting or aligning parameters in source models to avoid Authors' Contact Information: Arham Khan, University of Chicago, Chicago, Illinois, USA, arham@uchicago.edu; Todd Nief, University of Chicago, Chicago, Illinois, USA; Nathaniel Hudson, University of Chicago, Chicago, Illinois, USA; Mansi Sakarvadia, University of Chicago, Chicago, Illinois, USA; Daniel Grzenda, University of Chicago, Chicago, Illinois, USA; Aswathy Ajith, University of Chicago, Chicago, Illinois, USA; Jordan Pettyjohn, Colorado School of Mines, Golden, Colorado, USA; Kyle Chard, University of Chicago, Chicago, Illinois, USA; Ian Foster, Argonne National Laboratory, Lemont, Illinois, USA. \n\ndestructive interference when merging. This is often accomplished by distinguishing task-specific parameters from those that are generic or redundant. For example, when merging language models trained on two different languages, we might want to differentiate between parameters that encode language-specific knowledge and those that encapsulate higher-level semantic reasoning.",
            "score": 0.5617353233682012,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 522
                },
                {
                    "start": 525,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1937
                },
                {
                    "start": 1940,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 941,
                    "end": 945,
                    "matchedPaperCorpusId": "225040673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4521484375
        },
        {
            "corpus_id": "273482155",
            "title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery",
            "text": "Recent studies in the machine learning community have attempted to achieve MTL using model merging techniques [32,22,40,41,33]. However, the performance of simple parameter averaging [34,42] degrades significantly as the number of tasks increases, resulting in a substantial gap between its performance and that of traditional MTL model. Many recent works have made various attempts to fill this gap. We divide model merging into two stages: before and during merging. \n\n(i) The main concern before merging is how to provide more favorable preconditions for model merging, such as linearization or orthogonalization. Specifically, some works [20,74] independently fine-tune each task in the Tangent space [75] of the pre-trained model and demonstrate that this helps disentangle the weight space from the input space, leading to better model merging. Similarly, Linearization-LoRA [28] linearly fine-tunes some LoRA modules [76] in Tangent space. In addition, Task Arithmetic [36] pointed out that the orthogonality between task vectors is one of the conditions for successful model merging. From the perspective of loss landscape, OTFusion [77], GitReBasin [78] and REPAIR [79] rearrange the neurons in each layer to facilitate weight interpolation. \n\n(ii) The main focus during merging is how to mitigate interference and conflicts between models [36,43,21,29,24,25,23,44,26,33]. The first type of method explores how to better weigh and combine multiple models [19,45,40]. For example, Fisher-Merging [19] performs weighted merging utilizing the importance of each parameter through the Fisher information matrix [46]. RegMean [24] reweights and linearly combines rows in weight matrices based on statistics from training data. AdaMerging [26] leverages unlabeled test data to automatically learn a set of task-level or layer-level model merging coefficients. The second type of method explores how to merge models in sparse subspaces to reduce interference [47,48,30,49,50,31,51]. For example, Ties-Merging [21] removes the smaller magnitude parameters and eliminates the issue of parameter sign conflicts during model merging.",
            "score": 0.5612586514233651,
            "section_title": "A. Model Merging for Multi-Task Learning",
            "char_start_offset": 30851,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2131
                }
            ],
            "ref_mentions": [
                {
                    "start": 187,
                    "end": 190,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 642,
                    "end": 646,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 924,
                    "end": 928,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 1174,
                    "end": 1178,
                    "matchedPaperCorpusId": "253523197"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "258352277"
                },
                {
                    "start": 1616,
                    "end": 1620,
                    "matchedPaperCorpusId": "15354499"
                },
                {
                    "start": 1961,
                    "end": 1965,
                    "matchedPaperCorpusId": "256826987"
                },
                {
                    "start": 1977,
                    "end": 1980,
                    "matchedPaperCorpusId": "270067773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.474853515625
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.In addition, we introduce a tool that includes some methods in Toolkit.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.",
            "score": 0.5609793422255078,
            "section_title": "Methods based on Task Property",
            "char_start_offset": 20934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 190,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 750
                },
                {
                    "start": 752,
                    "end": 838
                },
                {
                    "start": 838,
                    "end": 1041
                },
                {
                    "start": 1041,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1369
                },
                {
                    "start": 1369,
                    "end": 1440
                },
                {
                    "start": 1442,
                    "end": 1644
                },
                {
                    "start": 1644,
                    "end": 1743
                },
                {
                    "start": 1743,
                    "end": 1960
                },
                {
                    "start": 1960,
                    "end": 2114
                },
                {
                    "start": 2114,
                    "end": 2210
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 211,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 365,
                    "end": 386,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1489,
                    "end": 1511,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1826,
                    "end": 1846,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "276557749",
            "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation",
            "text": "In the field of Natural Language Processing (NLP), the emergence of large language models (LLMs) (Brown et al., 2020;Touvron et al., 2023;OpenAI, 2023;Chowdhery et al., 2023) represents a revolutionary breakthrough. With their remarkable capabilities, these models have demonstrated outstanding performance across various tasks (Jiao et al., 2023;Chang et al., 2024b;Nam et al., 2024;Xing, 2024;Guo et al., 2024)  Supervised Fine-Tuning (SFT) is a crucial technique for adapting LLMs to specific tasks, refining their performance by training on domain-specific data (Hu et al., 2021;Ding et al., 2023;Xia et al., 2024). However, SFT requires substantial computational resources and long training times (Brown et al., 2020;Chang et al., 2024a). To address this challenge, Model Merging has emerged as an efficient solution, fusing the parameters of multiple fine-tuned LLMs into a unified model with diverse capabilities, without the need for additional training or computational costs (Yang et al., 2024;Akiba et al., 2024). It effectively reduces the resourceintensive demands of SFT while preserving and even enhancing model performance. \n\nA simple analogy for model merging is the Super Mario game, where the protagonist gains special abilities by absorbing power-up items. Similarly, merging model parameters integrates the strengths of different models, enabling more effective multi-task learning (Yu et al., 2024). However, existing model merging methods have some limitations, these methods heavily rely on predefined or heuristic parameter fusion strategies (Wortsman et al., 2022;Ilharco et al., 2022;Matena and Raffel, 2022;Jin et al., 2022;Yadav et al., 2023;Yu et al., 2024) such that they fail to fully explore the parameter space, thereby restricting the potential of the merged model. \n\nTo address this issue, we draw inspiration from Mixup (Zhang, 2017) and propose a novel technique called Mixup Model Merge (M 3 ).",
            "score": 0.5608835004268451,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1800
                },
                {
                    "start": 1803,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 117,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 151,
                    "end": 174,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 347,
                    "end": 367,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 367,
                    "end": 384,
                    "matchedPaperCorpusId": "259937834"
                },
                {
                    "start": 384,
                    "end": 395,
                    "matchedPaperCorpusId": "266933455"
                },
                {
                    "start": 583,
                    "end": 601,
                    "matchedPaperCorpusId": "257316425"
                },
                {
                    "start": 702,
                    "end": 722,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1403,
                    "end": 1420,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1567,
                    "end": 1590,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1611,
                    "end": 1635,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1671,
                    "end": 1687,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.802734375
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.",
            "score": 0.5555760159129728,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91552734375
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures. Extensive experiments across multiple domains demonstrate the effectiveness of our proposed methods, reducing fine-tuning costs, improving performance over state-of-the-art methods, and expanding the applicability of MoE merging.",
            "score": 0.5533059982096883,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96484375
        },
        {
            "corpus_id": "276557749",
            "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation",
            "text": "Model merging integrates the parameters of multiple models into a unified model, combining their diverse capabilities. Existing model merging methods are often constrained by fixed parameter merging ratios. In this study, we propose Mixup Model Merge (M$^3$), an innovative approach inspired by the Mixup data augmentation technique. This method merges the parameters of two large language models (LLMs) by randomly generating linear interpolation ratios, allowing for a more flexible and comprehensive exploration of the parameter space. Extensive experiments demonstrate the superiority of our proposed M$^3$ method in merging fine-tuned LLMs: (1) it significantly improves performance across multiple tasks, (2) it enhances LLMs' out-of-distribution (OOD) robustness and adversarial robustness, (3) it achieves superior results when combined with sparsification techniques such as DARE, and (4) it offers a simple yet efficient solution that does not require additional computational resources. In conclusion, M$^3$ is a simple yet effective model merging method that significantly enhances the performance of the merged model by randomly generating contribution ratios for two fine-tuned LLMs. The code is available at https://github.com/MLGroupJLU/MixupModelMerge.",
            "score": 0.5493638733132671,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "The rapid advancement of large language models has significantly enhanced performance across a diverse range of tasks (Touvron et al., 2023;Zhao et al., 2023). As these models continue to be fine-tuned for specialized domains, the necessity to merge these specialized models into a unified framework becomes increasingly critical (Yang et al., 2024;Goddard et al., 2024). While multi-task learning has been proposed as a solution, it incurs substantial training costs and requires simultaneous access to data and labels for all tasks (Sanh et al., 2022;Fifty et al., 2021). Recently, researchers have developed parameter-level model merging methods that not only comply with data privacy regulations but also improve efficiency by eliminating the need for retraining (Yadav et al., 2023;Yu et al., 2024). \n\nIn the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process. \n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others. By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers.",
            "score": 0.5480574796487947,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1682
                },
                {
                    "start": 1685,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2451
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 370,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 553,
                    "end": 572,
                    "matchedPaperCorpusId": "237485414"
                },
                {
                    "start": 767,
                    "end": 787,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 787,
                    "end": 803,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1193,
                    "end": 1209,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97021484375
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In the era of large language models, model merging is a promising way to combine multiple task-specific models into a single multitask model without extra training. However, two challenges remain: (a) interference between different models and (b) heterogeneous data during testing. Traditional model merging methods often show significant performance gaps compared to fine-tuned models due to these issues. Additionally, a one-size-fits-all model lacks flexibility for diverse test data, leading to performance degradation. We show that both shared and exclusive task-specific knowledge are crucial for merging performance, but directly merging exclusive knowledge hinders overall performance. In view of this, we propose Twin-Merging, a method that encompasses two principal stages: (1) modularizing knowledge into shared and exclusive components, with compression to reduce redundancy and enhance efficiency; (2) dynamically merging shared and task-specific knowledge based on the input. This approach narrows the performance gap between merged and fine-tuned models and improves adaptability to heterogeneous data. Extensive experiments on $20$ datasets for both language and vision tasks demonstrate the effectiveness of our method, showing an average improvement of $28.34\\%$ in absolute normalized score for discriminative tasks and even surpassing the fine-tuned upper bound on the generative tasks. Our implementation is available in \\url{https://github.com/LZY-the-boys/Twin-Merging}",
            "score": 0.5400923140816184,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9345703125
        },
        {
            "corpus_id": "276408756",
            "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
            "text": "Large Language Models (LLMs) have become ubiquitous in numerous real-world applications (Bommasani et al., 2021;Zhuang et al., 2020). The utilization of LLMs typically involves fine-tuning them for specific tasks, a process that often yields superior performance compared to general-purpose LLMs. A rapidly emerging technique in this domain is model merging (Garipov et al., 2018;Wortsman et al., 2022;Yu et al., 2024b), which aims to create a single multi-task model by combining the weights of multiple task-specific models. This approach facilitates the construction of multi-task models by integrating knowledge from fine-tuned (FT) models without requiring additional training. \n\nBuilding on recent studies (Ilharco et al., 2022;Yadav et al., 2024;Yu et al., 2024b), task vectorbased merging approaches have demonstrated significant effectiveness, where task vectors are de-fined as the parameter differences between finetuned models and the base LLM. Achieving optimal results in model merging often requires minimizing interference between task vectors associated with different tasks. To address this, existing approaches utilize modified task vectors instead of the original ones. For instance, Yu et al. (2024b) applied random dropping with probability p to obtain a sparse representation of task vectors, while Yadav et al. (2024) retained only the top-k elements of each task vector based on magnitude, setting the remaining elements to zero. These strategies aim to produce sparse estimations of task vectors, a common technique for mitigating interference. \n\nNevertheless, task vector-based model merging approaches remain constrained by two fundamental limitations. First, the computation of task vectors necessitates access to the base model parameters and demonstrates heightened sensitivity to parametric variations (Yu et al., 2024b). As fine-tuning progress goes deeper, substantial parametric divergence emerges between the original base model and its fine-tuned counterpart, thereby greatly hindering them merging effectiveness (Yu et al., 2024a). Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques.",
            "score": 0.5371706571405862,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2069
                },
                {
                    "start": 2070,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 112,
                    "end": 132,
                    "matchedPaperCorpusId": "207847753"
                },
                {
                    "start": 358,
                    "end": 380,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 380,
                    "end": 402,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 734,
                    "end": 753,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1322,
                    "end": 1341,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2050,
                    "end": 2068,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models. Previous research shows that parameter redundancy and sign discrepancies [76], as well as the distribution gap between tasks [31], hinder effective model merging. We demonstrate that task-specific models often contain mixed knowledge, where the expertise in one model may be exclusive or detrimental to others. This redundancy or interference can obstruct the integration of expertise across models [9]. (2) heterogeneity of data at test time. Previous methods pursue a single, static optimal solution for various tasks. While a one-size-fitsall model avoids introducing new parameters, it might be inadequate or suboptimal due to the unpredictable nature of test inputs [78]. It limits the utilization of complementary knowledge and leads to deteriorated performance [71]. \n\nTo address the above issues, in this paper, we introduce Twin Merging, involving two principal stages: \n\n(1) Knowledge Modularization: Unlike previous research that migrates merging interference in a parameter-wise manner or searches merging coefficients, we decompose the knowledge possessed by experts into shared knowledge and exclusive task-specific knowledge, as shown in Figure 1 (II). First, we compress common knowledge into a shared expert, serving to capture and consolidate common knowledge across varying tasks. Then we isolate exclusive knowledge based on the difference between the task experts and the shared expert, allowing diverse knowledge to be decomposed more finely. (2) Dynamic Merging: Inspired by Mixture of Experts (MoE) [80,84,85], we simplify the parameter merging problem into a conditional composition problem. Instead of pre-determining the best parameter combination for heterogeneous data at test time, as illustrated in Figure 1 (III), we introduce a router to dynamically merge shared and exclusive knowledge based on the test inputs. The shared model serves as the foundation, and task-specific knowledge is conditionally injected according to the router. \n\nWe demonstrate the effectiveness of our proposed Twin-Merging method through extensive experiments on 12 datasets, covering both discriminative and generative tasks, various model architectures, and in-domain and out-of-domain setups.",
            "score": 0.536617261352464,
            "section_title": "Introduction",
            "char_start_offset": 2052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 567
                },
                {
                    "start": 568,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 1042
                },
                {
                    "start": 1045,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 2009
                },
                {
                    "start": 2010,
                    "end": 2131
                },
                {
                    "start": 2134,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 289,
                    "end": 293,
                    "matchedPaperCorpusId": "258865647"
                },
                {
                    "start": 835,
                    "end": 839,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 932,
                    "end": 936,
                    "matchedPaperCorpusId": "263608332"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7109375
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "In this section, we focus on model merging research, for additional related work on multi-task learning and Mixture of Experts, please see Appendix B. Model merging aims to fuse multiple fine-tuned taskspecific models into one comprehensive multi-task model without additional training. FisherMerging [46] and RegMean [33], use straightforward weight averaging but require extra data and computation. Some works [1,21,58,60,70] bring models into a single low-loss basin and interpolate between them based on the linear mode connectivity (LMC) theory [15,18,20]. The weight permutations [1] and optimal transport [58] are utilized to better interpolate neural networks. However, recent studies [83] suggest that LMC might not always hold for fine-tuned models. Task-Arithmetic [28,51] extends averaging to arithmetic operations in the parameter space for finer control over model behaviors, but the interference between the multiple models can be an issue. To tackle this challenge, advanced merging methods like Ties-Merging [76], AdaMerging [78] and DARE [79] have been proposed. These methods aim to reduce task conflicts by addressing parameter redundancy or disagreements in signs, finding optimal merging coefficients, and reducing weight density, respectively. Jiang et al. [32] assume that test tasks are known and use task-specific knowledge to improve performance. However, this assumption is often unrealistic since real-world data distributions are unpredictable. In contrast, our method addresses merging interference by modularizing shared and task-specific knowledge. We handle heterogeneous test data scenarios by introducing dynamic merging techniques.",
            "score": 0.5336957718409912,
            "section_title": "Related Work",
            "char_start_offset": 5801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1668
                }
            ],
            "ref_mentions": [
                {
                    "start": 301,
                    "end": 305,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 415,
                    "end": 418,
                    "matchedPaperCorpusId": "256697368"
                },
                {
                    "start": 418,
                    "end": 421,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 424,
                    "end": 427,
                    "matchedPaperCorpusId": "211132598"
                },
                {
                    "start": 550,
                    "end": 554,
                    "matchedPaperCorpusId": "3845139"
                },
                {
                    "start": 554,
                    "end": 557,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 557,
                    "end": 560,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 612,
                    "end": 616,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 776,
                    "end": 780,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 780,
                    "end": 783,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1025,
                    "end": 1029,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1042,
                    "end": 1046,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76513671875
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Despite the significant progress made by magnitude-based methods, they often overlook the risk of merging highmagnitude components that may overwrite task-specific knowledge during integration. As illustrated in Figure 1, methods like Ties-Merging prioritize dimensions with larger magnitudes (e.g., the vertical axis in Task 1's vector), which can unintentionally overwrite critical information from Task 2. This imbalance skews the merged model towards Task 1, substantially degrading performance on Task 2 and undermining the overall multi-task capability. In contrast, the lowmagnitude horizontal axis associated with Task 2 carries valuable information while exerting minimal negative impact on Task 1's performance. Trimming these components is therefore counterproductive. This example highlights the dual objectives of effective model merging: (1) suppressing conflicts where dominant knowledge from one task undermines the performance of others, and (2) preserving the unique and essential knowledge required by each task. \n\nMotivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars. We evaluate CAT Merging on diverse visual, language, and visual-language datasets, demonstrating its superiority over state-of-the-art methods while maintaining robustness with limited exemplars.",
            "score": 0.5308672955454024,
            "section_title": "Introduction",
            "char_start_offset": 2119,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1945
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87255859375
        },
        {
            "corpus_id": "268537290",
            "title": "Evolutionary Optimization of Model Merging Recipes",
            "text": "Simple weight interpolation works well for merging image generation models that operate in a real valued latent and output space, but to our surprise, they also still work for merging language models under the right conditions, but have performance issues [49].Recently, new methods have been proposed to address merging language models specifically.One method, Task Arithmetic [21], involves building task vectors by subtracting pre-trained from fine-tuned model weights, enabling manipulation through arithmetic operations to steer the merged model's behavior.\n\nA key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation.A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance.The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences.In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49].\n\nImage generation model merging really took off in the community only after such methods were implemented in open-source toolkits [4].Similarly, language model merging really gained popularity after the implementation of mergekit [15,28].This toolkit provides all of the popular recipes for merging language models.In addition to simple linear and spherical interpolation, advanced recipes such as Task Arithmetic, TIES-Merging, DARE are also implemented, offering users the ability to experiment with combining these methods to work with fine-tunes of popular base models such as Mistral [22].With this toolkit, a large number of capable merged models are developed by the community, and most of the top models on the Open LLM Leaderboard [20] are gradually dominated by merged models produced by the community of language model enthusiasts.\n\nMergekit also introduced an additional method, called Frankenmerging which is not based on weight merging, but for the user to experiment with trying to stack different layers from multiple models to sequentially create a new model.",
            "score": 0.5306972868444395,
            "section_title": "Merging Language Models",
            "char_start_offset": 6677,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 562
                },
                {
                    "start": 564,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 923
                },
                {
                    "start": 923,
                    "end": 1140
                },
                {
                    "start": 1142,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1422
                },
                {
                    "start": 1424,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1661
                },
                {
                    "start": 1661,
                    "end": 1738
                },
                {
                    "start": 1738,
                    "end": 2017
                },
                {
                    "start": 2017,
                    "end": 2265
                },
                {
                    "start": 2267,
                    "end": 2499
                }
            ],
            "ref_mentions": [
                {
                    "start": 256,
                    "end": 260,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1417,
                    "end": 1421,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "274423281",
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "text": "Despite these advancements, these methods often rely on empirical strategies to resolve conflicts. In contrast, we clearly formulate the solution of model merging and derive a principled way for reducing interference between tasks. \n\nIn this paper, we revisit the Task Arithmetic Property and find that solving for task vectors satisfying this property in post-hoc methods is challenging. To more effectively guide task vector refinement, we propose the Task Consistency Property from the perspective of the merged model's performance. Specifically, this property requires that the performance of the merged model on each task should be close to that achieved when only the corresponding task vector is incorporated. In this view, networks that satisfy this property can be considered as being free from interference between tasks. Through theoretical derivation, we show that task consistency property can be approximately achieved by seeking orthogonal task vectors. This solution is also partially supported by the experimental phenomenon of prior studies [10,27], which indicate that a smaller cosine similarity between task vectors leads to reduced interference among them, thereby enhancing the performance of the merged model. Based on the previous discussions, we propose Adaptive Weight Disentanglement (AWD). AWD aims to decompose traditional task vectors into a redundant vector and several disentangled task vectors, ensuring the disentangled task vectors can (1) exhibit enhanced orthogonality mutually while (2) maintaining the performance of specific tasks. It achieves these two characteristics by two key optimization objectives respectively: (1) minimizing the cosine similarity of the disentangled task vectors; and (2) minimizing the norm of the redundant vector. Furthermore, AWD can be seamlessly integrated into existing merging methods, such as Task Arithmetic [10] and AdaMerging [42]. \n\nExtensive experimental validations have demonstrated AWD's effectiveness, and outperform existing merging techniques under various circumstances. When integrated with TA, AWD achieved absolute improvements in average accuracy of 2.8% on the ViT-B/32 model and 1.5% on the ViT-L/14 model compared to the advanced merging methods Ties-Merging. Moreover, we conducted further analyses, demonstrating that our method generalizes effectively on language models and exhibits enhanced robustness under various conditions. In summary, the main contributions of this research can be summarized as follows:",
            "score": 0.5260649562198598,
            "section_title": "Introduction",
            "char_start_offset": 2410,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1910
                },
                {
                    "start": 1913,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2427
                },
                {
                    "start": 2428,
                    "end": 2509
                }
            ],
            "ref_mentions": [
                {
                    "start": 1059,
                    "end": 1063,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1063,
                    "end": 1066,
                    "matchedPaperCorpusId": "263831551"
                },
                {
                    "start": 1885,
                    "end": 1889,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1905,
                    "end": 1909,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "278328055",
            "title": "Bielik 11B v2 Technical Report",
            "text": "To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) Wortsman et al. [2022], TIES (TIES-Merging: Resolving Interference When Merging Models) Yadav et al. [2023], Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training. \n\nTo further explore the potential of model merging, we conducted an experiment by introducing the original base model into the mixture of the three DPO-P fine-tuned models. This addition aimed to assess whether incorporating the foundational model could enhance the merged model's performance or stability. \n\nBeyond post-alignment merging, we also applied merging techniques during earlier stages of model development, specifically between different Supervised Fine-Tuning (SFT) runs. This strategy aimed to consolidate improvements from various SFT iterations, thereby enhancing the base model's quality prior to alignment through reinforcement learning from human feedback (RLHF).",
            "score": 0.5259122972250128,
            "section_title": "Model Merging",
            "char_start_offset": 25160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 972
                },
                {
                    "start": 975,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1280
                },
                {
                    "start": 1283,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1656
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 215,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 281,
                    "end": 300,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "276575632",
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "text": "First, we investigate the merging of finetuned models for different languages on the same task, which corresponds to multi-lingual single-task learning. \n\nLanguage knowledge interference yields imbalanced improvements: Table 1 shows the multilingual results of the ASR task with the highresource language set. On average, multi-lingual training slightly improves the pretrained model but significantly underperforms the finetuned models and merging methods. This may be due to negative interference between the knowledge of different languages, leading to gradient conflicts during training (Wang et al., 2020b). From a per-language perspective, it is observed that ca and fr achieve the largest improvements during fine-tuning while still showing significant improvements in multi-lingual training, whereas languages with smaller improvements during finetuning exhibit a substantial performance drop in multi-lingual training, even worse than the pretrained model. This indicates a strong language conflict in multi-lingual training, with ca and fr dominating. Additionally, we observe that the optimal learning rates for finetuned models vary significantly across languages (see Appendix A), while the unified learning rate configuration required by multi-lingual training prevents each language from reaching its optimal performance. \n\nModel merging mitigates language conflicts: In contrast, model merging methods show obvious improvements across almost all languages, demonstrating reduced conflict and better stability. Among model merging methods, TA outperforms WA due to its flexible scaling factor. Both MP-Merging and SVP-Merging further improve the performance of TA by reducing redundancy, and MP-Merging slightly outperforms SVP-Merging due to its finer-grained pruning. Combining the advantages of SVP and MP, LoRS-Merging achieves the best performance. \n\nTable 2 provides the multi-lingual results on ST task with the high-resource language set. The main conclusion is consistent with the ASR task: model merging methods still significantly outperform multi-lingual training, with LoRS-Merging achieving the best performance.",
            "score": 0.519430811816808,
            "section_title": "Multi-Lingual Model Merging",
            "char_start_offset": 16495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 155,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1525
                },
                {
                    "start": 1526,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1868
                },
                {
                    "start": 1871,
                    "end": 1961
                },
                {
                    "start": 1962,
                    "end": 2141
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "263831551",
            "title": "Parameter Efficient Multi-task Model Fusion with Partial Linearization",
            "text": "To address this concern, many parameter-efficient finetuning (PEFT) techniques are proposed, these approaches significantly reduce the number of parameters that need to be fine-tuned, meanwhile achieving comparable performance to full parameter fine-tuning. However, naively combining models that were fine-tuned in a parameter-efficient manner can more readily result in representational interference between tasks, which makes model fusion algorithms suboptimal. While some research has explored fusing parameter-efficient fine-tuned models for multi-task model fusion (Chronopoulou et al., 2023;Zhang et al., 2023;Huang et al., 2023), performance still lags considerably behind fusing fully fine-tuned models. Therefore, the key challenge is performing PEFT while also preventing negative interference between task-specific representations. Motivated by these concerns, we aim to enhance the multi-task model fusion capabilities of parameter-efficient fine-tuned models. \n\nIn this work, we present a novel approach to improve the multi-task fusion capability of parameterefficient fine-tuning models. Recent advances in understanding task arithmetic and weight disentanglement have demonstrated that linearizing the entire model and fine-tuning the corresponding tangent model in tangent space can enable more effective task arithmetic (Guillermo Ortiz-Jimenez et al., 2023). While promising, completely linearizing a large pre-trained model can be computationally expensive. Typically, this approach requires two to three times the computational resources needed for fine-tuning and inference. Our key insight is that we can perform efficient fine-tuning and disentangle task representations by only linearizing a subset of parameters appended to a fixed pretrained backbone. In essence, we are proposing a hybrid approach that leverages parameter-efficient fine-tuning for efficiency, while locally linearizing the adaptable modules to attain enhanced disentanglement and improved multi-task fusion capabilities. \n\nOur experiments on image classification and natural language processing tasks demonstrate that our partial linearization technique enables more effective model fusion, achieving superior performance across tasks compared to conventional PEFT methods and model fusion algorithms alone. In some cases, our proposed method is even comparable to full fine-tuning.",
            "score": 0.5192207346194067,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 2017
                },
                {
                    "start": 2020,
                    "end": 2304
                },
                {
                    "start": 2305,
                    "end": 2379
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7080078125
        },
        {
            "corpus_id": "278715220",
            "title": "RanDeS: Randomized Delta Superposition for Multi-Model Compression",
            "text": "Model Merging. Recent research on model merging is largely founded on linear mode connectivity (LMC) [Frankle et al., 2020, Neyshabur et al., 2020], which posits that models fine-tuned from the same pre-trained model are connected by a linear path along which performance remains constant. Building upon this concept, Wortsman et al. [2022] and Li et al. [2024] demonstrated that a set of specialist models can be directly interpolated to obtain a multi-task model. Ilharco et al. [2022] proposed interpolating the parameter deltas (referred to as \"task vectors\") instead. However, these methods suffer from task interference: when different models adjust the same parameters in conflicting ways, summing these adjustments leads to interference and degraded performance on individual tasks [Yadav et al., 2024, Tang et al., 2024b, Wang et al., 2024]. To mitigate this interference, various strategies have been proposed. Yang et al. [2023] optimized the merging coefficients for different tasks and layers to reduce interference. Yadav et al. [2024] addressed the conflict by removing redundant parameters and resolving sign disagreements. Tang et al. [2024c] reduced interference by upscaling the multilayer perceptron (MLP) layers. Tang et al. [2024b] compressed task vectors using singular value decomposition (SVD) and performed routing between them to further diminish interference. Both Wang et al. [2024] and Yu et al. [2024] sparsified the task vectors to prevent task conflicts. Additionally, Ortiz-Jimenez et al. [2024] proposed fine-tuning the linearized model along the tangent space of the pre-trained model to promote weight disentanglement and avoid interference. \n\nIn contrast to the above mentioned methods that aim to avoid conflicts, we intentionally accumulate interference among conflicting task vectors to facilitate their mutual cancellation. \n\nModel Compression. Model compression techniques aim to reduce the memory footprint of models while maintaining their performance.",
            "score": 0.5175978021053671,
            "section_title": "Related Work",
            "char_start_offset": 17335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1678
                },
                {
                    "start": 1681,
                    "end": 1865
                },
                {
                    "start": 1868,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 1997
                }
            ],
            "ref_mentions": [
                {
                    "start": 101,
                    "end": 122,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 318,
                    "end": 340,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 790,
                    "end": 809,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 921,
                    "end": 939,
                    "matchedPaperCorpusId": "257255597"
                },
                {
                    "start": 1030,
                    "end": 1049,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1416,
                    "end": 1432,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "271050386",
            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
            "text": "Single models have inherent limitations, such as potentially missing important information (Sagi and Rokach, 2018), and being prone to getting stuck in local optima or lacking multi-task capabilities.To address these limitations, researchers have explored model merging methods, which combine multiple models in the parameter space to create a unified, stronger model.Model merging has made significant progress in recent years, with various techniques cataloged in existing surveys (Li et al., 2023a).In the era of LLMs, model merging has become an important solution for model collaboration, usually employing basic merging methods and demonstrate the effectiveness.This section focuses on the merging techniques that are proven to be effective for LLMs2 .\n\nCurrent studies on model merging typically focus on two key issues: merging to approach a relatively optimal solution (M-ROS) and merging to enhance multi-task capability (M-MTC).Research on M-ROS is based on the finding that gradientoptimized solutions often converge near the boundary of a wide flat region rather than at the central point (Izmailov et al., 2018).Model merging offers a way to approach this relatively optimal point, thereby yielding a stronger model.M-MTC, on the other hand, aims to utilize model merging techniques to enrich a single model with capabilities across multiple tasks (Ilharco et al., 2023;Yadav et al., 2023).In the following subsection, we will introduce the techniques for each objective and their application to LLMs.\n\nIt is important to note that for both M-ROS and M-MTC, current model merging methods are applicable only to models with the same architecture and parameters within the same space.Therefore, most candidate models for merging should be trained with identical initialization.For instance, the candidate models M = {M 1 , M 2 , \u2022 \u2022 \u2022 , M k } should be fine-tuned from the same pre-trained model M 0 .This requirement ensures compatibility and coherence among the model parameters, promoting successful merging.Unfortunately, for models with incompatible parameters, such as LLaMA and QWen, current merging techniques are ineffective.",
            "score": 0.5069264410292431,
            "section_title": "Merging",
            "char_start_offset": 11520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 368
                },
                {
                    "start": 368,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 668
                },
                {
                    "start": 668,
                    "end": 758
                },
                {
                    "start": 760,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1126
                },
                {
                    "start": 1126,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1515
                },
                {
                    "start": 1517,
                    "end": 1696
                },
                {
                    "start": 1696,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2023
                },
                {
                    "start": 2023,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 114,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 1362,
                    "end": 1384,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1384,
                    "end": 1403,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76708984375
        },
        {
            "corpus_id": "271902780",
            "title": "Activated Parameter Locating via Causal Intervention for Model Merging",
            "text": "Recently, large language models (LLM) have achieved significant breakthroughs across various NLP tasks, with numerous task-specific checkpoints, fine-tuned on open-source LLMs, now publicly available (Wolf et al. 2019). These fine-tuned models incorporate valuable, high-quality taskspecific information on top of the pre-trained models (Devlin et al. 2018). However, obtaining supervised task-specific data remains challenging due to security and privacy concerns, while training LLMs on extensive datasets is also costly (Hu et al. 2021). Thus, a method that directly leverages the capabilities of these fine-tuned models with minimal or no additional training would be highly beneficial. Another relevant concept is linear model connectivity (Frankle et al. 2020), which suggests that fine-tuned models originating from the same pre-trained model, even when using different hyperparameter settings, generally lie within a single low-error basin (Izmailov et al. 2018;Kuditipudi et al. 2019;Garipov et al. 2018). Merging these models in parameter space can identify points that provide improved generalization (Wortsman et al. 2022) multiple homologous models are combined to achieve better generalization without necessitating a large volume of highquality data to fully fine-tune LLMs (Goddard et al. 2024;Stoica et al. 2023;Huang et al. 2023). \n\nA backbone of model merging involves manipulating and merging the task vector (Ilharco et al. 2022), also known as delta parameters (see Eq. 1), which quantify the difference between a pre-trained model and its fine-tuned counterpart. Clearly, delta parameters contain task-specific knowledge learned during fine-tuning, reflected in the shifts within the parameter space. Furthermore, while (Yu et al. 2024;Yadav et al. 2024) highlight the critical role of delta parameters in model merging, they also observe a major challenge: resolving redundancies and conflicts among the delta parameters of multiple models, particularly as the number of models being merged increases, which tends to aggravate these conflicts.",
            "score": 0.5065165541460859,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 200,
                    "end": 217,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 337,
                    "end": 356,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 745,
                    "end": 766,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 970,
                    "end": 993,
                    "matchedPaperCorpusId": "189897792"
                },
                {
                    "start": 993,
                    "end": 1013,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1289,
                    "end": 1310,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 1310,
                    "end": 1329,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74658203125
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "Model merging, while gaining traction and demonstrating significant potential, still encounters several key challenges that must be addressed to achieve broader adoption and improved efficiency. Firstly, as the number of tasks increases, merged models often underperform compared to independent expert models. Maintaining consistent performance across diverse tasks without extensive task-specific tuning remains a significant hurdle. Moreover, merging models trained on different tasks or domains can result in conflicts where knowledge from one model interferes with another. This issue is particularly pronounced in scenarios like multi-task learning and continual learning, where tasks often have distinct requirements. \n\nAdditionally, the lack of comprehensive theoretical frameworks for model merging limits the ability to predict and guarantee performance. Many current methods rely heavily on heuristic or empirical strategies, leaving room for improvement through deeper theoretical exploration. \n\nLastly, identifying optimal merging strategies, such as determining appropriate weight coefficients for models or parameters, is difficult. Fine-grained optimization approaches often come with high computational and resource demands, making them less feasible in practice. \n\nAddressing these challenges will require innovative methodologies, advanced computational tools, and a stronger theoretical foundation to support the efficient and reliable implementation of model merging across diverse applications. \n\nCombining model compression with model merging represents one of the promising future directions for advancing model merging techniques [Wang et al., 2024;Lu et al., 2024b]. Model compression, which involves reducing the size and complexity of a model while preserving its performance, helps mitigate interference between models during the merging process. By streamlining individual models before merging, compression can enhance compatibility and lead to more effective and seamless integration of model parameters, ultimately improving the overall merging outcomes. \n\nApart from leveraging model merging techniques, task merging or classification can also be explored as potential approaches. For instance, the Disperse-Then-Merge (DTM) method provides an innovative framework for addressing alignment tax in supervised fine-tuning of large language models [Fu et al., 2024]. This method tackles the issue of data biases by dividing the instruction-following dataset into several clusters, training separate sub-models on these clusters, and subsequently merging the sub-models into a single model. By doing so, DTM effectively distributes and mitigates dataset-specific biases, maintaining the model's instruction-following capacity while reducing the detrimental effects of such biases on knowledge and reasoning benchmarks.",
            "score": 0.505489592580645,
            "section_title": "Challenges and Future Perspectives of Model Merging",
            "char_start_offset": 29181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 723
                },
                {
                    "start": 726,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1146
                },
                {
                    "start": 1147,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1515
                },
                {
                    "start": 1518,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1874
                },
                {
                    "start": 1875,
                    "end": 2086
                },
                {
                    "start": 2089,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2396
                },
                {
                    "start": 2397,
                    "end": 2619
                },
                {
                    "start": 2620,
                    "end": 2847
                }
            ],
            "ref_mentions": [
                {
                    "start": 1654,
                    "end": 1673,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1673,
                    "end": 1690,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81494140625
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "The key advantage of model merging over traditional multi-task learning (MTL) (Caruana, 1997;Zhang & Yang, 2021;Hu et al., 2024;He et al., 2024) is its efficiency, in that it does not require joint training on data across all tasks, but only involves arithmetic operations in the weight space. Existing methods merge models by averaging model parameters via arithmetic mean (Wortsman et al., 2022a;Ilharco et al., 2023), Fisher information (Matena & Raffel, 2022), regression mean (Jin et al., 2022) or learned merging weights (Yang et al., 2023). Those methods all average the models in a global manner, meaning that they perform arithmetic operations to all parameters of the finetuned models. However, similar to the conflicting gradient problem in MTL (Yu et al., 2020;Liu et al., 2021), parameters in different finetuned models often have interference with each other, leading to suboptimal performance of the merged model. Recent works find that redundant parameter updates in finetuning are sources of conflicts (Yadav et al., 2023). Although the majority of model parameters are updated during finetuning, only very few contribute to improving the performance on downstream tasks (Chen et al., 2020a;Hoefler et al., 2021). \n\nTo overcome these limitations, we propose Localize-and-Stitch, an efficient algorithm that merges models in a localized manner. The algorithm (Figure 1) involves two steps: i) Localization: identify tiny localized regions in the finetuned models containing essential skills for the downstream tasks. ii) Stitching: reintegrate only these essential regions back into the pretrained model. In the experiments, we verify that the changes in finetuend parameters are highly redundant, as we can efficiently identify just 1% of the total parameters that recovers over 99% of the finetuned performance. We evaluate our method on various language and vision tasks, showing that it outperforms existing model merging methods under different data availability scenarios.",
            "score": 0.5053267041906131,
            "section_title": "Introduction",
            "char_start_offset": 1506,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1230
                },
                {
                    "start": 1233,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1994
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 93,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 93,
                    "end": 112,
                    "matchedPaperCorpusId": "11311635"
                },
                {
                    "start": 112,
                    "end": 128,
                    "matchedPaperCorpusId": "261243229"
                },
                {
                    "start": 128,
                    "end": 144,
                    "matchedPaperCorpusId": "267413071"
                },
                {
                    "start": 374,
                    "end": 398,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 398,
                    "end": 419,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 440,
                    "end": 463,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 756,
                    "end": 773,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 773,
                    "end": 790,
                    "matchedPaperCorpusId": "239998731"
                },
                {
                    "start": 1019,
                    "end": 1039,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "220768628"
                },
                {
                    "start": 1208,
                    "end": 1229,
                    "matchedPaperCorpusId": "231740691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.810546875
        },
        {
            "corpus_id": "267412030",
            "title": "Representation Surgery for Multi-Task Model Merging",
            "text": "For example, Ties-Merging (Yadav et al., 2023) eliminates the problem of parameter sign conflicts during model merging. DARE (Yu et al., 2023) removes a large number of useless neuron updates and then scales the neurons for merging. ReBasin (Ainsworth et al., 2023;Pe\u00f1a et al., 2023) rearranges and aligns the neurons of multiple models to establish connectivity paths between multiple models. Fisher-Merging (Matena & Raffel, 2022) performs weighted merging utilizing the importance of each parameter through the Fisher information matrix (Fisher, 1922). \n\nRegMean (Jin et al., 2023) reweights and linearly combines rows in weight matrices based on statistics from training data. Concrete (Tang et al., 2023) finds a shared subspace between multiple tasks for model merging. AdaMerging (Yang et al., 2024) leverages unlabeled test data to automatically learn a set of task-or layer-level model merging coefficients. \n\nWhile existing methods predominantly concentrate on merging in weight space, they often neglect a crucial concern stemming from weight merging-the representation bias. A substantial disparity emerges in the representation space between the merged model and individually-trained models. \n\nIn contrast, our surgery method addresses this gap, aiming to minimize the representation discrepancy. Moreover, our approach operates in the representation space, offering a complementary and orthogonal perspective to traditional weight-space merging methods. Consequently, our method can be seamlessly integrated with them.",
            "score": 0.5053166020593199,
            "section_title": "Model Merging for Multi-Task Learning",
            "char_start_offset": 7180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 555
                },
                {
                    "start": 558,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1204
                },
                {
                    "start": 1207,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1532
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 283,
                    "matchedPaperCorpusId": "255096607"
                },
                {
                    "start": 409,
                    "end": 432,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6494140625
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "In this work, we propose CAT Merging, a novel training-free technique for multi-task model merging. By introducing parameter-specific strategies, such as projection for linear weights and masking for normalization and shift parameters, CAT Merging effectively resolves inter-task knowledge conflicts while preserving task-specific information. Experimental results across diverse vision, language, and vision-language tasks demonstrate the effectiveness of CAT Merging. We achieve significant accuracy improvements of up to 2.5% on ViT-B/32 and 2.0% on ViT-L/14 compared to existing methods. CAT Merging also exhibits strong robustness in the case with only a few exemplars, providing a practical solution for real-world multi-task scenarios where retraining is infeasible.",
            "score": 0.5041781731057327,
            "section_title": "Conclusion",
            "char_start_offset": 23148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 773
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85498046875
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "As introduced in Section 4, our approach adopts a layer-wise strategy to analyze and mitigate knowledge conflict locally at each layer. Several existing methods attempt to reduce conflict by preserving components deemed important to each task. For example, Fisher Merging (Matena & Raffel, 2022) utilizes the Fisher Information Matrix to retain taskimportant weights, while Ties Merging (Yadav et al., 2023) and PCB Merging (DU et al., 2024) prune low-magnitude parameters. \n\nAlthough suppressing unimportant parameters can reduce some conflicts, these approaches overlook a critical is-sue: high-magnitude parameters-often considered \"important\"-can themselves be major sources of knowledge conflict. These components may overpower others during merging, distorting or suppressing the feature representations vital to different tasks. As a result, even when lowmagnitude parameters are pruned, significant knowledge conflict still exists, degenerating multi-task performance. \n\nIn contrast to existing trimming techniques, we propose a method that explicitly balances two objectives: (1) minimizing interference between tasks and (2) preserving the knowledge encoded in task vectors. For each task k, we define the task-specific transformation \u03a6 k that \"trims\" other task vectors in the same layer, i.e., T i (i \u0338 = k). The objective for \u03a6 k (.) is to minimize the layer-wise knowledge conflict \u2206f l k,i while maintaining the representational integrity of T i . Formally, considering all task vectors yields the following objective for \u03a6 k (.): \n\nwhere \u03bb controls the trade-off between reducing conflict and preserving knowledge. \n\nBased on the computational rules of different parameter types, we design tailored trimming operations. Specifically, for modern transformer-based networks, the parameters can be broadly classified into three categories: \n\n\u2022 Weight parameters in linear layers, which involve matrix multiplication operations; \n\n\u2022 Scale parameters in normalization layers, which scale normalized features through element-wise Hadamard products; \n\n\u2022 Shift parameters, which modify features through element-wise addition. \n\nMost parameters in modern architectures fall into these categories.",
            "score": 0.5035321675408732,
            "section_title": "Methodology",
            "char_start_offset": 11244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1545
                },
                {
                    "start": 1548,
                    "end": 1630
                },
                {
                    "start": 1633,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1940
                },
                {
                    "start": 1943,
                    "end": 2058
                },
                {
                    "start": 2061,
                    "end": 2133
                },
                {
                    "start": 2136,
                    "end": 2203
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 407,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 424,
                    "end": 441,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78515625
        },
        {
            "corpus_id": "274423281",
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "text": "This approach leverages the advantages of linearized fine-tuning for model merging while efficiently performing both fine-tuning and inference. For post-hoc methods: Early merging methods primarily focused on integrating individual models. Simple Averaging [36] constructs the merged weights by independently computing the arithmetic mean of each corresponding parameter across all models. Fisher Merging [19] performs weighted parameter fusion by utilizing the fisher information matrix to assess the importance of individually fine-tuned model parameters. RegMean [11] addresses model merging by minimizing the predictive discrepancies between the merged model and the task-specific models. \n\nRecently, Ilharco et al. [10] demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging [39] resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE [44] mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging [33] eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality.",
            "score": 0.5030678068477844,
            "section_title": "Related Work",
            "char_start_offset": 7837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 692
                },
                {
                    "start": 695,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1221
                },
                {
                    "start": 1224,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1614
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 261,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 405,
                    "end": 409,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 566,
                    "end": 570,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 720,
                    "end": 724,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 919,
                    "end": 923,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1085,
                    "end": 1089,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1242,
                    "end": 1246,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87353515625
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "In this paper, we delve deep into the critical challenge of knowledge conflict in multi-task model merging with a focus on task arithmetic. We began by formalizing the concept of knowledge conflict as the degradation in model performance caused by the interference between task vectors. Our analysis and empirical findings suggest that components of task vectors orthogonal to the gradient direction exhibit minimal knowledge conflict. This insight motivates us to define a trust region based on orthogonality and propose Task Arithmetic in the Trust Region (TATR). Extensive experiments across eight diverse datasets demonstrate that TATR effectively mitigates the knowledge conflict, enhancing the overall multi-task performance of task arithmetic-based methods. \n\n\u2022 AdaMerging (Yang et al., 2024b) leverages an unlabeled test set to adaptively learn the merging coefficients at either a layer-wise or task-wise level in Task Arithmetic. \n\n\u2022 AdaMerging++ (Yang et al., 2024b) an enhanced version of AdaMerging, integrates the principles of Ties-Merging (Yadav et al., 2023). \n\n\u2022 Surgery (Yang et al., 2024a) introduces a feature transformation module, trained to align features during the merging process. In this work, we adopt the basic version of Surgery combined with task arithmetic for evaluation iii) Training-free methods: \n\n\u2022 Weight Averaging directly averages model parameters from multiple tasks into a single model, enabling multi-task learning without additional training. \n\n\u2022 Fisher Merging (Matena & Raffel, 2022) leverages the Fisher information matrix to assess parameter importance, merging model parameters based on this importance. \n\n\u2022 RegMean (Jin et al., 2023) refines weight matrices by adjusting and linearly combining rows, utilizing statistical information derived from the training data. \n\n\u2022 Task Arithmetic (Ilharco et al., 2023b) introduces the concept of a \"task vector,\" defined as the difference between fine-tuned model parameters and pre-trained model parameters. \n\nMultiple task vectors are then combined and added to the pre-trained model to facilitate multi-task learning. \n\n\u2022 Ties-Merging (Yadav et al., 2023) eliminates unimportant parameters from the task vector and resolves sign conflicts among parameters, reducing interference during the final task vector merging process. \n\niv) Our methods:",
            "score": 0.5026948913867212,
            "section_title": "CONCLUSION",
            "char_start_offset": 24124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 939
                },
                {
                    "start": 942,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 1999
                },
                {
                    "start": 2002,
                    "end": 2111
                },
                {
                    "start": 2114,
                    "end": 2318
                },
                {
                    "start": 2321,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 780,
                    "end": 799,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 957,
                    "end": 977,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1055,
                    "end": 1075,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1089,
                    "end": 1109,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1666,
                    "end": 1684,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1837,
                    "end": 1860,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 2129,
                    "end": 2149,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "277621771",
            "title": "Breach in the Shield: Unveiling the Vulnerabilities of Large Language Models",
            "text": "In this section, we leverage FI to design a parameter protection mechanism that enhances the performance of various model merging algorithms. Model merging is a technique for acquiring domain-specific knowledge by combining models from different domains, thereby reducing the computational cost of additional fine-tuning (see [50] for a review). However, a persistent challenge is that merging parameters introduces perturbations that can hinder a model's ability to retain previously learned information. To address this, we use FI to identify parameters susceptible to forgetting and exclude them from the merging process. \n\nWe demonstrate that FI can be seamlessly integrated into mainstream model merging methods, including Average Merging [45], Task Arithmetic [20], and TIES [47]. Additionally, we include DARE [51] as a competing baseline for completeness. \n\nWe consider merging two models,  and , both fine-tuned from the same base model. Let   ,   , and  Base denote the parameters of models , , and the base model, respectively. The objective is to obtain a new model that performs well across both domains. We first introduce the merging methods and then demonstrate how FI can be integrated into these algorithms to mitigate perturbation effects. For simplicity, we restrict our consideration to equally weighted merging. \n\nAverage Merging Average merging obtains the merged model by averaging   and   , resulting in parameters  Avg =   +  2 . Task Arithmetic Task arithmetic constructs \"task vectors\" by subtracting a base model from each task-specific model and then \n\nwhere   =   \u2212  Base and similarly for   . Both Average Merging and Task Arithmetic modify all parameters in models  and , potentially degrading performance by disturbing their most sensitive parameters. To address this, we employ a protection strategy that preserves these vulnerable parameters while merging only the less critical ones. Specifically, we identify the top % of high-FI parameters in both models and record their locations in \u0398  and \u0398  . Then, for each layer in both  Task and  Avg , we revert parameters at locations in \u0398  \u2229 \u0398 \u2201  to their original values from   , and parameters at locations in \u0398  \u2229 \u0398 \u2201  to their original values from   . \n\nTIES (TrIm, Elect Sign) operates in two steps. First, it reduces redundancy by setting a fraction of the \"task vectors\"   and   to zero. Then, for each remaining entry, it retains the weight from the vector with the larger absolute value.",
            "score": 0.5023545060721921,
            "section_title": "FI-Guided Parameter Protection in Model Merging",
            "char_start_offset": 23627,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2237
                },
                {
                    "start": 2240,
                    "end": 2286
                },
                {
                    "start": 2287,
                    "end": 2376
                },
                {
                    "start": 2377,
                    "end": 2478
                }
            ],
            "ref_mentions": [
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 781,
                    "end": 785,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 817,
                    "end": 821,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82421875
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "Model merging offers an efficient approach to integrating the strengths of multiple expert models into a single model [12,17,13,42,14,15,19,47,43,44,48,49,50]. By performing weight interpolation directly at the parameter level, it serves as a powerful alternative to traditional multi-task learning [10,11]. \n\nBy exploring the loss landscape, researchers have uncovered the phenomenon of mode connectivity [51,52], which suggests that different solutions can be linked by paths in the parameter space that maintain low loss values. This insight has facilitated model fusion [53,54,55]. This technique involves aligning and interpolating corresponding components across models to minimize discrepancies [56,57]. Common methods include matching activations or weights [58], applying graph matching for channel alignment [59], and leveraging the principle of permutation invariance [60]. In general, mode connectivity ensures the feasibility of model merging, while parameter alignment facilitates more effective merging by providing a crucial prerequisite. This approach complements research efforts to mitigate task interference during model merging, representing an orthogonal yet synergistic direction. \n\nThis paper focuses on developing improved model merging techniques to reduce task interference. Existing approaches in this area include weighted merging, subspace merging, and others. (1) Weighted-based merging methods posit that different models should carry varying levels of importance during the merging process, with the importance dimension potentially refined to the task, layer, or parameter level [17,13,18,19,61,62,63]. The core objective is to identify the optimal merging coefficients based on predefined rules or learnable methods. (2) Subspace-based Merging methods aim to eliminate unimportant neurons in the models being merged, thereby alleviating interference between tasks [15,16]. The core of these approaches is to identify a small set of neurons that are most critical for downstream tasks [20,21,22,23,64,65,66,67,68]. However, these methods rely on static merging strategies, meaning that once the merging is completed, the model's structure and parameters are fixed, and all input samples are processed using the same merging weights.",
            "score": 0.5015324384769875,
            "section_title": "A. Multi-Task Model Merging",
            "char_start_offset": 35191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 710
                },
                {
                    "start": 711,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2266
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "258352277"
                },
                {
                    "start": 152,
                    "end": 155,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 155,
                    "end": 158,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "61153527"
                },
                {
                    "start": 706,
                    "end": 709,
                    "matchedPaperCorpusId": "221516809"
                },
                {
                    "start": 2026,
                    "end": 2029,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 2029,
                    "end": 2032,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 2044,
                    "end": 2047,
                    "matchedPaperCorpusId": "256826987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "In this paper, we introduced Mixture of Distributions (MoD), a novel approach for merging Large Language Models that preserves and leverages the strengths of constituent models through probabilistic distribution combination. Our method demonstrates significant advantages over existing parameter-merging techniques by maintaining critical density characteristics while enabling selective integration of model capabilities. The experimental results across diverse mathematical benchmarks validate MoD's effectiveness, achieving state-ofthe-art performance on both fundamental and advanced tasks. Our findings suggest that distributionbased merging approaches offer a promising direc-tion for developing more capable and adaptable language models, particularly in specialized domains requiring precise knowledge integration.",
            "score": 0.5013858510612836,
            "section_title": "Conclusions",
            "char_start_offset": 15057,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 822
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9462890625
        },
        {
            "corpus_id": "274788575",
            "title": "Revisiting Weight Averaging for Model Merging",
            "text": "To overcome the limitations of traditional multi-task learning, various methods have been proposed to construct a single model capable of handling multiple tasks by manipulating model parameters alone [16,40,48]. However, due to the non-convexity of loss functions and the complexity of the loss landscape in deep learning tasks, naively summing the parameters of models trained on different tasks often leads to significant performance degradation on each task [25,33]. Intriguingly, Ilharco et al. [17] demonstrated that arithmetic operations in parameter space using task vec-tors-differences between the parameters of task-specific fine-tuned models and those of a common pretrained model-consistently influence model behavior. Specifically, adding task vectors enhances performance on multiple tasks (task addition), while subtracting a task vector diminishes performance on the associated task (task negation). This suggests that task vectors capture task-specific knowledge in a way that can be algebraically manipulated. Ortiz-Jimenez et al. [35] analyzed this phenomenon, attributing it to weight disentanglement-where different directions in the weight space of a model independently govern distinct regions in the input space, allowing the model to manipulate these regions separately for specific tasks-and described it as an emergent property inherent to pretrained parameters. \n\nModel Merging and Task Interference Building upon the findings of Ilharco et al. [17], multi-task models can be constructed without retraining or access to training data by performing a weighted sum of task vectors. However, task interference still leads to reduced performance on individual tasks. To mitigate task interference in multitask model merging, recent studies have proposed methods that directly operate on task vectors: Ties-Merging [51] employs element-wise manipulations by trimming parameters with small magnitudes and resolving sign conflicts among parameters with large magnitudes via majority voting across tasks; similarly, Consensus Merging [47] leverages task-specific binary masks to retain only those parameters deemed important by multiple tasks, thereby reducing interference-inducing elements. Recently, Yang et al. [52] introduced a method that employs test-time adaptation [46] to dynamically determine the coefficients for the weighted sum of task vectors based on the input data distribution, thereby reducing task interference.",
            "score": 0.5011247222583163,
            "section_title": "Parameter Manipulation and Task Arithmetic",
            "char_start_offset": 5302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1390
                },
                {
                    "start": 1393,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 2213
                },
                {
                    "start": 2214,
                    "end": 2452
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 208,
                    "end": 211,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 500,
                    "end": 504,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1050,
                    "end": 1054,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1474,
                    "end": 1478,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1839,
                    "end": 1843,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2055,
                    "end": 2059,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78173828125
        },
        {
            "corpus_id": "267412030",
            "title": "Representation Surgery for Multi-Task Model Merging",
            "text": "Model merging is to merge multiple individual models into one (Li et al., 2023b). It has two main application scenarios: First, merge models trained on the same task to improve the accuracy or generalization of the final model (Gupta et al., 2020;Cha et al., 2021;Wortsman et al., 2022;Lu et al., 2022;Li et al., 2023a). Second, merge multiple models trained on different tasks to perform MTL (Ilharco et al., 2023;Yadav et al., 2023;Ram\u00e9 et al., 2023;Zhang et al., 2023;Stoica et al., 2023;Huang et al., 2023;Yang et al., 2024), which is the focus of this paper. We further divide model merging into two stages: before and during merging. \n\n(i) The main concern before merging is how to provide more favorable preconditions for model merging, such as linearization or orthogonalization. Specifically, (Ortiz-Jimenez et al., 2023) independently fine-tunes each task in the Tangent space (Jacot et al., 2018) of the pre-trained model and demonstrates that this helps decouple the weight space from the input space, leading to better model merging. Similarly, Linearization-LoRA (Tang et al., 2024) linearly fine-tunes some LoRA modules (Hu et al., 2022) in Tangent space. \n\nIn addition, Task Arithmetic (Ilharco et al., 2023) pointed out that the orthogonality between task vectors is one of the conditions for successful model merging. \n\n(ii) The main focus during merging is how to mitigate interference and conflicts between models (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2023;Pe\u00f1a et al., 2023;Jin et al., 2023;Ram\u00e9 et al., 2023;Zhang et al., 2023;Yang et al., 2024). For example, Ties-Merging (Yadav et al., 2023) eliminates the problem of parameter sign conflicts during model merging.",
            "score": 0.5006268753522903,
            "section_title": "Model Merging for Multi-Task Learning",
            "char_start_offset": 5597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1335
                },
                {
                    "start": 1338,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 247,
                    "matchedPaperCorpusId": "209315801"
                },
                {
                    "start": 247,
                    "end": 264,
                    "matchedPaperCorpusId": "235367622"
                },
                {
                    "start": 264,
                    "end": 286,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 286,
                    "end": 302,
                    "matchedPaperCorpusId": "254564561"
                },
                {
                    "start": 434,
                    "end": 452,
                    "matchedPaperCorpusId": "259096117"
                },
                {
                    "start": 491,
                    "end": 510,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 802,
                    "end": 830,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 887,
                    "end": 907,
                    "matchedPaperCorpusId": "49321232"
                },
                {
                    "start": 1491,
                    "end": 1509,
                    "matchedPaperCorpusId": "255096607"
                },
                {
                    "start": 1526,
                    "end": 1544,
                    "matchedPaperCorpusId": "259096117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42529296875
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance. \n\nModel merging offers advantages over other domain adaptation techniques like finetuning [4] and continual learning [5,6], particularly regarding data requirements and computational costs. Fine-tuning often necessitates substantial labeled domain-specific data, which may be scarce and can be computationally expensive, while continual learning can be susceptible to catastrophic forgetting [7]. Model merging, in contrast, leverages pretrained models, reducing the need for extensive retraining and minimizing computational overhead. However, the choice of merging method can introduce constraints; some methods may require models of similar size and architecture, potentially limiting the flexibility of model selection and hindering the benefits of combining models with complementary strengths. Current research aims to overcome these limitations by developing more flexible and efficient model merging strategies for optimal LLM domain adaptation.",
            "score": 0.5005448962142872,
            "section_title": "Model Merging for LLM Domain Adaptation",
            "char_start_offset": 4872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1821
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "276928602",
            "title": "Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors",
            "text": "According to 3.2, the conflict among different experts arises from the interaction between the interference vector and the input. Moreover, this conflict may be exacerbated by the overlap of input domains. This phenomenon is further evidenced by the results of the vision and language tasks. Specifically, the domain overlap among various visual tasks is limited, whereas the inputs across different language tasks exhibit a larger overlap. Consequently, the current model-merging approach allows for a high degree of recovery in vision tasks, while there remains a notable disparity between the performance of the merged model and that of individual experts in language tasks. To address this limitation, it may be necessary to adopt a data processing perspective aimed at reducing the overlap of input domains across different language tasks. \n\nA.3. The impact of linear layer Motivated by (Jin et al., 2023), we only apply our method to the linear layer in the model. To further demonstrate the effectiveness, we only apply the task vector of the linear layer to the pretrained model. The result is shown in Fig. 7. The result shows that only using the task vector of the linear keeps most of the ability of the model. This means focusing on the linear layer is the key to the model merging problems.",
            "score": 0.49990815143316203,
            "section_title": "A.2. Towards understanding the conflict in Model Merging",
            "char_start_offset": 26041,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1303
                }
            ],
            "ref_mentions": [
                {
                    "start": 892,
                    "end": 910,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.478271484375
        },
        {
            "corpus_id": "277634330",
            "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
            "text": "Standard full-model fine-tuning methods update all parameters when learning each new task, fully exploiting the model's expressive power but risking severe catastrophic forgetting due to conflicting updates (Luo et al., 2025). On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging. \n\nPositioning Our Work: Our approach introduces a novel constrained full-parameter update method that differs fundamentally from existing categories. Unlike parameter-efficient approaches, we leverage the entire parameter space, maximizing expressive capacity. Unlike isolation approaches, we don't partition parameters or require additional task-specific modules. Unlike constrained full fine-tuning, we explicitly mitigate interference through geometric constraints. Specifically, we dynamically identify low-rank subspaces via Singular Value Decomposition (SVD) and constrain updates to be orthogonal to previously learned task representations. This geometric approach to interference minimization ensures knowledge preservation while maintaining update flexibility. By operating in the full parameter space while enforcing orthogonality constraints, our method achieves a unique balance between knowledge retention and model plasticity, providing a theoretically grounded and practically scalable solution for continual learning in large language models.",
            "score": 0.499691950164191,
            "section_title": "Full-Model Fine-Tuning and Model Merging:",
            "char_start_offset": 8495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 637
                },
                {
                    "start": 640,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1696
                }
            ],
            "ref_mentions": [
                {
                    "start": 289,
                    "end": 311,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 318,
                    "end": 338,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93896484375
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations. In this paper, we formally define this issue as knowledge conflicts, characterized by the performance degradation of one task after merging with a model fine-tuned for another task. Through in-depth analysis, we show that these conflicts stem primarily from the components of task vectors that align with the gradient of task-specific losses at $\\theta_{\\text{pre}}$. To address this, we propose Task Arithmetic in Trust Region (TATR), which defines the trust region as dimensions in the model parameter space that cause only small changes (corresponding to the task vector components with gradient orthogonal direction) in the task-specific losses. Restricting parameter merging within this trust region, TATR can effectively alleviate knowledge conflicts. Moreover, TATR serves as both an independent approach and a plug-and-play module compatible with a wide range of TA-based methods. Extensive empirical evaluations on eight distinct datasets robustly demonstrate that TATR improves the multi-task performance of several TA-based model merging methods by an observable margin.",
            "score": 0.49759107952096454,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "267365047",
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "text": "Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://github.com/tanganke/weight-ensembling_MoE",
            "score": 0.49717569931568073,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "Model merging techniques, which aim to integrate knowledge across models, have attracted increasing attention in recent years. As a precursor, Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) averages model weights near the end of training. This concept was further advanced by approaches like SWAD (Cha et al., 2021) and Ensemble of Averages (EoA) (Arpit et al., 2022). Empirical evidence from Ilharco et al. (2023a) demonstrates that parameter averaging effectively integrates knowledge from models trained on diverse tasks. DLCPA (Sun et al., 2023) proposes to apply cumulative parameter averaging (CPA) to continually assimilate knowledge across distinct tasks. Fisher-Merging (Matena & Raffel, 2022) leverages the Fisher information matrix Fisher (1925) to measure the importance of model parameters and merge models using weighted averaging. Additionally, RegMean (Jin et al., 2023) formulates an optimal merging model by minimizing the distance to each model in the parameter space. \n\nRecently, Task Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors. Furthermore, several approaches combine test-time adaptation techniques with TA, yielding superior MTL performance. These test-time adaptation-based methods typically allocate merging weights and fine-tune them during testing using unsupervised test data.",
            "score": 0.49189524074728636,
            "section_title": "MULTI-TASK LEARNING THROUGH MODEL MERGING",
            "char_start_offset": 4020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1859
                },
                {
                    "start": 1860,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2115
                }
            ],
            "ref_mentions": [
                {
                    "start": 177,
                    "end": 200,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 308,
                    "end": 326,
                    "matchedPaperCorpusId": "235367622"
                },
                {
                    "start": 358,
                    "end": 378,
                    "matchedPaperCorpusId": "239049452"
                },
                {
                    "start": 404,
                    "end": 426,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 879,
                    "end": 897,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1032,
                    "end": 1055,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1465,
                    "end": 1485,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1585,
                    "end": 1606,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1616,
                    "end": 1633,
                    "matchedPaperCorpusId": "269293160"
                },
                {
                    "start": 1680,
                    "end": 1697,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1720,
                    "end": 1747,
                    "matchedPaperCorpusId": "258832777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88427734375
        },
        {
            "corpus_id": "273482450",
            "title": "Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace",
            "text": "The rapid advancement of artificial intelligence has led to the emergence of pre-trained models that demonstrate exceptional performance across various tasks (Yang et al., 2024a). However, training and deploying individual models for each specific task not only incurs substantial computational costs but also results in knowledge redundancy and storage inefficiencies. To address these challenges, multi-task model merging, as a promising solution, integrates parameters from multiple single-task models into a unified model (Tang et al., 2024a), which not only enhances task-specific performance but also significantly improves computational efficiency and cost-effectiveness (Izmailov et al., 2018;Frankle et al., 2020;Ilharco et al., 2022b). \n\nCurrent research in model merging primarily focuses on resolving conflicts among task-specific models to achieve effective knowledge transfer and inheritance. Pioneering merging strategies based on task vectors include gradient conflict-based methods (Yadav et al., 2024) and subspace-based approaches (Tang et al., Clean Model Backdoor Model 0",
            "score": 0.49174212151995855,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 745
                },
                {
                    "start": 748,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1092
                }
            ],
            "ref_mentions": [
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 722,
                    "end": 744,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 999,
                    "end": 1019,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.529296875
        },
        {
            "corpus_id": "276574617",
            "title": "Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation",
            "text": "Rapid development of foundation models has facilitated the construction of expert model from custom data. Modern models like large language model (LLMs) are pre-trained on various datasets to obtain general knowledge and employing pre-trained models typically involves fine-tuning on task- specific data to gain ability on specific areas. When dealing with tasks of different domains, multi-task learning (Sanh et al., 2022) is a common paradigm to mitigate performance variations. However, particular knowledge may be required progressively over time. As the model becomes larger (Dehghani et al., 2023;Wang et al., 2024b), once the model is specialized on specific datasets, it is time consuming and resource intensive to retrain models to get knowledge of another area, even encountering catastrophic forgetting (Zeng et al., 2024). Furthermore, issues regarding data privacy may obstacle its practical application. To address these issues, model merging (Stoica et al., 2024) has been proposed to integrate multiple separate models of specific knowledge off-the-shelf into one model with multi-task ability without the demand of training or accessing data. Its effectiveness and convenience show great potential in various downstream tasks (Guo et al., 2024;Shah et al., 2024). \n\nDespite its popularity, crucial problems for model merging remain unsolved, hindering its real-world application. First, with larger model size like multimodal large language models (MLLMs) and massive data, parameter efficient tuning (PEFT) (Hu et al., 2022) has become the most popular and effective tuning method for large models. However, existing model merging method focuses on combining full fine-tuning (FFT) models (Yadav et al., 2024;Guodong et al., 2024), which is challenging in distribution shift and undergoes performance drop when directly applied to parameter efficient model merging, as is illustrated in Figure 1. Another issue lies in that current high-performance methods rely on extra information of seen tasks (e.g., validation data (Yang et al., 2024b), extra storage (Huang et al., 2024)) to boost the performance.",
            "score": 0.4916082445428937,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2122
                }
            ],
            "ref_mentions": [
                {
                    "start": 405,
                    "end": 424,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 581,
                    "end": 604,
                    "matchedPaperCorpusId": "256808367"
                },
                {
                    "start": 958,
                    "end": 979,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 1262,
                    "end": 1280,
                    "matchedPaperCorpusId": "265351656"
                },
                {
                    "start": 1708,
                    "end": 1728,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1728,
                    "end": 1749,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65771484375
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "In order to study the relative merits of merging for different objectives across a wide set of languages, we conduct extensive ablations. We detail some of the most critical experiment variants below: \n\nObjective-based merging. To evaluate the relative merits of merging on balancing dual-objectives, we merge models that have been separately optimized for general-purpose abilities and safety. This builds upon our multilingual 0% and 100% Safety Mixes (see Section 2.2) to balance the trade-offs between safety and general performance. Language-based merging. Multilinguality remains one of the most challenging tasks in language modeling. We aim to determine whether language-specific models can be used off-the-shelf to incorporate language capabilities and explore how merging models based exclusively on different languages affects their downstream performance. \n\nSpecifically, we investigate whether combining models optimized for both safety and general performance with a 15% language-specific safety mix for our target languages leads to better performance than training on a mixture of those languages. For clarity, to produce a multilingual model with safe and general-purpose abilities for English, French, and Spanish (referred to as the EN-FR-SP group later), we merge models optimized independently on a 15% Safety Mix for each of these languages. \n\nComparison of merging applied to DPO and SFT. Model merging is a highly adaptable technique that can be applied at any stage of the training process owing to its simple input requirement of model checkpoints. To determine the optimal stage for maximizing its benefits, we merge and evaluate SFT and DPO checkpoints independently as these techniques have shown great success towards the alignment of language models [Aakanksha et al., 2024;Shen et al., 2024]. \n\nSensitivity to hyperparameters. Previous works [Ilharco et al., 2023] have shown that merging is sensitive to the hyperparameters involved and have developed sophisticated algorithms [Akiba et al., 2024;Xiao et al., 2023;Davari & Belilovsky, 2024] to find the optimal values for the same. \n\nTo this end, we seek to find the impact of varying the weighting scheme of Linear merging on both general performance and safety.",
            "score": 0.4905704021692839,
            "section_title": "Key Ablations",
            "char_start_offset": 11237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 200
                },
                {
                    "start": 203,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1824
                },
                {
                    "start": 1827,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2247
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.497314453125
        },
        {
            "corpus_id": "277322544",
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "text": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models. \n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential. \n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization. \n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks. DARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: \n\n(1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.",
            "score": 0.4901107837504234,
            "section_title": "MODEL MERGING",
            "char_start_offset": 9525,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 774
                },
                {
                    "start": 777,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1357
                },
                {
                    "start": 1360,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 2000
                },
                {
                    "start": 2003,
                    "end": 2285
                }
            ],
            "ref_mentions": [
                {
                    "start": 377,
                    "end": 400,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 904,
                    "end": 926,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "Additionally, Zheng et al. [239] discuss the topic of \"learning from models\" and it only mentions model merging as a subsection (single page only) in the whole paper. The most related work to the \"model merging\" topic is [108], but in terms of application, it only discusses model merging in three scenarios: federated learning, fine-tuning, and distillation. It also ignores a lot of recently published articles due to the rapid evolution of the model merging direction. To address these gaps, this survey aims to elucidate the methods, theories, applications, and future trends in model merging direction, providing a comprehensive classification of relevant approaches. In particular, this paper enhances the comprehensive understanding of model merging by covering three main aspects: First, how are existing model merging methods classified? We first propose a new taxonomy in Figure 2 (upper part) that divides existing model merging methods into two phases ( \u00a72): pre-merging and during-merging. (i) Pre-merging methods aim to create better conditions for merging. It is further divided into using linearized fine-tuning to achieve weight space and input space disentanglement, performing architectural transformations to convert heterogeneous models into homogeneous models, and aligning weights to place them in the same basin. (ii) During-merging methods focus on designing sophisticated techniques to merge multiple models into one. These methods address task conflict and interference problems when merging models. They can be further divided into basic merging methods that perform the simplest parameter merging strategy; weighted merging methods that merge multiple models according to the importance calculated by specific rules; subspace merging methods that project multiple models into sparse subspaces for merging; routing-based methods that dynamically merge models according to input samples during inference; and the post-calibration based method that corrects the merged model. In addition to these methods, we also discuss the theoretical or empirical analysis of model merging. \n\nSecond, which applications can benefit from model merging? We discuss in detail the various use cases of model merging in foundation models ( \u00a73) and over ten subfields of machine learning ( \u00a74). As shown in Figure 2 (lower part), model merging can be applied to a variety of foundation models, including large language models, multimodal large language models, and visual generative models.",
            "score": 0.48941755956765565,
            "section_title": "Introduction",
            "char_start_offset": 2177,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2103
                },
                {
                    "start": 2106,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2497
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61572265625
        },
        {
            "corpus_id": "277452617",
            "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization",
            "text": "Recent researches on model merging techniques have contributed to building more capable models by providing an efficient approach to combine abilities on various tasks from different models that requires less data and compute. Some studies focus on merging homogeneous models with identical architecture, while others focus on tackle the challenge on heterogeneous models which have different architectures. In merging homogeneous models, Task Arithmetic [10] proposes the concept of task vectors, which subtracts fine-tuned weights from pre-train weights to obtain task-related weight difference as the object of merging. Ties-Merging [26] and DARE [29] further improve the performance by mitigating parameter interference during the merging process through parameter pruning and conflict resolving. MetaGPT [32] scales the task vectors with task-agnostic coefficients in closed-form by seperating data term and scaling coefficients in the optimization objective. Although these methods improves the performance of the merged models, they cannot be directly applied on models with architecture difference. In fusing heterogeneous models, DAMC [1] employs parameter decoupling and adaptive adjustment to enhance model merging strategies for fusing modalities on MLLMs with different modality encoders, but this work still focus on merging identical language model architecture. To consolidate LLMs with different architectures, FuseLLM [22] and FuseChat [23] applies token alignment and model fusion strategies with knowledge distillation before continue training the model, but they need labeled data and computation resources for continue training. In fact, the majority of previous works on model merging requires labeled data for validation search or supervised training [1,10,22,23,26,29]. In this work, we eliminate the need of labeled data by leveraging our unsupervised hyper-parameter selection method, and enable model merging strategies to be applied on heterogeneous MLLMs with architecture differences.",
            "score": 0.4885795498771336,
            "section_title": "Model Merging",
            "char_start_offset": 4643,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 407
                },
                {
                    "start": 408,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 636,
                    "end": 640,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 650,
                    "end": 654,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1144,
                    "end": 1147,
                    "matchedPaperCorpusId": "267760102"
                },
                {
                    "start": 1775,
                    "end": 1778,
                    "matchedPaperCorpusId": "267760102"
                },
                {
                    "start": 1787,
                    "end": 1790,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72509765625
        },
        {
            "corpus_id": "273345779",
            "title": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing",
            "text": "The most commonly used model merging techniques themselves are Simple-Average (SA) (Chronopoulou et al., 2023;Wortsman et al., 2022) and Task-Arithmetic (TA) (Ortiz-Jimenez et al., 2023). Moreover, TIES-merging (Yadav et al., 2023) has recently proposed to further enhance the merging performance by solving the symbol conflicts among different models. \n\nHowever, all existing model merging techniques only achieve destructive editing performance when used for collaborative KE, which inevitably results in knowledge loss during the merging process.",
            "score": 0.48789539982000163,
            "section_title": "RELATED WORK",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 549
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41064453125
        },
        {
            "corpus_id": "271843401",
            "title": "ProFuser: Progressive Fusion of Large Language Models",
            "text": "Model merging involves combining the weights of two or more models into one by directly editing the weight space. There are two primary types of research in this area: 1. Merging Models Trained on the Same Task: Enhances a model's generalization by merging multiple models trained on the same task. Model Soups (Wortsman et al., 2022) fine-tune a model using the same dataset but with different strategies, and then combine the resulting models through linear averaging. 2. Merging Models Trained on Different Tasks: Integrates models trained on different tasks to enable multitask learning (MTL). Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2024) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. \n\nThe aforementioned methods are limited to merging models with same structure. FuseLLM (Wan et al., 2024) introduces a novel approach for knowledge fusion of heterogeneous LLMs, selecting the advantageous model with Min-CE on GT. It leverages logits distribution from source LLMs to transfer their advantages into a target LLM. This study proposes to evaluate a model's advantages from both the training mode and inference mode, enabling a more comprehensive demonstration of its strengths.",
            "score": 0.48783784968028454,
            "section_title": "Model Merging",
            "char_start_offset": 5998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1534
                },
                {
                    "start": 1537,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 311,
                    "end": 334,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1339,
                    "end": 1359,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "273403685",
            "title": "Deep Model Merging: The Sister of Neural Network Interpretability -- A Survey",
            "text": "For example, when merging language models trained on two different languages, we might want to differentiate between parameters that encode language-specific knowledge and those that encapsulate higher-level semantic reasoning. Serendipitously, in the process of trying to prevent destructive interference, model merging algorithms yield procedures for mapping neural network components (e.g., layers, neurons, or weights) to task-specific behaviors. This ability to map learned parameters to semantic behaviors is one of the overarching goals of deep learning interpretability and robustness research. Therefore, studying model merging and developing more performant model merging techniques is tantamount to developing algorithms that can help explain neural network behavior by localizing distinct predictive behaviors in neural networks to subnetworks of their neurons. Indeed, many model merging techniques have shown promise in their ability to induce predictable and interpretable changes in model behavior [38,57,123]. These can be considered parameter space analogs to popular interpretability techniques, such as activation steering or patching, that seek to identify and manipulate a model's activations (rather than weights) in ways that govern specific downstream behaviors [101,115,117]. Moreover, principles from model merging can yield insight into the susceptibility of models to adversaries [104], improve model robustness [37,127,141], and decrease the risk of data leakage in the federated learning regime [63]. \n\nRecent advances in our understanding of model merging have shed light on each of these essential areas of neural network theory. Accordingly, we seek to present a comprehensive analysis and categorization of cutting-edge research from the model merging literature. In so doing, we distill insights on how neural network representations are structured and learned, and demonstrate how these insights could be applied to relate learned representations to model behavior (in the spirit of model interpretability) and enhance model robustness.",
            "score": 0.4876032786511936,
            "section_title": "Introduction",
            "char_start_offset": 2106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1531
                },
                {
                    "start": 1534,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2073
                }
            ],
            "ref_mentions": [
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1409,
                    "end": 1414,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1441,
                    "end": 1445,
                    "matchedPaperCorpusId": "252760718"
                },
                {
                    "start": 1445,
                    "end": 1449,
                    "matchedPaperCorpusId": "198148018"
                },
                {
                    "start": 1449,
                    "end": 1453,
                    "matchedPaperCorpusId": "259937385"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56640625
        },
        {
            "corpus_id": "270379651",
            "title": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation",
            "text": "Model merging has emerged as an effective approach to combine multiple single-task models into a multitask model. This process typically involves computing a weighted average of the model parameters without any additional training. Existing model-merging methods focus on enhancing average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during the merging process. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel and low-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP efficiently identifies a Pareto set of scaling coefficients for merging multiple models, reflecting the trade-offs involved. It amortizes the substantial computational cost of evaluations needed to estimate the Pareto front by using quadratic approximation surrogate models derived from a pre-selected set of scaling coefficients. Experimental results on vision and natural language processing tasks demonstrate that MAP can accurately identify the Pareto front, providing practitioners with flexible solutions to balance competing task objectives. We also introduce Bayesian MAP for scenarios with a relatively low number of tasks and Nested MAP for situations with a high number of tasks, further reducing the computational cost of evaluation.",
            "score": 0.48735500748466876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7529296875
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "In addition, the conclusion drawn from a single model family may not generalize to other models. \n\nOther works explore specialized settings, such as temporal merging [13], multilingual merging [1], and domain-specific merging in material science [39]. A recent LLM merging competition [60,82] has also emerged, though its evaluation details remain undisclosed. \n\nMergeBench addresses these limitations by incorporating diverse model families, including Llama-3 and Gemma-2, and evaluating models up to 9B parameters. It focuses on domain-specific tasks beyond conventional NLP benchmarks and includes advanced merging methods. Both the specialized models and the evaluation pipeline are open-sourced, facilitating reproducibility and further research. \n\n6 Discussion and Future Directions \n\nOpportunities for improving merging efficiency. Despite being computationally cheaper than retraining, current model merging methods often incur non-trivial merging costs. Hyperparameter tuning, especially for scaling and sparsity, remains inefficient and largely trial-and-error, limiting the practicality of applying these methods to large-scale models. \n\nMix data or merge models? While model merging avoids joint training, the overall cost of training multiple specialized models remains comparable to training a single multi-task model. Our results show that multi-task models generally achieve stronger in-domain performance, particularly when the tasks are non-conflicting and a balanced data mixture can be constructed. This raises questions about the fundamental limitations of model merging compared to MTL in such settings. Nevertheless, model merging shows clear benefits in low-resource or imbalanced settings, such as fine-grained safety alignment [77] and multilingual language models [1], where data mixing is inherently challenging [14,21]. A deeper understanding of the trade-offs between data mixing and model merging remains an important future direction. \n\nPositioning model merging in LLM Pipelines. Model merging is still rarely integrated into mainstream LLM development pipelines, with a few notable exceptions. For example, Llama-3 employs model soup to average models trained with different hyperparameter settings for improved robustness [12]. Command A [11] applies merging similarly to our setting, combining separately trained specialized models. However, the potential applications of model merging could extend beyond these use cases.",
            "score": 0.4871928769355922,
            "section_title": "Related Works",
            "char_start_offset": 25831,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 99,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1848
                },
                {
                    "start": 1849,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2127
                },
                {
                    "start": 2128,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2368
                },
                {
                    "start": 2369,
                    "end": 2458
                }
            ],
            "ref_mentions": [
                {
                    "start": 246,
                    "end": 250,
                    "matchedPaperCorpusId": "272423492"
                },
                {
                    "start": 289,
                    "end": 292,
                    "matchedPaperCorpusId": "275949902"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "257038048"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.658203125
        },
        {
            "corpus_id": "276580914",
            "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning",
            "text": "Incorporating continual learning (CL) capability into large language models (LLMs) is essential for enabling them to acquire knowledge from diverse tasks sequentially, a critical requirement for adapting to ever-changing environments without extensive retraining (Wang et al., 2024b;Jiang et al., 2024;Yu et al., 2024;Chang et al., 2024). An effective CL system must address two key challenges: (1) Catastrophic Forgetting (CF) (McCloskey and Cohen, 1989), where previously acquired knowledge is lost when learning new tasks, and (2) Knowledge Transfer (KT) (Ke et al., 2021), which involves leveraging new, related tasks to improve performance on prior tasks, and vice versa. \n\nRecently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023;Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024;Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024;Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024).",
            "score": 0.48619466414894885,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 283,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 318,
                    "end": 336,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 428,
                    "end": 455,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 558,
                    "end": 575,
                    "matchedPaperCorpusId": "244908578"
                },
                {
                    "start": 770,
                    "end": 789,
                    "matchedPaperCorpusId": "258833488"
                },
                {
                    "start": 1201,
                    "end": 1219,
                    "matchedPaperCorpusId": "270703371"
                },
                {
                    "start": 1504,
                    "end": 1522,
                    "matchedPaperCorpusId": "271915471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56103515625
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "Large Language Models (LLMs) have demonstrated unparalleled capability in a diverse array of natural language tasks, encompassing commonsense reasoning, question answering, and specialized domains such as mathematics and programming [39,43,52]. The effectiveness of LLMs is based on the scaling law, which posits that proportionally increasing model and training data size leads to enhanced model performance [27]. Nevertheless, the computation overhead and data requirement surge as LLM continues to scale. With the widespread of open-sourced general or specialized LLMs, aggregating existing models to construct a more versatile LLM emerges as an economical alternative to training a larger LLM from scratch [13,16,54]. This not only mitigates the computation cost but also leverages the collective advancements of previous efforts in building LLMs. Within different methods to combine existing LLMs, a major class is merging [2,4,22,24,35,59,63,64]. Model merging combines multiple models into a single one of the same size through weight-space transformation. Wortsman et al. [59] first propose to merge a few fine-tuned models as a training trick for the flat loss-landscape, and Ilharco et al. [22] extends it to multi-task scenario, both of which employ the simple averaging. Other works propose more complicated merging methods, leveraging weight sparsity [63,64] and non-uniform coefficient [4,35]. However, they assume that all candidate models are \"useful\" when merging. While this may hold for small-sized designed model collections, it may not be the case in real-world scenarios given a large and divergent model zoo. How to ensure the benefits of merging different model zoo sizes and similarities, and exclude \"harmful\" candidates, remains underexplored. Since merging is limited to the same model structures and initial weights, another alternative is Mixture-of-Experts (MoE) [16]. MoE is a conditional computation architecture that activates only a subset of model parameters for each specific input example [47]. MoE LLMs have already demonstrated performance and computational efficiency advantages over their dense counterparts [15,25,30,68].",
            "score": 0.48602971872608264,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2164
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "267061245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "273482155",
            "title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery",
            "text": "In this section, we briefly introduce the related work, with a more comprehensive discussion provided in Appendix VII. \n\nRecent studies have attempted to achieve MTL using model merging techniques [32,22,40,41,33]. However, the performance of simple parameter averaging [34,42] degrades significantly as the number of tasks increases, resulting in a substantial gap between its performance and that of traditional MTL model. Many recent works have made various attempts to fill this gap [36,21,29,24,43,25,23,44,26,33]. The first type of method explores how to better weigh and combine multiple models [19,45,40]. For example, Fisher-Merging [19] performs weighted merging utilizing the importance of each parameter through the Fisher information matrix [46]. RegMean [24] reweights and linearly combines rows in weight matrices based on statistics from training data. AdaMerging [26] leverages unlabeled test data to automatically learn a set of task-level or layer-level model merging coefficients. The second type of method explores how to merge models in sparse subspaces to reduce interference [47,48,30,49,50,31,51]. For example, Ties-Merging [21] removes the smaller magnitude parameters and eliminates the issue of parameter sign conflicts during model merging. DARE [29] removes a large number of useless neuron updates and then scales the neurons for merging. Concrete [52] finds a shared subspace between multiple tasks for model merging. The third type of method dynamically merges multiple expert modules during inference [53,54,55,56,57]. For example, WEMoE [53] dynamically merges linear layers by routing, and static merges nonlinear layers. It should be noted that the dynamic merging method requires additional maintenance of more model parameters than the first two categories of methods, and it also reduces the inference efficiency. \n\nWhile existing methods predominantly concentrate on merging in weight space, they often neglect a crucial concern stemming from weight merging-the representation bias. A substantial disparity emerges in the representation space between the merged model and individually trained expert models. In contrast, our method addresses this gap, aiming to minimize the representation discrepancy.",
            "score": 0.4855278159765708,
            "section_title": "II. RELATED WORKS",
            "char_start_offset": 7245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1853
                },
                {
                    "start": 1856,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2148
                },
                {
                    "start": 2149,
                    "end": 2243
                }
            ],
            "ref_mentions": [
                {
                    "start": 274,
                    "end": 277,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 500,
                    "end": 503,
                    "matchedPaperCorpusId": "258352277"
                },
                {
                    "start": 754,
                    "end": 758,
                    "matchedPaperCorpusId": "15354499"
                },
                {
                    "start": 1099,
                    "end": 1103,
                    "matchedPaperCorpusId": "256826987"
                },
                {
                    "start": 1115,
                    "end": 1118,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1535,
                    "end": 1539,
                    "matchedPaperCorpusId": "267365047"
                },
                {
                    "start": 1539,
                    "end": 1542,
                    "matchedPaperCorpusId": "267547973"
                },
                {
                    "start": 1572,
                    "end": 1576,
                    "matchedPaperCorpusId": "267365047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60205078125
        },
        {
            "corpus_id": "269148434",
            "title": "MaxFusion: Plug&Play Multi-Modal Generation in Text-to-Image Diffusion Models",
            "text": "Recent advancements in Large Language Model (LLM) research have introduced the innovative concept of model merging [6,24,40,45,46], a process where weights from multiple pre-trained models for the same task are combined.This technique aims to create a superior model that outperforms the individual contributors.RegMean [13] pioneered a method for merging models trained on diverse datasets for identical tasks, enhancing overall performance.This method relies on a validation set to derive a closedform solution to a least squares regression problem, maintaining the merged model's weights in close alignment with that of the original models.Fisher merging [24] assigns varying degrees of importance to different model weights by utilizing the Fisher Information Matrix.Task Arithmetic [12] views the discrepancy between pre-trained and fine-tuned weights as task vectors, averaging these weights across tasks to forge a combined model.\n\nWhile these techniques show promise for token-based input and output scenarios in language models, they encounter limitations when input modalities shift.Direct application of these methods becomes problematic with changes in input modalities, explaining their rare utilization in multi-task conditioning for vision tasks.Specifically, in structural conditioning tasks where features vary significantly across modalities, simplistic averaging mechanisms like task merging prove inadequate, often resulting in the generation of multiple undesired artifacts in multi-task conditioned images.Recent efforts, such as GitRebasin [1], have explored fusing models trained on distinct tasks by aligning their weights.However, subsequent analyses, including work by Repair [14], have highlighted the emergence of a 'feature forgetting' issue, where signals from certain tasks weaken in the model's deeper layers.This challenge renders such strategies ineffective for integrating diffusion model weights across modalities for multi-modal generation, as it risks neglecting some conditions.\n\nTherefore, there exists a pressing need for a merging strategy capable of facilitating out-of-domain fusion for diffusion models.A potential solution is the framework proposed by Zipit [38], which maintains two parallel sets of model weights and merges them based on input features.Inspired by this approach, our work seeks to implement a similar strategy for effective model integration.\n\n3 Proposed Method",
            "score": 0.4839087175958341,
            "section_title": "Model Merging",
            "char_start_offset": 8137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 220,
                    "end": 312
                },
                {
                    "start": 312,
                    "end": 442
                },
                {
                    "start": 442,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 937
                },
                {
                    "start": 939,
                    "end": 1093
                },
                {
                    "start": 1093,
                    "end": 1261
                },
                {
                    "start": 1261,
                    "end": 1528
                },
                {
                    "start": 1528,
                    "end": 1648
                },
                {
                    "start": 1648,
                    "end": 1842
                },
                {
                    "start": 1842,
                    "end": 2018
                },
                {
                    "start": 2020,
                    "end": 2149
                },
                {
                    "start": 2149,
                    "end": 2302
                },
                {
                    "start": 2302,
                    "end": 2408
                },
                {
                    "start": 2410,
                    "end": 2427
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 124,
                    "end": 127,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 658,
                    "end": 662,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "276409347",
            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
            "text": "We propose 1bit-Merging, a novel framework that effectively combines specialized language models while addressing the fundamental trade-off between performance and storage efficiency. By incorporating dynamic routing with binary quantization, our approach maintains task-specific exper-tise while significantly reducing storage overhead. \n\nExtensive experiments across general knowledge, mathematical reasoning, and code generation tasks demonstrate that 1bit-Merging not only preserves the specialized capabilities of individual models but often enhances their performance.",
            "score": 0.4824178195175888,
            "section_title": "Conclusion",
            "char_start_offset": 21957,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 574
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88037109375
        },
        {
            "corpus_id": "277043311",
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "text": "Merging typically aims for efficiency by creating a single, cohesive model (Singh and Jaggi, 2020), whereas fusion often combines multiple models to enhance quality, potentially at the expense of speed (Ravaut et al., 2022b;Jiang et al., 2023b). \n\nWithin the merging domain, weighted-average techniques-refined by methods like Hessianbased estimates (Daheim et al., 2023) or pruningenhanced Fisher weights (Nathan et al., 2024)-adjust parameter significance but may fail to capture task-specific subtleties, resulting in performance degradation (e.g., a reported 10% drop with basic averaging (Ilharco et al., 2023b)). To counter this, the notion of task vectors, defined as \u03c4 t = \u03b8 ft t \u2212\u03b8 pre (Ilharco et al., 2023b), has gained prominence. These vectors encapsulate task-specific shifts in parameter space, facilitating precise conflict resolution during merging. Building on this, methods like Task Arithmetic (Ilharco et al., 2023b), AdaMerging (Yang et al., 2023), and TIES-Merging (Yadav et al., 2023b) address redundancy and sign discrepancies, improving cross-model compatibility. \n\nResolving parameter conflicts remains a core challenge, inspiring a variety of innovative strategies. Task Arithmetic (Ilharco et al., 2023b) introduced arithmetic-based vector merging, while TIES-Merging (Yadav et al., 2023b) and AdaMerging enhance this by targeting interference sources. Evolutionary methods (Akiba et al., 2024b) blend TIES with optimized inference routes, and practical applications like MetaGPT (Zhou et al., 2024) and LLM evaluators (Kim et al., 2024) showcase realworld efficacy. Alternatively, ZipIt (Stoica et al., 2024) retains correlated parameters while preserving distinct layers, offering adaptability. Additional advancements, such as geometric weight analysis (Shoemake, 1985;Jang et al., 2024) and safety alignment (Hammoud et al., 2024), further enrich the field.",
            "score": 0.48193812126890645,
            "section_title": "C.5 LLM Model Fusion",
            "char_start_offset": 44495,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1089
                },
                {
                    "start": 1092,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 244,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 593,
                    "end": 616,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 695,
                    "end": 718,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 914,
                    "end": 937,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 988,
                    "end": 1009,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1210,
                    "end": 1233,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1297,
                    "end": 1318,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "276574910",
            "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks (Brown et al., 2020;OpenAI, 2024;Cai et al., 2024;Touvron et al., 2023;Bai et al., 2023). Al-technique that combines parameters from multiple fine-tuned models into a unified model, has emerged as a promising solution. \n\nPrevious research has shown that merging methods can lead to safety-utility conflicts, where improvements in general ability (e.g., mathematical reasoning) degrade safety safeguards (Hammoud et al., 2024). For instance, merging safety-aligned and math-specific fine-tuned models get an unsafe mathematical AI expert (left conversation in Fig. 1a), reducing safety capabilities by over 30%, as shown in Fig. 1b. To address this problem, additional alignment training has been employed to improve the safety capabilities of the merged model (Thakkar et al., 2024;Aakanksha et al., 2024). However, such consequential safety-specific training requires labeled data and training costs, limiting their applicability in privacy-sensitive or resource-constrained scenarios. More critically, these methods address symptoms rather than root causes-they neither analyze neuron-level conflicts nor resolve interference mechanisms. \n\nThe safety-utility conflicts stem from two fundamental limitations in existing methods: (i) Neuron misidentification: Previous merging methods rely on simplistic metrics like parameter magnitude to select neurons, failing to distinguish safety-related regions from LLMs and impair safety capacity (ii) Neuron interference: Neurons optimized for different tasks (e.g., safety and code generation) exhibit antagonistic updates during merging, causing destructive parameter collisions and severely reduced performance, as shown in Fig. 1b, and Fig. 1c. \n\nIn this paper, we propose LED-Merging, a simple and effective merging method to address the above problems. Specifically, LED-Merging has three steps, including Location, Election, and Disjoint Merging. For the Location, LED-Merging identifies critical neurons in both base and fine-tuned models using gradient-based attribution scores to avoid neuron misidentification.",
            "score": 0.4808399140205074,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2156
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82568359375
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "Model merging [26,40,69,70,76] uses arithmetic operations on model parameters to combine the strengths of multiple models. It efficiently produces a single model with multi-task capabilities without necessitating joint training on data across all tasks. This significantly saves storage and maintenance costs compared with deploying multiple finetuned models independently. Moreover, model merging enables asynchronous development of model capabilities [11], allowing different teams to independently apply the most suitable optimization strategies for their target tasks. For instance, reasoning capabilities can be enhanced with RL tuning [56], while instruction following benefits from preference learning [43]. Those optimization procedures are non-trivial to integrate directly, and post-hoc merging provides a viable solution. \n\nDespite recent progress in model merging algorithms [22,26,29,40,66,69,73,79], existing evaluations [59,61,74] remain constrained in two critical dimensions: model size and task scale, making it difficult to quantify and compare the performance of different merging methods in real-world applications. On the model side, most evaluations rely on relatively small language models, such as GPT-2 (124M) [49], RoBERTa-base (125M) [37] and mT5 (2.85B) [51]. These choices inherently constrain the complexity and capability of the merged models, making it unclear whether Starting from open-source base models (Llama and Gemma), we perform task-specific post-training on five diverse domains: mathematics, coding, multilinguality, instruction following, and safety. This process produces five task-specialized models that perform well on their respective domains but likely poorly on others. We then apply a range of model merging algorithms to combine these specialized models into a single multi-task model. MergeBench evaluates the effectiveness of these merging approaches along three key dimensions: multi-task performance, retention of pretrained knowledge (forgetting), and runtime efficiency. \n\nobserved trends generalize to modern, large-scale language models. On the task side, evaluations typically focus on conventional NLP benchmarks such as sentiment classification and natural language inference. These tasks are narrow in scope, often solvable via shallow pattern recognition or memorization.",
            "score": 0.48043287118995354,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2030
                },
                {
                    "start": 2033,
                    "end": 2099
                },
                {
                    "start": 2100,
                    "end": 2241
                },
                {
                    "start": 2242,
                    "end": 2338
                }
            ],
            "ref_mentions": [
                {
                    "start": 14,
                    "end": 18,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 18,
                    "end": 21,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 21,
                    "end": 24,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 24,
                    "end": 27,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 709,
                    "end": 713,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 887,
                    "end": 891,
                    "matchedPaperCorpusId": "271957310"
                },
                {
                    "start": 891,
                    "end": 894,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 897,
                    "end": 900,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 900,
                    "end": 903,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 903,
                    "end": 906,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 909,
                    "end": 912,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1283,
                    "end": 1287,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74267578125
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "Weight interpolation has proven to be a simple yet effective strategy for scalable model merging, requiring minimal computational overhead [12,17,13,42,14,15,19,43,44]. A comprehensive discussion on model merging is available in [11]. However, achieving successful model merging remains challenging due to the potential for task interference. \n\nSeveral advanced methods have been proposed to address performance degradation in model merging. (1) Some approaches emphasize that the choice of merging coefficients is crucial to final performance, leading to the development of various strategies for optimizing these weights [17,18,19]. However, these methods typically use static merging weights, whereas this paper introduces a dynamic approach to weight assignment. ( 2) Some works argue that neural networks are over-parameterized, with many neurons being ineffective for downstream tasks and potentially causing task conflicts and interference; thus, they propose merging in a subspace [15,16,20,21,22,23]. In contrast, this paper differs by applying sparsification specifically to the MLP modules rather than the entire parameter space. Moreover, while previous methods perform static merging, this paper combines sparse task vectors through dynamic merging. We discuss more model merging works in the Appendix VII.",
            "score": 0.4797179144253806,
            "section_title": "V. RELATED WORK",
            "char_start_offset": 32768,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1262
                },
                {
                    "start": 1263,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 161,
                    "end": 164,
                    "matchedPaperCorpusId": "258352277"
                },
                {
                    "start": 1002,
                    "end": 1005,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1005,
                    "end": 1008,
                    "matchedPaperCorpusId": "270067773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32177734375
        },
        {
            "corpus_id": "276961298",
            "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
            "text": "In our approach, we combine several fine-tuned models into a single unified model to reduce storage and deployment costs while maintaining high task performance. Although basic methods such as parameter averaging (e.g., [14]), Fisher-weighted merging [22], and task arithmetic [14] have been explored, our work focuses on the TIES-MERGING framework [33] that particular fits our multi-task scenario. \n\nRather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference. The final merged parameters are obtained by \n\nwhere the coefficients \u03bb t are determined by the alignment and pruning process. By combining representation alignment with targeted pruning, TIES-MERGING effectively leverages shared knowledge across tasks while maintaining nuanced task-specific distinctions. This makes our method particularly well-suited for applications such as automated scoring systems, where both accuracy and efficiency are critical.",
            "score": 0.47961550425292243,
            "section_title": "Model Merging",
            "char_start_offset": 8245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 1095
                },
                {
                    "start": 1098,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1377
                },
                {
                    "start": 1380,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1787
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94189453125
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "key question remains -how does it compare to traditional data mixing and weighting approaches? In this paper, we explore whether model merging can effectively balance safety and overall performance and how it compares to data mixing techniques, particularly for multilingual alignment. \n\nWe evaluate these trade-offs under severe multi-task constraints -optimizing for general and safe performance in a multilingual setting. The inherent difficulties of handling multiple languages, each with its unique linguistic structures, cultural nuances, and potential biases, present a formidable task in establishing alignment for these models [Schwartz et al., 2022;Kotek et al., 2023;Khandelwal et al., 2023;Vashishtha et al., 2023;Khondaker et al., 2023;\u00dcst\u00fcn et al., 2024;Aryabumi et al., 2024;Singh et al., 2024]. Mitigating harm across multiple languages is critical given the wide adoption of large models across the world. However, a common issue in safety and alignment work is the narrow focus on addressing safety primarily for English. And so, the challenges are compounded in this scenario by the trivial amount of safety data available across different languages [Singh et al., 2024]. However, it is precisely because of these severe constraints that this presents an interesting setting to thoroughly evaluate the benefits of merging. \n\nWe conduct an exhaustive study to compare traditional approaches for balancing multi-objective training by curating and varying an expansive set of training data mixtures with approaches that merge model checkpoints trained on different subsets of data. Our large-scale evaluation is across six languages from five different language families and encompasses both finetuning and preference training across four different merging techniques. Through our comprehensive experimental setup, we summarize the key findings and contributions of our work as follows: \n\n1. Merging outperforms mixing. We find that model merging is more effective than weighting data mixtures for achieving a good balance between safety and generalizability in language models. The top-performing methods for individual objectives were TIES, which reduced harm by 10.4%, and Linear merging, which improved general performance by 8.6% beyond the data mixing approach.",
            "score": 0.4787584189180725,
            "section_title": "Introduction",
            "char_start_offset": 1862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 285
                },
                {
                    "start": 288,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2283
                }
            ],
            "ref_mentions": [
                {
                    "start": 659,
                    "end": 678,
                    "matchedPaperCorpusId": "261276445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "These tasks are narrow in scope, often solvable via shallow pattern recognition or memorization. As such, they fail to surface the generalization, compositionality and interference challenges that arise when merging stronger and more specialized models for real-world applications. \n\nTo address the limitations of existing model merging evaluations, we introduce MergeBench, a scalable and comprehensive benchmark designed to rigorously assess merging performance, illustrated in Figure 1. First, MergeBench improves model selection by adopting state-of-the-art, open-source language models as base models. Specifically, we include both pretrained and instruction-tuned versions of Llama-3.2-3B, Llama-3.1-8B [17], Gemma-2-2B, and Gemma-2-9B [62], resulting in a total of eight base models. Second, we construct a more challenging and representative task suite for evaluating merged models. Each base model is further finetuned on one of five carefully selected task categories, including instruction following, mathematics, multilingual understanding, coding and safety, to produce specialized models with minimal skill overlap 1 . By standardizing the finetuning and evaluation procedures, MergeBench ensures a fair and reproducible platform for comparing model merging algorithms. In addition to multi-task performance, MergeBench evaluates retention of pretrained generalization through forgetting analysis and reports runtime efficiency, offering a comprehensive view of both utility and computational cost of existing merging algorithms. \n\nOur extensive experiments reveal that model merging tends to perform better on stronger base models, and techniques such as scaling coefficient tuning and sparsification help preserve pretrained knowledge, often improving generalization compared to multi-task models. However, the computational cost of the merging process is non-trivial, leaving room for further optimization. In addition, when tasks are non-conflicting and relatively balanced, multi-task models still achieve stronger in-domain performance. The broader role of model merging in standard LLM training pipelines also remains underexplored. We hope MergeBench provides a foundation for future work to advance the understanding and practical adoption of model merging.",
            "score": 0.47799320827561553,
            "section_title": "Introduction",
            "char_start_offset": 2257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73779296875
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Model merging has emerged as a promising approach for integrating knowledge across different models without retraining. Early techniques like Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) introduced weight averaging to enhance generalization during the final stages of training. This concept was further advanced by approaches like SWAD (Cha et al., 2021) and Ensemble of Averages (EoA) (Arpit et al., 2022). Additionally, Ilharco et al. (2023a) empirically demonstrate that parameter averaging can effectively combine knowledge from models trained on diverse tasks. Based on this, Fisher-Merging (Matena & Raffel, 2022) applies the Fisher information matrix (Fisher, 1925) to weigh the averaging process. Similarly, RegMean (Jin et al., 2023) formulates merging as an optimization problem: minimizing the distance between the merged model and individuals in the parameter space. \n\nAs a milestone work, Task Arithmetic (Ilharco et al., 2023b) introduces the concept of \"task vectors\"-parameter offsets of finetuned models from a pretrained model as the origin. By scaling and combining these vectors, Task Arithmetic effectively balances the task-general and task-specific knowledge and achieves notable enhancement in model merging. Building on this idea, Ties-Merging (Yadav et al., 2023) and PCBMerging (DU et al., 2024) enhance the process by removing components in task vectors with negligible magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) integrate Task Arithmetic with LoRA modules (Hu et al., 2022) to enhance flexibility. Additionally, STA (Zhou et al., 2025) randomly shuffle network layers to eliminate knowledge conflict. Some advanced approaches incorporate test-time training techniques to further improve merging performance. AdaMerging (Yang et al., 2024b) trains layer-specific merging coefficients. Additionally, representation surgery (Yang et al., 2024a) and MoE router (Tang et al., 2024) train an extra adapter to align representations. Since test-time adaptation requires additional training and samples, their application may be limited.",
            "score": 0.47778333416834784,
            "section_title": "Multi-Task Learning through Model Merging",
            "char_start_offset": 5166,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1989
                },
                {
                    "start": 1990,
                    "end": 2092
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 199,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 349,
                    "end": 367,
                    "matchedPaperCorpusId": "235367622"
                },
                {
                    "start": 399,
                    "end": 419,
                    "matchedPaperCorpusId": "239049452"
                },
                {
                    "start": 435,
                    "end": 457,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 671,
                    "end": 685,
                    "matchedPaperCorpusId": "62064968"
                },
                {
                    "start": 737,
                    "end": 755,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 931,
                    "end": 954,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1282,
                    "end": 1302,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1318,
                    "end": 1335,
                    "matchedPaperCorpusId": "273098230"
                },
                {
                    "start": 1458,
                    "end": 1475,
                    "matchedPaperCorpusId": "269293160"
                },
                {
                    "start": 1783,
                    "end": 1803,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1885,
                    "end": 1905,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1921,
                    "end": 1940,
                    "matchedPaperCorpusId": "267365047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.783203125
        },
        {
            "corpus_id": "276813206",
            "title": "LEWIS (LayEr WIse Sparsity) - A Training Free Guided Model Merging Approach",
            "text": "As specialized large language models (LLMs) become increasingly prevalent, model merging methods are being used to combine them to create a single multi-task model without requiring any additional data or training. However, these approaches fall short when the objective of merging is to increase the downstream model's performance on a particular task-specific benchmark. In this work, we propose LEWIS (Layer Wise Sparsity), a guided model-merging framework that uses activation-based layer importance to dynamically adjust layer-wise task-vector sparsity required for the merge process. LEWIS uses a calibration dataset to prioritize critical layers during the task-vector pruning process required for model merging. This approach guides existing merging methods by preserving essential layer-wise task-specific knowledge while ensuring the merged model performs the best at benchmarks resembling the calibration dataset. Our experiments demonstrate the effectiveness of LEWIS with performance improvements of code instruction-following and math-solving models created through model merging up to 4 percent and 11.3 percent, respectively, outperforming unguided data-less model merging approaches that use uniform-sparsity.",
            "score": 0.47762870567439525,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85595703125
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "Multi-task learning (MTL) [4] enables models to handle multiple tasks efficiently by facilitating knowledge transfer. Model merging [22,67,68] emerged as an alternative to conventional MTL [20,28,40,76] by combining independently fine-tuned models into a single model. This approach allows for flexible adaptation, reduces computational overhead, and better preserves task-specific knowledge. A key challenge in model merging is balancing generalization and task specialization. On one hand, a merged model should retain shared knowledge across tasks [72,74], but on the other hand, it must preserve task-specific adaptations that are crucial for individual task performance [21,73]. \n\nEarly methods [23,71,72,74,75] primarily focused on constructing task-agnostic models by averaging weights or aligning parameters across models, assuming that similar models in weight space would naturally form a meaningful shared representation. However, this generalization-first * Co-corresponding author. T a s k A r i t h m e t i c [ICLR `23] RegMean [ICLR `23] T i e s M e r g i n g [NeurIPS `23] MagMax [ECCV `24] AdaMerging [ICLR `24] Consensus TA [ICML `24] Surgery [ICML `24] LiNeS [ICLR `25] Ours strategy often led to a loss of task-specific information, reducing performance on individual tasks. This issue became more pronounced when merging models trained on diverse or conflicting tasks, as simple averaging often failed to preserve specialized adaptations essential for high accuracy. \n\nRecent approaches [21,36,58,73] explored task-specific parameterization, with some incorporating additional components, in some cases, are trainable to enhance adaptation. These methods typically added task-specific modules or learnable merging coefficients, selectively adjusting how different tasks influence the final model. While this enhanced task adaptation, it also introduced challenges such as increased model complexity, higher computational costs in both training and inference, and scalability issues when applied to many tasks. Furthermore, many merging strategies still relied on a fixed shared model [11,21,23,36,58,72,73,75], often formed by merging encoders without adaptation.",
            "score": 0.47757975323992313,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 932
                },
                {
                    "start": 933,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1487
                },
                {
                    "start": 1490,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 29,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 136,
                    "end": 139,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 189,
                    "end": 193,
                    "matchedPaperCorpusId": "261243229"
                },
                {
                    "start": 193,
                    "end": 196,
                    "matchedPaperCorpusId": "4800342"
                },
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 199,
                    "end": 202,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 555,
                    "end": 558,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 679,
                    "end": 682,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 700,
                    "end": 704,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 704,
                    "end": 707,
                    "matchedPaperCorpusId": "268248251"
                },
                {
                    "start": 707,
                    "end": 710,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 710,
                    "end": 713,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 713,
                    "end": 716,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1508,
                    "end": 1512,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1512,
                    "end": 1515,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 1515,
                    "end": 1518,
                    "matchedPaperCorpusId": "267365047"
                },
                {
                    "start": 1518,
                    "end": 1521,
                    "matchedPaperCorpusId": "267412030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55224609375
        },
        {
            "corpus_id": "273404154",
            "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "text": "Merging methods. The traditional approach to learning multiple skills/tasks simultaneously is joint training on a mixture of task datasets [7]. Over the years, several works have improved this multi-task learning approach [41,53,48,20,38,9,48]. As data collection for specialized tasks, and training large models from scratch get more expensive; coupled with the rapid expansion in the availability of well-trained open-source models -model merging has emerged as a convenient way of building powerful models from existing ones [17,32]. The richly studied simplest way of merging by averaging model weights [57,52,16,26] paved the way to linear weight averaging [60]. Some methods like Fisher Merging [39] and RegMean [27] need training data based pre-computations to measure individual parameter importance but these are highly memory and data intensive. Expanding on weight averaging, Task Arithmetic [25] involving the creation and combination of task vectors facilitated multi-task learning. While this weight interpolation was heavily used for merging image generation models, recent methods like TIES [67] and DARE [2] reset redundant parameters, resolve sign conflicts, and exclusively merge parameters that exhibit sign-consistency, and SLERP [59] by spherical linear interpolation build upon this for language models. In all these methods, the coefficients governing the model merging are determined by trial-error; while works in the vision domain demonstrate the pivotal role played by these coefficients [68]. In contrast to the cumbersome grid-searching, our method CAT learns these coefficients layer-wise with access to very few examples in the natural language domain. \n\nLoRA merging methods. PEM composition [73] adopts the task arithmetic framework to incorporate the merging of LoRAs. Recently, the vision community witnessed the widespread application LoRAs [5,14,37,43,62,75,69] as an effective approach to multi-task learning and composing styles and subjects [49]. Many of these utilize Mixture of Experts (MoE) [5,14,37,62] based architectures having input-dependent learnable routers. These models have been primarily used in the context of multitask learning.",
            "score": 0.4771728233906244,
            "section_title": "Related Work",
            "char_start_offset": 4820,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1684
                },
                {
                    "start": 1687,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2109
                },
                {
                    "start": 2110,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 139,
                    "end": 142,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 226,
                    "end": 229,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 229,
                    "end": 232,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 232,
                    "end": 235,
                    "matchedPaperCorpusId": "219259832"
                },
                {
                    "start": 235,
                    "end": 238,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 240,
                    "end": 243,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 611,
                    "end": 614,
                    "matchedPaperCorpusId": "263858956"
                },
                {
                    "start": 614,
                    "end": 617,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 662,
                    "end": 666,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 701,
                    "end": 705,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1107,
                    "end": 1111,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1725,
                    "end": 1729,
                    "matchedPaperCorpusId": "259262373"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "270620432",
            "title": "Model Merging and Safety Alignment: One Bad Model Spoils the Bunch",
            "text": "\u2022 We propose a safety-aware merging pipeline that achieves greater alignment of the merged model without sacrificing its accuracy.(Khanov et al., 2024) addressing decoding, FIGA (Guo et al., 2024) for token-level signals, and f-DPO (Wang et al., 2024) for efficient alignment.Zhao et al. (2023) designed GPO to consider different interest groups.Some method enhance generalization (Zheng et al., 2024), while Dai et al. (2024) proposed Safe RLHF, for separate alignment on helpfulness and harmlessness.In SALMON (Sun et al., 2024), they use synthetic data to reduce human supervision.Although these may be effective, we show that model merging can mitigate the effects of alignment procedures.Importantly, Inan et al. (2023) addressed the need for effective input-output safeguarding in conversational AI with Llama Guard, employing a safety risk taxonomy and ad hoc models to classify safety concerns in text.\n\nModel Merging Techniques for merging multiple models have been proposed as efficient ways to benefit from the capabilities of multiple LLMs without retraining or accessing the original datasets.In Model Soups (Wortsman et al., 2022), they first propose to combine models with weight averaging, showing improved performance compared to a single model.Ilharco et al. (2023) build on this by performing task arithmetics, i.e. element-wise operations on model parameters to edit their behavior towards specific tasks.Similar alternatives are RegMean (Jin et al., 2023), andFisher Merging (Matena andRaffel, 2022).Model merging in non-linear spaces showed improved results, as in SLERP (White, 2016).Some, such as TIES (Yadav et al., 2024) and DARE (Yu et al., 2024), propose methods to improve model merging, focusing on sparsification.Similarly, Model Breadcrumbs (Davari and Belilovsky, 2023) exploits sparse masks for better combination.Importantly, some extend merging capabilities across multiple modalities (Sung et al., 2023).",
            "score": 0.47599456016456443,
            "section_title": "Introduction",
            "char_start_offset": 4220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 130,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 693
                },
                {
                    "start": 693,
                    "end": 910
                },
                {
                    "start": 912,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1521
                },
                {
                    "start": 1521,
                    "end": 1607
                },
                {
                    "start": 1607,
                    "end": 1744
                },
                {
                    "start": 1744,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 196,
                    "matchedPaperCorpusId": "265043685"
                },
                {
                    "start": 232,
                    "end": 251,
                    "matchedPaperCorpusId": "263142109"
                },
                {
                    "start": 381,
                    "end": 401,
                    "matchedPaperCorpusId": "264289051"
                },
                {
                    "start": 409,
                    "end": 426,
                    "matchedPaperCorpusId": "264306078"
                },
                {
                    "start": 512,
                    "end": 530,
                    "matchedPaperCorpusId": "263831633"
                },
                {
                    "start": 706,
                    "end": 724,
                    "matchedPaperCorpusId": "265308865"
                },
                {
                    "start": 1121,
                    "end": 1144,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1262,
                    "end": 1283,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1458,
                    "end": 1481,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1626,
                    "end": 1646,
                    "matchedPaperCorpusId": "259991144"
                },
                {
                    "start": 1656,
                    "end": 1673,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.349365234375
        },
        {
            "corpus_id": "271903078",
            "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
            "text": "Merging models involves combining the parameters of two or more specialized models to create a unified multi-task model [20]. Recent work has demonstrated that merging Llama2-7b-chat, a general-purpose chat model, with Meditron-7b, specialized for the medical domain, resulted in a merged model that outperformed its constituent models across both general and medical benchmarks [9]. The increasing number of merged models on the Open LLM leaderboard [38] further proves the success of applying this approach to various benchmarks. Several studies have proposed different merging techniques, such as weight-space averaging [27], Fisher-Weighted averaging [20], Git Re-Basin [21], TIES-Merging [28], and DARE [22], to efficiently merge models trained on distinct domains. Considering the trend of scaling models to billions of parameters and the cost of fully fine-tuning these models, there is an increasing need to explore the feasibility of merging multiple adapters within a single language model when adapters are used to fine-tune the models. Works such as Adaptersoup [23] have investigated this approach for both in-domain and out-of-domain evaluation of mixed adapters. Another study has conducted a comprehensive analysis of the effectiveness of different adapter architectures for the indomain generalizability of merged adapters [24]. In this work, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods.",
            "score": 0.4757357502733871,
            "section_title": "C. Merging",
            "char_start_offset": 7852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1714
                }
            ],
            "ref_mentions": [
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 623,
                    "end": 627,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 655,
                    "end": 659,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 674,
                    "end": 678,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 693,
                    "end": 697,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1074,
                    "end": 1078,
                    "matchedPaperCorpusId": "256846453"
                },
                {
                    "start": 1425,
                    "end": 1429,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1444,
                    "end": 1448,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "273403613",
            "title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
            "text": "Model Fusion emphasizes integrating language models with diverse architectures to boost their collective capabilities. The Composition to Augment Language Models (CALM) (Bansal et al., 2024), utilizes cross-attention mechanisms to blend representations from various models, harmonizing different neural network structures to capitalize on their combined strengths. Similarly, FUSELLM (Wan et al., 2024a) focuses on aligning and merging the probabilistic distributions of source large language models to enrich the knowledge of the fused model. Extending to chat-based models, FUSECHAT (Wan et al., 2024b) introduces a twostage fuse-and-merge framework: initially, it conducts pairwise knowledge fusion of source chat models to create several target models with uniform structures and sizes; subsequently, it combines these models in the parameter space. Despite these advancements, there is a notable gap in detailed analysis of unconstrained model merging, particularly in reasoning tasks for LLMs, with few efforts addressing the fusion of models with heterogeneous architectures. \n\n3 Modeling",
            "score": 0.47475109677784655,
            "section_title": "Model Fusion",
            "char_start_offset": 7077,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1095
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 190,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58056640625
        },
        {
            "corpus_id": "276928602",
            "title": "Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors",
            "text": "In this paper, we demonstrate that the task vectors constitute an approximate linear subspace of the corresponding input for the linear layer. Therefore, we can minimize the interference among expert models guided by the task vector. Based on the insight, we propose WUDI-Merging, a simple yet effective model merging method that resolves interferences without requiring any additional data or rescaling coefficients. Extensive empirical evaluations demonstrate the effectiveness of our method. We believe that this work is one step toward a simple and general-purpose data-free model merging technique. Further research may consider a more detailed way to resolve the conflict based on our method, which has the potential to reach the performance of multitask learning in the data-free model merging paradigm.",
            "score": 0.47462133367637127,
            "section_title": "Conclusion",
            "char_start_offset": 21047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.477783203125
        },
        {
            "corpus_id": "276409315",
            "title": "Be Cautious When Merging Unfamiliar LLMs: A Phishing Model Capable of Stealing Privacy",
            "text": "Let \u03b8 pre denote a weight of pre-trained LLM, \u03b8 1 , \u03b8 2 , ..., \u03b8 n denote n LLMs fine-tuned on different tasks and \u03b8 mer denote the merged model. \u03c4 i = \u03b8 i \u2212 \u03b8 pre is the task vector and \u03bb i \u2208 (0, 1) refers to the ith model's merging coefficient. In this paper, we consider several state-of-the-art model merging techniques as below. \n\nSimple Averaging (Wortsman et al., 2022) is a straightforward approach to model merging, where the element-wise weights of LLMs are simply averaged. It can be formulated as \u03b8 mer = n i=1 \u03bb i * \u03b8 i . Task Arithmetic (Ilharco et al., 2022;Ortiz-Jimenez et al., 2023) is a more standard model merging approach that proposed working with the task vectors \u03c4 i . These task vectors are averaged with the merging coefficient and then added to the pre-trained LLM. It can be formulated as \u03b8 mer = \u03b8 pre + n i=1 \u03bb i * \u03c4 i TIES (Yadav et al., 2024) is a recent model merging approach that addresses the issue of interference between tasks when merging models. This approach improves Task Arithmetic by removing the smaller magnitude weights and only averaging the weights with the same sign. It can be formulated as \u03b8 mer = \u03b8 pre + n i=1 \u03bb i * \u03d5(\u03c4 i ), where \u03d5(\u2022) is a function to filter smaller and diff-signs values. DARE (Yu et al., 2024) an advancement model merging approach that randomly masks model parameters with mask matrix M i \u223c Bernoulli(p) to mitigate task conflicts, where p is the probability. It can be formulated as",
            "score": 0.4742987749915113,
            "section_title": "A.1 Model Merging",
            "char_start_offset": 23920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 333
                },
                {
                    "start": 336,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1458
                }
            ],
            "ref_mentions": [
                {
                    "start": 353,
                    "end": 376,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 854,
                    "end": 873,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1250,
                    "end": 1267,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6650390625
        },
        {
            "corpus_id": "276259373",
            "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
            "text": "Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular \\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model merging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release our models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for further investigations.",
            "score": 0.47363097710665125,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8720703125
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "Model merging. Model merging aims at efficiently integrating multiple finetuned models into a single model that retains the capabilities of each. This approach enhances the efficiency, generalization and multi-task capabilities of finetuned models. In scenarios where models are trained on the same task with different training configurations, Singh & Jaggi (2020); Ainsworth et al. (2022);Jolicoeur-Martineau et al. (2024) show that merged models perform comparably to ensemble models but with significantly lower deployment costs. Additionally, Wortsman et al. (2022a;b) demonstrate that the merged model improves the out-of-distribution (OOD) robustness. When merging finetuned models from different tasks, the merged model can provide better initialization for new tasks (Choshen et al., 2022;Gueta et al., 2023). Finetuned models with different specialized skills can also be combined to enhance multi-task capabilities (Ilharco et al., 2023;Tam et al., 2023;Matena & Raffel, 2022;Jin et al., 2022;Yang et al., 2023;Yu et al., 2023;Wang et al., 2024b;a). More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020;Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem. \n\nOur approach stands out with four key advantages: i) Localized merging: Instead of global merging, we localize merging to specific regions with finetuned skills, effectively decreasing task conflicts. ii) Simplified process: Existing works often require computationally intensive grid search or optimization to determine the optimal merging coefficients, while our stitching procedure does not have the requirement.",
            "score": 0.47277779325507385,
            "section_title": "Related works",
            "char_start_offset": 30230,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 423,
                    "matchedPaperCorpusId": "257984998"
                },
                {
                    "start": 547,
                    "end": 570,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 925,
                    "end": 947,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 964,
                    "end": 986,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1037,
                    "end": 1056,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1490,
                    "end": 1507,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1507,
                    "end": 1524,
                    "matchedPaperCorpusId": "239998731"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "263608674",
            "title": "BYOM: Building Your Own Multi-Task Model For Free",
            "text": "As discussed in the Introduction and Figure 1, existing merging methods suffer a large performance gap compared with the Single-Task method. In this section, we introduce the task interference issue and conduct empirical analysis to show that interference cannot be resolved by simply merging without task-specific knowledge. \n\nIn merging models, task interference means merging one task-specific model can negatively impact the performance of other tasks. One consequence of task interference is the overall performance of a merged model is much worse than using multiple task-specific models. \n\nTask interference is more serious when merging more tasks. We conduct an experiment to study the performance of existing merging methods when varying the number of tasks being merged. For a merging method A, we normalize its accuracy on each task by the accuracy on the task-specific model \u03b8 t and report the average normalized accuracy, i.e.,",
            "score": 0.4725336856278085,
            "section_title": "Task Interference in Merging Models",
            "char_start_offset": 8125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 325
                },
                {
                    "start": 328,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "273098230",
            "title": "Parameter Competition Balancing for Model Merging",
            "text": "This section provides a detailed baseline description. Our experiments encompass seven comparison methods: \n\n\u2022 Individual means that each task uses an independent fine-tuned model, which has no interference between tasks, but cannot perform multiple tasks simultaneously. \u2022 Traditional MTL collects the original training data of all tasks together to train a multi-task model. It can be used as a reference upper bound for model merging \u2022 Weight Averaging is the simplest method of model merging, which directly averages the parameters of multiple models using \u03b8 m = n t=1 \u03b8 t /n, calculating the element-wise mean of all individual models. It can be used as a lower bound for model merging. [8,86]. \u2022 Fisher Merging [46] calculates the Fisher information matrix [17] Ft = E x\u223cDt E y\u223cp \u03b8 t (y|x) \u2207 \u03b8t (log p \u03b8t (y|x t )) 2 to measure the importance of each parameter when merging models for task t, where and model merging is performed according to the guidance of this importance. \u2022 RegMean [30] imposes a constraint when merging models, that is, the L 2 distance between the merged model's and the individual models' activations. It computes a least-squares solution as \n\n, where X t is the input activation of the corresponding layer. \n\n\u2022 Task Arithmetic [28] first defines the concept of \"task vectors\" and merges these vectors into a pre-trained model to execute multi-task learning. The model is produced by scaling and adding the task vectors to the initial model as \u03b8 m = \u03b8 init + \u03bb * n t=1 \u03c4 t . \u2022 Ties-Merging [89] further solves the task conflict problem in Task Arithmetic [28]. It eliminates redundant parameters and resolves symbol conflicts through three steps: Trim, Elect Sign, and Disjoint Merge. \u2022 AdaMerging automatically learns a merging coefficient for each layer of each task vector in Task Arithmetic [28]. \u2022 LoraHub [25] employs Low-rank Adaptations to dynamically combine task-specific modules for cross-task generalization, and adapts to new tasks by configuring \u03b8 \u2032 = K k=1 w k \u2022 \u03b8 k .",
            "score": 0.4721395624702984,
            "section_title": "E Baseline details",
            "char_start_offset": 37140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1171
                },
                {
                    "start": 1174,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2013
                }
            ],
            "ref_mentions": [
                {
                    "start": 695,
                    "end": 698,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 992,
                    "end": 996,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1520,
                    "end": 1524,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1825,
                    "end": 1829,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "276574910",
            "title": "LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint",
            "text": "Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: \\textbf{neuron misidentification} due to simplistic parameter magnitude-based selection, and \\textbf{cross-task neuron interference} during merging. To address these challenges, we propose \\textbf{LED-Merging}, a three-stage framework that \\textbf{L}ocates task-specific neurons via gradient-based attribution, dynamically \\textbf{E}lects critical neurons through multi-model importance fusion, and \\textbf{D}isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging reduces harmful response rates(\\emph{e.g.}, a 31.4\\% decrease on Llama-3-8B-Instruct on HarmBench) while preserving 95\\% of utility performance(\\emph{e.g.}, 52.39\\% accuracy on GSM8K). LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs.",
            "score": 0.47184878095542715,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "273025960",
            "title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models",
            "text": "Given that here, model souping appears to be susceptible to negative interference when merging, we additionally evaluate TIES-merging (Yadav et al., 2023) for Swahili. This merging method resolves interference between experts by sequentially trimming conflicting values, determining the sign of each weight, and then merging the resulting weights. In addition, we present an alternative for Swahili where, instead of post hoc model merging, we mix the language and math datasets and do a single fine-tuning run with samples from both. Instead of training two experts, each on 30-40k samples, the joint SFT is over the union (80k samples). Identical to the training of experts, we launch many SFT runs with different hyperparameters and select the best three checkpoints. \n\nTable 2: MGSM 8-shot results of layer swapping for Swahili in more detail and with two additional comparisons, TIES-merging and dataset merging. We display the minimum performance in Swahili, as well as the average across all 9 languages in MGSM and in English. Table 2 displays these more detailed results for Swahili. We find that TIES-merging, similar to model souping, consistently underperforms compared to layer swapping. For mixing the Swahili and math data prior to fine-tuning, we are able to achieve results comparable to layer swapping on average, with the average MGSM score for this joint SFT being less than one point lower. How-ever, the maximum performance of these checkpoints lagged the best layer-swapped pairs by 4.4 percentage points. This means that the ceiling for cross-lingual transfer is significantly higher with this methodology than simply mixing datasets together. This is significant because our method of merging two variants fine-tuned on separate datasets proves to be more effective than an extended fine-tuning on the combined datasets, consistent with Aakanksha et al. (2024). \n\nWe note that by swapping in layers from the Swahili expert, MGSM performance in English and other non-Swahili languages decreases from the math expert. This is expected given that we optimize for a different language, yet the decrease is relatively small nevertheless.",
            "score": 0.4699371392040328,
            "section_title": "FURTHER EVALUATIONS ON SWAHILI",
            "char_start_offset": 24108,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2157
                }
            ],
            "ref_mentions": [
                {
                    "start": 134,
                    "end": 154,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "By merging models, AI systems can combine the distinct strengths of separate language models, achieving a balance between multiple capabilities without requiring substantial retraining. However, the integration process can be intricate due to differences in training methods and fine-tuning, typically necessitating specialized knowledge and repeated refinement. This paper explores model merging techniques across a spectrum of complexity, examining where automated methods like evolutionary strategies stand compared to hyperparameter-driven approaches such as DARE, TIES-Merging and simpler methods like Model Soups. In addition, we introduce Differentiable Adaptive Merging (DAM), an efficient, adaptive merging approach as an alternative to evolutionary merging that optimizes model integration through scaling coefficients, minimizing computational demands. Our findings reveal that even simple averaging methods, like Model Soups, perform competitively when model similarity is high, underscoring each technique's unique strengths and limitations. We open-sourced DAM, including the implementation code and experiment pipeline, on GitHub: https://github.com/arcee-ai/DAM.",
            "score": 0.46966191195142326,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73974609375
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "The goal of multi-task learning (MTL) is to enable a single model to perform multiple tasks simultaneously, thereby facilitating knowledge transfer between these tasks [20,123,162,172,218,228]. As shown in Figure 1(c), to avoid the high cost of joint training, a straightforward approach is to merge multiple independently trained models on different tasks to accomplish MTL. Almost all of the model merging methods discussed in \u00a72.3 can be used to merge multiple models trained on different tasks to perform MTL. In this section, we take some representative tasks as examples. For MTL tasks in computer vision, Task Arithmetic [71], Ties-Merging [216], AdaMerging [220] and other studies [176,219,223] proposed to combine ViT models trained on different visual classification tasks, and the obtained model can complete the object classification of multiple tasks. The results of Task Arithmetic [71] demonstrate that merging independently trained models from any two datasets yields a merged model whose performance is comparable to that of a singletask model. Similarly, ZipIt [170], which merges ResNet architectures trained on different tasks, achieves comparable results. For MTL tasks in natural language processing, DARE [226] introduces a method to assimilate homologous models, augmenting LLMs as a \"free lunch\". For instance, merging WizardLM with WizardMath significantly boosts WizardLM's performance on GSM8K (a benchmark for evaluating the mathematical reasoning ability of LLMs) from 2.2 to 66.3. Akiba et al. [6] suggest that directly merging an LLM with mathematical capabilities and an LLM with Japanese language proficiency results in a model capable of solving Japanese mathematical problems. Furthermore, numerous studies have demonstrated that combining PEFT modules (such as Adapter, LoRA or Soft-prompts) trained on different tasks can also achieve MTL [11,180,234].",
            "score": 0.46959232881199575,
            "section_title": "Knowledge Transfer in Multi-Task Learning",
            "char_start_offset": 58669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1511
                },
                {
                    "start": 1512,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 168,
                    "end": 172,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 172,
                    "end": 176,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 180,
                    "end": 184,
                    "matchedPaperCorpusId": "208513386"
                },
                {
                    "start": 184,
                    "end": 188,
                    "matchedPaperCorpusId": "254043876"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1228,
                    "end": 1233,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1905517578125
        },
        {
            "corpus_id": "272968955",
            "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
            "text": "Recent advancements in large pretrained models have demonstrated remarkable performance and strong generalization abilities across various domains, such as natural language processing (Chang et al. 2024;Zhou et al. 2024b;Wu et al. 2024) and computer vision (Wang et al. 2024;Radford et al. 2021). Open-source communities have provided many pretrained models for various data types, as well as finetuned versions tailored to specific tasks. However, large finetuning models is often a complex process that requires vast amounts of high-quality data and computational resources. To address the challenge of building foundational models capable of handling diverse tasks under limited computational resources, model merging has gained increasing attention (Jang, Yun, and Han 2024). Model merging leverages existing pretrained models to flexibly transfer and inte-grate knowledge without requiring the original training data or additional model training (White 2016). This approach enables the creation of new models with higher generalization capabilities, suited to multiple tasks and scenarios. In recent years, model merging has become a simple yet popular method in pretrained models development, as illustrated in Fig. 1(a), with merged models showing significant potential on the Open LLM leaderboard (Myrzakhan, Bsharat, and Shen 2024). Current model merging methods primarily focus on merging models with the same architecture in the parameter space. In recent years, research in the parameter space has become quite extensive, including approaches like weight averaging (Matena and Raffel 2022), Model Soup (Wortsman et al. 2022), Ties merging (Yadav et al. 2024), and DARE (Drop And REscale) method (Yu et al. 2024). \n\nHowever, focusing solely on merging models within the parameter space significantly limits their practical utility. Models with different architectures exhibit broader diversity in representation capabilities and task types (Mellor et al. 2021;Zoph et al. 2018), potentially expanding the performance boundaries of merged models beyond those of a single architecture. Nevertheless, merging models across different architectures presents several practical challenges (Dong et al. 2021), leading to limited research in this area.",
            "score": 0.4695616873886382,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1724
                },
                {
                    "start": 1727,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2254
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 203,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 257,
                    "end": 275,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 275,
                    "end": 294,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 1651,
                    "end": 1670,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1971,
                    "end": 1988,
                    "matchedPaperCorpusId": "12227989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408203125
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "As shown in Figure 2 (lower part), model merging can be applied to a variety of foundation models, including large language models, multimodal large language models, and visual generative models. For example, model merging in large language models can help mitigate untruthfulness and toxicity output, accomplish knowledge unlearning, and speed up training. Moreover, model merging also arises in different machine learning subfields, such as continual learning, multi-task/multi-domain learning, few-shot learning, and other subfields, to solve a variety of challenges. For instance, in continual learning, model merging can mitigate catastrophic forgetting of old tasks. In multi-task learning, multi-objective learning and multidomain learning, it facilitates knowledge transfer. Additionally, in adversarial learning, model merging can be employed for both attack and defense strategies. \n\nThird, what are the remaining challenges and future research opportunities for model merging? Despite the advancements in merging methods and their well-developed applications, there are still numerous open challenges and future research directions in the field ( \u00a75). For example, as the number of tasks increases, the performance gap between existing methods and independent expert models becomes signifi-Figure 2: The taxonomy of model merging in machine learning. This general framework covers advanced model merging methods and theories (top part), as well as practical applications of model merging techniques to foundation models and more than 10 machine learning subfields (bottom part). cantly larger. Additionally, current model merging methods incur enormous memory costs during merging and lack trust guarantees as well as in-depth theoretical analysis. Addressing these gaps will require substantial efforts from researchers to further advance the flourishing development of this field. \n\nTo summarize, the main contributions of this paper include the following three aspects: \n\n\u2022 Methodology Overview: We provide a comprehensive summary of the technical aspects of model merging. Specifically, we propose a new taxonomy that divides existing model merging methods into two stages and further subdivides the methods in each stage according to key techniques. Additionally, we discuss theoretical analysis work related to model merging. \n\n\u2022 Application Overview: We offer a comprehensive summary of the application aspects of model merging. Specifically, we explore the application of model merging to foundation models and 10+ machine learning subfields, demonstrating how model merging can address existing challenges in these areas.",
            "score": 0.4678019565120687,
            "section_title": "Introduction",
            "char_start_offset": 4479,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1893
                },
                {
                    "start": 1896,
                    "end": 1983
                },
                {
                    "start": 1986,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2265
                },
                {
                    "start": 2266,
                    "end": 2342
                },
                {
                    "start": 2345,
                    "end": 2446
                },
                {
                    "start": 2447,
                    "end": 2641
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43701171875
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Fine-tuning pretrained foundation models has become a standard paradigm for addressing downstream applications (Brown et al., 2020). However, as the number of applications grows, managing and deploying numerous finetuned models introduces significant costs and operational complexity. To address this issue, multi-task model merging has emerged as a promising solution by consolidating multiple While masking low-magnitude components (horizontal axis, Task 2) mitigates some interference, high-magnitude dimensions (vertical axis, Task 1) can still induce significant conflicts. In contrast, our proposed CAT Merging proactively identifies and trims highconflict components, ensuring a more stable model merging. \n\nexpert models into a single unified model without additional training (Matena & Raffel, 2022). \n\nA notable advancement in model merging is Task Arithmetic (Ilharco et al., 2023b), which introduces the concept of task vectors-defined as the difference vector between pretrained and finetuned models in the parameter space. Task Arithmetic demonstrates that task-specific knowledge can be effectively integrated into the pretrained model through simple arithmetic operations, such as model merging by adding task vectors to the pretrained parameters. Despite its effectiveness, the technique is vulnerable to performance degradation caused by knowledge conflict (Sun et al., 2025;Ortiz-Jimenez et al., 2023), or the imbalance and contradiction among the accumulated task vectors. \n\nTo address knowledge conflict, prior research has explored enhancing Task Arithmetic by trimming unimportant components within task vectors. One typical importance metric is the parameter magnitude. For instance, Ties-Merging (Yadav et al., 2023) trims small-magnitude elements in task vectors. PCBMerging (DU et al., 2024) refines this importance measure by considering inter-task correlations. Similarly, Twin-Merging (Lu et al., 2024) applies singular value decomposition to task vectors; leveraging singular values as the importance metric, it retains components corresponding to large singular values. \n\nDespite the significant progress made by magnitude-based methods, they often overlook the risk of merging highmagnitude components that may overwrite task-specific knowledge during integration.",
            "score": 0.4675671299614632,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 284
                },
                {
                    "start": 285,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 712
                },
                {
                    "start": 715,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1492
                },
                {
                    "start": 1495,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2101
                },
                {
                    "start": 2104,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 111,
                    "end": 131,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 870,
                    "end": 893,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1393,
                    "end": 1420,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1721,
                    "end": 1741,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1915,
                    "end": 1932,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "273098230",
            "title": "Parameter Competition Balancing for Model Merging",
            "text": "While fine-tuning pretrained models has become common practice, these models often underperform outside their specific domains. Recently developed model merging techniques enable the direct integration of multiple models, each fine-tuned for distinct tasks, into a single model. This strategy promotes multitasking capabilities without requiring retraining on the original datasets. However, existing methods fall short in addressing potential conflicts and complex correlations between tasks, especially in parameter-level adjustments, posing a challenge in effectively balancing parameter competition across various tasks. This paper introduces an innovative technique named PCB-Merging (Parameter Competition Balancing), a lightweight and training-free technique that adjusts the coefficients of each parameter for effective model merging. PCB-Merging employs intra-balancing to gauge parameter significance within individual tasks and inter-balancing to assess parameter similarities across different tasks. Parameters with low importance scores are dropped, and the remaining ones are rescaled to form the final merged model. We assessed our approach in diverse merging scenarios, including cross-task, cross-domain, and cross-training configurations, as well as out-of-domain generalization. The experimental results reveal that our approach achieves substantial performance enhancements across multiple modalities, domains, model sizes, number of tasks, fine-tuning forms, and large language models, outperforming existing model merging methods. The code is publicly available at: \\url{https://github.com/duguodong7/pcb-merging}.",
            "score": 0.46632156761599663,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94921875
        },
        {
            "corpus_id": "266053657",
            "title": "Merging by Matching Models in Task Parameter Subspaces",
            "text": "The widespread fine-tuning of public pre-trained models has produced a huge number of specialized models. These specialized models may be trained on different tasks, where a \"task\" is simply the input-output relationship that we aim to train a model to perform (e.g. sentiment analysis of text, object recognition in images, etc.). Alternatively, the Stable Diffusion XL model (Podell et al., 2023) forms the basis of over a thousand specialized image generation models on the Hugging Face Model Hub that are specialized to different styles or content types. How can we recycle these specialized models to create better base models (Choshen et al., 2022;Ram\u00e9 et al., 2022)? Model merging (Wortsman et al., 2022b;Matena & Raffel, 2022) aims to tackle this problem by combining specialized models into a single model that retains the individual models' capabilities. A common example application of merging is constructing a multitask model from individual-task models, which is the primary application we explore in our paper. Compared to multitask learning, merging does not require simultaneous access to the individual-task datasets. Compared to outputspace ensembling of M models, merging produces a model that is M times cheaper to run. \n\nWhile merging via simple parameter averaging can work well for models that share an architecture and initialization (McMahan et al., 2017;Stich, 2018), recent merging methods improve over simple averaging by considering parameter importance (Matena & Raffel, 2022), matching activations (Jin et al., 2022), omitting the contribution of the pre-trained model (Ilharco et al., 2022), or resolving interference across models (Yadav et al., 2023). \n\nIn our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task.",
            "score": 0.4653588087788896,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1240
                },
                {
                    "start": 1243,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 712,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 712,
                    "end": 734,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1359,
                    "end": 1381,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 1484,
                    "end": 1507,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8388671875
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "The increasing prevalence of open-source models trained on diverse tasks provides unprecedented opportunities to utilize pre-trained weights for various applications. However, access to the original training data is often restricted due to privacy concerns, proprietary limitations, or other constraints, posing significant challenges for tasks requiring cross-domain capabilities [Jin et al., 2022]. Model merging techniques address this issue by enabling the combination of model weights without relying on original data, thereby equipping models with the ability to handle multiple tasks effectively. Currently, model merging has emerged as a promising solution, enabling the combination of multiple models with similar architectures to harness complementary strengths. This approach not only enhances task-specific performance but also fosters greater adaptability across tasks [Yang et al., 2024a;Tam et al., 2024]. \n\nModel merging provides several key advantages [Yang et al., 2024a;Yu et al., 2024b;Zhao et al., 2024]. Firstly, it allows for the aggregation of knowledge across multiple models without requiring extensive retraining, thereby offering a more resource-efficient alternative to traditional fine-tuning and transfer learning. Additionally, model merging can mitigate issues like catastrophic forgetting and offers a pathway to create models that encapsulate the strengths of multiple training regimes. For instance, weight average [Wortsman et al., 2022;Choshen et al., 2022] and task arithmetic merging [Ilharco et al., 2022] are widely adopted methods for retaining model capabilities while maintaining robustness across varied domains. \n\nIn recent years, model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements. These advancements underscore the potential of model merging as a versatile solution, capable of adapting to a range of tasks and minimizing resource demands.",
            "score": 0.4644058763540075,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2406
                }
            ],
            "ref_mentions": [
                {
                    "start": 969,
                    "end": 989,
                    "matchedPaperCorpusId": "267499590"
                },
                {
                    "start": 1006,
                    "end": 1024,
                    "matchedPaperCorpusId": "259937385"
                },
                {
                    "start": 1524,
                    "end": 1546,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.583984375
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "In this paper, we propose a novel trust-region criterion for model merging, Task Arithmetic in the Trust Region (TATR), which addresses the knowledge conflict problem. The trust region contains dimensions in the model parameter space that cause only small changes in the task-specific losses. When merging models, only the components of task vectors in the trust region are added to the pre-trained weights; other dimensions are discarded. TATR can be used independently or jointly with other techniques like Ties-Merging (Yadav et al., 2023), AdaMerging (Yang et al., 2024b), and Surgery (Yang et al., 2024a). (Tang et al., 2020;Ma et al., 2018), sparsification (Ding et al., 2021;Sun et al., 2020;Liu et al., 2019), and soft parameter sharing (Gao et al., 2020;Hazimeh et al., 2021). Other strategies focus on optimizing task interactions, such as adjusting task-specific loss weights (Sener & Koltun, 2018;Liu et al., 2019;2022;Hu et al., 2023;Chen et al., 2022a), resolving gradient direction conflicts (Yu et al., 2020;Chen et al., 2020;Liu et al., 2021a;Javaloy & Valera, 2022;Navon et al., 2022), or preventing the dominance of certain tasks (Chen et al., 2018;He et al., 2022;Yang et al., 2023). \n\nTraditional MTL are not well-suited for merging foundation models. First, retraining these models using vast amounts of data incurs significant computational costs. Large-scale foundation models are already resource-intensive, and training them with multi-task objectives further amplifies these demands, requiring immense computation and time. Additionally, retraining from scratch wastes valuable knowledge optimized in each individual expert model. These considerations have driven the development of model merging as an alternative to multi-task learning.",
            "score": 0.46301997009914153,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2209,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1765
                }
            ],
            "ref_mentions": [
                {
                    "start": 522,
                    "end": 542,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 555,
                    "end": 575,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 589,
                    "end": 609,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 611,
                    "end": 630,
                    "matchedPaperCorpusId": "221784966"
                },
                {
                    "start": 630,
                    "end": 646,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 663,
                    "end": 682,
                    "matchedPaperCorpusId": "235792414"
                },
                {
                    "start": 699,
                    "end": 716,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "214727852"
                },
                {
                    "start": 763,
                    "end": 784,
                    "matchedPaperCorpusId": "235358484"
                },
                {
                    "start": 887,
                    "end": 909,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 909,
                    "end": 926,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 931,
                    "end": 947,
                    "matchedPaperCorpusId": "261243229"
                },
                {
                    "start": 947,
                    "end": 966,
                    "matchedPaperCorpusId": "235253907"
                },
                {
                    "start": 1007,
                    "end": 1024,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1024,
                    "end": 1042,
                    "matchedPaperCorpusId": "222341884"
                },
                {
                    "start": 1042,
                    "end": 1060,
                    "matchedPaperCorpusId": "239998731"
                },
                {
                    "start": 1083,
                    "end": 1102,
                    "matchedPaperCorpusId": "246473259"
                },
                {
                    "start": 1168,
                    "end": 1184,
                    "matchedPaperCorpusId": "247446582"
                },
                {
                    "start": 1184,
                    "end": 1202,
                    "matchedPaperCorpusId": "254043876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8525390625
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "As the demand for versatile and powerful AI systems grows, the need to merge Large Language Models (LLMs) with specialized capabilities, such as multilingual skills or domain-specific knowledge, has become increasingly pressing. Effective model merging enables systems to leverage the unique strengths of individual models without necessitating extensive retraining. Merging also offers the potential to reduce catastrophic forgetting, a significant advantage in maintaining learned knowledge from each model (Sukhbaatar et al., 2024;Siriwardhana et al., 2024;Labrak et al., 2024). However, model merging remains inherently complex due to differences in training and fine-tuning processes, often requiring deep expertise and iterative tuning to achieve a balanced integration of the models' contributions. \n\nModel merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2024) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale. To gain deeper insight into the strengths and weaknesses of these approaches, we performed an in-depth comparative analysis of model merging techniques, spanning from basic averaging methods to more sophisticated automated approaches. \n\nBuilding on these insights, we introduce Differentiable Adaptive Merging (DAM), a new approach developed as a more efficient alternative to compute-heavy evolutionary strategies.",
            "score": 0.46178917923116386,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 1018,
                    "end": 1041,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1085,
                    "end": 1105,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86279296875
        },
        {
            "corpus_id": "271039884",
            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "text": "Model Merging Model merging is a promising way to combine the abilities of multiple models. Pioneering works explore strategies to find the best weights for averaging (Choshen et al., 2022;Wortsman et al., 2022;Matena and Raffel, 2022;Jin et al., 2022). Task Arithmetic (Ilharco et al., 2022) employs task vectors, enabling control through arithmetic operations to steer the merged model's behavior. TIES (Yadav et al., 2023) further addresses the problem of information loss by handling parameter conflict more carefully. DARE (Yu et al., 2023) zeros out redundant parameters and amplifies the remaining ones. Evolutionary Model Merge (Akiba et al., 2024) automatically discovers optimal model combinations through evolutionary algorithms. \n\nThere is little discussion of model merging in the context of multilinguality. Instead, previous works attempt to introduce language-specific and task-specific modular adapters (Pfeiffer et al., 2020;Parovic et al., 2023;Parovi\u0107 et al., 2024;Zhao et al., 2024), which require additional training. These works focus on high-resource languages and specific tasks. In contrast, model merging can utilize existing models without additional training, making it a versatile approach for building more general task-solving LLMs. Besides, we are the first to study model merging for low-resource languages.",
            "score": 0.46157410929752263,
            "section_title": "Related Works",
            "char_start_offset": 4327,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1341
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 211,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 211,
                    "end": 235,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 235,
                    "end": 252,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 270,
                    "end": 292,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 528,
                    "end": 545,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 920,
                    "end": 943,
                    "matchedPaperCorpusId": "218470133"
                },
                {
                    "start": 943,
                    "end": 964,
                    "matchedPaperCorpusId": "259075252"
                },
                {
                    "start": 964,
                    "end": 985,
                    "matchedPaperCorpusId": "268417117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61572265625
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "Model merging provides a scalable alternative to multi-task training by combining specialized finetuned models through parameter arithmetic, enabling efficient deployment without the need for joint training or access to all task data. While recent methods have shown promise, existing evaluations are limited in both model scale and task diversity, leaving open questions about their applicability to large, domain-specialized LLMs. To tackle the challenges, we introduce MergeBench, a comprehensive evaluation suite designed to assess model merging at scale. MergeBench builds on state-of-the-art open-source language models, including Llama and Gemma families at 2B to 9B scales, and covers five key domains: instruction following, mathematics, multilingual understanding, coding and safety. We standardize finetuning and evaluation protocols, and assess eight representative merging methods across multi-task performance, forgetting and runtime efficiency. Based on extensive experiments, we provide practical guidelines for algorithm selection and share insights showing that model merging tends to perform better on stronger base models, with techniques such as merging coefficient tuning and sparsification improving knowledge retention. However, several challenges remain, including the computational cost on large models, the gap for in-domain performance compared to multi-task models, and the underexplored role of model merging in standard LLM training pipelines. We hope MergeBench provides a foundation for future research to advance the understanding and practical application of model merging. We open source our code at \\href{https://github.com/uiuctml/MergeBench}{https://github.com/uiuctml/MergeBench}.",
            "score": 0.4594498851944958,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "276409347",
            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
            "text": "Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers-enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that \\texttt{1bit}-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.",
            "score": 0.4593163932843843,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9404296875
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "In this study, we explored the potential of model merging methods to enhance the integration of technical vocabulary in language models, particularly focusing on cross-lingual knowledge transfer. Our findings reveal that while model merging can facilitate knowledge transfer at a mono-lingual level, as evidenced by satisfactory performance on general benchmarks, it struggles with the effective acquisition and integration of technical terminology across languages. The merged models' performance remained similar to that of the non-expert Japanese model, with a tendency towards decline, highlighting the challenges of incorporating specialized vocabulary. \n\nThe results underscore the complexity of achieving effective cross-lingual knowledge transfer, particularly in domains requiring precise technical language comprehension. The observed performance suggests that current merging methods may introduce complexities that hinder the integration of domain-specific terminology. Despite these challenges, the study provides valuable insights into the strengths and limitations of model merging, offering a foundation for future research aimed at developing more sophisticated methods for domain adaptation and cross-lingual knowledge transfer. \n\nResearch should then focus on refining existing merging methods to better handle technical vocabulary and explore alternative strategies that enhance the integration of specialized knowledge without compromising the models' general capabilities. By addressing these challenges, we can advance the development of more versatile and capable language models suitable for specialized applications across diverse linguistic contexts. \n\nWhile this study provides valuable insights into the potential of model merging for cross-lingual technical vocabulary acquisition, several limitations must be acknowledged. Firstly, the reliance on judge LLMs for evaluating definition accuracy introduces a degree of uncertainty, as these models may not fully capture the nuances of human judgment. Secondly, the study's focus on the Japanese-English language pair and the medical domain may limit the generalizability of the findings. The significant linguistic differences between these languages likely influence the results, and effectiveness could vary with other language pairs or domains. Thirdly, the investigation is limited to six specific model merging methods, and exploring alternative approaches could reveal more effective strategies. Finally, the minimal impact of hyperparameter tuning suggests robustness, but further optimization could potentially yield improved results. Addressing these limitations is crucial for future research to achieve a comprehensive and detailed understanding of these mechanisms and to develop new, more performant techniques.",
            "score": 0.45862092277833,
            "section_title": "CONCLUSION",
            "char_start_offset": 23524,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1246
                },
                {
                    "start": 1249,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2166
                },
                {
                    "start": 2167,
                    "end": 2326
                },
                {
                    "start": 2327,
                    "end": 2480
                },
                {
                    "start": 2481,
                    "end": 2621
                },
                {
                    "start": 2622,
                    "end": 2803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67578125
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "This type of model merging not only considers weight magnitudes to address conflicts and interference but also decomposes the weights to analyze both their magnitude and direction. This approach further mitigates conflicts between models. Below, we introduce representative methods. \n\nPCB-Masks: This method introduces task-specific binary masks to improve model merging and compression. Binary masks are generated for each task using task vectors from fine-tuned models, highlighting parameters important for that task while ignoring irrelevant ones. During merging, the method eliminates \"selfish weights\" (important to one task only) and \"catastrophic weights\" (irrelevant to all tasks), preserving only shared parameters that benefit multiple tasks [Wang et al., 2024]. \n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024]. \n\nWIDEN(Weight Disentanglement ): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]. Additionally, it employs a Softmax-based score calibration to adaptively balance the contributions of different models, ensuring that the merged model retains and optimally integrates their abili-ties (Table1). \n\nFREE-Merging: It is a novel model merging approach that leverages Fourier transform-based filtering and lightweight expert modules [Zheng and Wang, 2024]. It mitigates task conflicts by applying high-pass filtering in the frequency domain, removing low-frequency signals that reduce generalization while preserving essential model structures.",
            "score": 0.45832550820554707,
            "section_title": "Decomposing weight type",
            "char_start_offset": 19865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1276
                },
                {
                    "start": 1279,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2337
                }
            ],
            "ref_mentions": [
                {
                    "start": 753,
                    "end": 772,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1255,
                    "end": 1275,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1762,
                    "end": 1780,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2126,
                    "end": 2148,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90185546875
        },
        {
            "corpus_id": "267759980",
            "title": "OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data",
            "text": "We also propose an ensemble learning approach for performance improvement, utilizing fusion strategies at both the model level and the inference level. \n\nModel fusion. MergeKit1 is a toolkit designed for merging trained language models. We carefully selected a few high-accuracy models and utilized MergeKit to perform model fusion using the SLERP (Shoemake, 1985), TIES (Yadav et al., 2023) and linear (Wortsman et al., 2022) methods. Traditionally, model merging often resorts to weight averaging which, although straightforward, might not always capture the intricate features of the models being merged. The SLERP technique addresses this limitation, producing a blended model with characteristics smoothly interpolated from both parent models, ensuring the resultant model captures the essence of both its parents. Meanwhile, the TIES method is proposed to resolve interference issues by resetting parameters, resolving sign conflicts, and merging only compatible parameters. TIES outperforms many existing methods across diverse settings, emphasizing the importance of addressing interference in model merging for enhanced performance and versatility. \n\nModel Voting. In addition to the model-level fusion, we also explored fusion at the probability level of model generation, which can be understood as a form of model voting. We selected another group of highly accurate candidate models and performed linear fusion at the probability level. Specifically, we calculate the weighted summation of the probability values on \u00ebxisting hallucinationpredicted by different candidate models for different tasks. By tuning the linear weight combination, we are able to determine the optimal combination of weights for each task. Finally, combining different tasks together yields the final fusion result. In this way, we implement weighted voting of models at the inference result level.",
            "score": 0.45749148409794094,
            "section_title": "Ensemble Learning",
            "char_start_offset": 7732,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 154,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1157
                },
                {
                    "start": 1160,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1886
                }
            ],
            "ref_mentions": [
                {
                    "start": 348,
                    "end": 364,
                    "matchedPaperCorpusId": "11290566"
                },
                {
                    "start": 371,
                    "end": 391,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 403,
                    "end": 426,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75927734375
        },
        {
            "corpus_id": "276575899",
            "title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing",
            "text": "However, when generating responses based on newly integrated knowledge, the model may encounter conflicts between the new and outdated knowledge, leading to degraded performance (Li et al., 2024b). Ni et al. (2024) propose a full fine-tuning-based approach that first performs forgetting outdated knowledge before editing the model with new information. However, fine-tuning-based editing is susceptible to overfitting (Cao et al., 2021b), and updating all layers incurs significant memory overhead. Additionally, the gap between the unlearning and editing stages may lead to unintended knowledge distortions. \n\nTo address these issues, we propose Conflictfree Model Editing (CoME), which selectively removes outdated knowledge while simultaneously updating the model with new knowledge. This process mirrors the way the human brain refines arXiv:2502.15826v1 [cs.CL] 20 Feb 2025 its understanding-when we learn new information, the brain selectively weakens outdated or conflicting memories to avoid cognitive interference and confusion (Geiselman et al., 1983;Bjork and Bjork, 1996;Wixted, 2004;Alves and Bueno, 2017;Kliegl and B\u00e4uml, 2021). In a similar manner, CoME identifies parameters associated with outdated knowledge and unlearns them during the integration of new knowledge, thereby reducing knowledge conflicts within the LLM. By performing both steps simultaneously, CoME minimizes unintended knowledge transformations. This process is analogous to how humans enhance cognitive clarity by discarding irrelevant or erroneous memories. Importantly, CoME achieves this without unnecessary loss of linguistic understanding, as we carefully preserve critical language-processing features shared between outdated and new knowledge. Furthermore, we limit the parameter space subject to modification during the unlearning process to minimize unnecessary parameter adjustments. \n\nWe apply CoME to state-of-art model editing methods, including MEMIT (Meng et al., 2023b) and PMET (Li et al., 2024a), which are designed to mitigate overfitting and memory overhead issues in knowledge editing.",
            "score": 0.4574310730001244,
            "section_title": "Introduction",
            "char_start_offset": 1738,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 1038,
                    "end": 1062,
                    "matchedPaperCorpusId": "16317124"
                },
                {
                    "start": 1062,
                    "end": 1084,
                    "matchedPaperCorpusId": "13115556"
                },
                {
                    "start": 1084,
                    "end": 1097,
                    "matchedPaperCorpusId": "3057114"
                },
                {
                    "start": 1097,
                    "end": 1119,
                    "matchedPaperCorpusId": "64695568"
                },
                {
                    "start": 1119,
                    "end": 1142,
                    "matchedPaperCorpusId": "237938563"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1363525390625
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "Our experiments are associated with several baseline approaches. The details of these baselines are as follows: \n\n\u2022 Pretrained directly employs a pretrained model to predict across multiple tasks. Since it does not incorporate any downstream task-specific information during model training, its performance on downstream tasks is typically suboptimal. \n\n\u2022 Individual. In this approach, an independent finetuned model is used for each task. While it avoids interference between tasks, it cannot perform multiple tasks simultaneously. It serves as a reference upper bound for model merging approaches. \n\n\u2022 Traditional MTL aggregates the original training data from all tasks to train a single multi-task model. \n\n\u2022 Weight Averaging directly averages model parameters from multiple tasks into a single model, enabling multi-task learning without additional training. \n\n\u2022 Fisher Merging (Matena & Raffel, 2022) leverages the Fisher information matrix to assess parameter importance, merging model parameters based on this importance. \n\n\u2022 RegMean (Jin et al., 2023) refines weight matrices by adjusting and linearly combining rows, utilizing statistical information derived from the training data. \n\n\u2022 Task Arithmetic (Ilharco et al., 2023b) introduces the concept of a \"task vector,\" defined as the difference between finetuned model parameters and pretrained model parameters. Multiple task vectors are then combined and added to the pretrained model to facilitate multi-task learning. \n\n\u2022 Ties-Merging (Yadav et al., 2023) eliminates unimportant parameters from the task vector and resolves sign conflicts among parameters, reducing interference during the final task vector merging process. \n\n\u2022 TATR (Sun et al., 2025). This method advances task arithmetic by restricting the merging within a trust region to mitigate knowledge conflict. \n\n\u2022 TATR & Ties-Merging (Sun et al., 2025;Yadav et al., 2023). This method combines the trust region restriction in TATR into Ties-Merging to enhance the performance. \n\n\u2022 Consensus Merging (Wang et al., 2024) computing a group of masks for each task vector to minimize the distance in parameter space between the merged model and the finetuned model. \n\n\u2022 PCB Merging (DU et al., 2024) trims components in the task vector that have small magnitudes and are not significantly related to other tasks.",
            "score": 0.4572428359527208,
            "section_title": "B.3. Baselines.",
            "char_start_offset": 30509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 111
                },
                {
                    "start": 114,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 351
                },
                {
                    "start": 354,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 863
                },
                {
                    "start": 866,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2187
                },
                {
                    "start": 2190,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 1042,
                    "end": 1060,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1213,
                    "end": 1236,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1500,
                    "end": 1520,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1879,
                    "end": 1898,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2026,
                    "end": 2045,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 2204,
                    "end": 2221,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7060546875
        },
        {
            "corpus_id": "272831995",
            "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering",
            "text": "A natural approach to performing LoRA merging involves a simple element-wise averaging of the parameters from each LoRA: \u03d5 \u2032 = 1 p p i=1 \u03d5 i . However, parameter interference poses a significant challenge to effective LoRA merging. We identify two potential sources of parameter interference during LoRA merging and demonstrate through experiments that such interference can lead to performance degradation in the merged LoRA. The first cause of parameter interference stems from parameter misalignment in LoRAs, as depicted in the left part of Fig. 2. Accoding to Property 2.1, the MSUs of each LoRA can be permuted arbitrarily without affecting the functionality of the LoRA module. However, misalignment of MSU parameters when merging LoRAs can result in parameter interference. To investigate the impact of parameter misalignment on model performance, we conducted a controlled experiment using the Llama-2-7b model, training LoRAs on different tasks. For the parameters A and B of a task, we randomly generated a permutation matrix P and adjusted the parameters to A \u2032 = (A + P A)/2 and B \u2032 = (B + BP T )/2. This adjustment simulates the merging of two identical LoRAs with misaligned parameters. The results, presented in Tab.1, indicate that parameter misalignment can lead to a decline in model performance, with some tasks experiencing significant performance degradation. Therefore, ideal merging entails alignming MSUs during LoRA merging to mitigate parameter interference. \n\nAnother source of parameter interference stems from knowledge conflict during LoRA merging. As depicted on the right side of Fig. 2, knowledge conflict occurs when the merged LoRA lacks sufficient parameter space to encapsulate the comprehensive knowledge. This deficiency forces the merging of task-specific MSUs, resulting in parameter interference. To investigate the impact of knowledge conflict during LoRA merging, we conducted an experiment to demonstrate the performance degradation resulting from merging task-specific MSUs. With a base LoRA trained on the CoLA task, we adapted this LoRA for two new tasks (MNLI and MRPC) by appending an additional MSU to create two separate task-specific LoRAs. Throughout the training process for the new tasks, only the newly introduced MSU for each task was trainable.",
            "score": 0.4563878726374113,
            "section_title": "PROBLEM FORMULATION AND CHALLENGES",
            "char_start_offset": 9834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "274788575",
            "title": "Revisiting Weight Averaging for Model Merging",
            "text": "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors. Our method demonstrates robust and scalable performance on vision benchmarks across varying numbers of tasks and model sizes. Furthermore, we observe that our approach is applicable to natural language processing tasks with competitive performance.",
            "score": 0.4563258671421564,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "259064039",
            "title": "TIES-Merging: Resolving Interference When Merging Models",
            "text": "RegMean [31] proposed a closed-form solution for the merged model's parameters by solving a local linear regression problem for each individual linear layer in the model. However, this requires transmitting additional data statistics that are the same size as the model and requires additional inference steps to calculate them. Fisher Merging [45] goes beyond simple averaging to identify the importance of individual parameters using Fisher Information Matrix [18,3,34] and uses it to weigh the parameters in each model when merging. However, this shows little gains when merging multiple checkpoints and also requires computing gradients which has a high memory cost. Task Arithmetic [29] presented a method for merging models by generating task vectors and performing arithmetic operations, such as addition, to obtain a multitask checkpoint. A concurrent work by Ortiz-Jim\u00e9nez et al. [51] provided theoretical insights on model merging based on the weight disentanglement property that arises during pretraining. They showed that finetuning models in their tangent space enhance this property, leading to better-merged models. Our method follows these past works on model merging but additionally takes into account the interference between different parameters during merging. Figure 3: Performance depends on a small fraction of high-magnitude parameters. For each task vector, we keep only the largest -top-k% parameters and plot the average performance across eleven tasks. Keeping only the top-20% of the parameter does not degrade the performance. We plot the fraction of parameters that have a sign conflict after trimming versus the number of models being merged.",
            "score": 0.4559010562875965,
            "section_title": "Related Work",
            "char_start_offset": 7893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1676
                }
            ],
            "ref_mentions": [
                {
                    "start": 8,
                    "end": 12,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 344,
                    "end": 348,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 466,
                    "end": 468,
                    "matchedPaperCorpusId": "7589577"
                },
                {
                    "start": 468,
                    "end": 471,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 687,
                    "end": 691,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 889,
                    "end": 893,
                    "matchedPaperCorpusId": "258832777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.467041015625
        },
        {
            "corpus_id": "273228908",
            "title": "Efficient Few-shot Learning for Multi-label Classification of Scientific Documents with Many Classes",
            "text": "Model fusion, which involves the integration of capabilities from different models, can be mainly divided into two categories. Firstly, ensemble approaches combine the output of multiple models to enhance the overall prediction performance (Littlestone and Warmuth, 1994;Sagi and Rokach, 2018). Outputs are typically combined by weight averaging (Littlestone and Warmuth, 1994) or majority voting (Monteith et al., 2011). These ensemble approaches can improve the prediction performance of large-scale language models (Jiang et al., 2023). Secondly, weight merging approaches enable model fusion at the parameter level. Wortsman et al. (2022) show that weight averaging of multiple models fine-tuned with different hyperparameters improves prediction accuracy and robustness. Task vectors derived from model weights can be modified and combined together through arithmetic operations to steer the behavior of a resulting model (Ilharco et al., 2023). This approach can be enhanced by trimming task vectors and resolving sign conflicts before merging them (Yadav et al., 2023). In addition, Drop And Rescale (DARE) can be used as a general preprocessing technique for existing model merging methods to merge multiple task-specific fine-tuned models into a single model with diverse abilities (Yu et al., 2023).",
            "score": 0.4540085664750361,
            "section_title": "Model Fusion",
            "char_start_offset": 6437,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1309
                }
            ],
            "ref_mentions": [
                {
                    "start": 240,
                    "end": 271,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 271,
                    "end": 293,
                    "matchedPaperCorpusId": "49291826"
                },
                {
                    "start": 346,
                    "end": 377,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 518,
                    "end": 538,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 620,
                    "end": 642,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 927,
                    "end": 949,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1055,
                    "end": 1075,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65283203125
        },
        {
            "corpus_id": "277043311",
            "title": "RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs",
            "text": "Model merging (Lu et al., 2024a) has become a pivotal technique in the realm of LLMs, enabling the integration of strengths from multiple pre-trained or fine-tuned models to boost performance, adaptability, and efficiency. This approach can be broadly divided into two paradigms: zero-shot merging, which fuses models without further training, and merge-then-train, which involves refining the combined model after integration. \n\nEarly zero-shot techniques, such as weight averaging (Nagarajan and Kolter, 2021; Wortsman et al., 2022) and Linear Mode Connectivity, laid the groundwork, evolving into more advanced methods like Task Arithmetic (Ilharco et al., 2023a)-where task vectors steer parameter adjustments-and TIES (Yadav et al., 2023a), which reduces interference through trimming and conflict resolution. Recent innovations, including DARE (Yu et al., 2024) and Evolutionary Model Merge (Akiba et al., 2024a), further refine this by optimizing selective parameters or inference pathways, all without additional training. On the other hand, merge-thentrain strategies, such as Fisher Merging (Matena and Raffel, 2022), utilize the Fisher information matrix to assign parameter weights, while Reg-Mean (Jin et al., 2023) fine-tunes linear merging on a per-layer basis, carefully balancing embeddings and biases. However, both approaches encounter difficulties when merging models with divergent initializations, prompting research into permutation symmetries (Ainsworth et al., 2022;Verma and Elbayad, 2024) to better align parameters. A notable distinction exists between model merging and model fusion. Merging typically aims for efficiency by creating a single, cohesive model (Singh and Jaggi, 2020), whereas fusion often combines multiple models to enhance quality, potentially at the expense of speed (Ravaut et al., 2022b;Jiang et al., 2023b).",
            "score": 0.4525600035518008,
            "section_title": "C.5 LLM Model Fusion",
            "char_start_offset": 42882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1858
                }
            ],
            "ref_mentions": [
                {
                    "start": 512,
                    "end": 534,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "270067818",
            "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
            "text": "Some works aim to enhance robustness by eliminating invaluable parameter updates during training to achieve robust generalization (Lee et al., 2020;Jiang et al., 2022;Xu et al., 2021).Our proposed optimizer can be viewed as an extension of these gradient dropout methods.However, we further derive that merging the gradients with the delta parameters of the SFT model can better balance alignment rewards and tax.Comprehensive experiments show our methods surpass established robust fine-tuning baselines in alignment scenarios.\n\nOffline Model Merging.Model merging is widely studied as an effective training-free method to combine the capabilities of fine-tuned large language models (Wortsman et al., 2022;Lin et al., 2023;Matena and Raffel, 2022).Ilharco et al. (2023) proposed the concept of a \"task vector\" (also referred to as \"delta parameters\") and demonstrates that the arithmetic combination of them can adjust model behaviors.Yadav et al. (2023) further addressed conflicts between different task vectors through norm-based sparsification and consensus on the signs of weights.Tam et al. (2024) employed the conjugate gradient method to find the optimal merging combination.Yu et al. (2024) shown LLMs are highly robust to sparsify task vectors and proposes DARE for merging LLMs with random sparsification.Recently, Lin et al. (2024) demonstrates that simple offline model averaging can help mitigate alignment tax.Unlike these offline model merging methods that combine the parameters of different models in one shot, our work introduces an approach where, at each step of RLHF optimization, we online merge the gradients of the policy model with the delta parameters of the SFT model.Our results show that this optimizer is more effective than offline merging in boosting alignment rewards while eliminating alignment tax.",
            "score": 0.45080351625860426,
            "section_title": "Continual Learning (CL).",
            "char_start_offset": 6853,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 413
                },
                {
                    "start": 413,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 552
                },
                {
                    "start": 552,
                    "end": 750
                },
                {
                    "start": 750,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 750,
                    "end": 771,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1088,
                    "end": 1105,
                    "matchedPaperCorpusId": "266053657"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52099609375
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "Model Merging. Merging methods can be divided into zero-shot merging and merge-then-train approaches. Early zero-shot merging methods are weight averaging and Linear Mode Connectivity [38,59]. Later popular methods include Task Arithmetic [22] manipulating task vectors, and TIES [63] addressing parameter interference through trimming and conflict resolution. DARE [64] optimizes parameters selectively to enhance merging without extra training. Others focus on geometric properties of weights for merging [49,24]. Recent Evolutionary Model Merge [4] improves weight configuration and data token pathways during inference. For the merge-then-train approach, Fisher merging [35] uses the Fisher information matrix to weigh model parameters to maximize their joint likelihood. RegMean [26] adapts the linear merging to each linear layer while averaging embeddings and biases. However, both zero-shot and merge-then-train approaches are less effective for models initialized differently. [2,23,53,62] exploit the permutation symmetry inherent in neural networks on small to large models. To boost merging efficiency, our focus on merging lies in the zero-shot merging of models with the same architecture and initialization. Model Mixture. Mixture-of-Experts (MoE) [47] scales up neural networks by utilizing router networks to activate different parts of the model for different input tokens. Its integration with Large Language Models (LLMs) has gained notable recognition for its exceptional generative capabilities and unparalleled efficiency. Recently, Mixtral [25] demonstrates that the MoE methodology can achieve the performance of dense LLM counterparts while employing significantly fewer active parameters. Model mixture combines a collection of dense LLM models, irrespective of their sizes, into a MoE model. Some studies discover model fusion [54,55] integrating the outputs of expert models to exploit the unique insights into the data distribution. Recent initiatives include Branch-Train-MiX [50], which starts with a seed-dense LLM and then branches out, facilitating the parallel training of expert models. These trained dense models are subsequently incorporated as experts within MoE layers, with other parameters being averaged. However, this approach is limited to dense models that share identical architectures and sizes.",
            "score": 0.4506340122886839,
            "section_title": "Related Works",
            "char_start_offset": 5095,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 623
                },
                {
                    "start": 624,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2344
                }
            ],
            "ref_mentions": [
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "11290566"
                },
                {
                    "start": 1855,
                    "end": 1859,
                    "matchedPaperCorpusId": "267061245"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "In this paper, we introduce a novel approach for addressing the multi-objective optimization problem in large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, model merging faces two significant challenges: First, existing methods rely heavily on human knowledge or intuition. Second, it's difficult to obtain the great model merging configuration in limited evaluations. To address these challenges, we formalize model merging as a multi-objective optimization problem and propose an automated optimization approach named MM-MO. This method leverages multi-objective optimization algorithms to autonomously search for optimal merging configurations across various tasks, alleviating the need for human intervention. In MM-MO, a weak-to-strong method is employed to enhance the acquisition function, allowing previously evaluated superior configurations to guide the search for new ones. Meanwhile, Fisher information is applied to screen these configurations, increasing the possibility of identifying high-quality merging configuration. Additionally, we designed a sparsity metric as an additional optimization objective to enhance the model's generalization performance across different tasks. We conducted comprehensive experiments with other mainstream model merging methods, demonstrating that the proposed MM-MO algorithm is competitive and effective in achieving high-quality model merging.",
            "score": 0.4496921066484291,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "277322544",
            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
            "text": "Existing merging methods rely on sparsely estimated task vectors but face two key limitations: dependence on base model parameters and task vector interference. LoRE-Merging (Liu et al., 2025d), a low-rank estimation framework, is proposed to address such issues. It constructs an approximate base model and low-rank task vectors via optimization, minimizing discrepancies between fine-tuned and merged models. Using coordinate descent and singular value thresholding, LoRE-Merging reduces task vector interference, demonstrating the effectiveness of low-rank estimation in model merging. \n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments. In our study, we eliminate the router training and directly utilize its singular value decomposition (SVD) merging part. \n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance.",
            "score": 0.4493950924903757,
            "section_title": "LoRE-Merging",
            "char_start_offset": 12716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 730,
                    "end": 747,
                    "matchedPaperCorpusId": "270702345"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "One of the primary advantages of model merging is its ability to combine the strengths of multiple specialized models into a single, multi-task model. Therefore, we first evaluate the multi-task performance of the merged models produced by different algorithms. Given the varying difficulty levels across tasks, we report normalized performance [26] as the main evaluation metric. Specifically, normalized performance is computed as \n\nfinetuned , where perf \n\nmerged and perf \n\nfinetuned denote the performance of the merged and specialized models on task i, respectively. This metric captures the proportion of finetuned performance retained by the merged model, with a value of 1 indicating that the merged model matches the performance of the task-specific finetuned models across all tasks. We report the multi-task performance in Figure 2, and summarize our observations as follows. Full numeric reesults are presented in Appendix D.  knowledge. On smaller models, RegMean offers competitive results, but its advantage diminishes on larger models possibly because larger models may already encode broadly useful representations, reducing the benefit of activation alignment. Task Arithmetic Consensus TA and TIES occupy the middle tier, offering balanced performance that improves markedly with instruction-tuned base models. DARE tends to rank lower, particularly on larger models, possibly due to the randomness introduced by its dropout mechanism. Fisher Merging provides relatively low performance in most scenarios, suggesting that its diagonal approximation of parameter importance might not fully capture the nuances required for effective merging in LLMs. \n\nModel merging is more effective on stronger base models. This is consistent with findings from Yadav et al. [74]. Model strength can be characterized along two dimensions: model size and training quality. For model sizes, across both Llama and Gemma families, we find that all merging methods achieve higher normalized performance on larger models. Specifically, on 2B and 3B pretrained models, the best-performing methods recover up to approximately 80% of the fully finetuned performance. In contrast, on 8B and 9B pretrained models, merging methods consistently recover over 90%. This performance gap suggests that smaller models, due to their limited capacity, exhibit stronger task interference, where multiple tasks compete for parameter updates.",
            "score": 0.4486865723574196,
            "section_title": "Multi-Task Performance",
            "char_start_offset": 15003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 475
                },
                {
                    "start": 478,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1668
                },
                {
                    "start": 1671,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2253
                },
                {
                    "start": 2254,
                    "end": 2423
                }
            ],
            "ref_mentions": [
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8056640625
        },
        {
            "corpus_id": "254591386",
            "title": "Do Text-to-Text Multi-Task Learners Suffer from Task Conflict?",
            "text": "Traditional multi-task learning architectures train a single model across multiple tasks through a shared encoder followed by task-specific decoders. Learning these models often requires specialized training algorithms that address task-conflict in the shared parameter updates, which otherwise can lead to negative transfer. A new type of multi-task learning within NLP homogenizes multi-task architectures as a shared encoder and language model decoder, which does surprisingly well across a range of diverse tasks. Does this new architecture suffer from task-conflicts that require specialized training algorithms? We study how certain factors in the shift towards text-to-text models affects multi-task conflict and negative transfer, finding that both directional conflict and transfer are surprisingly constant across architectures.",
            "score": 0.4485292900044229,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "270067818",
            "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
            "text": "These connections are often categorized under model merging methods.In this work, we specifically focus on general task arithmetic merging methods, which have proven effective in combining the abilities of fine-tuned models and have good characteristics such as linearity.Here, we present a straightforward merging case where parameters \u03b8 s and \u03b8 r , trained from the same pre-trained model \u03b8 b , are merged.The new capabilities learned by \u03b8 s and \u03b8 r relative to \u03b8 b can be represented by their delta parameters: \u03c4 s = \u03b8 s \u2212 \u03b8 b and \u03c4 r = \u03b8 r \u2212 \u03b8 b .Merging methods typically begin by applying a sparsification operation to each delta parameter and then linearly combining them using consensus methods to derive the merged model\n\n, where F(\u2022) denotes a sparsification operator with corresponding hyper-parameters p, and \u2295 represents the consensus method used to handle parameter interference.For instance, Yadav et al. ( 2023) employed top-k sparsification and sign-based consensus, while Yu et al. (2024) utilized random sparsification and linear combination to achieve this merging process.\n\nOffline Merging Mitigates Alignment Tax at Cost.To investigate whether Model Merging can restore the ability of the aligned model to recover the SFT model's capability, we conducted preliminary experiments.As shown in Tab. 4, we observe that the current mainstream model merging methods, when merging the SFT model and RLHF model, indeed improve performance across various linguistic abilities compared to the RLHF model, and outperform the SFT model on average.However, this improvement comes at the cost of decreased human preference, evidenced by a significant drop in MT-Bench and AlpcaEval 2.0 scores.In other words, while merging the SFT and RLHF models once can alleviate alignment tax, it also substantially diminishes the alignment bonus.\n\nFrom Offline Merging to Online.It is not surprising that simply merging the SFT and RLHF models in an offline, one-time manner did not yield satisfactory results, as our reliance on the Mode Connectivity assumption has indicated that interpolating parameters results in interpolated model performance.Thus, finding a single point along this continuum that outperforms both ends of the spectrum simultaneously is indeed challenging.",
            "score": 0.4483924175418692,
            "section_title": "Preliminaries and Motivations",
            "char_start_offset": 10775,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 68,
                    "end": 272
                },
                {
                    "start": 272,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 551
                },
                {
                    "start": 551,
                    "end": 729
                },
                {
                    "start": 731,
                    "end": 893
                },
                {
                    "start": 893,
                    "end": 1093
                },
                {
                    "start": 1095,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1701
                },
                {
                    "start": 1701,
                    "end": 1842
                },
                {
                    "start": 1844,
                    "end": 1875
                },
                {
                    "start": 1875,
                    "end": 2145
                },
                {
                    "start": 2145,
                    "end": 2275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "As the name suggests, this method assumes that parameters with larger magnitudes have a greater impact on model performance and should be preserved. The approach involves sorting parameters by their absolute values, retaining those with larger magnitudes while dropping those with smaller absolute values. This helps reduce parameter interference during model merging, ensuring that essential information is maintained. Model merging methods of this type include the following. \n\nTIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024]. Model Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024]. In contrast to DARE, which randomly drops delta parameters with a fixed probability, Model Breadcrumbs applies a deterministic, layerwise masking process that accounts for task vector distributions, allowing for finer control over parameter adjustments. Compared to DELLA-Merging's stochastic pruning strategy, MAGPRUNE, which relies on magnitude-based probability sampling, Model Breadcrumbs adopts a more direct filtering mechanism that eliminates extreme values and maintains task relevance through robust aggregation. Furthermore, Model Breadcrumbs emphasizes layer-specific processing over global operations, enhancing its adaptability to diverse tasks. These distinctions make Model Breadcrumbs particularly effective in reducing noise accumulation and achieving consistent performance across multi-task scenarios, while requiring less hyperparameter tuning than prior methods.",
            "score": 0.44766946630237203,
            "section_title": "Weight sorting type",
            "char_start_offset": 17365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1488
                },
                {
                    "start": 1489,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2110
                },
                {
                    "start": 2111,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2472
                }
            ],
            "ref_mentions": [
                {
                    "start": 1558,
                    "end": 1587,
                    "matchedPaperCorpusId": "273903113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "259064039",
            "title": "TIES-Merging: Resolving Interference When Merging Models",
            "text": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
            "score": 0.4476459269498014,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86376953125
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "A fundamental issue in any model merging scenario is ensuring alignment. Alignment refers to the process of mapping functionally equivalent features to the same relative positions in the weight matrices of the models being merged. Without alignment, merging operations might combine disparate features, leading to interference and performance degradation. \n\nMerging models trained from different initializations, or even on entirely different tasks, compounds this difficulty. Significant contributions to the field include methods like ZipIt (Stoica et al., 2023), Git Re-Basin (Ainsworth et al., 2022), and Optimal Transport (OT) Fusion (Singh and Jaggi, 2020). Git Re-Basin generalizes the merging process by permuting model parameters into a common space before averaging them, an approach that works well for models trained on similar tasks but struggles when tasks diverge significantly. OT Fusion employs Optimal Transport to achieve a similar goal, introducing soft matching for increased flexibility over strict permutations. ZipIt relaxes the constraints on merging, allowing for partial merging both across layers-merging up to a specified depth-and within layers by combining correlated features within models, as well as between models. This method excels in multi-task scenarios by accounting for unshared features, outperforming conventional permutation methods, especially when models are trained on disjoint tasks. Though even at their most successful, no current method has been shown to reliably merge a set of models trained from different initializations on different tasks and achieve high multi-task performance. \n\nThis remains an open problem in the field. \n\nFortunately, when merging models fine-tuned from the same base or 'parent' model, alignment is naturally preserved through the fine-tuning process. The merging methods discussed in subsequent sections assume this inherent alignment, enabling more effective integration of models.",
            "score": 0.4460770702806751,
            "section_title": "Model Alignment",
            "char_start_offset": 3688,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 355
                },
                {
                    "start": 358,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1635
                },
                {
                    "start": 1638,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 639,
                    "end": 662,
                    "matchedPaperCorpusId": "204512191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.701171875
        },
        {
            "corpus_id": "274436302",
            "title": "Task Singular Vectors: Reducing Task Interference in Model Merging",
            "text": "Model Merging offers an efficient alternative to ensembling by combining existing models without the need for further training. Several approaches address this by determining the neuron permutations that align the models into a shared optimization basin, after which the models are combined using a straightforward averaging method. [1,8,25,36,43]. An alternative line of approaches focuses on the multi-task scenario, where a single pre-trained model is fine-tuned for different tasks [10,22,32,40,46,48,51,53,54]. The prerequisites for methods in this category are outlined in Table 1. Task Arithmetic [22] introduces the concept of task vectors, which are the weight differences \n\nbetween fine-tuned models and the pre-trained base model. \n\nBy averaging these vectors, a merged multi-task model can be created; conversely, negating a task vector allows for forgetting a specific task. TIES [51] addresses redundancies in model parameters by first selecting the top-k most significant parameter changes and then constructing a sign vector based on the majority sign across all models. The latter is used to merge the task vectors disjointly, meaning the average is not computed when a parameter is zero or when parameters disagree in sign. Similarly, DARE [53] randomly resets redundant parameters to their pre-trained values and rescales the remaining parameters by a factor proportional to the dropped ones, aiming to reduce interference among tasks. Fisher Merging [32] and RegMean [24] merge models by performing weighted averaging, utilizing the Fisher information matrix and the inner product of input vectors, respectively. Model Breadcrumbs [10] focuses on merging only significant weights by discarding outliers and both minor and large perturbations in the finetuned parameters. More recently, Wang et al. [46] observed that tasks often utilize non-overlapping sets of weights. They propose two methods: the first, TALL-Mask, uses binary masks to activate important task-specific weights, requiring extra storage (as does our method TSV-C) for the masks. The second, Consensus Merging, leverages these masks to remove parameters that are important to less than two tasks. Like our approach, TwinMerging [31] and SMILE [44] apply SVD to layer task matrices.",
            "score": 0.4452996880056701,
            "section_title": "Related Work",
            "char_start_offset": 4025,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 741
                },
                {
                    "start": 744,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1086
                },
                {
                    "start": 1087,
                    "end": 1241
                },
                {
                    "start": 1242,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1790
                },
                {
                    "start": 1791,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2066
                },
                {
                    "start": 2067,
                    "end": 2183
                },
                {
                    "start": 2184,
                    "end": 2268
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 336,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 338,
                    "end": 341,
                    "matchedPaperCorpusId": "253523197"
                },
                {
                    "start": 344,
                    "end": 346,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 486,
                    "end": 490,
                    "matchedPaperCorpusId": "266174505"
                },
                {
                    "start": 490,
                    "end": 493,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 493,
                    "end": 496,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 499,
                    "end": 502,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 502,
                    "end": 505,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 505,
                    "end": 508,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 508,
                    "end": 511,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 604,
                    "end": 608,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 893,
                    "end": 897,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1258,
                    "end": 1262,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "266174505"
                },
                {
                    "start": 1818,
                    "end": 1822,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63525390625
        },
        {
            "corpus_id": "274981481",
            "title": "Parameter-Efficient Interventions for Enhanced Model Merging",
            "text": "Multi-task learning (MTL) assumes simultaneous accommodation of knowledge from multiple tasks using a common backbone [2,34]. This approach offers several benefits, including better performance, eliminating the need for separate task-specific models, and facilitating knowledge transfer between related tasks [39]. However, this approach is challenging, as it can lead to negative transfer or task interference. Furthermore, MTL necessitates collecting data from all tasks for joint training, which can be costly in terms of larger model size, extended training times, and usage of computational resources. Additionally, data privacy concerns may complicate the implementation of MTL, especially in sensitive domains. MTL also lacks the flexibility to incorporate new tasks without retraining the entire model [47]. \n\nModel merging has emerged as a promising alternative to traditional joint MTL to address these limitations [20]. Model merging aims to fuse the abilities of multiple task-specific models into a single multi-task model while keeping the parameter size the same as that of the individual models. Practitioners now follow a standard workflow where they fine-tune a pre-trained foundational model for various target tasks. This approach corresponds to model merging more than tradi- IntervMerge consistently demonstrates superior performance compared to the state-of-the-art Surgery approach in multi-task model merging. This advantage is particularly evident when utilizing our efficient mini-intervention mechanism, which achieves better results than Surgery while employing three times fewer parameters. It is important to note that Inter-vMerge may exhibit more additional parameters than Surgery for certain ranks, as it is applied across many network blocks. \n\ntional multi-task learning. Consequently, large language models (LLMs) have effectively utilized the benefits of this procedure [17,37,42,12,48,1,29,28]. Merging offers several advantages. It eliminates the need to collect and manage data from all tasks, reducing training costs and data privacy concerns. Model merging enables greater flexibility in incorporating new tasks, supporting continual learning [22]. Furthermore, model merging can enhance the robustness and improve the handling of distribution shifts [27,38]. To achieve these benefits, merging methods interpolate between the parameters of task-specific models [15,14,37].",
            "score": 0.4451690793608076,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2086
                },
                {
                    "start": 2087,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2303
                },
                {
                    "start": 2304,
                    "end": 2417
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 121,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 121,
                    "end": 124,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1922,
                    "end": 1925,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1930,
                    "end": 1933,
                    "matchedPaperCorpusId": "259096117"
                },
                {
                    "start": 2295,
                    "end": 2299,
                    "matchedPaperCorpusId": "254877458"
                },
                {
                    "start": 2299,
                    "end": 2302,
                    "matchedPaperCorpusId": "237420687"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6767578125
        },
        {
            "corpus_id": "275212307",
            "title": "Training-free Heterogeneous Model Merging",
            "text": "Direct weight average. Weight averaging [25] is a widely used model merging technique that constructs the merged model by averaging parameters. Task Arithmetic [9] employs a predefined scaling factor to differentiate the significance of various models. Fisher Merging [17] performs weighted parameter fusion, where the weights are determined using the Fisher information matrix [6]. RegMean [12] adeptly addresses model merging by optimizing a linear regression problem with closed-form solutions. TIES-Merging [28] resolves task conflicts in [9] by pruning low-magnitude parameters, rectifying sign disagreements, and merging parameters with consistent signs in isolation. DARE [29] further mitigates parameter interference from previous approaches by randomly dropping delta parameters and rescaling the remaining ones. \n\nAlign-then-average. Git Re-Basin [1] and Neuron Alignment [22] permute models by evaluating the similarity between their weights or activations. REPAIR [13] enhances the precision of Git Re-Basin by calculating the correlation between intermediate layer feature activations and incorporating multiple batch normalization layers into the network. OTFusion [20] introduces a permutation-based approach grounded in optimal transport theory, utilizing the Wasserstein distance, where neuron associations facilitate the one-shot fusion of pre-existing models with identical depths. Several studies [10], [23] extend these methods to accommodate Transformerbased architectures, though substantial performance degradation persists without fine-tuning. Zipit! [21] addresses intra-model merging, aligning all models within the same basin by \"zipping\" redundant features both within and across models. Furthermore, MuDSC [27] proposes the simultaneous alignment of models in both weight and activation spaces.",
            "score": 0.4445760874604132,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 4954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1824
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 44,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "15354499"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "221516809"
                },
                {
                    "start": 1736,
                    "end": 1740,
                    "matchedPaperCorpusId": "268248251"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "276408321",
            "title": "Superpose Singular Features for Model Merging",
            "text": "Model merging is a technique that combines multiple models into a single model to enhance performance or enable the model to perform multiple tasks. Previous studies have shown that averaging the weights of multiple models fine-tuned from the same pre-trained initialization is a promising approach for model merging. Fisher Merging [Matena and Raffel, 2022] advances beyond simple averaging by utilizing the Fisher information matrix to assess the importance of individual parameters, which are then weighted accordingly during the merging process. Similarly, RegMean [Jin et al., 2022] forms a linear regression problem with extra data for each layer and offers a closed-form solution for the merged model's parameters by solving the regression problem. \n\nBeyond parameter averaging, Task Arithmetic [Ilharco et al., 2022a] introduces task vectors and adding the task vectors of individual tasks to merge model, demonstrating their effectiveness and lightweight nature in facilitating cross-task generalization. Building on this concept, PEM Composition [Zhang et al., 2023a] extends the task arithmetic framework to merge LoRA [Hu et al., 2021], while Ties-Merging [Yadav et al., 2024] addresses task conflicts by resetting redundant parameters and resolving sign conflicts. These methods, however, use a single merging coefficient across all task vectors, which limits their flexibility. In contrast, Lorahub [Huang et al., 2023] and AdaMerging [Yang et al., 2023] use different coefficients for enhanced adaptability. Lorahub's performance is limited as it only searches for coefficients at the task level, while AdaMerging requires complex training and unlabeled test datasets, making it applicable solely to classification problems. DARE [Yu et al., 2024] proposes drop and rescale as preprocessing steps when merging fine-tuned LLMs. PCB-Merging [Du et al., 2024] is a lightweight, training-free technique for model merging that balances parameter competition by intra-balancing parameter significance within tasks and inter-balancing parameter similarities across tasks, effectively enhancing performance across various scenarios and domains.",
            "score": 0.4443649045462602,
            "section_title": "Model Merging of Fine-tuned Models",
            "char_start_offset": 5222,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 755
                },
                {
                    "start": 758,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2151
                }
            ],
            "ref_mentions": [
                {
                    "start": 1056,
                    "end": 1077,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1168,
                    "end": 1188,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1745,
                    "end": 1762,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1854,
                    "end": 1871,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7626953125
        },
        {
            "corpus_id": "277940324",
            "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
            "text": "Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss. \n\nTypical model merging scenarios often require combining pre-existing expert models-each specialized in a specific task-into one unified system. However, this static approach falls short in scenarios where new tasks emerge over time. In continual learning, we face the challenge of incrementally integrating new task-specific models without retraining the entire system. Recent advances in dynamic model merging address this by tackling issues such as parameter interference, memory efficiency, and sequential integration, enabling systems that adapt more effectively as new tasks are encountered. For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios.",
            "score": 0.44435391872512936,
            "section_title": "Model Merging",
            "char_start_offset": 74079,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2123
                },
                {
                    "start": 2124,
                    "end": 2227
                },
                {
                    "start": 2228,
                    "end": 2345
                },
                {
                    "start": 2346,
                    "end": 2574
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77783203125
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "Large language models (LLMs) have enabled the development of numerous specialized, task-specific variants. However, the maintenance and deployment of these individual models present substantial challenges in terms of resource utilization and operational efficiency. In this work, we propose the \\textit{Mixture of Distributions (MoD)} framework, a novel approach for merging LLMs that operates directly on their output probability distributions, rather than on model weights. Unlike traditional weight-averaging methods, MoD effectively preserves the specialized capabilities of individual models while enabling efficient knowledge sharing across tasks. Through extensive experimentation on mathematical reasoning benchmarks using Qwen2.5 models, we demonstrate that MoD significantly outperforms existing model merging techniques across multiple benchmarks. All code, data, and experimental materials are published at https://github.com/knovel-eng/mod.",
            "score": 0.44406834439708287,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "273482155",
            "title": "SurgeryV2: Bridging the Gap Between Model Merging and Multi-Task Learning with Deep Representation Surgery",
            "text": "The baselines we compare include three non-model merging methods, and seven representative model merging methods: \n\n(i) Non-model Merging Methods: \u2022 Pretrained directly uses the pre-trained model to test multiple downstream tasks. The performance of this method is generally less than acceptable due to the lack of task-related knowledge. \n\n\u2022 Individual involves full fine-tuning of the pretrained model with task-specific training data and often yields superior performance. However, it requires maintaining a copy of model parameters for each task, which is very costly when the number of tasks is large. \u2022 Traditional MTL collects data from multiple tasks and collaboratively trains a multi-task model. Due to potential task interference, the performance of a multi-task model is usually slightly lower than that of individual models, but the advantage is that it only has a single model. \n\n(ii) Model Merging Methods: \u2022 Weight Averaging is the simplest model merging method, which directly adds the parameters of well-trained multiple models according to average values. \u2022 Fisher Merging [19] measures the importance of each parameter based on the Fisher information matrix [46] and merges models based on that importance. \u2022 RegMean [24] utilizes pre-computed inner product matrices of inputs at each layer from the original training data to conduct a linear combination of the model's parameters. \u2022 Task Arithmetic [36] defines the parameter difference between the fine-tuned model and the pre-trained model as a 'task vector', and merges multiple task vectors into the pretrained model based on grid search coefficients for model merging. \n\n\u2022 Ties-Merging [21] eliminates sign conflicts between parameters of task vectors based on Task Arithmetic [36], thereby effectively alleviating interference during model merging. \n\n\u2022 AdaMerging [26] designed a layer-wise model merging strategy, which assigns a merging coefficient to each layer of the model to be merged and optimizes these coefficients by entropy minimization. \u2022 Concrete AdaMerging [52] performs model merging in the shared subspace of multiple models. Similar to AdaMerging [26], it employs entropy minimization to identify the shared subspace. \n\n(iii) Our Representation Surgery Methods: Our Surgery (in \u00a7IV-B) is orthogonal to the model merging methods in (ii).",
            "score": 0.4438956022762999,
            "section_title": "B. Baselines",
            "char_start_offset": 37143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 116,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 338
                },
                {
                    "start": 341,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2025
                },
                {
                    "start": 2026,
                    "end": 2118
                },
                {
                    "start": 2119,
                    "end": 2211
                },
                {
                    "start": 2214,
                    "end": 2330
                }
            ],
            "ref_mentions": [
                {
                    "start": 1178,
                    "end": 1182,
                    "matchedPaperCorpusId": "15354499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "274822342",
            "title": "Fine-tuning Aligned Classifiers for Merging Outputs: Towards a Superior Evaluation Protocol in Model Merging",
            "text": "Model merging aims to combine multiple homologous models into a single model capable of performing better across multiple related tasks (Wortsman et al., 2022;Jang et al., 2024;Daheim et al., 2023;Panigrahi et al., 2023;Marczak et al., 2024). Beyond basic Weight Averaging of multiple models, the primary merging methods can be categorized into two main types (Yadav et al., 2023). \n\nMerging Weight Based Methods Instead of assigning an averaging weight for each fine-tuned model, some methods utilize dataset statistics or gradients to adjust the merging weights. Fisher Merging (Matena and Raffel, 2022) computes perweight importance for each model by the Fisher Information matrix, and reweights the weights with this importance factor during merging. Reg-Mean (Jin et al., 2022) conducts a local linear regression problem that minimizes prediction differences between the merged model and the individual models, and the closed-form solution is the merging weight. AdaMerging (Yang et al., 2024c) autonomously learns task-wise or layer-wise merging coefficients without using the original training data. Specifically, it employs entropy minimization on unlabeled test samples as a surrogate objective to iteratively refine the merging coefficients. \n\nTask Vector Based Methods The task vector indicates the offset of a certain task based on the pre-trained model in the model parameter space. We introduce the following latest methods except for the representative referred to before. Motivated by model pruning that eliminating unnecessary parameters could also maintain performance, Dare (Yu et al., 2024) reduces potential conflicts in the delta parameters by randomly dropping. To mitigate forgetting while training, MagMax (Marczak et al., 2024) combines sequential fine-tuning with maximum-magnitude weight selection to enable effective knowledge integration across tasks. TALL-masks (Wang et al., 2024) localizes taskspecific information within the multi-task vector by deactivating irrelevant parts for each task using binary masks.",
            "score": 0.44314033831593014,
            "section_title": "Related Works",
            "char_start_offset": 23072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 381
                },
                {
                    "start": 384,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2043
                }
            ],
            "ref_mentions": [
                {
                    "start": 136,
                    "end": 159,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 360,
                    "end": 380,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 979,
                    "end": 999,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1593,
                    "end": 1610,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59619140625
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "The major pitfall of the unweighted merging is that there exists parameter interference, as explored in the previous work on dense model merging (Yu et al., 2024;Yadav et al., 2024). As suggested in Figure 2, when influential parameters (large magnitude parameters) in the task vector merge with redundant parameters (small magnitude parameters) or parameters with sign conflict, simple averaging will output a small magnitude parameter, which may reduce the effect of the original task vector. In contrast to BTX, we mitigate model interference by employing previous SoTA methods in this MoE setup, namely Dare and Ties. First, we calculate the task vector \u03c4 i = \u03b8 b \u2212 \u03b8 i with the base model parameter \u03b8 b and the parameter \u03b8 i for the model CPTed on domain i. For Ties merging, we first drop the bottom (100 \u2212 p)% of the redundant parameters (smallest magnitude) by resetting them to 0. For each parameter, we determine the sign with the highest total magnitude in all task vectors and sum all task vectors together to \u03c4 m but only by keeping the parameter values whose signs are the same as the determined sign. For Dare merging, we randomly drop the (100 \u2212 p)% parameters. We rescale each task vector with \u03c4 i = \u03c4 i 0.01p . We sum all task vectors to \u03c4 m . Finally, we add the summed task vector back to the base model with the scaling term \u03bb and obtain the merged layer parameters: \u03b8 m = \u03b8 b +\u03bb\u2022\u03c4 m . We expect that the drop operation in both methods will address the parameter interference issue, as revealed in dense model merging, and produce a consistent performance boost (Yu et al., 2024;Yadav et al., 2024). \n\nSimilar to BTX, after combining each expert model into an MoE, we fine-tune all parameters in the MoE in the fine-tuning stage. By addressing parameter interference, our approach achieves performance improvements over BTX especially in earlier stages of fine-tuning. Next, we describe how to further reduce the fine-tuning needs.",
            "score": 0.44178538920789356,
            "section_title": "Addressing Parameter Interference",
            "char_start_offset": 8934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 1952
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 162,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 162,
                    "end": 181,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1583,
                    "end": 1600,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1600,
                    "end": 1619,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "278207827",
            "title": "Param$\\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost",
            "text": "Parameter space has been explored in previous research by Plappert et al. (2017); Sedlmair et al. (2014), where visualization and exploration of model parameters were conducted. Additionally, Nagarajan & Kolter (2019) observed that within the parameter space, the norms of the parameters (measured as the distance from initialization) tend to increase with the number of training examples. \n\nModel parameters averaging is a technique that can improve the generalization performance of machine learning models by reducing the variance of their predictions. Neyshabur et al. (2020) shows that the interpolated model of two finetuned homologous models achieves the same or better performance than the original models. Wortsman et al. (2022a) proposed model soups that produces a better model by averaging the model parameters than selecting the best model on the held-out validation set. Rame et al. (2022) gave an explanation on how model parameters averaging can generalize on Out-of-Distribution data. Nikishin et al. (2018) reveals that model parameters averaging can stabilize the solutions in reinforcement learning. Model merging, which transcends the averaging of weights, is an emerging field focused on integrating multiple task-specific models into a unified model that retains the capabilities of the original models (Ilharco et al., 2022;Jin et al., 2022;Matena & Raffel, 2022;Neyshabur et al., 2020;Nikishin et al., 2018;Wortsman et al., 2022a;b;Yadav et al., 2023;Yu et al., 2024;Zhang et al., 2023). Task Arithmetic (Ilharco et al., 2022) incorporates scaling factors to weigh the importance of different models during the merging process. Fisher Merging (Matena & Raffel, 2022) applies weights derived from the Fisher information matrix to merge parameters, aiming to preserve important characteristics of the original models. RegMean Jin et al. (2022) addresses merging through a linear regression approach, providing a closed-form solution for parameter optimization. TIES-Merging Yadav et al. (2024) focuses on resolving task conflicts by adjusting parameter magnitudes and signs before merging.",
            "score": 0.44095201948803614,
            "section_title": "RELATED WORK",
            "char_start_offset": 21270,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 389
                },
                {
                    "start": 392,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2112
                }
            ],
            "ref_mentions": [
                {
                    "start": 82,
                    "end": 104,
                    "matchedPaperCorpusId": "1360769"
                },
                {
                    "start": 192,
                    "end": 217,
                    "matchedPaperCorpusId": "61153527"
                },
                {
                    "start": 715,
                    "end": 738,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 885,
                    "end": 903,
                    "matchedPaperCorpusId": "248887264"
                },
                {
                    "start": 1002,
                    "end": 1024,
                    "matchedPaperCorpusId": "209522332"
                },
                {
                    "start": 1326,
                    "end": 1348,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1365,
                    "end": 1387,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1410,
                    "end": 1432,
                    "matchedPaperCorpusId": "209522332"
                },
                {
                    "start": 1432,
                    "end": 1455,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1476,
                    "end": 1492,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1492,
                    "end": 1511,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1529,
                    "end": 1551,
                    "matchedPaperCorpusId": "232134851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6826171875
        },
        {
            "corpus_id": "277349207",
            "title": "Reinforced Model Merging",
            "text": "The success of large language models has garnered widespread attention for model merging techniques, especially training-free methods which combine model capabilities within the parameter space. However, two challenges remain: (1) uniform treatment of all parameters leads to performance degradation; (2) search-based algorithms are often inefficient. In this paper, we present an innovative framework termed Reinforced Model Merging (RMM), which encompasses an environment and agent tailored for merging tasks. These components interact to execute layer-wise merging actions, aiming to search the optimal merging architecture. Notably, RMM operates without any gradient computations on the original models, rendering it feasible for edge devices. Furthermore, by utilizing data subsets during the evaluation process, we addressed the bottleneck in the reward feedback phase, thereby accelerating RMM by up to 100 times. Extensive experiments demonstrate that RMM achieves state-of-the-art performance across various vision and NLP datasets and effectively overcomes the limitations of the existing baseline methods. Our code is available at https://github.com/WuDiHJQ/Reinforced-Model-Merging.",
            "score": 0.44002835580286354,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86767578125
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge [4,60,78]. However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference [37,40,50]. Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts [6,76]. Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating [17,20,28,34,51]. Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework [13,25,70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework.",
            "score": 0.4398404991013493,
            "section_title": "Multi-task Learning",
            "char_start_offset": 5007,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1063
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 116,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 119,
                    "end": 122,
                    "matchedPaperCorpusId": "235790783"
                },
                {
                    "start": 357,
                    "end": 361,
                    "matchedPaperCorpusId": "50770252"
                },
                {
                    "start": 361,
                    "end": 364,
                    "matchedPaperCorpusId": "1923223"
                },
                {
                    "start": 364,
                    "end": 367,
                    "matchedPaperCorpusId": "22014305"
                },
                {
                    "start": 480,
                    "end": 483,
                    "matchedPaperCorpusId": "4703661"
                },
                {
                    "start": 483,
                    "end": 486,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 617,
                    "end": 621,
                    "matchedPaperCorpusId": "52952193"
                },
                {
                    "start": 621,
                    "end": 624,
                    "matchedPaperCorpusId": "261243229"
                },
                {
                    "start": 624,
                    "end": 627,
                    "matchedPaperCorpusId": "4800342"
                },
                {
                    "start": 627,
                    "end": 630,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "52957972"
                },
                {
                    "start": 751,
                    "end": 755,
                    "matchedPaperCorpusId": "237291521"
                },
                {
                    "start": 755,
                    "end": 758,
                    "matchedPaperCorpusId": "256658804"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69775390625
        },
        {
            "corpus_id": "266053657",
            "title": "Merging by Matching Models in Task Parameter Subspaces",
            "text": "We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task. To match models in their task parameter subspace, merging methods upweight each model in its task parameter subspace, which aims to ensure that the task-relevant components of a given model will not be washed out after the models are combined. In particular, we show how Fisher merging (Matena & Raffel, 2022), RegMean (Jin et al., 2022), and simple parameter averaging (McMahan et al., 2017;Stich, 2018) all perform merging in this way and differ only in their choice of task parameter subspace. Concurrently, other works have focused on inaccuracies in model merging stemming from gradient mismatches in different models, and use these insights to connect diagonal Fisher merging and Task Arithmetic (Daheim et al., 2023). \n\nMatching models in their task parameter subspace requires solving a linear system of equations. This linear system implicitly defines a merging objective that relates to a given merging method's choice of task parameter subspace. While previous merging methods used merging objectives with a tractable closed-form solution, we instead develop a merging framework that uses the conjugate gradient method (Hestenes & Stiefel, 1952) to solve a given linear system. We refer to our merging framework as MaTS (Matching Models in their Task Subspace). By using the conjugate gradient method, MaTS flexibly supports different merging objectives and initializations (which can impact convergence speed). MaTS also enables the use of merging objectives for linear systems that don't have a tractable closed-form solution. To explore this possibility, we leverage insights from K-FAC (Grosse & Martens, 2016;Martens & Grosse, 2015) and introduce a merging method where a model's task parameter subspace is based on a block-diagonal approximation of the Fisher information matrix. \n\nTo explore the effectiveness of MaTS, we comprehensively compare it to existing merging methods on multitask and intermediate-task merging of language models and vision models trained via parameter-efficient or fullmodel fine-tuning.",
            "score": 0.4396506628741437,
            "section_title": "Introduction",
            "char_start_offset": 1882,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1718
                },
                {
                    "start": 1719,
                    "end": 1975
                },
                {
                    "start": 1978,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 465,
                    "end": 488,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 549,
                    "end": 571,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 1309,
                    "end": 1335,
                    "matchedPaperCorpusId": "2207234"
                },
                {
                    "start": 1780,
                    "end": 1804,
                    "matchedPaperCorpusId": "11724661"
                },
                {
                    "start": 1804,
                    "end": 1827,
                    "matchedPaperCorpusId": "11480464"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "274776961",
            "title": "SUPERMERGE: An Approach For Gradient-Based Model Merging",
            "text": "LLM Fine-Tuning Techniques. The core idea of fine-tuning is to extend the knowledge acquired from pre-training and adapt it to a specific target domain through additional training using a taskspecific data set. In the natural language processing domain, instruction fine-tuning (Wei et al., 2022;Chung et al., 2024) serves as a widely adopted approach to enhance the model's ability to comprehend and accurately execute desired tasks. To enhance fine-tuning efficiency, parameter-efficient fine-tuning (PEFT) methods have been proposed. PEFT methods typically introduce lightweight task-specific adaptations to the pre-trained model. One approach is to add adapters (Houlsby et al., 2019) as sub-modules to the pre-trained model, which enables efficient learning of new knowledge. Low-Rank Adaptation(LoRA) (Hu et al., 2022) factorizes each weight matrix into a low-rank decomposition that has minimal number of tunable parameters. Infused Adapter by Inhibiting and Amplifying Inner Activations (IA 3 ) (Liu et al., 2022) proposes a method of learning a subset of additional parameters to re-scale inner activations in the attention and feed-forward modules of transformer-based language models. These methods aim to achieve comparable performance to traditional full fine-tuning, while achieving significant reduction of tunable parameters. \n\nMulti-Task Learning via Model Merging. Model merging methods aim to combine two or more fine-tuned models into a single model. Most existing methods focus on merging models that are derived from the same base architecture and initialization. The most intuitive approach is calculating a smooth average of the model parameters across different tasks, such as Task Arithmetic (Ilharco et al., 2022) and Fisher-Weighted averaging (Matena & Raffel, 2024). TIES-Merging (Yadav et al., 2024) takes a rule-based approach to resolve sign conflicts and merges with least feature redundancy. DARE (Yu et al., 2024) performs sparsification on each fine-tuned model by randomly dropping and ensembles them with rescaling. AdaMerging (Yang et al., 2024) is the most relevant related work to SUPERMERGE.",
            "score": 0.4394868936745121,
            "section_title": "RELATED WORK",
            "char_start_offset": 7712,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1925
                },
                {
                    "start": 1926,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2133
                }
            ],
            "ref_mentions": [
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 666,
                    "end": 688,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 807,
                    "end": 824,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1003,
                    "end": 1021,
                    "matchedPaperCorpusId": "248693283"
                },
                {
                    "start": 1718,
                    "end": 1740,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1771,
                    "end": 1794,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1931,
                    "end": 1948,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71484375
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "We compare our method against a diverse set of baselines, ranging from simple merging strategies to advanced methods leveraging task-specific or shared representations. Pretrained indicates a model that predicts multiple tasks without additional fine-tuning for task-specific requirements. However, the absence of task-specific information for downstream tasks generally leads to poor performance. Individual refers to the fine-tuning of individual pre-trained models for each task. Since there is no interference between tasks, it has been regarded as the upper bound of task-specific performance. Traditional MTL trains a multi-task model by combining the original training data for all tasks. While effective, this approach depends on access to raw data and labels for each task, which is not always practical. Weight Averaging merges multiple individual models by directly averaging their parameters to create a single model for multi-task learning. Although simple, this method lacks task-specific adjustments. Fisher Merging [39] uses the Fisher information matrix to estimate the importance of each parameter. Model parameters are merged based on their relative contributions. RegMean [27] merges models by aligning the inputs of linear layers to minimize the L2 distance between the individual models and the merged model. Task Arithmetic [23] defines the difference between the fine-tuned and pre-trained model parameters as a task vector. By combining multiple task vectors and adding them to the pre-trained model, it enables multi-task learning. MagMax [38] merges task vectors [23] by selecting the parameter with the largest magnitude for each position, consolidating knowledge into a single model without retaining task-specific data. Ties Merging [72] highlight the importance of addressing interference in task arithmetic-based merging. It involves removing redundant parameters from the task vector and resolving parameter sign conflicts. LiNeS [64] observes that reducing the influence of shallow layers helps prevent distortion of general representations, thereby simplifying merging coefficient selection by allowing them to increase linearly with layer depth. \n\nConsensus TA [63] enhances Task Arithmetic [23] by using Consensus Merging, which retains only weights beneficial to multiple tasks while eliminating irrelevant or taskspecific weights. This process uses task-specific binary masks to identify relevant weights and forms a consensus mask to minimize task interference.",
            "score": 0.43891916788417545,
            "section_title": "B.2. Baseline Details",
            "char_start_offset": 34982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1448
                },
                {
                    "start": 1449,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2181
                },
                {
                    "start": 2184,
                    "end": 2369
                },
                {
                    "start": 2370,
                    "end": 2501
                }
            ],
            "ref_mentions": [
                {
                    "start": 1031,
                    "end": 1035,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1192,
                    "end": 1196,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1347,
                    "end": 1351,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1565,
                    "end": 1569,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 1590,
                    "end": 1594,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1763,
                    "end": 1767,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1963,
                    "end": 1967,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 2197,
                    "end": 2201,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 2227,
                    "end": 2231,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.712890625
        },
        {
            "corpus_id": "270067773",
            "title": "EMR-Merging: Tuning-Free High-Performance Model Merging",
            "text": "With the rapid development of deep learning, different model architectures [36,22,71,88] are proposed, along with multiple training strategies [89,86]. Pre-trained models' capabilities are enhanced, thus showing increasing significance [54,22,7,19]. Finetuning models on downstream tasks from a pre-trained model has become a standard paradigm in both NLP and vision fields [20,51,19,22,5,87], which usually leads to improved performance with less labeled data. With the development of open-source repositories such as Huggingface [79], timm [77], and torchvision [44], the number of pre-trained and finetuned checkpoints exponentially rise. However, applying individual models to different tasks results in high storage and deployment costs. Multi-task learning (MTL) partially solves this problem by jointly training a model using multiple datasets [70,93,95], but it suffers from (i) high computational costs and (ii) data unavailability due to privacy [33]. Recently, model merging attempts to solve these drawbacks by combining weights instead of additional training, thus showing vital significance and broad application prospects. \n\nA simple strategy of model merging is averaging the model weights [80], but it usually causes obvious performance degradation, as shown in Fig. 1. To this end, there are multiple model merging methods proposed to improve the performance of the merged model, which can be roughly divided into three categories: (i) Weighted averaging of model weights include Fisher-Merging [46] and RegMean [33]. They use pre-computed Fisher information matrices [23] and inner-product matrices [33] to tune the coefficients for weighted averaging. (ii) Task vector-based methods that add task vectors together instead of model weights, include Task Arithmetic [30], Ties-Merging [84], and AdaMerging [85]. Ties-Merging handles the interference issue and AdaMerging adaptively tunes the merging coefficients. (iii) Pre-processing techniques include DARE [90]. It reduces interference by dropping most elements and rescaling the others in task vectors.",
            "score": 0.4384287173540378,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1137
                },
                {
                    "start": 1140,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 1982
                },
                {
                    "start": 1983,
                    "end": 2074
                }
            ],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 88,
                    "matchedPaperCorpusId": "247222744"
                },
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "252780856"
                },
                {
                    "start": 236,
                    "end": 240,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 243,
                    "end": 245,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "234742221"
                },
                {
                    "start": 851,
                    "end": 855,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 855,
                    "end": 858,
                    "matchedPaperCorpusId": "257496595"
                },
                {
                    "start": 858,
                    "end": 861,
                    "matchedPaperCorpusId": "11311635"
                },
                {
                    "start": 956,
                    "end": 960,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1206,
                    "end": 1210,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1513,
                    "end": 1517,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1530,
                    "end": 1534,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1618,
                    "end": 1622,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1803,
                    "end": 1807,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1824,
                    "end": 1828,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.48779296875
        },
        {
            "corpus_id": "267547973",
            "title": "Learning to Route Among Specialized Experts for Zero-Shot Generalization",
            "text": "Pfeiffer et al. (2020) independently learns adapters for each task, then uses a knowledge composition module to combine adapters at different layers, outperforming independent multitask training and full-model finetuning on 16 natural language understanding tasks. Gou et al. (2023) trains LoRA for data clusters and a universal LoRA for the entire dataset, enhancing generalization to unseen instructions. Shah et al. (2023) trains content and style LoRAs independently, uses merge vectors to minimize interference, and combines LoRAs by using training data from both domains. Wang et al. (2023) merges SAM and CLIP models to produce SAM-CLIP for language-based segmentation. Since all of these works use labeled target-task datasets, we exclude them from comparison. \n\nMerging expert models Model merging (Choshen et al., 2022;Wortsman et al., 2022;Ram\u00e9 et al., 2022;Matena & Raffel, 2022;Ilharco et al., 2022;Yadav et al., 2023;Tam et al., 2023;Jin et al., 2022;Yang et al., 2023)   on a dataset to compute statistics or tune hyperparameters, so we focused on comparing to simple parameter averaging in our experiments (which remains a widespread method). \n\nIn addition, state-of-the-art merging methods typically underperform multitask training (Tam et al., 2023;Ilharco et al., 2022). Merging has also been used as a component of systems that aim to enable zero-shot generalization. For example, Chronopoulou et al. ( 2023) merges separate task and language adapters to enable cross-lingual generalization. \n\nMultitask fine-tuning for zero-shot generalization In multitask learning, a model is trained simultaneously on a collection of datasets from different tasks. Multitask learning generally assumes access to all datasets at once, which differs from the focus of our work. In the simplest case, a base model is fine-tuned on a multitask mixture of datasets.",
            "score": 0.4382221516001221,
            "section_title": "Qualitative Analysis",
            "char_start_offset": 29458,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1867
                }
            ],
            "ref_mentions": [
                {
                    "start": 829,
                    "end": 851,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 912,
                    "end": 931,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 948,
                    "end": 965,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.572265625
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "By combining these two factors, we derive the final merging coefficients, which are then applied to merge the corresponding layers. Figure 1 highlights how Sens-Merging enhances existing taskvector techniques like Task Arithmetic (Ilharco et al., 2023b) and DARE (Yu et al., 2024). Notably, when combined with DARE method, Sens-Merging enables merged models to outperform specialized fine-tuned models, particularly in code generation tasks. \n\nTo empirically demonstrate the effectiveness of Sens-Merging, we conduct extensive experiments by combining it with existing model merging approaches. We merged three widely adopted finetuned models-specializing in general knowledge (Chat), mathematical reasoning (Math), and code generation (Code)-derived from the LLaMA2-7B/13B and Mistral 7B families. The integration of our Sens-Merging not only improves baseline merging performance but enables merged models to surpass individual fine-tuned models. Notably, when merging Code model with Math and Chat models using Sens-Merging, it achieves superior performance on coding tasks compared to codespecific fine-tuning alone. These results indicate that model merging can effectively address the challenges of training a single model for complex tasks by integrating the specialized capabilities of multiple fine-tuned models. \n\nTo sum up, our contributions include: (1) We propose a novel model merging coefficient determination method based on both task-specific and cross-task sensitivity analysis. (2) Through comprehensive evaluations, we validate that our proposed method enhances model merging performance across various domains. (3) We empirically demonstrate that different task-specific models contribute unequally to model merging, and parameter importance varies across different layers within each model. (4) We validate that each scaling ap-proach presents distinct trade-offs: task-specific scaling excels in specialized domains like mathematics but offers limited general benefits, while cross-task scaling achieves broader performance gains at the cost of peak task-specialized performance.",
            "score": 0.4379216848691117,
            "section_title": "Introduction",
            "char_start_offset": 2335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 253,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 263,
                    "end": 280,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96337890625
        },
        {
            "corpus_id": "273374869",
            "title": "The Non-Local Model Merging Problem: Permutation Symmetries and Variance Collapse",
            "text": "LocalMerging work by first finding a so-called task vectors, defined as \u03c4 t = \u03b8 t \u2212 \u03b8 init , then potentially processing it further via, e.g., masking, rescaling, etc., and eventually averaging to obtain \u03c4 MTL . The merged model is then obtained by \n\nWe review standard techniques below. \n\nAverage merging. Average-based merging is a straightforward approach to model merging, where the weights of the individual expert models \u03b8 t are simply averaged, \u03b8 merged := 1 T t \u03b8 t . This technique has been shown to be surprisingly effective in some cases and can be linked to the concept of linear mode connectivity. If the expert models are located in the same (multi-task) loss basin and are linearly connected, averaging their weights can be seen as finding a point along the linear path connecting them. There are several variations and extensions of the basic approach. Some studies have explored weighted averaging, where different weights are assigned to different expert models based on their performance or importance. Other approaches have investigated layer-wise averaging, where the weights of each layer are averaged separately, allowing for more flexibility in combining models with different architectures. \n\nTask arithmetic. Task arithmetic is a more recent model merging approach that proposed working with the task vectors \u03c4 t 's, instead of the \u03b8 t 's (Ilharco et al., 2023). These task vectors are then averaged and added to or subtracted from the foundation model to produce a merged model with desirable properties, i.e., \u03c4 MTL := \u03bb \u2022 t \u03c4 t . This method has been shown to be more effective than simple averaging, particularly when merging models trained on diverse tasks. Ties merging. TIES-merging, introduced by Yadav et al. ( 2024), is a model merging approach that specifically addresses the issue of interference between tasks when merging models. It recognizes that simply averaging or arithmetically combining model weights can lead to negative interference, where the performance on individual tasks deteriorates after merging. To mitigate this, TIES-merging employs a technique to identify and resolve interference by removing smallest magnitude weights on the task vectors, and only averaging the weights that agree in their sign (where the chosen sign is determined by averaging the task vectors). \n\nTALL-mask merging.",
            "score": 0.4373870789958392,
            "section_title": "Standard model merging functions",
            "char_start_offset": 9637,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 287
                },
                {
                    "start": 290,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1688
                },
                {
                    "start": 1689,
                    "end": 1702
                },
                {
                    "start": 1703,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2325
                },
                {
                    "start": 2328,
                    "end": 2346
                }
            ],
            "ref_mentions": [
                {
                    "start": 1365,
                    "end": 1387,
                    "matchedPaperCorpusId": "254408495"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74755859375
        },
        {
            "corpus_id": "273404154",
            "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "text": "Low-Rank Adaptation (LoRA) is a popular technique for parameter-efficient fine-tuning of Large Language Models (LLMs). We study how different LoRA modules can be merged to achieve skill composition -- testing the performance of the merged model on a target task that involves combining multiple skills, each skill coming from a single LoRA. This setup is favorable when it is difficult to obtain training data for the target task and when it can be decomposed into multiple skills. First, we identify practically occurring use-cases that can be studied under the realm of skill composition, e.g. solving hard math-word problems with code, creating a bot to answer questions on proprietary manuals or about domain-specialized corpora. Our main contribution is to show that concatenation of LoRAs (CAT), which optimally weights LoRAs that were individually trained on different skills, outperforms existing model- and data- merging techniques; for instance on math-word problems, CAT beats these methods by an average of 43% and 12% respectively. Thus, this paper advocates model merging as an efficient way to solve compositional tasks and underscores CAT as a simple, compute-friendly and effective procedure. To our knowledge, this is the first work demonstrating the superiority of model merging over data mixing for binary skill composition tasks. Code and data are available at https://github.com/aksh555/LoRA-Soups",
            "score": 0.437154869464611,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6962890625
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "In the pretraining-finetuning paradigm, different model branches that share a common pretrained initialization exhibit specific linear properties. These properties, such as Cross-Task Linearity (CTL), allow for the linear interpolation of weights and corresponding feature spaces across tasks. This phenomenon enables effective model merging by ensuring consistency between parameter spaces and feature representations, even when the models are fine-tuned on different tasks. It provides a foundation for combining diverse taskspecific models into a unified framework [Zhou et al., 2024b]. \n\nDirect Merging type is a relatively simple model merging method that avoids the complexities of resolving conflicts and interferences during the merging of model weights. This approach does not require retraining, significantly reducing computational costs. Moreover, many advanced model merging methods are built upon these straightforward foundational approaches, highlighting their importance in the field. Despite their simplicity, these foundational methods serve as the cornerstone for further innovations and advancements in model merging techniques. Several model merging methods fall into this category. The following sections introduce these approaches. \n\nModel Soup ( Weight Averaging ): It is a method for merging fine-tuned models by averaging their weights, improving accuracy and robustness without additional training or inference costs. It works well when models share the same pretrained initialization but differ in hyperparameters [Wortsman et al., 2022]. Variants like uniform averaging and greedy soup selectively optimize performance. Model soup is efficient, flexible, and outperforms individual models, making it a robust alternative to ensembles for tasks like image and text classification. However, this method does not account for weight conflicts during model merging, which often results in performance that falls short of more sophisticated model merging approaches. \n\nTask arithmetic: It is a method for editing pre-trained models by leveraging task vectors, which are computed as the difference between a model's fine-tuned weights and its pre-trained weights (\u03c4 t = \u03b8 t \u2212 \u03b8 0 ). These vectors encapsulate the changes needed for a model to perform a specific task and can be applied to modify model weights through simple operations like addition, subtraction, and analogy-based reasoning [Ilharco et al., 2022].",
            "score": 0.4367455823398396,
            "section_title": "Direct Merging",
            "char_start_offset": 9842,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1255
                },
                {
                    "start": 1258,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2205
                },
                {
                    "start": 2206,
                    "end": 2438
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 588,
                    "matchedPaperCorpusId": "267499590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7451171875
        },
        {
            "corpus_id": "274423281",
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "text": "As the pretraining-finetuning paradigm gains increasing popularity [38], the research community has witnessed a proliferation of finetuned models [20], typically derived from foundational models such as T5 [23], and CLIP [22], among others. However, these models are often finetuned on task-specific training data, which limits their capacity for out-of-domain generalization [25,29,30]. In diverse realworld applications, the independent deployment of multiple fine-tuned models increases storage costs and computational demands. While traditional multi-task learning methods can mitigate these issues, they typically necessitate concurrent training across multiple task-specific datasets. Nonetheless, managing original datasets incurs significant expenses and potential privacy risks [16]. Moreover, when confronted with new tasks, traditional multi-task learning methods necessitate training from scratch. Consequently, exploiting existing models to construct efficient multi-task models has become a crucial challenge. \n\nFortunately, model merging [10] has garnered growing attention as an economical and efficient method for obtaining multi-task models. This approach aims to merge multiple task-specific models, requiring no original training data or only a small amount of unlabeled data, thereby enabling the merged model to perform efficiently across diverse tasks [40]. As a foundational technique in this field, Ilharco et al. [10] introduced the concept of Task Arithmetic. Specifically, Task Arithmetic combines task vectors through arithmetic operations, facilitating the efficient transfer of capabilities among different tasks and thus enabling the construction of multi-task models. Here, task vectors are derived by computing the difference between the weights of fine-tuned models and the pre-trained model. However, related studies have indicated that task interference has become the main challenge for this method [21,39]. Previ-ous studies have proposed various techniques to mitigate this issue [8,33,39,44]. Ties-Merging [39] addresses this challenge by pruning redundant parameters, resolving sign conflicts, and averaging parameters aligned along dominant directions. DARE [44] reduces merging conflicts through random parameter drop and maintains model performance via rescaling operations. Consensus Merging [33] aims to eliminate selfish and catastrophic weights. Despite these advancements, these methods often rely on empirical strategies to resolve conflicts.",
            "score": 0.43670549914950807,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1023
                },
                {
                    "start": 1026,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2319
                },
                {
                    "start": 2320,
                    "end": 2394
                },
                {
                    "start": 2395,
                    "end": 2493
                }
            ],
            "ref_mentions": [
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "257766557"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "267547973"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 376,
                    "end": 380,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 380,
                    "end": 383,
                    "matchedPaperCorpusId": "252780443"
                },
                {
                    "start": 383,
                    "end": 386,
                    "matchedPaperCorpusId": "258832820"
                },
                {
                    "start": 787,
                    "end": 791,
                    "matchedPaperCorpusId": "52297310"
                },
                {
                    "start": 1053,
                    "end": 1057,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1439,
                    "end": 1443,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1937,
                    "end": 1941,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1941,
                    "end": 1944,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2023,
                    "end": 2026,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 2026,
                    "end": 2029,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2029,
                    "end": 2032,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2047,
                    "end": 2051,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2201,
                    "end": 2205,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2338,
                    "end": 2342,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68310546875
        },
        {
            "corpus_id": "274788575",
            "title": "Revisiting Weight Averaging for Model Merging",
            "text": "Model merging has emerged as an efficient way to construct multi-task learners [26]. Unlike traditional multi-task learning approaches that directly train a single model on multiple tasks [1], model merging leverages individually fine-tuned models and fuses their parameters to create a model that preserves their original capabilities. This approach eliminates the need to prepare training data or store separate sets of parameters for each task, thereby reducing the costs associated with modern deep neural networks, which often require large amounts of data and numerous parameters. Consequently, model merging has been favored in various applications, including federated learning [37], model compression [14,47], and continual learning [15,29]. \n\nAlongside fundamental observations about mode connectivity in parameter space [9,10,12], model merging through direct interpolation between fine-tuned parameters has become prevalent in the literature. For example, the most straightforward approach is simply averaging the parameters of different models, a method known as weight averaging [3,15,18,30,48]. However, when the knowledge encoded in each model differs significantly, weight averaging can lead to substantial interference between parameters. This interference often results in considerable performance degradation compared to traditional multi-task learning approaches, especially as the number of merged models increases. \n\nTo mitigate the limitations of the weight averaging, recent approaches have leveraged the task arithmetic framework [17], which allows for the extrapolation of the parameters. In this framework, models are assumed to be fine-tuned from a common initialization, enabling the definition of task vectors as the directions pointing from the initialization to the fine-tuned parameters in the parameter space. Among several arithmetic operations on task vectors, addition with a scaling coefficient has shown to be effective in merging models trained on various tasks. Subsequent approaches have improved this arithmetic by either resolving interference between task vectors [11,51] or applying test-time adaptation techniques for better scaling coefficient [52]. \n\nIn this paper, we revisit the weight averaging strategy through the lens of task arithmetic. We begin by formulating weight averaging as a task arithmetic that induces centered task vectors around the weight average itself. We then observe that applying low-rank approximations on the centered task vectors dramatically improves the performance of the merged model produced by the corresponding task arithmetic.",
            "score": 0.43588135618317925,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 750
                },
                {
                    "start": 753,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2198
                },
                {
                    "start": 2201,
                    "end": 2293
                },
                {
                    "start": 2294,
                    "end": 2424
                },
                {
                    "start": 2425,
                    "end": 2612
                }
            ],
            "ref_mentions": [
                {
                    "start": 188,
                    "end": 191,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 686,
                    "end": 690,
                    "matchedPaperCorpusId": "261787828"
                },
                {
                    "start": 710,
                    "end": 714,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 714,
                    "end": 717,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 742,
                    "end": 746,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "3845139"
                },
                {
                    "start": 834,
                    "end": 837,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 837,
                    "end": 840,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1096,
                    "end": 1099,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 1099,
                    "end": 1102,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1102,
                    "end": 1105,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1105,
                    "end": 1108,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1556,
                    "end": 1560,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 2114,
                    "end": 2117,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 2193,
                    "end": 2197,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7734375
        },
        {
            "corpus_id": "276884783",
            "title": "TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation",
            "text": "Recently, the DeepSeek-R1 model has achieved great success, and its released R1-Distill models (DeepSeek-AI, 2025) demonstrated that distilled small models can be superior in reasoning. Building smaller-scale models is also beneficial for deployment and reducing inference costs. However, developing smaller yet powerful models is a key challenge in Large Language Models (LLMs). \n\nThe most effective method, to our knowledge, is distilling a smaller model from a bigger teacher model across various domains (Jiao et al., 2020;DeepSeek-AI, 2025;Team, 2025a;Muennighoff et al., 2025). However, this method has a fundamental limitation: it requires carefully selecting the most relevant data/domains and tuning their proportions for joint training, which is typically time-consuming and error-prone (Guo et al., 2019;Ji et al., 2024). Furthermore, optimizing many domains simultaneously can lead to conflicting gradients, where tasks interfere, impeding overall learning progress (Yu et al., 2020;Jiang et al., 2024). These problems limit the effectiveness and efficiency of naive data mixed distillation, often resulting in models that cannot achieve the performance levels desired for specialized tasks. \n\nTo address these issues and optimize performance across multiple areas, we propose an approach, namely branch-merge, which integrates a model-merging technique during the distillation. Our branch-merge distillation approach contains two phases as follows. \n\n\u2022 Branch Phase: Knowledge is selectively distilled from a unified large teacher model (e.g., DeepSeek-R1 671B) to instruct several specialized student models (e.g., math, coding, science) through domain-specific SFT. \n\n\u2022 Merge Phase: The specialized models are combined into a single unified model, enabling crossdomain knowledge transfer while preserving their original specialized capabilities.",
            "score": 0.43572065228820367,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1680
                },
                {
                    "start": 1683,
                    "end": 1860
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 815,
                    "matchedPaperCorpusId": "102353198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "276813206",
            "title": "LEWIS (LayEr WIse Sparsity) - A Training Free Guided Model Merging Approach",
            "text": "As specialized large language models (LLMs) fine-tuned for tasks such as math solving or instruction following become more prevalent, efficient model-merging methods have gained critical importance. State-of-the-art techniques like TIES Yadav et al. (2024), DARE Yu et al. (2024), andDeLLA Deep et al. (2024) rely on task vectors Ilharco et al. (2022)-parameter deltas between a pre-trained model and its fine-tuned variant-to merge models. Although these data-less strategies prune task vectors and fuse them into multi-task models, they often yield only moderate performance across tasks. To address this, recent works such as Model Breadcrumbs Davari & Belilovsky (2025), AdaMerging++ Yang et al. (2023), and Localize and Stitch He et al. (2024) have explored optimizing layer-or parameter-level importance to reduce task interference, but at a higher computational cost. Earlier model-merging methods, including simple averaging Choshen et al. (2022), Fisher-weighted approaches Matena & Raffel (2022), and geometric-based solutions Ainsworth et al. (2022); Stoica et al. (2023), often suffer from task interference or disregard crucial details like outlier activations. These outlier activations emerge in large-scale transformers Dettmers et al. (2022) and can be 100 times larger than typical hidden states, making naive pruning detrimental to LLM performance Sun et al. (2023); Wei et al. (2024). Empirical findings also indicate that different fine-tunes exhibit varying layer norms, magnitudes, and angles Jang et al. (2025), hinting that layer-wise treatment can be beneficial. \n\nIn this work, we propose a guided model-merging strategy that augments state-of-the-art methods (e.g., TIES Yadav et al. (2024) and DARE Yu et al. (2024)) by leveraging insights from Wanda pruning Sun et al. ( 2023) and a calibration dataset to fine-tune layer-level task-vector sparsity.",
            "score": 0.4352909386762899,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1879
                }
            ],
            "ref_mentions": [
                {
                    "start": 647,
                    "end": 673,
                    "matchedPaperCorpusId": "266174505"
                },
                {
                    "start": 983,
                    "end": 1005,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1062,
                    "end": 1082,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 1236,
                    "end": 1258,
                    "matchedPaperCorpusId": "251564521"
                },
                {
                    "start": 1367,
                    "end": 1384,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 1516,
                    "end": 1534,
                    "matchedPaperCorpusId": "268733341"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "267938070",
            "title": "Knowledge Fusion of Chat LLMs: A Preliminary Technical Report",
            "text": "Large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series have demonstrated remarkable success across a wide range of natural language processing (NLP) tasks. However, the computational resources and time costs associated with LLM development remain prohibitively high for most entities. Despite the structural and functional differences among LLMs, they often exhibit similar capabilities across various tasks. Therefore, moving beyond the traditional approach of training a LLM from scratch, an alternative option is to combine existing LLMs into a new, more powerful one, which is termed knowledge fusion of LLMs by Wan et al. (2024). If successful, this fusion not only reduces the initial training costs but also enables the combined model to leverage the strengths of multiple LLMs. \n\nThe endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone and Warmuth, 1994;Jiang et al., 2023) directly aggregate the outputs of different models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models and executing each during inference, which is inefficient for LLMs due to their substantial memory and inference time requirements. Another approach is to directly merge several neural networks into a single network through arithmetic operations in the parameter space (Gupta et al., 2020). This approach typically assumes uniform network architectures and seeks to merge the parameters of different neural networks either through manual merging weights (Wortsman et al., 2022;Yadav et al., 2023) or by automatically obtaining merging weights based on model gradients or representations of additional data (Matena and Raffel, 2022;Jin et al., 2022). Recently, FUSELLM (Wan et al., 2024) introduced a new paradigm for integrating the capabilities of multiple LLMs. This approach externalizes the knowledge of multiple source LLMs using their generated probability distribution matrices and transfers their collective knowledge into a target LLM through lightweight continual training. Consequently, FUSELLM facilitates the fusion of multiple pre-trained LLMs with distinct architectures into a unified LLM.",
            "score": 0.4349813783574933,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 832
                },
                {
                    "start": 835,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1112
                },
                {
                    "start": 1113,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2286
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 961,
                    "end": 992,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 1635,
                    "end": 1658,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1658,
                    "end": 1677,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1787,
                    "end": 1812,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1812,
                    "end": 1829,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "Sparsity is important, but how to locate sparse regions is the key. Previous research identifies that during the finetuning stage, a significant portion of parameter updates is redundant, introducing interference in model merging (Yadav et al., 2023). This underscores the need for locating sparse regions to reduce such interference. While the importance of sparsity is recognized, strategies for achieving it remain underexplored. Earlier approaches typically identify sparse regions through random selection (Yu et al., 2023) or selecting regions with the top-k% largest magnitudes in task vectors (Yadav et al., 2023). However, they often fall short in identifying the most effective sparse regions for model merging. In Figure 2, we evaluate the efficacy of different localization methods across twelve language tasks, comparing the quality of their localized regions (specified by the binary mask \u03b3 i ). The performance is assessed on individual grafted models for each task, denoted as \u03b8 pre + \u03b3 i \u2299 \u03c4 i , where \u03c4 i is the task vector of the i-th task and \u2299 is the element-wise product. This grafted performance measures how well the finetuned skills are preserved when only keeping parameter updates during finetuning in localized regions. Unlike previous methods, we directly optimize the binary masks to maximally retain finetuned performance, detailed in Section 3.2. Our method significantly outperforms others, especially at lower sparsity levels. The strength of our approach lies in its precision in identifying small but informative regions, which is particularly advantageous for model merging. \n\nSparse regions with less overlap reduce task conflicts. Identifying the smallest possible regions with essential finetuned skills is key to minimizing potential conflicts among task vectors, as smaller localized regions naturally incur less overlap among tasks. With reduced overlap, each task can occupy its own, relatively disjoint localized region, thereby reducing task conflicts. This has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation.",
            "score": 0.4342247482509064,
            "section_title": "Motivation and objectives",
            "char_start_offset": 7245,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 622
                },
                {
                    "start": 623,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1611
                },
                {
                    "start": 1614,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2248
                }
            ],
            "ref_mentions": [
                {
                    "start": 230,
                    "end": 250,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 601,
                    "end": 621,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84423828125
        },
        {
            "corpus_id": "275119334",
            "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging",
            "text": "Negative transfer remains a fundamental challenge in model merging for MTL, as merging often inherits task interference within the shared representation. The goal is to construct a single model that generalizes well across all tasks while minimizing such interference. Existing weight alignment methods [1,55,71] enable merging fine-tuned weights obtained from different pre-trained models. However, these approaches face scalability limitations as the number of tasks increases. A foundational work in model merging for MTL is Task Arithmetic [23], which introduces task vectors, defined as the difference between pre-trained and fine-tuned models. This enables arithmetic-based weight combinations across multiple tasks, allowing the merging of a large number of individual models. Subsequent methods refine this approach by explicitly addressing task conflict within the merged model. Ties-Merging [72] mitigates redundancy and sign conflicts, while DARE [75] removes redundant components and rescales key parameters. AdaMerging [74] learns task vector importance at test time, and LiNeS [64] reduces the impact of shallow layers to preserve general representations. While merged models have traditionally been considered task-agnostic, recent findings [19,63] suggest that they inherently encode task-specific knowledge. Motivated by these observations, recent approaches incorporate task-specific modules into the merged encoder to better align with the performance of individual models. Surgery [73] refines merged encoders by introducing taskspecific adapters, while EMR-Merging [21] applies taskspecific masking to a pre-defined merged model. Routerbased methods [36,58] determine task vector coefficients via routing mechanisms that depend on each task's input. However, these methods introduce additional parameters and often overlook the role of a well-structured shared expert in maximizing task-specific module effectiveness. \n\nModel Tinting explicitly refines both shared and taskspecific layers in a complementary manner, which allows it to easily capture hidden task-specific knowledge. This balanced design improves task adaptation and often surpasses individual model performance.",
            "score": 0.434054519374139,
            "section_title": "Model Merging for Multi-task Learning",
            "char_start_offset": 6112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 390
                },
                {
                    "start": 391,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1938
                },
                {
                    "start": 1941,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 303,
                    "end": 306,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 309,
                    "end": 312,
                    "matchedPaperCorpusId": "268248251"
                },
                {
                    "start": 544,
                    "end": 548,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 901,
                    "end": 905,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 958,
                    "end": 962,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1032,
                    "end": 1036,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1091,
                    "end": 1095,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 1256,
                    "end": 1260,
                    "matchedPaperCorpusId": "271957310"
                },
                {
                    "start": 1260,
                    "end": 1263,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1586,
                    "end": 1590,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1671,
                    "end": 1675,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "267365047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80517578125
        },
        {
            "corpus_id": "263831551",
            "title": "Parameter Efficient Multi-task Model Fusion with Partial Linearization",
            "text": "Ties-meriging introduces several innovations to address the challenges of parameter interference. This indicates that the degree of interference between visual tasks is higher than that between NLP tasks. Compare the cosine similarities matrix between Figure 2(a) and Figure 8(a), indeed, the task vectors on vision domain have higher cosine similarity, which is consistent with this model fusion phenomenon. Since the higher cosine similarity between task vectors implies greater redundancy and overlap in the knowledge captured. This results in more destructive task interference when naively merging the specialized models. \n\nFor both image classification tasks and NLP tasks, we observe that the normalized scores of L-LoRA surpass LoRA fine-tuning in most cases, and L-LoRA even exceeds full fine-tuning with task arithmetic and ties-merging. In addition, LoRA is more effective than L-LoRA for a smaller number of tasks in some cases, but as the number of task vectors increases, L-LoRA surpasses LoRA. This phenomenon also suggests the effectiveness of L-LoRA fine-tuning for multi-task model fusion, we further discuss it in Appendix E.2. \n\nA more in-depth analysis comparison between LoRA and L-LoRA is provided in Figures 10 and 11. For further elaboration and supplementary details, please refer to Appendix E.",
            "score": 0.4338505137691021,
            "section_title": "MULTI-TASK MODEL FUSION ON VISION AND LANGUAGE DOMAINS",
            "char_start_offset": 23364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1146
                },
                {
                    "start": 1149,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73876953125
        },
        {
            "corpus_id": "272968955",
            "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
            "text": "Model merge refers to combining the parameters and features of multiple large pretrained models to generate a unified model that can perform better in multiple tasks. Through model merge, the advantages of different models can be utilized to enhance the model's generalization and multitask processing capabilities. In the general setting of model merge, given a set of K tasks and the corresponding pretrained or fine-tuned models, whose parameters are denoted as {\u03b8 \u03b8 \u03b8 1 , \u03b8 \u03b8 \u03b8 2 , . . . , \u03b8 \u03b8 \u03b8 K }. The goal of model merging is to combine these K models into a single model that can effectively handle all K tasks. It is important to note that these models are fine-tuned from the same base model with parameters \u03b8 \u03b8 \u03b8 base . The merging process can be represented as (Cong et al. 2024): \n\nwhere \u03b8 \u03b8 \u03b8 merge is the parameters of the merged model that can efficiently perform all K tasks; and g merge represents the model merging method. The illustration of the mathematical representation of model merging is shown in Fig. 1(b). Among existing model merge methods, average merging (Matena and Raffel 2022) is a common approach, which constructs merged models by averaging parameters expressed as \u03b8 \u03b8 \u03b8 merge = K k=1 \u03b8 \u03b8 \u03b8 k K . Model soups (Wortsman et al. 2022) generates a multifunctional composite merged pretrained model by simply linearly combining the parameters of multiple fine-tuned models, denoted as \u03b8 \u03b8 \u03b8 merge = K k=1 \u03bb k \u03b8 \u03b8 \u03b8 k . Task arithmetic (Ilharco et al. 2023) uses predefined scaling coefficients to differentiate the importance of various models, which is described as \n\n. Ties merging method (Yadav et al. 2024) addresses the task conflict problem in task arithmetic by pruning low-magnitude parameters, resolving sign discrepancies, and non-overlappingly merging parameters with consistent signs. DARE (Drop And REscale) merge method (Yu et al. 2024) sets most delta parameters denoted as \u03b4 k = \u03b8 \u03b8 \u03b8 k \u2212\u03b8 \u03b8 \u03b8 base to zero and rescales the remaining embeddings to approximate the original embeddings.",
            "score": 0.4336231251223166,
            "section_title": "Background",
            "char_start_offset": 6107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 504
                },
                {
                    "start": 505,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1828
                },
                {
                    "start": 1829,
                    "end": 2032
                }
            ],
            "ref_mentions": [
                {
                    "start": 1467,
                    "end": 1487,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1623,
                    "end": 1642,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51806640625
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "Multi-task learning (MTL) leverages a shared model to accomplish multiple tasks and facilitate knowledge transfer. Recent research on task arithmetic-based MTL demonstrates that merging the parameters of independently fine-tuned models can effectively achieve MTL. However, existing merging methods primarily seek a static optimal solution within the original model parameter space, which often results in performance degradation due to the inherent diversity among tasks and potential interferences. To address this challenge, in this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach. Building on WEMoE, we further introduce an efficient-and-effective WEMoE (E-WEMoE) method, whose core mechanism involves eliminating non-essential elements in the critical modules of WEMoE and implementing shared routing across multiple MoE modules, thereby significantly reducing both the trainable parameters, the overall parameter count, and computational overhead of the merged model by WEMoE. Experimental results across various architectures and tasks demonstrate that both WEMoE and E-WEMoE outperform state-of-the-art (SOTA) model merging methods in terms of MTL performance, generalization, and robustness.",
            "score": 0.4331618574161741,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "In this section, we first introduce the notation and problem definition of model merging in \u00a72.1. We then elaborate on advanced model merging methods (Table 1 summarizes the primary purpose of each category of methods). Existing model merging techniques can be roughly divided into the following two categories: (i) Before Merging Methods in \u00a72.2: it provides better prior knowledge for model merging. (ii) During Merging Methods in \u00a72.3: it resolves task conflict/interference by various strategies, and then performs parameter merging operations. Finally, we conclude with theories or explanations for the effectiveness of model merging in \u00a72.4. Merge multiple models by projecting them into a sparse subspace Routing-based Merging Methods ( \u00a72. 3.4) Dynamically merge multiple models based on input during the inference phase Post-calibration-based Merging Methods ( \u00a72.3.5) Calibrating the merged model to be closer to the individual models reduces the knowledge loss",
            "score": 0.4327239558549122,
            "section_title": "Advanced Model Merging Methods",
            "char_start_offset": 7907,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 971
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8212890625
        },
        {
            "corpus_id": "266520969",
            "title": "Multimodal Attention Merging for Improved Speech Recognition and Audio Event Classification",
            "text": "Prior work in multimodal merging includes OTKGE [21], that uses Optimal Transport to align structural knowledge, linguistic information, and image embeddings in knowledge graphs. However, they do not extend their approach to merging model weights and focus on embeddings instead. Voice2Series [5] and Frozen Pretrained Transformer [4] demonstrate knowledge transfer across modalities through frozen self-attention weights. While both works study the transferability of self-attention across modalities, they stop short of merging models trained from different modalities and do not address sequence-to-sequence tasks such as ASR. In contrast, works such as Fisher merging [22], local fine-tuning [16], MODEL SOUPS [19], DMC [23], ADAPTERSOUP [24], MLM [25] discuss model merging but do not consider the multimodal scenario. Perhaps the closest work to ours is Multimodal Model Merging [15], an empirical study of merging vision and text models for combined vision-language tasks such as Visual Question Answering [26] and image-text retrieval [27]. The approach studies the use of simple interpolation, REGMEAN [15], and Task Vectors [28] to determine the best model merging approach. However, they require contrastive model alignment, use a shared seed pre-training phase to initialize models prior to merging, and address joint vision+language tasks. In contrast, MAM merges off-the-shelf models from different modalities without constraints on pre-training tasks or weight initialization. Through L-MAM, we also present an approach to learn the interpolation factor in the case where limited data/compute is available, reducing the requirement on empirical experimentation.",
            "score": 0.4324544495774363,
            "section_title": "RELATED WORK",
            "char_start_offset": 3859,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1676
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 52,
                    "matchedPaperCorpusId": "258509157"
                },
                {
                    "start": 293,
                    "end": 296,
                    "matchedPaperCorpusId": "235458522"
                },
                {
                    "start": 672,
                    "end": 676,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 714,
                    "end": 718,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 752,
                    "end": 756,
                    "matchedPaperCorpusId": "13753663"
                },
                {
                    "start": 1013,
                    "end": 1017,
                    "matchedPaperCorpusId": "3180429"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.280517578125
        },
        {
            "corpus_id": "267365047",
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "text": "Another significant challenge of merging multi-task models is mitigating the interference between parameters of different models, which can substantially deteriorate the average performance (Yadav et al., 2023;Yu et al., 2023;Tang et al., 2023b). Existing methods, while effective in some scenarios, may not be flexible enough to handle the dynamic nature of multi-task learning, where the optimal solution can vary depending on the input. \n\nTo address these challenges, we propose a novel approach to merge vision Transformers (ViTs). Our method merges most of the parameters while upscaling the multilayer perceptron (MLP) of the Transformer layers to a weight-ensembling Mixture of Experts (MoE) module. This module can dynamically integrate shared and task-specific knowledge based on the input sample, thereby providing a more flexible solution that can adapt to the specific needs of each instance. \n\nOur primary realization is that the issue of parameter inter-  Here we outline the detailed structure of the Weight-Ensembling MoE module, composed of the router, pre-trained MLP weights, and a collection of task vectors. Collaboration between shared weights and task vectors is employed to create input-conditioned weights dynamically. In this way, we separate shared information and task-specific knowledge, which are then combined based on input in time. \n\nference can be significantly alleviated by identifying and separating shared and task-specific knowledge and then dynamically integrating them. So we can leverage the shared knowledge that is beneficial across all tasks, while also taking into account the unique requirements. By dynamically combining these two types of knowledge based on the specific input data, we can create a more flexible and adaptable model that can effectively handle a wide range of tasks. \n\nWe validate our method through conventional multi-task model merging experiments and evaluate its generalization and robustness. The results demonstrate the effectiveness of our method and provide a comprehensive understanding. \n\nTo summarize, our contributions are as follows: \n\n\u2022 We propose a novel method to merge Transformerbased models. Our method is effective in transferring knowledge from various task-specific fine-tuned models and constructing a unified multi-task model.",
            "score": 0.4311587147226664,
            "section_title": "Introduction",
            "char_start_offset": 1869,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 904
                },
                {
                    "start": 907,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1364
                },
                {
                    "start": 1367,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1832
                },
                {
                    "start": 1835,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2062
                },
                {
                    "start": 2065,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2316
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1153564453125
        },
        {
            "corpus_id": "276776501",
            "title": "CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging",
            "text": "Model merging has gained increasing attention in the deep learning community, particularly in the context of using task vectors for model merging in large language models (LLMs) (Ilharco et al., 2022;Li et al., 2023;Wortsman et al., 2022;Jin et al., 2022;Matena & Raffel, 2022;Singh & Jaggi, 2020;Akiba et al., 2024). This technique has become especially popular for merging homologous models, those derived by fine-tuning the same base model on different 1 State Key Laboratory of Complex & Critical Software Environment (CCSE), Beihang University, Beijing, China 2 Hangzhou Innovation Institute of Beihang University, Hangzhou, China 3 National University of Singapore, Singapore, Singapore . Correspondence to: Hailong Sun <sunhl@buaa.edu.cn>. tasks, to create a better-performing model. Many of the bestperforming models on the LLM leaderboard (Beeching et al., 2023) are built by fine-tuning the base models and subsequently merging them to optimize task-specific performance. Additionally, major enterprises have employed model merging techniques in the development of pre-training models, such as Llama3 (Dubey et al., 2024) and Qwen2 (Yang et al., 2024a;Lu et al., 2024), to enhance generalization capabilities and improve performance across a range of tasks. \n\nRecent studies have further shown that sparsifying task vectors before merging can mitigate parameter conflicts between different task vectors, leading to measurable improvements in merging performance (Yu et al., 2024;Yadav et al., 2024;Davari & Belilovsky, 2023;He et al., 2024). These conflicts can be categorized into two types: (a) conflicts due to redundant parameters, where parameters that contribute little to performance are unnecessarily retained, and (b) conflicts due to overlapping parameters, where task vectors retain parameters that overlap, potentially with significantly different magnitudes or signs. Such overlaps hinder the effectiveness of the merging process. \n\nSparsifying task vectors, whether selectively or randomly, aims to reduce conflicts in model merging.",
            "score": 0.4310919816026011,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1267
                },
                {
                    "start": 1270,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2057
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 200,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 216,
                    "end": 238,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 238,
                    "end": 255,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 277,
                    "end": 297,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 1472,
                    "end": 1489,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1489,
                    "end": 1508,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "271874793",
            "title": "FuseChat: Knowledge Fusion of Chat Models",
            "text": "Large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series have demonstrated remarkable success across a wide range of natural language processing (NLP) tasks. Currently, it has become prevalent and imperative for individuals and corporations to build their own LLMs. However, the computational resources and time costs associated with LLM development remain prohibitively high. Furthermore, despite the structural and functional differences among LLMs, they often exhibit similar capabilities across various tasks. Therefore, besides training an LLM from scratch, another option is to combine the distinct advantages of existing LLMs into a more potent LLM, which is termed knowledge fusion of LLMs (Wan et al., 2024). Figure 1(a) illustrates the results of our preliminary experiment conducted on AlpacaEval 2.0 and MT-Bench, where we plot the percentage of questions each LLM answers best (measured by PairRM (Jiang et al., 2023)) among six prominent chat LLMs. These established LLMs, regardless of their potency, exhibit distinct strengths. Therefore, knowledge fusion not only reduces the developmental costs of creating a new LLM but also has the potential to integrate the diverse strengths of existing models. \n\nThe endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone and Warmuth, 1994;Jiang et al., 2023) directly aggregate the outputs of multiple models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models during inference, which is inefficient for LLMs due to their substantial memory and inference time requirements. Another approach is to directly merge several neural networks into a single network through arithmetic operations in the parameter space (Gupta et al., 2020), whereas this approach typically assumes uniform network architectures and requires manually-tuned (Wortsman et al., 2022;Yadav et al., 2024) or automatically-learned (Matena and Raffel, 2022;Jin et al., 2023) coefficients to merge the parameters of different neural networks.",
            "score": 0.42926161995008616,
            "section_title": "Introduction",
            "char_start_offset": 259,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 61,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 743,
                    "end": 761,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 955,
                    "end": 975,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1390,
                    "end": 1421,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 1421,
                    "end": 1440,
                    "matchedPaperCorpusId": "259075564"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3447265625
        },
        {
            "corpus_id": "276885406",
            "title": "Statistical Deficiency for Task Inclusion Estimation",
            "text": "Inspired from ensemble methods (Dietterich, 2000), model merging focuses on combining several existing models to create a new one. Recent methods either use arithmetic operations (Ilharco et al., 2023;Tao et al., 2024;Ortiz-Jimenez et al., 2024;Zhou et al., 2024;Zeng et al., 2025) or more complex aggregation methods (Yadav et al., 2023;Jin et al., 2023;Yang et al., 2023), with the goal of solving conflicts or interferences between models (Yu et al., 2020;Sener and Koltun, 2018) and thus tasks. While task and model merging focuses on the parameter space (whose dimension is excessively large), the tools developed here focus on the activation space. However, we propose in App. H an analysis of parameter space based approaches and we show some limitations, despite some interesting behaviors. \n\nTask Transfer. Transfer learning (Torrey and Shavlik, 2010;Hanneke and Kpotufe, 2024;Lange et al., 2021) consists in leveraging a model pretrained for a new task for a given task, either as an initialization point for further training or to generate useful representations. Although most of the time one uses a generic pre-trained model and trains directly for the new task, Vu et al. (2020) showed that some tasks might benefit from training on an intermediate task, effectively building a path of (easily) transferable tasks. In computer vision, this phenomenon has been studied and quantified by Bao et al. (2019). Zamir et al. (2018), obtained similar results showing connections between various visual tasks and were able to leverage these structures to optimize training of multitask models (Zhang and Yang, 2021). Knowledge transferability between different tasks is also at the heart of modern machine learning (ML) and generalization as exhibited by models such as T5 (Wei et al., 2021;Khashabi et al., 2020), or more recently instructtype models (Zhang et al., 2023b) with multi-task training leading to significantly stronger results.",
            "score": 0.4291151130143398,
            "section_title": "Introduction",
            "char_start_offset": 4028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 31,
                    "end": 49,
                    "matchedPaperCorpusId": "10765854"
                },
                {
                    "start": 442,
                    "end": 459,
                    "matchedPaperCorpusId": "210839011"
                },
                {
                    "start": 1400,
                    "end": 1417,
                    "matchedPaperCorpusId": "202782600"
                },
                {
                    "start": 1419,
                    "end": 1438,
                    "matchedPaperCorpusId": "5046249"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2322998046875
        },
        {
            "corpus_id": "274423281",
            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
            "text": "Model merging has recently gained attention as an economical and scalable approach to incorporate task-specific weights from various tasks into a unified multi-task model. For example, in Task Arithmetic (TA), adding the fine-tuned weights of different tasks can enhance the model's performance on those tasks, while subtracting them leads to task forgetting. Although TA is highly effective, interference among task still hampers the performance of the merged model. Existing methods for handling conflicts between task generally rely on empirical selection, resulting in suboptimal performance. In this paper, we introduce an Adaptive Weight Disentanglement method. We begin by theoretically proving that task vectors employed in model merging should be orthogonal to minimize interference among tasks. Guided by this insight, we initialize redundant vectors such that, when subtracted from the original task vectors, the resulting vectors exhibit increased orthogonality. Additionally, we impose an norm constraint on the redundant vectors to preserve the performance of the task-specific models. Experimental results demonstrate the effectiveness of our proposed technique: it successfully extracts redundant vectors, and after their subtraction, the task vectors not only retain robust performance but also achieve superior fusion outcomes. Our code is available at \\href{https://github.com/FarisXiong/AWD.git}{https://github.com/FarisXiong/AWD.git}.",
            "score": 0.4288813076589421,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8125
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "Relying solely on weight magnitude for pruning is insufficient, as it does not fully capture a model's functional contributions. A common alternative is pruning based on model activations or output responses, which helps reduce conflicts and improve merging effectiveness. By selectively retaining important neurons and representations, this approach enhances model performance. Representative methods include: \n\nZipIt!: This method merges models trained on disjoint tasks by identifying shared feature activations and selectively \"zipping\" them together. It introduces a partial merging strategy, allowing models to retain distinct task-specific heads while merging up to a certain layer [Stoica et al., 2023]. \n\nSurgeryV2: Addresses representation bias in model merging by performing layer-wise representation surgery. Instead of directly averaging weights, it aligns activations between expert models and the merged model, progressively refining feature representations to reduce task interference [Yang et al., 2024b]. \n\nLocalize-and-Stitch: A localized merging approach that first identifies sparse task-specific activation regions and stitches only these into the final model. By selecting highly activated neurons, it minimizes task conflicts and retains essential task-specific knowledge [He et al., 2024]. \n\nThese methods improve model merging efficiency by pruning low-importance activations, ensuring better generalization across tasks while reducing parameter redundancy. \n\nAdaMerging: It assigns different sparsity rates to different layers, pruning less important layers more aggressively while preserving critical layers [Yang et al., 2023]. This ensures that merging does not indiscriminately combine task-specific features, reducing task interference. Instead of relying solely on weight magnitudes, AdaMerging minimizes entropy to ensure more deterministic outputs during merging, making it an activation-aware approach.",
            "score": 0.4281094919367957,
            "section_title": "Activation-based Pruning Merging type",
            "char_start_offset": 22679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1022
                },
                {
                    "start": 1025,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1314
                },
                {
                    "start": 1317,
                    "end": 1483
                },
                {
                    "start": 1486,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 689,
                    "end": 710,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 1001,
                    "end": 1021,
                    "matchedPaperCorpusId": "267499590"
                },
                {
                    "start": 1296,
                    "end": 1313,
                    "matchedPaperCorpusId": "231740691"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91650390625
        },
        {
            "corpus_id": "270560921",
            "title": "DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling",
            "text": "DELLA focuses on pruning delta parameters instead of fine-tuned model parameters.\n\nModel merging techniques focus on combining multiple task-specific models into a single model with diverse capabilities without requiring the original training data (Zhang et al., 2023a).Widely recognised methods in model merging include Average Merging (Wortsman et al., 2022), which constructs merged models using averaged SFT parameters; Task Arithmetic (Ilharco et al., 2023), which uses domain-specific offsets and pre-defined scaling terms to distinguish the importance of each model being merged; Fisher Merging (Matena and Raffel, 2022), utilizing the Fisher information matrix for weighted parameter fusion (Fisher, 1922); RegMean (Jin et al., 2023), which optimizes a linear regression problem for model merging; and TIES-Merging (Yadav et al., 2024), which resolves conflicts by trimming low-magnitude parameters, resolving sign disagreements, and disjointly merging parameters with consistent signs.Recent approaches such as DARE (Yu et al., 2023) and DPPA (Zhu et al., 2024) aim to reduce interference during merging by dropping delta parameters and rescaling remaining delta parameters to maintain each model's performance before merging.\n\nThis paper focuses its contribution on merging homologous models, where model merging is achieved by manipulating delta parameters.The baselines we use in this paper primarily involve manipulating delta parameters of homologous models.",
            "score": 0.42745570419712037,
            "section_title": "Related Work",
            "char_start_offset": 19343,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 83,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1368
                },
                {
                    "start": 1368,
                    "end": 1472
                }
            ],
            "ref_mentions": [
                {
                    "start": 248,
                    "end": 269,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 337,
                    "end": 360,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 440,
                    "end": 462,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 723,
                    "end": 741,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 823,
                    "end": 843,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63720703125
        },
        {
            "corpus_id": "53037995",
            "title": "Meta-Learning Multi-task Communication",
            "text": "In this paper, we propose an orthogonal technique to alleviate the above problem. Specifically, we first describe a general framework for multi-task learning called Parameters Read-Write Networks (PRaWNs) that can simply abstract the commonalities between existing typical neural models for multi-task learning. Then, we explain the reason for pretend-to-share problem within PRaWN that each task has the permission to modify shared any communication protocol, which makes it easier to update parameters along diverse directions. \n\nTherefore, the problem is translated into this question: how to address the conflict among different tasks happening in the process of parameter updating. We resolve this conflict by introducing an inductive bias for multi-task learning. Specifically, different tasks can communicate with each other by sending (or receiving) both hidden variables and gradients. In this way, each task can keep informed of not only the knowledge stored in other tasks but the way how other tasks maintain their knowledge. \n\nAs shown in Fig. 1(b), during the forward phase, tasks A, B, C are capable of sending (and receiving) the gradients to (and from) each other. Then during the backward phase, each task is aware of how other tasks are modifying the parameters, making it possible to modify in a more consistent way. Mathematically, by communicating with gradients, we can apply consistency constraint to each task during the process of parameter updating, which can prevent private features from slipping into shared space thereby alleviating the pretend-to-share problem. Technically, we explore two kinds of communication mechanisms for gradient passing: pairwise and list-wise communication, where the latter one can take task relatedness into consideration. \n\nWe evaluate our models on three groups of multi-task learning datasets, range from natural language processing tasks to computer vision. The results show that our models are more expressive since we can learn a shared space which is more pure, driving task-dependent features away. \n\nTo summarize, we make the following contributions: \n\n\u2022 We describe a general framework, which can not only help us analyze current models in a unified way [Section 3], but find potential limitations, and move forward with suitable approaches [Section 4].",
            "score": 0.4270772041968689,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 529
                },
                {
                    "start": 532,
                    "end": 686
                },
                {
                    "start": 687,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2066
                },
                {
                    "start": 2069,
                    "end": 2119
                },
                {
                    "start": 2122,
                    "end": 2323
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1507568359375
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "Model Merging Recent advances in large language models (LLMs) have highlighted model 1 https://github.com/knovel-eng/mod merging as a crucial strategy for combining the capabilities of multiple models into a unified system (Ainsworth et al., 2023;Goddard et al., 2024;Labrak et al., 2024). This approach has gained prominence for its ability to enhance multitask performance and enable continual learning without requiring costly retraining procedures. Initial investigations in this domain explored weight averaging techniques, which directly combined parameters of models sharing identical architectures and initializations (Matena and Raffel, 2022;Garipov et al., 2018). While these methods demonstrated promising results, they revealed significant limitations when applied to models trained on heterogeneous tasks or initialized differently, prompting the development of more sophisticated approaches.",
            "score": 0.4268795056642635,
            "section_title": "Related Work",
            "char_start_offset": 3640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 905
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "267740180"
                },
                {
                    "start": 626,
                    "end": 651,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 651,
                    "end": 672,
                    "matchedPaperCorpusId": "4055784"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55419921875
        },
        {
            "corpus_id": "276557749",
            "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation",
            "text": "Inspired by Mixup (Zhang, 2017), we propose a simple yet effective model merging method called Mixup Model Merge (M 3 ). Unlike existing methods that use fixed merging ratios, M 3 generates the model contribution ratio randomly, harnessing randomness to inject fresh vitality into the model merging process. Specifically, M 3 further explores the parameter space of the merged model to unlock the potential of model merging. \n\nTo further elaborate, given two fine-tuned LLMs with parameters {\u03b8 t 1 SFT , \u03b8 t 2 SFT }, we combine M 3 with established model merging methods to fuse these parameters and obtain a single merged model with parameters \u03b8 M . As illustrative examples, we consider two widely used merging methods: Average Merging (Wortsman et al., 2022) and Task Arithmetic (Ilharco et al., 2022). \n\nThe official computation process for Average Merging is described as follows: \n\nThe official computation process for Task Arith-metic is: \n\nwhere \u03b8 PRE \u2208 R d represents the parameters of the pre-trained language model (PLM), such as Llama 2 (Touvron et al., 2023). \u03bb is a scaling factor that weights the contribution of each model during the merging process. \u03b4 t i denotes the delta parameter (Ilharco et al., 2022), which is defined as the difference between the parameters of the language models (LMs) before and after SFT, i.e., \u03b4 t = \u03b8 t SFT \u2212 \u03b8 PRE \u2208 R d , where t refers to task t. When introducing M 3 , the process for Average Merging is reformulated as: \n\nwhile the process for Task Arithmetic is reformulated as: \n\nwhere \u03bb m determines the linear interpolation ratio between the two fine-tuned LLMs, and is generally a value between 0 and 1. \u03bb m is sampled from a Beta distribution, typically \u03bb m \u223c Beta(\u03b1, \u03b1), where \u03b1 controls the shape of the Beta distribution. \n\nThe hyperparameter \u03b1 for M 3 is selected from the range [0.2, 0.4, 0.5, 1, 2, 3, 5].",
            "score": 0.4268795056642635,
            "section_title": "Mixup Model Merge",
            "char_start_offset": 7462,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 424
                },
                {
                    "start": 427,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 805
                },
                {
                    "start": 808,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1470
                },
                {
                    "start": 1473,
                    "end": 1530
                },
                {
                    "start": 1533,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1868
                }
            ],
            "ref_mentions": [
                {
                    "start": 738,
                    "end": 761,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "273404154",
            "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "text": "Prior works [19,49,70] have investigated LoRA merging in computer vision where each skill is a visual concept or style and the objective is image generation. On the other hand, natural language tasks are more challenging since identifying the skills needed for solving a task is not always clear. On natural language tasks, most prior works [5,14,37,43,62] merged LoRAs with the objective of multitask learning. In this setting, the individual LoRA modules are trained on (potentially) independent tasks and the merged model is tested on the original tasks. A successful model retains the skills of each individual LoRA. Differently, Huang et al. [24] devise LoraHub, a strategy to merge LoRAs for cross-task learning. By finetuning LoRAs on FLAN [35], they achieve performance equivalent to few-shot prompting on some Big-Bench Hard (BBH) tasks [54]. Closer to our work, Akiba et al. [2] merge specialized LMs in Japanese and in math [11] to solve word-math problems in Japanese [50]. Though this can be viewed as a skill composition task, they focus on studying only one such task, and their contribution is an evolutionary algorithm for merging models. \n\nGiven LoRAs trained on specialized domains (biology, math, code, reading comprehension, questionanswering), is it possible to merge them to effectively solve a new problem that requires a combination of these domains? \n\nWe underline that most settings we consider are out-of-domain since the specialized LoRAs have been trained on datasets that are very different from the target task. To our knowledge, this is the first paper exhibiting model merging is superior to data mixing for binary skill composition problems. \n\nOur key contributions are summarized as follows: \n\n-In Section 3, we define the skill composition problem and show some practical applications in domains spanning code, science, robustness, and in-house use cases. -We introduce Learnable Concatenation (CAT), a LoRA merging technique that involves a simple weighted average of the encompassed skill LoRAs. While concatenating LoRAs as a technique already exists, our contribution lies in cheaply learning the weights layer-wise and applying this to compositional natural language tasks.",
            "score": 0.424961324270268,
            "section_title": "Introduction",
            "char_start_offset": 2176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1375
                },
                {
                    "start": 1378,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1676
                },
                {
                    "start": 1679,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 12,
                    "end": 16,
                    "matchedPaperCorpusId": "258960192"
                },
                {
                    "start": 747,
                    "end": 751,
                    "matchedPaperCorpusId": "256415991"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6953125
        },
        {
            "corpus_id": "277313159",
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "text": "Thus, we propose FREE-Merging, a two-stage method that ensures an efficient backbone and lightweight experts as shown in Fig 1 . For backbone merging, the key lies in eliminating task interference [51]. We identify a compelling phenomenon that task interference is evident in the frequency domain of model parameters. And we present in Sec. 4 that frequency-domain interference is strongly correlated with performance. Notably, existing methods, which focus only on spatial-domain operations, fail to address frequencydomain interference, leading to suboptimal outcomes. Experimental evidence reveals that task interference is severe in the low-frequency region. This occurs because, high-Figure 1. The workflow of FREE-Merging involves two main steps. First, FR-Merging utilizes high-pass filtering to remove harmful specialized information from each model, thereby constructing a high-quality merged backbone. Second, it employs lightweight extraction of task experts, which are dynamically added during inference to mitigate the impact of task interference. frequency signals represent fine-grained variations, while low-frequency parts capture the global structure [18], which is more likely to contain task-specific information that leads to task interference. Leveraging the inherent redundancy in fine-tuned parameters [27,47,53], we propose to directly filter out low-frequency parts with severe task interference. As detailed in Sec. 5.2, this high-pass filtering substantially enhances generalization with minimal performance loss, preventing task interference during merging. \n\nOur lightweight yet highly effective approach, FR-Merging, is the first to apply Fourier filtering to neural network parameters, showing its potential for broader applications. \n\nAfter obtaining a high-performance backbone through FR-Merging, we use a lightweight task-specific expert to recover the inevitable information loss with minimal storage. Unlike existing methods that store extensive knowledge, we rescale low-frequency signals, which store task-specific information, retaining task expertise with only 1% of the parameters. Inspired by MoE [39,56], we employ a router for dynamic expert routing, enhancing model flexibility. \n\nIn summary, we introduce FREE-Merging, which utilizes FR-Merging to reduce task interference and incorporates lightweight experts to eliminate information loss of model merging. Our main contributions are: 1).",
            "score": 0.42459481725964476,
            "section_title": "Introduction",
            "char_start_offset": 2244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1765
                },
                {
                    "start": 1768,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2225
                },
                {
                    "start": 2228,
                    "end": 2405
                },
                {
                    "start": 2406,
                    "end": 2437
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 201,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1326,
                    "end": 1330,
                    "matchedPaperCorpusId": "267682180"
                },
                {
                    "start": 1330,
                    "end": 1333,
                    "matchedPaperCorpusId": "269605332"
                },
                {
                    "start": 1333,
                    "end": 1336,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2141,
                    "end": 2145,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 2145,
                    "end": 2148,
                    "matchedPaperCorpusId": "247011948"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77001953125
        },
        {
            "corpus_id": "254877510",
            "title": "Dataless Knowledge Fusion by Merging Weights of Language Models",
            "text": "The dominant paradigm for solving NLP tasks ranging from classification to sequence tagging involves fine-tuning a pretrained language model (PLM) using task-specific labeled data (Devlin et al., 2019;He et al., 2021). This results in specialized models that are explicitly trained to run inference over a single domain and task. Multi-task learning has shown that leveraging information across domains or tasks can be beneficial if the data sets, data set size and algorithms are well selected (Phang et al., 2018;Pruksachatkun et al., 2020;Poth et al., 2021;Weller et al., 2022). Combining knowledge of multiple data sets in a single model can lead to better overall performance on in-domain data (Poth et al., 2021), can better generalize on out-of-domain data (Wang et al., 2020b) and results in a model that is more practical and parameter efficient than maintaining specialized models. \n\nHowever, the multi-task learning setup suffers from two practical limitations. First, the training process requires access to the original labeled data, which may not be realistic as annotated data may be private to the agent fine-tuning the model which can happen in order to ensure data or annotation privacy or to guard intellectual property to annotations. Second, because a significant amount of data or task combinations are not beneficial to performance (Poth et al., 2021), building a single model requires training on all data set combinations to identify the optimal one, which can be prohibitive especially if there are many available source data sets or models. \n\nModel merging is defined as combining multiple models into a single one in parameter space without access to data (Matena & Raffel, 2021). This technique provides an alternative to building a single model while satisfying data privacy constraints. Weight merging algorithms usually also have a closed-form solution, making them very efficient as no retraining is necessary, thus enabling usage even when a large number of data sets or model combinations are available. Merging can be considered as an alternative to model ensembling (Opitz & Maclin, 1999;Rokach, 2010), where the outputs of individual models are combined to produce the final prediction.",
            "score": 0.4239688769150558,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1567
                },
                {
                    "start": 1570,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 201,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 515,
                    "end": 542,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 542,
                    "end": 560,
                    "matchedPaperCorpusId": "233289699"
                },
                {
                    "start": 560,
                    "end": 580,
                    "matchedPaperCorpusId": "248780114"
                },
                {
                    "start": 699,
                    "end": 718,
                    "matchedPaperCorpusId": "233289699"
                },
                {
                    "start": 764,
                    "end": 784,
                    "matchedPaperCorpusId": "220045358"
                },
                {
                    "start": 1355,
                    "end": 1374,
                    "matchedPaperCorpusId": "233289699"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59765625
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "The growing adoption of large foundation models is accompanied by significant practical challenges in terms of computational and storage demands (Kaplan et al., 2020). To address these challenges, multi-task model merging (Matena & Raffel, 2022) has emerged as a promising solution. For example, Task Arithmetic (Ilharco et al., 2023b) merges models by summing the task vectors from multiple tasks and applying them to the pre-trained model. Here task vectors are the difference in model parameters between the pre-trained foundation model and its fine-tuned version on a specific task. This approach builds a high-performance multi-task model by simple arithmetic operations in the model parameter space, thereby reducing computational overheads associated with fine-tuning on multiple tasks. \n\nDespite their successes, task arithmetic and its variants (Yadav et al., 2023;Wang et al., 2024;Yang et al., 2024b;a) still suffer from conflicts between task vectors. As illustrated in Figure 1, adding task vectors pointing to largely opposite directions may lead to catastrophic forgetting, and inconsistent task vector magnitudes may cause unbalanced merging, allowing the resulting model to be disproportionately influenced by a small subset of tasks. We refer to this issue as knowledge conflicts, represented as the expected performance variation of one task observed before and after merging another task vector. Knowledge conflicts differ from the typical notion of negative transfer (Yang et al., 2022;Meng et al., 2021;Liu et al., 2021b;Wang et al., 2023), as the former specifically refers to conflicts between predetermined, static task vectors, whereas the latter typically describes dynamic interference among tasks during training. Although current methods like sign alignment and test-time adaptation partially address knowledge conflicts, a thorough analysis of the root causes and a dedicated solution remain elusive. In scenario (a), the two task vectors contain largemagnitude components in opposite directions. In scenario (b), the difference in vector magnitudes causes the merged model to be dominated by one task. Both lead to suboptimal performance in one or more tasks. \n\nIn this paper, we propose a novel trust-region criterion for model merging, Task Arithmetic in the Trust Region (TATR), which addresses the knowledge conflict problem.",
            "score": 0.4239588841002975,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 793
                },
                {
                    "start": 796,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2361
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 245,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 312,
                    "end": 335,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 854,
                    "end": 874,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 874,
                    "end": 892,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 892,
                    "end": 911,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1488,
                    "end": 1507,
                    "matchedPaperCorpusId": "247011924"
                },
                {
                    "start": 1507,
                    "end": 1525,
                    "matchedPaperCorpusId": "238656924"
                },
                {
                    "start": 1525,
                    "end": 1543,
                    "matchedPaperCorpusId": "239998731"
                },
                {
                    "start": 1543,
                    "end": 1561,
                    "matchedPaperCorpusId": "266163541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "Moreover, weight merging may lead to suboptimal results when substantial differences exist in the parameter space (Li et al., 2022). \n\nIn this paper, we explore the fusion of LLMs from a probabilistic distribution perspective. For an input text, we argue that the probabilistic distributions generated by different source LLMs can reflect their inherent knowledge in understanding this text. Therefore, the proposed FUSELLM leverages the generative distributions of source LLMs to externalize both their collective knowledge and individual strengths and transfer them to the target LLM through lightweight continual training. \n\nTo achieve this, we develop a new strategy for aligning tokenizations originating from different LLMs and explore two methods for fusing the probability distributions generated by these diverse LLMs. During the continual training, FUSELLM places significant emphasis on minimizing the divergence between the target LLM's probabilistic distributions and those of the source LLMs. \n\nTo empirically demonstrate the effectiveness of FUSELLM, we examine a challenging yet general scenario of LLMs fusion, where the source models share minimal commonalities. Specifically, we focus on three popular open-source LLMs that possess distinct architectures and functionalities: Llama-2 (Touvron et al., 2023), OpenLLaMA (Geng & Liu, 2023), and MPT (Team, 2023). Evaluations across three benchmarks, which consist of a total of 42 tasks spanning reasoning, commonsense, and code generation, confirm that the target model trained by our method outperforms each source LLM and the baseline in most tasks. Moreover, we simulate the existence of functionally distinct LLMs with identical architecture by continually training a single base model on several domain-specific corpora. When evaluated based on perplexity, our method demonstrates superior potential in combining the capabilities of these structurally identical LLMs compared to traditional ensemble and weight merging methods. \n\nTo sum up, this paper explores a novel challenge called LLMs fusion, with the goal of creating a unified model that effectively utilizes the collective capabilities and unique strengths of diverse LLMs. Illustrated in Figure 1, our proposed approach distinguishes itself from traditional ensemble and weight merging techniques by prioritizing the fusion of multiple LLMs through knowledge externalization and transfer.",
            "score": 0.42290041290111124,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 135,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1006
                },
                {
                    "start": 1009,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1999
                },
                {
                    "start": 2002,
                    "end": 2204
                },
                {
                    "start": 2205,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 131,
                    "matchedPaperCorpusId": "251371375"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "272968986",
            "title": "Realistic Evaluation of Model Merging for Compositional Generalization",
            "text": "An exhaustive comparison of merging methods is beyond the scope of this work, so we focus on eight popular merging methods that represent the diversity of approaches. We discuss additional methods in Section 5. We use \u03b8 m to denote the parameters of the merged model, \u03b8 i i \u2208 {1, . . . , M } as the M constituent models being merged, and \u03b8 p as the base model which we assume all constituent models are fine-tuned from. Throughout this work, we assume all models are \"open vocabulary\", i.e., they use natural language for classification or generation and do not require task-specific classification heads.",
            "score": 0.4227106823263234,
            "section_title": "MERGING METHODS",
            "char_start_offset": 5721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 605
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45166015625
        },
        {
            "corpus_id": "276575632",
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "text": "Model merging merges the parameters of multiple separate models with different capabilities to build a universal model. With its high flexibility, model merging enables the seamless incorporation of new languages or tasks without the need for retraining the entire model. Additionally, since model merging allows models for different languages or tasks to be trained independently, it can effectively alleviate negative transfer issues (Wang et al., 2019;Zhang et al., 2023b;Wang et al., 2020b) commonly observed in multi-lingual training. This training independence also enables the use of optimal training configurations for each language or task instead of the unified settings required in multi-lingual training. \n\nMoreover, we propose Low-Rank and Sparse model Merging (LoRS-Merging), which uses a low-rank component to capture the compact structure and a sparse component to capture the scattered details in the weights. LoRS-Merging retains effective parts of structure and details while reducing redundant parts to reduce task interference. Specifically, coarse-grained singular value pruning is used to retain the low-rank structure, while fine-grained magnitude pruning is used to remove redundant details. The main contribution of this paper can be summarised as follows. \n\n\u2022 We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models. \n\n\u2022 LoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model. 2 Related Work",
            "score": 0.4225654466923844,
            "section_title": "Introduction",
            "char_start_offset": 1979,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 716
                },
                {
                    "start": 719,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1282
                },
                {
                    "start": 1285,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1508
                },
                {
                    "start": 1511,
                    "end": 1823
                }
            ],
            "ref_mentions": [
                {
                    "start": 436,
                    "end": 455,
                    "matchedPaperCorpusId": "53748459"
                },
                {
                    "start": 455,
                    "end": 475,
                    "matchedPaperCorpusId": "235790783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "273821064",
            "title": "Collective Model Intelligence Requires Compatible Specialization",
            "text": "In this work, we explore the limitations of combining models by averaging intermediate features, referred to as model merging, and propose a new direction for achieving collective model intelligence through what we call compatible specialization. Current methods for model merging, such as parameter and feature averaging, struggle to effectively combine specialized models due to representational divergence during fine-tuning. As models specialize to their individual domains, their internal feature representations become increasingly incompatible, leading to poor performance when attempting to merge them for new tasks. We analyze this phenomenon using centered kernel alignment (CKA) and show that as models specialize, the similarity in their feature space structure diminishes, hindering their capacity for collective use. To address these challenges, we investigate routing-based merging strategies, which offer more flexible methods for combining specialized models by dynamically routing across different layers. This allows us to improve on existing methods by combining features from multiple layers rather than relying on fixed, layer-wise combinations. However, we find that these approaches still face limitations when layers within models are representationally incompatible. Our findings highlight the importance of designing new approaches for model merging that operate on well-defined input and output spaces, similar to how humans communicate through language rather than intermediate neural activations.",
            "score": 0.422468861799559,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8310546875
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "Generally speaking, there are four kinds of model merging (MM) methods. \n\nOne common approach is simply averaging the weights of different models, which has shown promising performance. Model soups [12] achieved notable performance improvements on computer vision tasks. Task arithmetic [13] merges LLMs by performing arithmetic operations on task vectors (also known as delta parameters), which represent the differences between the weights of fine-tuned LLMs and the original pre-trained model. Fisher-Weighted Averaging [14] is based on the Laplace approximation, where each model's posterior is approximated as a Gaussian distribution, with its precision matrix corresponding to its Fisher information. Spherical linear interpolation (SLERP) [15] can achieve smooth interpolation between two vectors and maintain directional consistency during model merging by calculating the angle between two vectors and determining the interpolation vector based on the interpolation factor and angle. \n\nRegMean [16] merges different models by minimizing the prediction differences between the merged model and the source models. \n\nTo address the interference between parameters of different models caused by redundant parameter values and disagreements on the sign of parameters across models, TIES-Merging [17] introduces three novel steps when merging models: (1) resetting parameters with small delta parameters, (2) resolving sign conflicts, and (3) merging only the consistent parameters. DARE [18] performs model merging by sparsifying delta parameters of source supervised fine-tuned (SFT) models with Drop and Re-Scale operations before actually merging them. \n\nRecently, Daheim et al. [4] connected the inaccuracy of weighted-averaging to mismatches in the gradients and proposed a new uncertainty-based scheme to improve performance by reducing this mismatch. \n\nOur approach builds upon the combination of TIES-Merging and DARE, leveraging their strengths to create a robust model merging strategy. By integrating the conflict resolution techniques of TIES-Merging with the parameter sparsification methods of DARE, we aim to automate the search for optimal model merging configurations, ensuring that the final merged model maintains high performance without the degradation typically associated with parameter conflicts.",
            "score": 0.42138703727490767,
            "section_title": "Model Merging",
            "char_start_offset": 4645,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 74,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 992
                },
                {
                    "start": 995,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2000
                },
                {
                    "start": 2001,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 523,
                    "end": 527,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "277940324",
            "title": "Parameter-Efficient Continual Fine-Tuning: A Survey",
            "text": "The Expand and Merge [51] framework introduces a parameter-efficient architecture that combines adapter layers with Vision-Language models. The approach is built on two key innovations: first, it employs specially designed adapter layers that expand for new tasks while keeping old knowledge intact through frozen parameters; second, it leverages a pretrained text encoder's fixed embedding space to guide the vision encoder's continual learning process through vision-language pretraining models like CLIP. To effectively manage knowledge integration, the framework implements an adaptive fusion mechanism using scaling weights at different network depths, complemented by a unique parameter merging stage that prevents performance degradation while controlling parameter growth. This design addresses two common limitations in the field: the parameter bloat, typical of expansion-based approaches, and the over-constraining of new learning found in regularization-based methods. Through extensive validation across three datasets, the framework demonstrates superior performance compared to current state-of-the-art methods, exhibiting robust performance maintenance on both old and new tasks while effectively managing parameter efficiency.",
            "score": 0.4212570618607869,
            "section_title": "Adapter-based PECFT",
            "char_start_offset": 55190,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1243
                }
            ],
            "ref_mentions": [
                {
                    "start": 21,
                    "end": 25,
                    "matchedPaperCorpusId": "272540835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "269187724",
            "title": "Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation",
            "text": "Multilingual Interference.Multilingual training enables knowledge transfer but also causes interference, largely due to optimization conflicts among various languages or tasks (Wang and Zhang, 2022).Methods addressing conflicts between tasks hold promise to reduce interference (Wang et al., 2020), yet they show limited effectiveness in practical applications (Xin et al., 2022).Scaling up model size reduces interference directly but may lead to overly large models (Chang et al., 2023), with risks of overfitting (Aharoni et al., 2019).\n\nLanguage-Specific Modeling.Modular-based approaches enhance the unified model by adding language-dependent modules such as adapters (Bapna and Firat, 2019) or languageaware layers (Zhang et al., 2020b).Although the unified model serves as a common foundation, these approaches struggle to facilitate knowledge transfer among isolated modules due to a lack of clear inductive biases and thus heavy reliance on heuristics.For instance, Chronopoulou et al. (2023) rely on priori knowledge to control parameter sharing in language family adapters, Bapna and Firat (2019); Pires et al. (2023) isolate modules per language, hindering knowledge sharing.\n\nAdditionally, these modular-based methods substantially increase the number of parameters, thereby leading to increased memory demands and slower inference times (Liao et al., 2023a,b).Despite adapters normally being lightweight, they can easily accumulate to a significant parameter growth when dealing with many languages.In contrast, our method leverages the model's intrinsic modularity signals to promote task separation, without adding extra parameters.\n\nSub-networks in Multi-task Models.The lottery ticket hypothesis (Frankle and Carbin, 2018) states that within dense neural networks, sparse subnetworks can be found with iterative pruning to achieve the original network's performance.Following this premise, recent studies attempt to isolate sub-networks of a pre-trained unified model that captures task-specific features (Lin et al., 2021;He et al., 2023;Choenni et al., 2023a).",
            "score": 0.4211400822039455,
            "section_title": "Related Work",
            "char_start_offset": 4126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 26,
                    "end": 199
                },
                {
                    "start": 199,
                    "end": 380
                },
                {
                    "start": 380,
                    "end": 539
                },
                {
                    "start": 541,
                    "end": 568
                },
                {
                    "start": 568,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 961
                },
                {
                    "start": 961,
                    "end": 1187
                },
                {
                    "start": 1189,
                    "end": 1374
                },
                {
                    "start": 1374,
                    "end": 1513
                },
                {
                    "start": 1513,
                    "end": 1648
                },
                {
                    "start": 1650,
                    "end": 1684
                },
                {
                    "start": 1684,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2080
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 198,
                    "matchedPaperCorpusId": "245502478"
                },
                {
                    "start": 278,
                    "end": 296,
                    "matchedPaperCorpusId": "222291168"
                },
                {
                    "start": 516,
                    "end": 538,
                    "matchedPaperCorpusId": "67855815"
                },
                {
                    "start": 673,
                    "end": 696,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 721,
                    "end": 742,
                    "matchedPaperCorpusId": "216144650"
                },
                {
                    "start": 975,
                    "end": 1001,
                    "matchedPaperCorpusId": "257833499"
                },
                {
                    "start": 1085,
                    "end": 1107,
                    "matchedPaperCorpusId": "202660912"
                },
                {
                    "start": 1109,
                    "end": 1128,
                    "matchedPaperCorpusId": "258479712"
                },
                {
                    "start": 1714,
                    "end": 1740,
                    "matchedPaperCorpusId": "53388625"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28564453125
        },
        {
            "corpus_id": "276885421",
            "title": "To See a World in a Spark of Neuron: Disentangling Multi-task Interference for Training-free Model Merging",
            "text": "Multi-task learning (MTL) (Fifty et al., 2021) leverages transferable knowledge to handle multiple related tasks simultaneously. Existing MTL approaches primarily rely on architectural design or optimization strategies. Architectural-based methods, such as Mixture of Experts (MoE) (Shazeer et al., 2017), introduce specialized subnetworks that dynamically route inputs to task-specific experts, effectively reducing interference. However, these methods require modifying the pretrained model structure, increasing computational complexity, and limiting scalability (Liu et al., 2019;Shen et al., 2024). Optimization-based approaches, on the other hand, focus on balancing task gradients or loss functions to mitigate task conflicts during (Rosenblatt, 1958) Figure 2: Illustration of our proposed framework. Neuronal Task Vector: Neuronal task vectors \u03c4 for the k th neuron are defined as the difference between the fine-tuned and pre-trained neuron for each task. Decomposition: The pre-trained k th neuron is decomposed into its parallel and orthogonal complementary subspaces, followed by the projection of neuronal task vectors onto these subspaces. NeuroMerging: Our proposed NeuroMerging operates within these complementary subspaces for neuronal model merging. \n\ntraining (Bai et al., 2023;Kendall et al., 2018). While these methods improve convergence, they still depend on task-specific training data, which may be impractical in real-world applications due to privacy concerns or data scarcity (Liang et al., 2020). In contrast, model merging offers an alternative paradigm by integrating knowledge from multiple fine-tuned models into a single unified model without requiring additional training data or architectural modifications (Wortsman et al., 2022;Ilharco et al., 2023). Notwithstanding the promising findings, a key challenge in model merging is task conflict (Yadav et al., 2024;Du et al., 2024), where different tasks compete for model capacity, potentially leading to suboptimal performance. \n\nTo resolve task conflicts, existing model merging methods can be categorized into three levels based on their granularity.",
            "score": 0.4208295396310246,
            "section_title": "Related Work",
            "char_start_offset": 5030,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 128
                },
                {
                    "start": 129,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2139
                }
            ],
            "ref_mentions": [
                {
                    "start": 566,
                    "end": 584,
                    "matchedPaperCorpusId": "4389348"
                },
                {
                    "start": 740,
                    "end": 758,
                    "matchedPaperCorpusId": "12781225"
                },
                {
                    "start": 1298,
                    "end": 1319,
                    "matchedPaperCorpusId": "4800342"
                },
                {
                    "start": 1744,
                    "end": 1767,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1767,
                    "end": 1788,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1880,
                    "end": 1900,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1900,
                    "end": 1916,
                    "matchedPaperCorpusId": "273098230"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227783203125
        },
        {
            "corpus_id": "270619714",
            "title": "WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia",
            "text": "The advent of large language models (LLMs) [Brown et al., 2020] has revolutionized the field of Natural Language Processing (NLP), enabling unprecedented capabilities in text understanding and generation.However, static LLMs often suffer from outdated information and hallucinations.To mitigate these shortcomings, retrieval-augmented generation (RAG) techniques [Lewis et al., 2020] have been developed, which combine the strengths of LLMs with retrieved up-to-date information from external sources.While RAG frameworks have shown significant promise, it remains unclear how LLMs handle knowledge conflicts from different sources, including \"context-memory conflicts\", which refers to the retrieved context knowledge being in conflict with the parametric knowledge (memory) encapsulated within the LLM's parameters, and \"inter-context conflicts\", which refers to the contradictions among the retrieved passages [Xu et al., 2024].Most prior research on LLM knowledge conflicts has concentrated on \"context-memory conflicts\" and relied on artificially generated datasets,",
            "score": 0.42081502318657005,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 501
                },
                {
                    "start": 501,
                    "end": 931
                },
                {
                    "start": 931,
                    "end": 1071
                }
            ],
            "ref_mentions": [
                {
                    "start": 363,
                    "end": 383,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1165771484375
        },
        {
            "corpus_id": "273403613",
            "title": "Unconstrained Model Merging for Enhanced LLM Reasoning",
            "text": "Large Language Models (LLMs) have demonstrated remarkable capabilities, attributed to emergent abilities that arise with scale and diverse training data. Different organizations are developing their own LLMs, each excelling in specific reasoning tasks. In the context of LLMs, reasoning refers to the capability to perform logical inference, understand complex relationships, and solve problems that require multi-step thought processes, e.g., natural language reasoning, code programming, mathematical problem-solving with or without tools. However, applications in many other domains might require a combination of these abilities. For instance, in educational platforms or automated theorem proving, an LLM needs to comprehend complex mathematical concepts, reason through problems, and generate correct and efficient code solutions. An intuitive solution is to aggregate the datasets used to train these specialized LLMs and develop a more comprehensive, generalpurpose model. However, this approach could be more practical due to the enormous computational resources and human labor required. Moreover, the proprietary or sensitive nature of the data used to train individual models frequently restricts access, limiting the feasibility of data-centric methods. Therefore, there is a pressing need for efficiently building large models over off-the-shelf models (MoM), combining abilities without retraining or access to the original training data. Model merging has emerged as a promising avenue to address this challenge. Early approaches, such as weight averaging (Utans, 1996;Smith and Gashler, 2017), laid the foundation for techniques like Linear Mode Connectivity (LMC) (Garipov et al., 2018), which facilitates the merging of models trained from a common base. Methods like Model Soups (Wortsman et al., 2022) and Task Arithmetic (Ilharco et al., 2022) further exploit these principles by averaging weights or performing arithmetic operations on task-specific vectors. More advanced strategies such as TIES (Yadav et al., 2023) and Git-Rebasin (Ainsworth et al., 2022) address challenges related to permutation symmetries, enabling the alignment of models with Figure 1: The framework on unconstrained model merging. We first establish a robust evaluator and select the top-ranking domain-specific small models (DSSMs) with the strongest math or coding abilities.",
            "score": 0.4206935807995721,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1981
                },
                {
                    "start": 1982,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 1585,
                    "end": 1609,
                    "matchedPaperCorpusId": "263858956"
                },
                {
                    "start": 1682,
                    "end": 1704,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 1799,
                    "end": 1822,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "Large language models (LLMs) pretrained on a wide-variety of corpora have achieved notable success in multiple tasks (Touvron et al., 2023;Ope-nAI, 2023;Brown et al., 2020;Liu et al., 2024a). With significant progress, there is increasing interest in how to continuously improve the performance of LLMs in new domains, including math (Yu et al., 2023), code (Roziere et al., 2023), Wikipedia knowledge (Shao et al., 2024), or legal domains (Cui et al., 2023). One straightforward approach is through continual pretraining (CPT) on domain-specific data, which, however, is challenging for multiple target domains, as it can cause catastrophic forgetting on previously learned tasks (Luo et al., 2023). \n\nAn alternative approach is Mixture-of-Experts (MoE) merging, where dense experts are first CPTed in parallel for each domain and then merged into a unified MoE model, usually by keeping feedforward neural network (FFN) layers separate and averaging non-FFN layers (Sukhbaatar et al., 2024;Kang et al., 2024). Compared with dense models of similar size, the MoE model uses just a subset of parameters during inference by learning to route tokens to the top few experts, thus reducing inference costs. Unlike training an MoE model from scratch, MoE merging offers modularity, as individual experts are domain-specialized, and is substantially less expensive, as CPT-ing experts in parallel requires less compute than training the entire MoE on large datasets from the beginning (Sukhbaatar et al., 2024). \n\nIn this paper, we investigate how to effectively merge different domain expert models into a unified MoE model. The current state-of-the-art (SoTA) MoE merging approach, such as Branch-Train-Mix (BTX) (Sukhbaatar et al., 2024) assumes experts are branched from the same ancestor model and merges experts by simply unweighted averaging the non-FFN layers.",
            "score": 0.42024852272684765,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 700
                },
                {
                    "start": 703,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1505
                },
                {
                    "start": 1508,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 172,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6982421875
        },
        {
            "corpus_id": "276574617",
            "title": "Parameter Efficient Merging for Multimodal Large Language Models with Complementary Parameter Adaptation",
            "text": "Another issue lies in that current high-performance methods rely on extra information of seen tasks (e.g., validation data (Yang et al., 2024b), extra storage (Huang et al., 2024)) to boost the performance. Therefore, they can only handle seen tasks and fail to generalize to unseen tasks, questioning their robustness, as is concluded in Table 1. The most related work is LoraHub (Huang et al., 2023). However, its requirement for coefficient optimization through test time sample adaptation severely hinders its application. \n\nIn this paper, we analyze the reason behind the performance drop. We observe (1) distinct wider distribution and (2) task-oriented singular values in efficient parameters that differ two ways of fine-tuning. To handle the problem, we propose CoPA-Merging, a novel parameter efficient model merging method for high-performance merging of multimodal large models. Starting from the perspective of principal directions and low-rank decomposition, we introduce effective complementary1 parameter adaptation to both enhance ability for seen tasks and generalization on unseen tasks. Specifically, we prune ineffective parameters and construct scaling coefficients from inter-parameter relation to to mitigate interference between tasks and bridge potential performance gap resulting from merging. Additionally, we perform cross-task normalization to balance tasks of different data scales and enhance unseen task generalization. It is notable that our method is free from any additional data or storage, which equips the method with more flexibility. \n\nWe conduct experiments on a benchmark consisting of eight seen tasks and four unseen tasks with diverse fields to evaluate the ability on multimodal generative tasks. We also report results on common evaluation benchmarks and it shows that our model achieves superior performance on both seen (3.4%), unseen tasks (4.5%) and comprehensive evaluation benchmark with a substantial margin, demonstrating the effectiveness and generalizability of our method. We additionally perform experiments on vision tasks along with extensive analyses to validate the utility of our method. \n\nOur contributions are summarized as follows: \n\n\u2022 We focus on parameter efficient merging for multimodal large language models, highlighting the distinct difference between two types of fine-tuning methods.",
            "score": 0.4200439046609646,
            "section_title": "Introduction",
            "char_start_offset": 1931,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 526
                },
                {
                    "start": 529,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2152
                },
                {
                    "start": 2155,
                    "end": 2199
                },
                {
                    "start": 2202,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 143,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 159,
                    "end": 179,
                    "matchedPaperCorpusId": "270067773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86083984375
        },
        {
            "corpus_id": "272968955",
            "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
            "text": "Recent research (Akiba et al. 2024) has explored the concept of knowledge distribution within language models, revealing promising avenues for model merging in the data flow space. Unlike traditional methods merging in the parameter space, model merging in the data flow space preserves the original layer weights while optimizing the paths of inference. This method allows tokens to transition across different layers of multiple models, such as moving from one layer in model A to other layers in model B, thereby enhancing the model's versatility. Early investigations into merging in the data flow space primarily focused on serial connections with fixed, non-adaptive configurations. Specifically, given a set of models and a budget (representing the length of the inference path), the goal was to determine the optimal layer indices, which define the inference paths through the models. Due to the vast search space, researchers proposed a modified approach using an index array to manage the inclusion and exclusion of layers, effectively reducing the search space. The primary focus of this search process is to maintain the integrity of model parameters while optimizing the inference paths. Notably, model merge methods searching within the data flow space effectively aim to enhance the performance of the merged model by exploring different model architectures.",
            "score": 0.4185848365485682,
            "section_title": "Model Merge In the Data Flow Space",
            "char_start_offset": 40865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 181,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1373
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60009765625
        },
        {
            "corpus_id": "276575178",
            "title": "MedForge: Building Medical Foundation Models Like Open Source Software Development",
            "text": "In collaborative systems, proper model merging becomes increasingly vital for improving model knowledge integration from multiple sources in a resource-limited environment (Li et al., 2023b;Yang et al., 2024a;Goddard et al., 2024). Conceptually, model merging strategies can be categorized into entire model merging and partial model merging. \n\nEntire model merging involves combining multiple model parameters to participate in the merging process by several means. Entire model merging can be viewed as an optimization problem (Matena and Raffel, 2021;Jin et al., 2022;Mavromatis et al., 2024) or an alignment problem (Ainsworth et al., 2022;Jordan et al., 2022;Xu et al., 2024;Ainsworth et al., 2022), each offering unique advantages depending on the task at hand. In the optimization-based approach, the goal is to find the best combination of multiple models to enhance performance and efficiency. For instance, using Fisher information approximation (Matena and Raffel, 2021), the optimization-based model merging can be interpreted as selecting parameters that maximize the joint likelihood of the models' posterior distributions. The optimization of model merging can also be guided by minimizing the prediction differences between the merged model and individual models (Jin et al., 2022). With the development of large language models (LLM), optimization-based method is used to fuse multiple LLMs at test-time by minimizing perplexity over the input prompt (Mavromatis et al., 2024). To highlight, optimization-based methods are beneficial for scenarios requiring enhanced model performance and efficiency to integrate model parameters, while alignment-based methods (Ainsworth et al., 2022;Jordan et al., 2022) are better suited for maintaining consistency and interpretability, facilitating critical information sharing across models. For example, a training-free model merging strategy aligns relevant models by using a similarity matrix of their representations in both activation and weight spaces (Xu et al., 2024). Further, the alignment between the independently trained model and a reference model not only works for models with the same architecture but also for arbitrary model architectures (Ainsworth et al., 2022).",
            "score": 0.4185848365485682,
            "section_title": "Model Merging",
            "char_start_offset": 9329,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 342
                },
                {
                    "start": 345,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 230,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 664,
                    "end": 680,
                    "matchedPaperCorpusId": "268248251"
                },
                {
                    "start": 2014,
                    "end": 2031,
                    "matchedPaperCorpusId": "268248251"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.533203125
        },
        {
            "corpus_id": "277065877",
            "title": "FW-Merging: Scaling Model Merging with Frank-Wolfe Optimization",
            "text": "Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration. In contrast to the data-free methods mentioned above, data-informed methods [56,69,70] optimize merging coefficients using additional data. Model merging is impactful for LLMs, enabling efficient knowledge integration without full retraining, facilitating distributed finetuning [62], multi-task learning [49], and cost-effective model adaptation.",
            "score": 0.4185848365485682,
            "section_title": "Related Work",
            "char_start_offset": 7319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 571
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "corpus_id": "276095183",
            "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs",
            "text": "Dense merging methods combine multiple dense models into one to achieve diverse capabilities (Wortsman et al., 2022;Ilharco et al., 2022;Goddard et al., 2024;Jin et al., 2022;Matena and Raffel, 2022;Roberts et al., 2024). Most approaches focus on merging homogeneous dense models into another dense model. For example, average merging (Wortsman et al., 2022) averages model parameters, while task vector merging (Ilharco et al., 2022) adds the unweighted sum of task vectors (the difference between base and expert parameters) back to the dense model with scaling. Other work determines task vector weights instead of using an unweighted sum (Jin et al., 2022;Matena and Raffel, 2022). SoTA methods like Dare and Ties (Yadav et al., 2024;Yu et al., 2024) trim the task vector to resolve parameter interference: Dare trims the task vector randomly and rescales, while Ties sets vector parameters to zero by magnitude and adjusts signs to reduce conflicts. \n\nIn addition to homogeneous model merging, Roberts et al. (2024) propose merging heterogeneous models into a dense model using projectors, while Wan et al. (2024) apply knowledge distillation to fuse heterogeneous models. In this work, we introduce a more efficient method for merging experts with limited or no further fine-tuning and, unlike previous work focusing on dense models, we explore merging homogeneous and heterogeneous experts into an MoE model.",
            "score": 0.41755996633559295,
            "section_title": "Dense Model Merging",
            "char_start_offset": 3993,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1177
                },
                {
                    "start": 1178,
                    "end": 1415
                }
            ],
            "ref_mentions": [
                {
                    "start": 93,
                    "end": 116,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 137,
                    "end": 158,
                    "matchedPaperCorpusId": "268537132"
                },
                {
                    "start": 175,
                    "end": 199,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 335,
                    "end": 358,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 660,
                    "end": 684,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 718,
                    "end": 738,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 738,
                    "end": 754,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6025390625
        },
        {
            "corpus_id": "278714994",
            "title": "MergeBench: A Benchmark for Merging Domain-Specialized LLMs",
            "text": "This performance gap suggests that smaller models, due to their limited capacity, exhibit stronger task interference, where multiple tasks compete for parameter updates. This aligns with observations in the multi-task learning literature, where smaller models are more prone to capacity bottlenecks and negative task interactions [24]. For training quality, we also observe that merging methods consistently achieve over 90% normalized performance when applied to instructiontuned models, compared to their pretrained counterparts. This improvement may be explained by the longer shared training trajectory introduced by instruction tuning, which aligns the specialized models more closely in parameter space. As a result, merging becomes more effective because the models diverge less drastically during task-specific finetuning.",
            "score": 0.41603224245192094,
            "section_title": "Multi-Task Performance",
            "char_start_offset": 17257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 330,
                    "end": 334,
                    "matchedPaperCorpusId": "261243229"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8505859375
        },
        {
            "corpus_id": "271903285",
            "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models",
            "text": "Understanding that addressing parameter interference by model merging is difficult, even just for the bias terms, the optimal method for weight combination has a closed-form solution that varies by task. To manage this challenge, we introduce an innovative approach with a Sparse MIxture of Low-rank Experts (SMILE) model in this section, which operates in a zero-shot fashion, meaning no data or training is required. An overview is shown in Figure 3. \n\nWe upscale the linear layers from source models to the SMILE model, which consists of a shared pre-trained part, a router, and several low-rank experts. Figure 3 is organized into two primary sections: the overall model architecture (left) and the routing mechanism (right). \n\nRecall the output decomposition in Eq.( 4) and Eq.( 10), we can express the output of a merged model as the output of the pre-trained model plus a weighted sum of the fine-tuned parts of the individual models. If we can identify the most relevant experts for a given input, we can dynamically select the corresponding fine-tuned parts to combine with the pre-trained part. Then the merging error in Eq.( 11) can be minimized. Mathematically, we can express this idea as: \n\nHere, \u03bb is a function that maps the input to a one-hot probability distribution over the tasks, i.e. \u03bb j x (i) = 1 if j = i, and \u03bb j x (i) = 0 otherwise. However, a naive implementation of this idea would require a training process to learn the parameters of the router and a large number of additional parameters to store the fine-tuned weights of all tasks. A more efficient approach is to remove less significant terms from the fine-tuned components in Eq.( 13), focusing on retaining the most pertinent knowledge for each task. Therefore, the parameter space must be ranked by the importance of its dimensions. However, from previous findings in Section 2, we know that the fine-tuned information is distributed across less significant dimensions (Space II & III), which is a large portion of the whole space. We opt to use SVD to decompose the parameter differences \u2206W (i) for each task, and then apply a low-rank approximation to extract the most important part as follows:",
            "score": 0.41530149927373883,
            "section_title": "Resolving Parameter Interference using Sparse Mixture of Low-Rank Experts",
            "char_start_offset": 15964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 452
                },
                {
                    "start": 455,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 729
                },
                {
                    "start": 732,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402587890625
        },
        {
            "corpus_id": "271916354",
            "title": "Plug, Play, and Fuse: Zero-Shot Joint Decoding via Word-Level Re-ranking Across Diverse Vocabularies",
            "text": "Given that many models are trained on different tasks, architectures, modalities, and data types, combining these models to leverage inputs from multiple modalities and facilitate knowledge sharing is highly beneficial. Moreover, it is ideal if the ensembling approaches satisfy the following constraints: 1) It should not rely on shared vocabularies for flexibility in choosing models and maximizing potential combinations. 2) Effective knowledge sharing should occur during decoding to better navigate the search space exploiting this knowledge at each step. 3) Avoid requiring additional training, parameters, or major dependence on task-specific data for maximum applicability and not cause deviations from the pre-trained model. This section presents our algorithm for ensembling models with different vocabularies that satisfy the aforementioned constraints. First, we explain why re-ranking partial hypotheses can lead to incorrect probability estimates if the word is incomplete. Next, we introduce and justify a heuristicbased approach that predicts whether a hypothesis is at the end of a word, allowing for accurate reranking of completed words in partial hypotheses. Finally, we formally describe the complete algorithm, detailing how we merge probabilities from different models and how this process can be integrated with decoding strategies.",
            "score": 0.41528938132575366,
            "section_title": "Methodology",
            "char_start_offset": 5206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1356
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37353515625
        },
        {
            "corpus_id": "271924011",
            "title": "Pareto Merging: Multi-Objective Optimization for Preference-Aware Model Merging",
            "text": "Model merging tries to combine multiple neural network models to a single model. Existing merging methods can generally be divided into two categories: (i) merging models trained with different initializations (Ainsworth, Hayase, and Srinivasa 2023;Jordan et al. 2023;Stoica et al. 2024), and (ii) merging models fine-tuned on different datasets (Ilharco et al. 2023;Matena and Raffel 2022;Jin et al. 2023;Yadav et al. 2023;Daheim et al. 2024;Yang et al. 2024). In this paper, we focus on the latter scenario. \n\nLet the neural network be f (x; \u03b8, h k ), where x is the input data, \u03b8 is the parameter of the shared bottom, and h k is the task-specific head for the kth dataset. Since model merging focuses on the shared bottom, we use the simplified notation f (x; \u03b8). We denote the parameter of the pre-trained model as \u03b8 0 , and the parameter after fine-tuning on dataset k as \u03b8 k . \n\nThe goal of model merging is to effectively combine these fine-tuned models f \u03b81 , . . . , f \u03b8 K . The simplest approach is weight averaging, but it often degrades performance. Fisher Merging (Matena and Raffel 2022) uses the Fisher information matrix to ensure that crucial features from each task are preserved effectively. RegMean (Jin et al. 2023) minimizes the prediction differences between the merged model and individual models by adjusting their weights. Ilharco et al. (2023) introduces Task Arithmetic, which combines the task vectors {V k \u2261 \u03b8 k \u2212 \u03b8 0 } K k=1 to form a merged model \u03b8 0 + \u03bb K k=1 V k , where \u03bb \u2208 R controls the importance of task vectors. However, this might lead to suboptimal performance due to potential conflicts among task vectors. To address this issue, Ties Merging (Yadav et al. 2023) designs operations to reduce redundancy and resolve sign conflicts among the task vectors. \n\nAdaMerging (Yang et al. 2024) is most related to the proposed method.",
            "score": 0.414942465423345,
            "section_title": "Model Merging",
            "char_start_offset": 4153,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 509
                },
                {
                    "start": 512,
                    "end": 676
                },
                {
                    "start": 677,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 883
                },
                {
                    "start": 886,
                    "end": 974
                },
                {
                    "start": 975,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1797
                },
                {
                    "start": 1800,
                    "end": 1869
                }
            ],
            "ref_mentions": [
                {
                    "start": 210,
                    "end": 249,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 249,
                    "end": 268,
                    "matchedPaperCorpusId": "253523197"
                },
                {
                    "start": 268,
                    "end": 287,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 346,
                    "end": 367,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 367,
                    "end": 390,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 390,
                    "end": 406,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 406,
                    "end": 424,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 424,
                    "end": 443,
                    "matchedPaperCorpusId": "264306115"
                },
                {
                    "start": 443,
                    "end": 460,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1078,
                    "end": 1102,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1220,
                    "end": 1236,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1350,
                    "end": 1371,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1687,
                    "end": 1705,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1593017578125
        },
        {
            "corpus_id": "271039884",
            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
            "text": "Model merging is a technique for combining multiple models possessing different capabilities into a single versatile model without additional training. For example, we can merge a model specialized for Japanese and a model specialized for math to obtain a model that excels at solving mathematical problems in Japanese (Akiba et al., 2024). In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}.",
            "score": 0.4139667360141592,
            "section_title": "Preliminary: Model Merging",
            "char_start_offset": 7352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 559
                },
                {
                    "start": 562,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1434
                },
                {
                    "start": 1437,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1750
                }
            ],
            "ref_mentions": [
                {
                    "start": 455,
                    "end": 477,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93359375
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "In this paper, we claim that the observed performance gap primarily arises from the fact that existing model merging methods focus on finding a static multi-task optimal solution within the original parameter space. This approach limits the effectiveness of merging models for different instances, as the inherent diversity among samples implies that the optimal model combination is often task-or instance-specific. Therefore, an ideal merging model should adapt dynamically according to the particular task or instance being processed. \n\nTo address this challenge, we propose a novel approach, Weight-Ensembling Mixture of Experts (WEMoE), to dynamically merge Vision Transformers (ViT)-based models. More specifically, we first analyze the core modules-Attention (Att) and Multilayer Perceptron (MLP)-in multiple ViTs and observe that, compared to the pre-trained model, the MLP module exhibits more significant parameter changes after fine-tuning on downstream tasks, whereas the Att module shows less variation. This indicates that the MLP parameters capture more task-specific information, making them more susceptible to task conflicts during the merging process. Based on these findings, we adopt a traditional static merging approach (e.g., task arithmetic) for the non-MLP modules in ViTs while upscaling the MLP modules to a mixture-of-experts (MoE) module equipped with a routing mechanism. The experts in the MoE module consist of task-shared and task-specific knowledge extracted from the fine-tuned MLPs, with the routing mechanism dynamically assigning optimal merge weights to each MLP expert module based on the input samples. \n\nAlthough WEMoE exhibits excellent MTL performance, it has potential limitations when upgrading the MLPs in each Transformer block to MoE modules: (1) Dynamically merging MLPs requires additional storage for the MLP parameters across all tasks, leading to a substantial storage burden when the number of tasks is large. (2) Maintaining separate trainable routing parameters for each Transformer block can significantly increase the number of parameters as the number of ViT blocks grows. To address these challenges, we build on WEMoE by introducing an Efficient-and-effective Weight-Ensembling Mixture of Experts (E-WEMoE) to perform multi-task model merging.",
            "score": 0.413770824803978,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2225,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 537
                },
                {
                    "start": 540,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1644
                },
                {
                    "start": 1647,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2306
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3232421875
        },
        {
            "corpus_id": "278165581",
            "title": "Dynamic Fisher-weighted Model Merging via Bayesian Optimization",
            "text": "Modern transformer-based pre-trained language models (PLMs) (Devlin et al., 2019;Raffel et al., 2020;Brown et al., 2020) have driven a paradigm shift towards fine-tuning PLMs for specific tasks, achieving state-of-the-art performance across various applications. The general-purpose representations learned through pretraining have significantly enhanced numerous downstream tasks, leading to the widespread development of fine-tuned expert models (Min et al., 2023). For example, over a million models have been uploaded to the Hugging Face repository (Wolf, 2019), with many publicly available for research study2 . \n\nMost off-the-shelf models are fine-tuned independently for individual tasks, which limits their performance outside of their specialized domains. Ideally, models should be capable of handling multiple tasks relevant to a particular use case. Although multi-task learning (MTL) (S\u00f8gaard and Goldberg, 2016;Deng et al., 2019) offers a straightforward solution, it must require simultaneous access to the labeled datasets of all tasks and training over those datasets. This challenge is pronounced given the increasing difficulty of fine-tuning PLMs with their ever-growing sizes. More importantly, since the original training data for each model is often proprietary, it is time-consuming or even infeasible for users to label a large amount of data for MTL. \n\nModel merging offers a cost-effective alternative for building multi-task models by combining off-the-shelf models in the parameter space without additional training. For example, this can be done by simply weight-averaging the task vectors (i.e. fine-tuned part of the parameters from the pretrained model) (Ilharco et al., 2023;Yang et al., 2024). The success of model merging is supported by recent findings that the local minima, optimized from pre-trained parameters, are linearly connected in a flat basin of the loss landscape with no barriers in between (Neyshabur et al., 2020;Zhou et al., 2024). As a result, linearly interpolating between fine-tuned models potentially produces a well-behaved model with multi-task capabilities.",
            "score": 0.4132305508782224,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1376
                },
                {
                    "start": 1379,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 60,
                    "end": 81,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 81,
                    "end": 101,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 448,
                    "end": 466,
                    "matchedPaperCorpusId": "240420063"
                },
                {
                    "start": 897,
                    "end": 925,
                    "matchedPaperCorpusId": "16661147"
                },
                {
                    "start": 925,
                    "end": 943,
                    "matchedPaperCorpusId": "54447613"
                },
                {
                    "start": 1687,
                    "end": 1709,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1709,
                    "end": 1727,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1965,
                    "end": 1983,
                    "matchedPaperCorpusId": "267499590"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "272827386",
            "title": "CONGRA: Benchmarking Automatic Conflict Resolution",
            "text": "We propose CONGRA, a complexity-graded conflict benchmarking system. CONGRA implements a highly efficient and accurate conflict classification algorithm to construct a complexity-graded conflict dataset, which is used to evaluate the performance of merging tools under various conflict scenarios. CONGRA utilizes three code matching metrics of different granularities and combines them to calculate the accuracy and precision of auto-generated resolutions. We evaluate six LLMs on CONGRA, and the results show that LLMs with longer context support often perform worse than those with shorter context support, and general LLMs outperform specialized code LLMs in precision. \n\nJialu Zhang, Todd Mytkowicz, Mike Kaufman, Ruzica Piskac, and Shuvendu K Lahiri. Using pretrained language models to resolve textual and semantic merge conflicts (experience paper",
            "score": 0.4130833750107341,
            "section_title": "Conclusion",
            "char_start_offset": 20580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 854
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1756591796875
        },
        {
            "corpus_id": "276422131",
            "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition",
            "text": "The rapid advancement of Natural Language Processing (NLP) has led to the widespread adoption of Large Language Models (LLMs) across diverse applications. While these models exhibit remarkable versatility [1,2], their effectiveness in specialized domains remains limited due to insufficient exposure to domain-specific knowledge during training. This issue becomes particularly pronounced in fields such as medicine, law, and engineering, where precise understanding and accurate generation of technical terminology are essential. Even minor misunderstandings in these domains can lead to significant misinterpretations, impacting decision-making and real-world applications. Therefore, developing methods to effectively incorporate domain-specific knowledge into LLMs is vital for enhancing their applicability and reliability in specialized contexts. \n\nOne promising approach to addressing this limitation is model merging, which integrates the strengths of multiple LLMs to enhance domain adaptation. Model merging presents a costeffective alternative to full-scale retraining or fine-tuning, allowing the integration of new knowledge without requiring large amounts of additional data or computational resources. However, the extent to which model merging facilitates domain-specific knowledge integration, particularly in multilingual settings, remains an open question. This limitation is particularly problematic for applications that require precise understanding and generation of technical language. An accurate interpretation of terms and concepts is essential in these fields, as even minor misunderstandings can lead to significant errors or miscommunications. \n\nThis study explores the potential of model merging for cross-lingual knowledge transfer, with a particular focus on integrating domain-specific technical vocabulary. The primary challenge lies in ensuring effective knowledge transfer without interference so that newly acquired domainspecific information enhances the model's proficiency while preserving its general linguistic capabilities. Another key issue is whether merging enables the model to retain and accurately utilize domain-specific terminology across different languages, maintaining both contextual meaning and usability in a multilingual setting. To investigate this, we conduct a comprehensive experiment, merging a general-purpose Japanese-specific model with an English medical domain-specific model and assessing various merging strategies. Through quantitative analysis, we evaluate the effectiveness of different approaches in transferring domain-specific terminology knowledge and improving the model's ability to understand technical language, particularly medical jargon.",
            "score": 0.41292745838010086,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 852
                },
                {
                    "start": 855,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1673
                },
                {
                    "start": 1676,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2067
                },
                {
                    "start": 2068,
                    "end": 2288
                },
                {
                    "start": 2289,
                    "end": 2486
                },
                {
                    "start": 2487,
                    "end": 2722
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8203125
        },
        {
            "corpus_id": "273098762",
            "title": "Llama SLayer 8B: Shallow Layers Hold the Key to Knowledge Injection",
            "text": "Large Language Models (LLMs) have significantly revolutionized the natural language processing area, showcasing unparalleled abilities across various tasks (Achiam et al., 2023). Despite their versatility, LLMs exhibit limitations in specialized areas such as mathematics, programming, etc., which * Work done during an internship at Alibaba Cloud. \u2020 Tao Gong is the corresponding author. hinder the potential of wide-ranging applications. \n\nTo address these gaps, existing work (Liu et al., 2023;Wang et al., 2023) has sought to enhance the diverse skills of pre-trained LLMs through customized data strategies. However, they require extensive computational efforts and massive data volumes, challenging the widespread accessibility of LLM research. Furthermore, while Parameter-Efficient Fine-Tuning (PEFT) techniques offer a reduction in training requirements, their effectiveness tends to diminish (Biderman et al., 2024;Wu et al., 2024) compared to traditional fine-tuning methods, especially as the size of the model and the dataset grows. \n\nSubsequently, another line of research emerged, focusing on methods such as model merging (Akiba et al., 2024) and model expansion (Wu et al., 2024;Choi and Gazeley, 2024;Kim et al., 2023). Model merging methods strive to synthesize a multifaceted model that amalgamates insights from various pre-trained domain-specific LLMs, potentially crafting a model adept at addressing a multitude of tasks concurrently. However, the process of training multiple domain-specific LLMs is resource-intensive. On the other hand, model expansion methods, exemplified by Llama Pro, seek to refine pre-trained models for domain-specific applications in the post-pretraining phase by only fine-tuning the expanded layers. Therefore, it can employ significantly fewer trainable parameters than full model fine-tuning. \n\nHowever, present model expansion methods generally treat each part of LLMs equally, although different layers may exhibit varying sensitivity to incorporated knowledge. This lack of differentiation can result in less-than-ideal knowledge injection results. An intuitive idea is to inject knowledge into the most important layers so that the LLM can more sufficiently leverage the new knowledge without the overhead of redundant adjustments across all layers.",
            "score": 0.4126380840029322,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 1045
                },
                {
                    "start": 1048,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1847
                },
                {
                    "start": 1850,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2106
                },
                {
                    "start": 2107,
                    "end": 2308
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239501953125
        },
        {
            "corpus_id": "273662099",
            "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging",
            "text": "The goal of multi-task learning (MTL) is to utilize a single model to perform multiple related tasks concurrently, thereby facilitating information sharing and knowledge transfer among the tasks. In recent years, the rapid development of deep learning has prompted a learning paradigm shift, where the mainstream paradigm now focuses on fine-tuning downstream tasks using powerful pre-trained models, rather than training an expert model from scratch [2,3,4,5,6,7]. This shift typically results in significant reductions in both data requirements and computational resources. Additionally, the open-source ethos of the deep learning community has encouraged developers to release a vast array of expert models fine-tuned on various downstream tasks. To date, over one million diverse models have been made available on Hugging Face 1 . These above diverse factors have given rise to a new MTL paradigm, enabling the direct merging of multiple independently trained expert models to create a multi-task model without requiring access to their original training data [8,9,10,11]. \n\nHowever, due to potential task conflicts and interferences among multiple tasks, simply merging parameters from independently fine-tuned models may lead to a sharp decline in MTL performance [12,13]. Recently, an increasing number of studies have aimed to address the MTL performance degradation resulting from model merging [11]. A notable example is task arithmetic [14], which introduces the concept of 'task vectors' to extract task-specific knowledge from the fine-tuned models. By linearly weighting the task-private knowledge of multiple tasks into the pre-trained model, task arithmetic enhances the model's ability to process multiple downstream tasks. Inspired by task arithmetic, recent advancements have proposed techniques to alleviate sign conflicts among task vectors [15,16], merge them in a fine-grained manner [17,18,19], or combine task vectors within subspaces [20,21,22,23]. While these methods have considerably improved task arithmetic's performance, a noticeable performance gap still exists between the merged MTL model and the independently fine-tuned expert model (or the joint-trained MTL model). \n\nIn this paper, we claim that the observed performance gap primarily arises from the fact that existing model merging methods focus on finding a static multi-task optimal solution within the original parameter space.",
            "score": 0.41255516182042784,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2204
                },
                {
                    "start": 2207,
                    "end": 2422
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 458,
                    "matchedPaperCorpusId": "257038341"
                },
                {
                    "start": 462,
                    "end": 464,
                    "matchedPaperCorpusId": "257505035"
                },
                {
                    "start": 1068,
                    "end": 1070,
                    "matchedPaperCorpusId": "259022411"
                },
                {
                    "start": 1968,
                    "end": 1971,
                    "matchedPaperCorpusId": "269757600"
                },
                {
                    "start": 1971,
                    "end": 1974,
                    "matchedPaperCorpusId": "270067773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "271064651",
            "title": "MagMax: Leveraging Model Merging for Seamless Continual Learning",
            "text": "Weights interpolation has recently emerged as an efficient technique for transfer learning that reduces forgetting. After fine-tuning LPM on target data, its weights are interpolated with the weights of (unchanged) LPM, which allows finding a good balance between accuracy on the target domain and zero-shot capabilities of LPM [46]. Such an approach was further extended when merging models across multiple models for OOD performance [45] or in multi-task learning (i.e., Task Vectors [13]). Since then multiple methods have been developed in this area. TIES-Merging [48] reduces the interference when merging models by trimming parameters and electing signs. In [29], the authors linearize the fine-tuning to disentangle weights and facilitate merging. ZipLoRA [36] adapts diffusion models by merging LoRA weights for different styles and subjects. However, those methods were, up-to-date, evaluated on a limited number of scenarios. In this work, we are interested in using those promising approaches to test how they work for different similarities between tasks, as well as when they are compared with simple CL baselines. A concurrent work, CoFiMA [24], utilizes Fisher Merging [26] sequentially after each task to continually train closed vocabulary image classifiers. In contrast, we focus on reducing parameter-level interferences in open vocabulaty models. \n\n3 Background and motivation",
            "score": 0.4122093330334212,
            "section_title": "Related Work",
            "char_start_offset": 6889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1396
                }
            ],
            "ref_mentions": [
                {
                    "start": 568,
                    "end": 572,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 664,
                    "end": 668,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 1184,
                    "end": 1188,
                    "matchedPaperCorpusId": "244345933"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.450927734375
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities across diverse Natural Language Processing (NLP) tasks, including language understanding, reasoning, and generation. However, general-domain LLMs often struggle with financial tasks due to the technical and specialized nature of financial texts. This study investigates the efficacy of instruction fine-tuning smaller-scale LLMs, including Mistral-7B, Llama3-8B, and Phi3-mini, to enhance their performance in financial text classification tasks. We fine-tuned both instruction-tuned and base models across four financial classification tasks, achieving significant improvements in task-specific performance. Furthermore, we evaluated the zero-shot capabilities of these fine-tuned models on three unseen complex financial tasks, including argument classification, deal completeness classification, and causal classification. Our results indicate while base model fine-tuning led to greater degradation, instruction-tuned models maintained more robust performance. To address this degradation, we employed model merging techniques, integrating single-task domain-specific fine-tuned models with the base model. Using this merging method resulted in significant enhancements in zero-shot performance, even exceeding the original model's accuracy on certain datasets. Our findings underscore the effectiveness of instruction fine-tuning and model merging for adapting LLMs to specialized financial text classification tasks.",
            "score": 0.4120684140794585,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "271865730",
            "title": "BadMerging: Backdoor Attacks Against Model Merging",
            "text": "Model merging algorithms merge task-specific models initialized from the same pre-trained model, such as CLIP-like pre-trained models. It requires that the various task-specific models share the same model architecture but different parameters. As illustrated in Figure 1, two CLIP-like pre-trained models are fine-tuned on distinct datasets to obtain two task-specific models. Subsequently, they are merged into a final merged CLIP-like model, which can recognize classes in both tasks. We note that besides keeping their generalization ability, current model merging algorithms freeze the text encoder to further make each class have an identical language feature representation among different models, avoiding feature space collapses and conflicts among different models [24]. We now formally introduce the merging process. Specially, we denote M  as the CLIP-like model M with weights  and V  as the visual encoder of the model M  . Let  pre be the weights of a pre-trained model, and   be the weights fine-tuned on a dataset D  . Then, we denote a task vector \u0394  as the element-wise difference between   and  pre , i.e., \u0394  =   \u2212  pre . Assume there are  task vectors {\u0394 1 , . . . , \u0394  } obtained from different training settings of the same/different tasks. We can derive a unified formulation of model merging to obtain merged weights  merged as  merged =  pre + \u0394 merged . Different merging algorithms mainly differ in their ways of obtaining the merged task vector \u0394 merged as follows: Task-Arithmetic (TA) [24] and Simple Average (SA) [66]. TA and SA merge task vectors via the weighted sum: \u0394 merged =   =1 \u2022\u0394  . Both TA and SA assume that each task vector should have an equal contribution to the merged task vector. TA scales each task vector using a fixed  = 0.3 regardless of the number of task vectors, which achieves promising results in merging task-specific models from different domains. SA calculates  as the arithmetic mean, i.e.,  = 1  , which achieves better results in merging taskspecific models from the same domain. Ties-Merging (Ties) [72]. Ties proposes three operations: TRIM, ELECT SIGN and MERGE to address three kinds of interference among original task vectors in \u0394 . We combine these three operations and call them  (\u2022).",
            "score": 0.411715923748203,
            "section_title": "Model Merging",
            "char_start_offset": 8656,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2257
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1517,
                    "end": 1521,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1546,
                    "end": 1550,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 2065,
                    "end": 2069,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "278418792",
            "title": "Analysis of Model Merging Methods for Continual Updating of Foundation Models in Distributed Data Settings",
            "text": "Model merging is an emerging technique designed to integrate the parameters of multiple models, each fine-tuned for distinct tasks or data distributions, into a single high-performance model without requiring a complete retraining cycle [14,15]. Unlike conventional ensemble methods [33], which combine the outputs of multiple models during inference, leading to increased computational costs and memory usage, model merging consolidates the learned representations into one model. This consolidation offers significant advantages in terms of efficiency, inference speed, and storage. \n\nOne straightforward and commonly used model merging approach is parameter averaging [34,35]. However, more advanced techniques, such as weighted averaging, assign different weights to models based on their relative contribution or performance regarding the target task [36,37]. The success of these techniques typically stems from observations indicating that fine-tuned models originating from the same pretrained initialization tend to occupy similar regions within the loss landscape, thereby facilitating effective linear interpolation [38]. \n\nA critical concept in this domain is the task vector [39], defined as the difference between the parameters of a fine-tuned model and those of the original pretrained model. Task vectors are considered to encapsulate the specific knowledge acquired during fine-tuning. By performing arithmetic operations on these vectors (e.g., addition or subtraction), the specialized skills of different models can be effectively manipulated or combined. Despite its promise, task vector manipulation is susceptible to interference when merging multiple vectors, and recent studies have proposed strategies to mitigate these effects [40,41]. \n\nBeyond simple parameter averaging, some studies have investigated layer-wise merging techniques that aim to independently align and integrate model parameters at each layer [42,43]. These methods offer more granular control over the merging process, potentially better preserving the specialized knowledge of individual models. However, most existing research on model merging evaluates the integrated model in a single merging round rather than a continuous update framework. \n\nOur study addresses this gap by adopting a continual pretraining strategy in which the base model undergoes cycles of fine-tuning and merging [16]. This iterative approach provides insights into the robustness and stability of model merging over time and highlights the challenges of maintaining consistent performance across multiple update cycles.",
            "score": 0.4113366938296894,
            "section_title": "Model Merging",
            "char_start_offset": 13345,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1132
                },
                {
                    "start": 1135,
                    "end": 1308
                },
                {
                    "start": 1309,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1763
                },
                {
                    "start": 1766,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2242
                },
                {
                    "start": 2245,
                    "end": 2392
                },
                {
                    "start": 2393,
                    "end": 2594
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "254877458"
                },
                {
                    "start": 856,
                    "end": 860,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 860,
                    "end": 863,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1127,
                    "end": 1131,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 1755,
                    "end": 1759,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1759,
                    "end": 1762,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1943,
                    "end": 1946,
                    "matchedPaperCorpusId": "268537290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "276776501",
            "title": "CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging",
            "text": "This section provides a theoretical analysis of how Conflict-Aware Sparsification (CA) reduces parameter overlap, ensures orthogonality of task vectors in parameter space, and mitigates interference during model merging. \n\nSparse and Non-Overlapping Task Vectors. CA employs a sequential pruning strategy to produce sparse task vectors \u03c4 A , \u03c4 B \u2208 R u\u00d7v with non-overlapping parameters. Their binary masks M A , M B \u2208 {0, 1} u\u00d7v satisfy: \n\nThe task vectors are defined as: \n\nwhere \u2206W A , \u2206W B are parameter updates from a base model, and \u2299 denotes elementwise multiplication. This ensures that \u03c4 A and \u03c4 B have disjoint non-zero entries. Prior studies (Yu et al., 2024;Yadav et al., 2024) and our experimental results in A.8 confirm that these sparse updates are nearly lossless in retaining task-specific information, as simple rescaling compensates for pruning-induced changes. \n\nNon-Overlap Implies Orthogonality. The Frobenius inner product of the task vectors \u03c4 A and \u03c4 B is: \n\nUnder the non-overlapping condition (M A ) ij (M B ) ij = 0, each term in the summation equals zero: \n\nThus, the inner product reduces to: \n\nThis guarantees that \u03c4 A and \u03c4 B are orthogonal. \n\nOrthogonality Reduces Interference. Consider the combined weight update: \n\nwhere \u03bb A , \u03bb B \u2208 R are the scaling coefficients for the task vectors. The squared Frobenius norm of the update is: \n\nWhen \u03c4 A and \u03c4 B are orthogonal (i.e., \u27e8\u03c4 A , \u03c4 B \u27e9 F = 0), the cross-term vanishes, and the norm simplifies to: \n\nThis decoupling ensures that adjusting \u03bb A affects only the contribution of \u03c4 A , with minimal direct interference to \u03c4 B . As a result, task vector contributions can be independently scaled, avoiding interference during model merging. \n\nOn Overlap and Possible Synergy. While overlap often leads to conflicts, there may be cases where overlapping coordinates have aligned updates, providing synergistic effects. However, identifying exactly which overlap is \"helpful\" can be challenging, as it requires deep insights into each task's loss surface.",
            "score": 0.41047221920584015,
            "section_title": "Theoretical Analysis",
            "char_start_offset": 16271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 223,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 980
                },
                {
                    "start": 983,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1121
                },
                {
                    "start": 1124,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1247
                },
                {
                    "start": 1250,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1365
                },
                {
                    "start": 1368,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1606
                },
                {
                    "start": 1607,
                    "end": 1718
                },
                {
                    "start": 1721,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 652,
                    "end": 669,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 669,
                    "end": 688,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59130859375
        },
        {
            "corpus_id": "273228210",
            "title": "Model-GLUE: Democratized LLM Scaling for A Large Model Zoo in the Wild",
            "text": "In this study, we consider a collection of n existing Large Language Models (LLMs), denoted as {M 1 , . . . , M n }, which have been fine-tuned on diverse corpora. Our objective is to outline a systematic approach towards producing one stronger aggregated model across all knowledge domains. Specifically, the unified LLM incorporates single LLMs mainly through merging and mixture. The concept of Model Merging Model merging is integrating multiple models into one unified model in the weight space, compatible with LLMs of the same initialization [16]. Popular merging methods can be divided into two types: \u2776 Merging entire model weights represented by Model Soup [59] (Linear), SLERP [49], and Model Stock [24]; \u2777 Task-vector based merging represented by Task Arithmetic [22], TIES [63], and DARE [64]. The former method directly interpolates model weights, while the latter subtracts the pre-trained model from the fine-tuned model to obtain task vectors and utilizes sparsity and consistency of parameters for refined merging. The basic Linear interpolation merging is defined as w u = n i=1 s i \u2022 w i , where w i and s i are the corresponding model weights and merging coefficient of M i \u2208 {M 1 , . . . M n }. Selective Merging Pipeline Merging can be easily applied to models with the same architecture, but does not guarantee better results. Therefore, before searching for the merging coefficient, we first pre-process the models by clustering all the models using cosine similarity and then searching for the optimal merging coefficient and method within each cluster. Details are explained in Appendix A. 5.",
            "score": 0.41045944639441756,
            "section_title": "Preliminaries",
            "char_start_offset": 9133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1619
                }
            ],
            "ref_mentions": [
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "11290566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67919921875
        },
        {
            "corpus_id": "267365047",
            "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts",
            "text": "From the previous observations and discussion, our key insight is that we can separate the shared information and task-specific knowledge and combine them dynamically in parameter space based on inputs. Based on this insight, we propose a novel framework to merge the pre-trained model and the fine-tuned models, as shown in Figure 1(a). \n\nSo far we have identified the shared information and taskspecific knowledge. The next step is to combine them in a way that allows the merged model to handle all the tasks in S. The ability to separate and leverage both knowledge is a key strength of our proposed framework. To achieve this, we propose to dynamically integrate the shared information and task-specific knowledge based on the input samples, rather than seeking a static solution within the original parameter space. This dynamic adjustment allows the model to better adapt to the nuances of each task, thereby improving its performance across source tasks. \n\nTo this end, we propose to upcycle the MLPs in the pre-trained model and the fine-tuned models into Weight-Ensembling MoE modules, which are designed to dynamically select task-specific knowledge and combine it with the shared information based on the input samples. We will discuss the details of the Weight-Ensembling MoE module in Section 3.2. For the remaining parts of the model, we utilize the Task Arithmetic method (Ilharco et al., 2023) to merge the weights. Task Arithmetic is a simple yet effective and scalable method for merging models, it operates in the parameter space element-wise without modification to the model structure. The merged model has parameters \u03b8 = \u03b8 0 + \u03bb n i=1 \u03c4 i , where \u03bb is a hyperparameter that controls the contribution of the task vectors to the model.",
            "score": 0.41045944639441756,
            "section_title": "Framework Overview",
            "char_start_offset": 11720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 337
                },
                {
                    "start": 340,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1231
                },
                {
                    "start": 1232,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1756
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51953125
        },
        {
            "corpus_id": "277787291",
            "title": "Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs",
            "text": "Weight Interpolation and Task Arithmetic. In recent years, the increasing scale of large language models has significantly heightened the demand for data and training costs associated with finetuning multi-task models. To merge the capabilities of various existing single-task models into a unified framework, several model merging techniques (Daheim et al., 2024;Jin et al., 2023;Wan et al., 2024;Yang et al., 2024b) have been developed. The simplest and most intuitive approach is weight interpolation (Frankle et al., 2020;Izmailov et al., 2018;Ram\u00e9 et al., 2023;2022), which has been applied to enhance model generalization (Wortsman et al., 2022b), improve specific singletask performance (Wortsman et al., 2022a), and boost multi-task effectiveness (Ilharco et al., 2022;Li et al., 2022). Subsequently, Task Arithmetic (Ilharco et al., 2023) was introduced, enabling the merging of multiple models through a weighted operation on the parameter difference \u03c4 . Many subsequent methods (Yang et al., 2024c;Yu et al., 2024;Yadav et al., 2023;Zhang et al., 2023) have been proposed based on the principles of Task Arithmetic. In conclusion, modifying fine-tuning methods to achieve a more linear model can reduce interference during model merging. However, these methods still require training data and retraining, which is often challenging in practical scenarios.",
            "score": 0.41045944639441756,
            "section_title": "RELATED WORK",
            "char_start_offset": 23237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 41
                },
                {
                    "start": 42,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1366
                }
            ],
            "ref_mentions": [
                {
                    "start": 343,
                    "end": 364,
                    "matchedPaperCorpusId": "264306115"
                },
                {
                    "start": 364,
                    "end": 381,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 381,
                    "end": 398,
                    "matchedPaperCorpusId": "267061245"
                },
                {
                    "start": 504,
                    "end": 526,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 526,
                    "end": 548,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 548,
                    "end": 566,
                    "matchedPaperCorpusId": "254877458"
                },
                {
                    "start": 628,
                    "end": 652,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 694,
                    "end": 718,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 755,
                    "end": 777,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 777,
                    "end": 793,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 825,
                    "end": 847,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 989,
                    "end": 1009,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1009,
                    "end": 1025,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1025,
                    "end": 1044,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "274306089",
            "title": "Neutralizing Backdoors through Information Conflicts for Large Language Models",
            "text": "Training the conflict model. We build a conflict model M \u03b8 by fine-tuning a pre-trained model M \u03b80 using a small amount of clean data D c . During fine-tuning, the objective is to optimize the pre-trained model's parameters \u03b8 0 by maximizing the sum of conditional probability P under taskspecific prompts x: \n\nIn full-model fine-tuning settings, all parameters within the model are updated, which can be highly time-consuming and computationally expensive. To mitigate the training overhead, we leverage a lightweight fine-tuning method Low-Rank Adaptation (LoRA) [13]. LoRA introduces and updates the parameters of low-rank-matrices \u03b8 \u2032 while maintaining the other parameters \u03b8 0 frozen. \n\nImportantly, our conflict model, fine-tuned with LoRA, can be used to mitigate backdoored models with various training methods, regardless of whether they exploit full-model finetuning or employ PEFT techniques. Model Merging. We proceed by merging the conflict model with the backdoored model. Model merging [65], [77] involves integrating multiple trained models into a single model, often leveraging the strengths of models trained on different tasks to enhance performance and robustness. Popular model merging algorithms inlcude Linear combination [65], Spherical linear interpolation (SLERP) [7], TIES merging [69], and Passthrough [7]. \n\nLinear combination. Linear Combination is a straightforward model merging method where the weights of two models are combined linearly: \n\nwhere t is the interpolation parameter that controls the proportion of each model's contribution. Spherical linear interpolation (SLERP). SLERP is used for smooth interpolation between two vectors, following the arc on the surface of a sphere rather than a linear path. This approach preserves the geometric properties of the spherical space while merging the parameters from two models: \n\nwhere t is the interpolation parameter, and \u03d5 represents the angle between \u03b8 and \u03b8. TIES merging. The TIES merging algorithm begins by extracting task vectors [16], defined as \u03b8 task = \u03b8 \u2212 \u03b8 0 from both backdoored and conflict model. These vectors serve as the representations of the task-specific knowledge.",
            "score": 0.4101568501226595,
            "section_title": "Internal Conflict Construction",
            "char_start_offset": 26114,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1334
                },
                {
                    "start": 1337,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1744
                },
                {
                    "start": 1745,
                    "end": 1862
                },
                {
                    "start": 1865,
                    "end": 1948
                },
                {
                    "start": 1949,
                    "end": 1962
                },
                {
                    "start": 1963,
                    "end": 2098
                },
                {
                    "start": 2099,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1245,
                    "end": 1249,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84521484375
        },
        {
            "corpus_id": "271947337",
            "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging",
            "text": "Training specialized, task-specific models presents several challenges, including the storage costs associated with maintaining multiple models, the substantial memory requirements for deploying these models, and the rapid obsolescence of models as training datasets age. One proposed solution to mitigate these issues is model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;Yadav et al., 2024;Yu et al., 2024) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values. \n\nAdditionally, DARE introduces random pruning to align more closely with the base model's performance (Goddard et al., 2024). More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures.",
            "score": 0.4099877301822839,
            "section_title": "A.1.2 Model Merging",
            "char_start_offset": 33952,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 759,
                    "end": 778,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 778,
                    "end": 794,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82861328125
        },
        {
            "corpus_id": "270702345",
            "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging",
            "text": "Here we will elaborate on the baselines utilized in our main comparison experiment, as outlined in Table 2 and Figure 2b. \n\n\u2022 Individual means that each task uses the corresponding fine-tuned model, which has no interference between tasks but cannot perform multiple tasks simultaneously. It serves as the upper-bound performance for each specific task. \n\n\u2022 Weight Averaging [6,74] is the simplest form of model merging, which straightforwardly averages the parameters of multiple models. It serves as a lower bound for model merging. \n\n\u2022 Task Arithmetic [28] first introduces the concept of \"task vectors\" and merges them into the pre-trained model to execute multi-task learning. \n\n\u2022 Ties-Merging [76] addresses task conflicts by eliminating redundant parameters. The process involves three steps: Trim, Elect Sign, and Disjoint Merge. \n\n\u2022 Task Arithmetic (w/ DARE) [79] This variant incorporates the Bernoulli-Dropout technique for 70% sparsification before employing Task Arithmetic [28] for merging. \n\n\u2022 Ties-Merging (w/ DARE) [79] Similar to the previous approach, this variant integrates Bernoulli-Dropout for 70% sparsification, followed by Ties-Merging [76] for the merging process. \n\n\u2022 AdaMerging [78] assumes access to an offline test set and dynamically adapts to it by introducing additional coefficients at every layer, conducting unsupervised training across multiple iterations on the test set (without labels) to refine the model. \n\n\u2022 Surgery [77] assumes that test data IDs are accessible during inference, allowing it to insert corresponding task-specific adapters to leverage task-specific knowledge. \n\nThe coefficient for Task Arithmetic and Ties-Merging are decided by a small scale grid search on validation datasets. The coefficient of 0.7 is consistently applied for DARE Merging, following the previous papers [79].",
            "score": 0.40962071563879954,
            "section_title": "D.6 Baselines Details",
            "char_start_offset": 35035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 124,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 681
                },
                {
                    "start": 684,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1447
                },
                {
                    "start": 1450,
                    "end": 1620
                },
                {
                    "start": 1623,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 381,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 555,
                    "end": 559,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 699,
                    "end": 703,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 987,
                    "end": 991,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1162,
                    "end": 1166,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1207,
                    "end": 1211,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6572265625
        },
        {
            "corpus_id": "272831995",
            "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering",
            "text": "In this paper, we address the critical challenge of merging multiple LoRAs, each tailored for distinct tasks, into a unified and comprehensive LoRA. We identify parameter interference as a primary obstacle in merging, with parameter misalignment and knowledge conflict being significant contributors. denotes the output. The performance of the ensemble of LoRAs tends to be more stable, but it incurs additional computational overhead. In contrast to the ensemble method, model merging presents an alternative composition strategy. A typical strategy involves employing element-wise fusion of these parameters, represented as A \u2032 = 1 n n j=1 A j and B \u2032 = 1 n n j=1 B j . This formulation allows the merged parameters to function similarly to a single LoRA. However, directly merging parameters can lead to performance degradation due to parameter interference. \n\nOur proposed LoRA-LEGO method serves as a bridge between the two strategies, ensuring an optimal balance between computational efficiency and performance. By selectively aligning and fusing MSUs based on their semantic similarity, LoRA-LEGO effectively condenses the most relevant semantic features into fewer clusters. This process allows for the merging of parameters within each cluster, reducing the overall parameter count in a manner similar to the model merging method. By adjusting the number of clusters, LoRA-LEGO can accommodate more parameters for inference, much like the ensemble method. In this way, our method leverages the strengths of both methodologies, ultimately enhancing model performance and inference efficiency.",
            "score": 0.40946893333455314,
            "section_title": "CONCLUSION",
            "char_start_offset": 25740,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 861
                },
                {
                    "start": 864,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1183
                },
                {
                    "start": 1184,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7392578125
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model. Empirical evaluation on various vision and language benchmarks validate the effectiveness of our approach. Beyond model merging, our approach performs effective model compression, which compresses the model size to be 1% of the original without sacrificing performance. Additionally, Localize-and-Stitch also excels at retaining the pretrained knowledge. Overall, our approach offers a novel pathway for flexible and continual skills composition from finetuned models with minimal storage and computational overhead. \n\nPublished in Transactions on Machine Learning Research (12/2024) (SST-2, CR, MR, MPQA). ii) Conceptually dissimilar subset: Including tasks from different categories (SST-2 for sentiment classification, TREC for question classification, SUBJ for subjectivity, and MNLI for entailment). \n\nIn Table 6, we report the average performance for each subset. In the similar subset, where tasks share similar skills, all merging methods perform equally well. However, in the dissimilar subset, where task skills differ and may even conflict, Localize-and-Stitch shows a significant advantage, demonstrating its ability to effectively resolve task interference. Where are the localized regions? We analyze the distribution of the localized regions for both language and vision tasks in Figure 10, both in terms of the layer index and the transformer components. For the layers, different tasks seem to occupy different layers, although the earlier layers in the network seldomly appear in the localized regions. Interestingly, most of the localized regions concentrate in the LayerNorm parameters. This pattern can possibly be attributed to a distribution shift observed in the finetuning data compared to the pretraining data, necessitating adjustments to the LayerNorm parameters to accommodate this shift. Full grafted performance. We evaluate the quality of the localized regions by the grafted performance.",
            "score": 0.4094483211987704,
            "section_title": "Conclusion",
            "char_start_offset": 35729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1313
                },
                {
                    "start": 1316,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2115
                },
                {
                    "start": 2116,
                    "end": 2326
                },
                {
                    "start": 2327,
                    "end": 2352
                },
                {
                    "start": 2353,
                    "end": 2429
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9033203125
        },
        {
            "corpus_id": "270869770",
            "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via Multi-objective Optimization",
            "text": "Large language models (LLMs) have shown great performance on tasks in various domains such as natural language processing [1], computer vision [2] etc. With the great effort of world-wide contributors in community, a large number of general-purpose pre-trained and task-specific fine-tuned language models have been proposed and made publicly available. However, LLM pre-training or fine-tuning is non-trivial and requires a lot of effort and financial budget. Recently, model merging (MM) has attracted many researchers' attention. By combining multiple LLMs into a single model with better performance and adaptability on more tasks, MM offers a novel cost-efficient way of obtaining new powerful language models without performing additional training [3,4], just like the Power Rangers merge their Zords together to form a mighty Megazord [5]. Ideally, MM is supposed to inherit and amplify the strengths from its source models while ignoring their weaknesses. Therefore, the obtained model will be able to tackle the union set of all the tasks where the source models are pre-trained/fine-tuned with better performance. Yet this is achieved without training, which saves a large amount of calculation/financial budget. With the help of open-source toolkits such as mergekit [6,7], MM has become popular for LLM developing and shown great potential on the Open LLM Leaderboard [8]. \n\nHowever, model merging requires the model maker to have profound knowledge or intuition. Automatically discovering more capable MM recipes is still in its infancy. To the best of our knowledge, The most related work is [3], where Akiba et al. proposed to use evolutionary algorithms (EAs) to generate powerful model merging recipes which operates in both parameter space and data flow space. However, the approach of using diverse task scores and optimizing the parameter space to explore the comprehensive potential of the final model has been largely overlooked. \n\nMM is similar to ensemble learning in the sense that they both try to produce one model from multiple source models. In ensemble learning, It is generally believed that diversity in an ensemble could be beneficial for improving its performance [9,10,11]. One could infer that maintaining diversity during MM may also result in powerful LLMs.",
            "score": 0.4091046033876865,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1550
                },
                {
                    "start": 1551,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2295
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 146,
                    "matchedPaperCorpusId": "258762579"
                },
                {
                    "start": 2201,
                    "end": 2204,
                    "matchedPaperCorpusId": "2976898"
                },
                {
                    "start": 2204,
                    "end": 2207,
                    "matchedPaperCorpusId": "8711351"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52783203125
        },
        {
            "corpus_id": "276647526",
            "title": "Layer-Aware Task Arithmetic: Disentangling Task-Specific and Instruction-Following Knowledge",
            "text": "Combining model capabilities without additional training has attracted growing attention. Model merging fuses weights of separately fine-tuned models for multi-task learning (Choi et al., 2024), and simple averaging can improve accuracy and robustness (Wortsman et al., 2022). TIES (Yadav et al., 2023) resets negligible changes to address sign conflicts, reducing performance drops; Deltasparsification (DARE) (Yu et al., 2024) discards up to 99% of fine-tuning deltas to merge multiple homologous models. Most research aims to minimize utility loss of merged LLMs (Matena and Raffel, 2022;Jin et al., 2023;Zhou et al., 2024;Du et al., 2024;Lu et al., 2024;Dai et al., 2025;Lai et al., 2025), while Yang et al. (2024b,a); Bowen et al. (2024);Gargiulo et al. (2025) explore merging computer vision models using key parts of task vectors. \n\nAn alternative line of research, task arithmetic (TA), views tasks as weight update vectors composed via vector operations. Ilharco et al. (2023) define a task vector as the difference between a fine-tuned model and its base, enabling multiple tasks to be learned simultaneously and new tasks to be inferred without retraining. Negating a task vector selectively unlearns a specific task with minimal impact on others, implying that model weights shift independently per task. TA has been considered in fine-tuning (Zhang et al., 2023;Choi et al., 2024) and alignment (Zhao et al., 2024;Li et al., 2025;Hazra et al., 2024) contexts. \n\nIn this paper, we focus on TA for both task learning and forgetting. Existing methods generally merge or edit entire models without distinguishing which layers encode task-specific versus general knowledge. In contrast, our proposed LATA performs a layer-wise analysis to separate generic utility from task-specific effects, enabling selective amplification or removal of tasks while preserving overall performance.",
            "score": 0.408474693361686,
            "section_title": "Related Work",
            "char_start_offset": 2956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 89
                },
                {
                    "start": 90,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 837
                },
                {
                    "start": 840,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1472
                },
                {
                    "start": 1475,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1890
                }
            ],
            "ref_mentions": [
                {
                    "start": 252,
                    "end": 275,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 282,
                    "end": 302,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 411,
                    "end": 428,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 566,
                    "end": 591,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 591,
                    "end": 608,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 608,
                    "end": 626,
                    "matchedPaperCorpusId": "270559703"
                },
                {
                    "start": 642,
                    "end": 658,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 964,
                    "end": 985,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1355,
                    "end": 1375,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1408,
                    "end": 1427,
                    "matchedPaperCorpusId": "270067915"
                },
                {
                    "start": 1427,
                    "end": 1443,
                    "matchedPaperCorpusId": "272310211"
                },
                {
                    "start": 1443,
                    "end": 1462,
                    "matchedPaperCorpusId": "270560348"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50048828125
        },
        {
            "corpus_id": "271769807",
            "title": "Chamain: Harmonizing Character Persona Integrity with Domain-Adaptive Knowledge in Dialogue Generation",
            "text": "The weight merging technique has emerged as a significant application of NLP in recent years, aiming to combine multiple task-specific models into a unified model. This methodology has been widely adopted in various benchmarks (Kim et al., 2023) due to its ability to enhance performance not only on the target task but also on out-of-domain tasks. Unlike model ensemble methods, which utilizes the predictions of multiple models to generate a final output, weight merging yields a single model through techniques such as interpolating the weights of multiple models or employing task arithmetic (Ilharco et al., 2023). There are various methods for merging the weights of models fine-tuned on different datasets, with traditional approaches including weight averaging. For instance, TIES-Merging (Yadav et al., 2023) selectively incorporates changes from fine-tuned models by discarding low-magnitude alterations and merging only those values that align with designated sign, while Dare-TIES (Yu et al., 2023) reduces redundancy by converting the majority of delta parameters to zero. We leverage these merging techniques to develop a chatbot that, by accounting for the distinct traits of chit-chat and knowledge-grounded dialogues, seamlessly integrates knowledge, maintains its persona, and effectively engages in multi-turn conversations to ensure enjoyable interactions.",
            "score": 0.4064586197702927,
            "section_title": "Weight Merging",
            "char_start_offset": 8856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1376
                }
            ],
            "ref_mentions": [
                {
                    "start": 596,
                    "end": 618,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 797,
                    "end": 816,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74560546875
        },
        {
            "corpus_id": "277271763",
            "title": "Personalized Language Models via Privacy-Preserving Evolutionary Model Merging",
            "text": "However, it introduces additional model complexities, lacks direct optimization for task-specific metrics, and does not explicitly address the privacy risks associated with using shared PEFT modules. To this end, PriME leverages evolutionary methods to solve the multi-objective optimization of capturing user preferences while preserving the privacy of the users sharing their PEFT modules. \n\nModel Merging. Model merging is a surprisingly effective approach to developing models by integrating multiple models with diverse capabilities into a unified architecture in parameter space. A simple approach of averaging the weights of multiple models fine-tuned from the same base model has demonstrated significant performance in image processing and classification tasks (Wortsman et al., 2022). \n\nTo extend the technique to the language domain, methods such as Task Arithmetic (Ilharco et al., 2022) construct task vectors that represent models weights encoding task-specific abilities and combine them to create a merged model with the desired capabilities. Despite the surprising effectiveness of merged models, the process is often considered an art, requiring human intuition and expertise in model selection and crafting merging recipes, such as interpolation weights. \n\nRecently, Akiba et al. (2024) have demonstrated the use of evolutionary algorithms to automatically discover effective model merging recipes, such as interpolation weights for mixing parameters and layer permutations, to further enhance the technique. In this work, we extend the evolutionary approach to model merging for personalizing language models, with an explicit component in optimization to safeguard the privacy of shared user information.",
            "score": 0.4056934450559768,
            "section_title": "Related Work",
            "char_start_offset": 25339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1725
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314453125
        },
        {
            "corpus_id": "245502478",
            "title": "Parameter Differentiation based Multilingual Neural Machine Translation",
            "text": "The key issue in parameter differentiation is the definition of differentiation criterion that helps to detect the shared parameters that should differentiate into more specialized types. We define the differentiation criterion based on intertask gradient cosine similarity, where the parameters facing conflicting gradients are more likely to be language-specific. \n\nAs shown in Figure 2, the parameter \u03b8 i is shared by tasks t 1 , t 2 , and t 3 at the beginning. To determine whether the shared parameter should be more specialized, we first define the interference degree of the parameter shared by the three tasks with the inter-task gradient cosine similarity. More formally, suppose the i-th parameter \u03b8 i in an MNMT model is shared by a set of tasks T i , the interference degree I of the parameter \u03b8 i is defined by: \n\nwhere g tj i and g t k i are the gradients of task t j and t k respectively on the parameter \u03b8 i . \n\nIntuitively, the gradients determine the optimization directions. For example in Figure 2, the gradient g tj i indicates the direction of global optimum for task t j . The gradients with maximum negative cosine similarity, such as g t1 i and g t3 i , point to opposite directions, which hinders the optimization and has been proved detrimental for multi-task learning (Yu et al. 2020;Wang et al. 2021). \n\nThe gradients of each task on each shared parameter are evaluated on held-out validation data. To minimize the gradient variance caused by inconsistent sentence semantics across languages, the validation data is created as multi-way aligned, i.e., each sentence has translations of all languages. With the held-out validation data, we evaluate gradients of each task on each shared parameter for calculating inter-task gradient similarities as well as the interference degree I for each parameter. The interference degree I helps the model to find out parameters that face severe interference and the parameters with high interference degrees are flagged for differentiation. Suppose the parameter \u03b8 i shared by tasks T i is flagged, we cluster the tasks in T i into two subsets T i and T i that minimize the overall interference. The partition P * is obtained by:",
            "score": 0.4053007611162179,
            "section_title": "The Differentiation Criterion",
            "char_start_offset": 7072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 824
                },
                {
                    "start": 827,
                    "end": 925
                },
                {
                    "start": 928,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2197
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1312,
                    "matchedPaperCorpusId": "210839011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35595703125
        },
        {
            "corpus_id": "276422064",
            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
            "text": "Merging Models with Sense-Merging. We first evaluate the effectiveness of our Sens-Merging method by utilizing it as a plug-and-play module to enhance existing task-vector-based baselines. Table 1 presents the performance of the baseline methods alongside their Sens-Merging enhanced counterparts across seven datasets. Specifically, when merging fine-tuned models specialized in general knowledge (Chat1 ), mathematical reasoning (Math2 ), and code generation (Code3 ), all derived from LLaMA2-7B4 , Sens-Merging demonstrates a consistent improvement in the average performance across all domains. Specifically, when comparing the average scores of each method with and without Sens-Merging, we find that: \n\n(1) Superior Improvement in Task Arithmetic: Task Arithmetic exhibits a particularly notable increase from an average score of 29.03 without Sens-Merging to 34.78 with Sens-Merging, achieving a 19.22% relative improvement of 5.58 points. As both Ties-Merging and DARE have implemented drop strategies to mitigate parameter interference, the integration of scaling coefficient adjustments through Sens-Merging does not achieve as substantial an enhancement as seen with Task Arithmetic. Nevertheless, Sens-Merging still contributes to performance improvements in these methods, with Ties-Merging increasing from an average score  (47.69) and MATH (7.80), surpassing their respective baselines. In code generation, Task Arithmetic shows substantial improvements, increasing from 13.5 to 33.1 on MBPP and from 7.3 to 18.9 on HumanEval. \n\n(3) Enhanced Performance than Individual Fine-tuned Models: Sens-Merging enables the combined models to achieve higher performance on general knowledge and code generation tasks, even surpassing the original code fine-tuned model. For example, when integrating the Chat, Math, and Code models using Sens-Merging, performance on the MBPP and HumanEval datasets increases significantly. Specifically, accuracy improves from 26.8 to 32.3 on the MBPP dataset and from 12.8 to 19.5 on the HumanEval dataset. This demonstrates that model merging can overcome the challenges associated with training a single model for complex tasks by effectively integrating capabilities from other specialized fine-tuned models.",
            "score": 0.4051437908555608,
            "section_title": "Main Results",
            "char_start_offset": 16128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 34
                },
                {
                    "start": 35,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 706
                },
                {
                    "start": 709,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1401
                },
                {
                    "start": 1402,
                    "end": 1541
                },
                {
                    "start": 1544,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91357421875
        },
        {
            "corpus_id": "276928602",
            "title": "Whoever Started the Interference Should End It: Guiding Data-Free Model Merging via Task Vectors",
            "text": "Consider a linear layer whose parameters are updated through gradient descent: each neuron's weight adjustment derives from the product of the learning rate, the gradient of its output, and the corresponding input vector. Crucially, since the learning rate and total optimization steps always remain constrained during standard fine-tuning procedures, the corresponding inputs for individual samples remain consistent across successive update iterations. The temporal consistency indicated that the cumulative update to each neuron can be approximated as a weighted summation of fixed input vectors, with coefficients determined by the product of learning rates and corresponding gradient magnitudes. This implies that the task vectors of a linear layer constitute an approximate linear subspace of the input, allowing us to implicitly utilize training data information solely through the task vectors. In this context, the task vectors are computed by calculating the difference between the weights of the expert models and those of the pre-trained model. \n\nBased on this insight, we propose WUDI-Merging (Whoever started the interference shoUd enD It), a simple yet effective data-free model merging method. We evaluate our method on both vision and natural language processing (NLP) tasks, where our method significantly surpasses recent data-free model merging techniques and outperforms mainstream test-time adaptation model merging methods. In Summary, our contributions are summarized as follows: \n\n\u2022 Through detailed theoretical analysis, we have demonstrated that the task vectors within the linear layer approximately form a linear subspace of the input space. \n\n\u2022 We propose WUDI-Merging, a simple yet effective data-free model merging method that minimizes interference to the task vector without requiring any additional data, extra storage, or rescaling coefficients. \n\n\u2022 Extensive evaluations have demonstrated that WUDI-Merging achieves state-of-the-art performance in datafree model merging, surpassing mainstream test-time adaptation model merging methods.",
            "score": 0.4045600845587924,
            "section_title": "Introduction",
            "char_start_offset": 2183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1056
                },
                {
                    "start": 1059,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1670
                },
                {
                    "start": 1673,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41064453125
        },
        {
            "corpus_id": "268041323",
            "title": "Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models",
            "text": "According to our findings, the mechanism by which LMs use both internal memory and external context can be summarized as three stages in Figure 1: (1) Enriching semantic information; (2) Propagating question information; and (3) Extracting attribute information, where knowledge conflicts arise at the third stage, due to the inconsistent information flows between memory heads and context heads. \n\nInspired by our insights into knowledge conflicts, we propose a minimally-invasive control method called Pruning Head via PatH PatcHing (PH3), which can efficiently mitigate knowledge conflicts by intervening on attention heads without updating model parameters. First, we use the path patching (Goldowsky-Dill et al., 2023;Wang et al., 2023a) technique to localize important memory heads and context heads. Our method can avoid the noise interference of other heads, enabling a more accurate calculation of the importance score for the target head. Then, we perform structured pruning on those negative attention heads to mitigate conflicts. In this way, our method can flexibly control LMs to use internal memory or external context. Experimental results on the World Capital dataset show that our method can not only reliably and consistently increase the average internal memory usage rate of eight LMs by 44.0% (from 49.7% to 93.7%) but also increase the external context usage rate by 38.5% (from 50.3% to 88.8%). PH3 also enables LMs to generate answers more faithfully according to retrieved passages in open-domain QA tasks. We conduct extensive experiments to demonstrate the cross-model (e.g., from GPT series to LLaMA2 series), cross-relation (e.g., from World Capital to Official Language), and cross-format (e.g., from triple format to document format) generalization. Our contributions are summarized as follows: \n\n\u2022 We perform an exploration into the mechanism of interpreting knowledge conflicts, and reveal that memory heads and context heads at later layers can cause knowledge conflicts when inconsistent information flows merge. In this work, we mainly focus on the autoregressive transformer-based language models. Given a sequence of input tokens",
            "score": 0.4045129652267877,
            "section_title": "Introduction",
            "char_start_offset": 3587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 396
                },
                {
                    "start": 399,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1041
                },
                {
                    "start": 1042,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1826
                },
                {
                    "start": 1829,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2135
                },
                {
                    "start": 2136,
                    "end": 2168
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "273345526",
            "title": "Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning",
            "text": "Large language models demonstrate strong multitask capabilities, effectively addressing a wide range of tasks across diverse domains [Brown et al., 2020;Radford et al., 2019]. \"Safety\" in a model can be viewed as another \"task-solving\" ability that a model can learn. It is well established that equipping a model with any kind of capabilities with the standard paradigm of training requires copious amounts of data. Multi-tasking abilities typically arise from fine-tuning models on mixed datasets, which combine data from various sources and across many tasks [Raffel et al., 2023;Wang et al., 2019;\u00dcst\u00fcn et al., 2024]. However, determining the optimal strategy for mixing datasets in multi-task training is often complex and resource-intensive, as it must ensure that all tasks benefit from the shared training process -especially in the context of safety, where the general performance of models often gets cannibalized in exchange for safety [Tsipras et al., 2019;Bianchi et al., 2024;Ray & Bhalani, 2024;\u00dcst\u00fcn et al., 2024]. \n\nMore recently, an emerging approach for enabling multi-tasking has focused on training distinct models for specific tasks, followed by a weight-merging process governed by a pre-defined algorithm [Tam et al., 2023;Yang et al., 2024;Li et al., 2024a;Wan et al., 2024;Zhou et al., 2024;Davari & Belilovsky, 2024]. This method has shown great promise in building models with new capabilities without incurring additional costs and challenges that accompany training from scratch. However, a We analyze the differences in merging models on trained with specialized multilingual datasets, particularly in the context of safety, in contrast to those trained directly on mixtures of these datasets. We follow the LLM-as-ajudge approach for evaluating the performance of these models along two axes -general and safety. \n\nkey question remains -how does it compare to traditional data mixing and weighting approaches?",
            "score": 0.4042696647400017,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1941
                }
            ],
            "ref_mentions": [
                {
                    "start": 133,
                    "end": 153,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371337890625
        },
        {
            "corpus_id": "276903216",
            "title": "Task Vector Quantization for Memory-Efficient Model Merging",
            "text": "Early model merging studies [24,51,52] explore combining models trained on the same task in weight space, demonstrating robustness to distribution shifts and improved performance on unseen data. Recent studies have extended this paradigm to multi-task learning [5,46,63], where merging multiple models trained on different tasks enables efficient knowledge sharing across domains. \n\nTask arithmetic [23] construct multi-task models through additive task vectors, which are computed by subtracting the pre-trained model's parameter from those of the finetuned model. Unlike direct interpolation, task vectors enable modular composition of specialized models, making them more flexible for multi-task settings. Sparsificationbased merging [55,61], where only the most significant parameters are retained to reduce interference, often complemented by rescaling strategies to enhance stability before integration. However, these approaches require manual tuning of task vector coefficients, which increases computational cost and limits scalability. To mitigate this issue, AdaMerging [58] determines the coefficients adaptively through test-time training. LiNeS [49] simplifies coefficient selection by increasing them linearly with layer depth. To move beyond fixed coefficients for all inputs, router-based approaches [33,45] dynamically determine instance-wise merging coefficients. Representation Surgery [57] independently learn task-specific parameters at test time to align the merged model's output with that of each task's individual model. \n\nAs the number of tasks increases, storing multiple finetuned checkpoints becomes a major bottleneck. Despite efforts to address storage issues, existing methods like EMR-Merging [20] and TALL Mask [48] rely solely on taskspecific binary masks. In contrast, we introduce a more universally applicable strategy that quantizes each task vector, significantly reducing storage overhead while preserving performance. Crucially, our approach integrates seamlessly with existing task vector-based methods, requiring only minimal modifications.",
            "score": 0.4042649703094485,
            "section_title": "Related Work 2.1. Model Merging",
            "char_start_offset": 5133,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1546
                },
                {
                    "start": 1549,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 28,
                    "end": 32,
                    "matchedPaperCorpusId": "268733341"
                },
                {
                    "start": 32,
                    "end": 35,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 35,
                    "end": 38,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 261,
                    "end": 264,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 264,
                    "end": 267,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 267,
                    "end": 270,
                    "matchedPaperCorpusId": "235790783"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 737,
                    "end": 741,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 741,
                    "end": 744,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1081,
                    "end": 1085,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1159,
                    "end": 1163,
                    "matchedPaperCorpusId": "273507837"
                },
                {
                    "start": 1317,
                    "end": 1321,
                    "matchedPaperCorpusId": "270702345"
                },
                {
                    "start": 1321,
                    "end": 1324,
                    "matchedPaperCorpusId": "267365047"
                },
                {
                    "start": 1406,
                    "end": 1410,
                    "matchedPaperCorpusId": "267412030"
                },
                {
                    "start": 1727,
                    "end": 1731,
                    "matchedPaperCorpusId": "270067773"
                },
                {
                    "start": 1746,
                    "end": 1750,
                    "matchedPaperCorpusId": "269757600"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8037109375
        },
        {
            "corpus_id": "278534535",
            "title": "Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces",
            "text": "Various techniques have been proposed to mitigate these collisions, including N-LoRA methods, which attempt to preserve or restore orthogonality between subspaces associated with each training instance, thereby limiting the unintended rewriting of previously acquired knowledge [197]. \n\nLarge-scale language models with greater parametric capacity generally exhibit an increased tendency toward orthogonality [193,192]. Their expanded representational space allows for a finer separation of relevant dimensions, reducing interference noise. Consequently, this finer-grained separation of concepts and tasks translates into superior performance, particularly in complex tasks requiring intensive knowledge management [193]. However, larger models also face novel challenges in detecting and correcting internal collisions, as the multiplicity of parameters and potentially addressed tasks makes controlling residual orthogonality more difficult [192]. \n\nA deeper understanding of superposition in LLMs, whether manifested through quasi-orthogonality or parameter collisions, largely depends on disentanglement techniques. Concept Activation Vectors (CAVs) enable the projection of neural activity into semantically interpretable directions [194]. Sparse Autoencoders (SAEs) offer a dimensionality reduction method to isolate semantic features into distinct latent codes [190], making residual overlaps visible. Furthermore, causal intervention approaches, which involve selectively manipulating the activation of certain neurons, have demonstrated the importance of overlaps in determining model outputs : any targeted modification can simultaneously affect multiple tasks or concepts. For instance, [199] applied causal interventions to transformer models to analyze pronoun resolution, showing that targeted manipulation of specific attention heads influences the propagation of contextual information and simultaneously affects multiple possible interpretations. Similarly, [200] studied causal relationships across different layers of language models, revealing that specific interventions modulate neural activity hierarchically and affect various cognitive functions in parallel [199,200]. \n\nThis superposition has significant implications for the explainability of large language models (LLMs) and the localization of their internal knowledge. Probing methods indicate that neurons initially appearing specialized and monosemantic (e.g., concerning grammatical categories) actually participate in more abstract functions when considering the entire context. For instance, [50] demonstrated that the early layers of LLMs use sparse neuron combinations to represent multiple superimposed features, while intermediate layers contain neurons that appear less poly-semantic and more dedicated to high-level contextual characteristics.",
            "score": 0.40414248275865244,
            "section_title": "Superposition and Quasi-Orthogonality",
            "char_start_offset": 11391,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1120
                },
                {
                    "start": 1121,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2349
                },
                {
                    "start": 2350,
                    "end": 2563
                },
                {
                    "start": 2564,
                    "end": 2835
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "276742465",
            "title": "Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think",
            "text": "Model merging, widely explored in natural language processing and image classification, typically combines multiple fine-tuned task-specific models into a unified multitask model without additional training. Early methods [1,11,13,14,26,33] leverage interpolation techniques to integrate multi-task knowledge effectively. Task-Arithmetic [25] extends these approaches by enabling complex arithmetic operations in weight space, offering finer control over the merging process. Recent studies [30,39,61,63,66] have further reduced interference during merging. Beyond interpolation-based methods, recent works have shown that sparsifying [66] or amplifying delta weights [74] can enhance model knowledge in merging. Additionally, merging techniques have been introduced to multimodal understanding such as question-answering [9,47]. However, the video generation domain has remained unexplored until now. \n\nWe pioneer the introduction of model merging techniques into the video generation domain. Specifically, we leverage model merging to extrapolate motion degree knowledge and extract fine-grained decoupled knowledge, which are selectively merged at different diffusion stages.",
            "score": 0.40324772683189375,
            "section_title": "Model Merging",
            "char_start_offset": 5542,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1178
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 225,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 225,
                    "end": 228,
                    "matchedPaperCorpusId": "3845139"
                },
                {
                    "start": 228,
                    "end": 231,
                    "matchedPaperCorpusId": "209324341"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "4055784"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 237,
                    "end": 240,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 495,
                    "end": 498,
                    "matchedPaperCorpusId": "258832777"
                },
                {
                    "start": 498,
                    "end": 501,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 501,
                    "end": 504,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 504,
                    "end": 507,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 635,
                    "end": 639,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.435302734375
        },
        {
            "corpus_id": "272831995",
            "title": "Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering",
            "text": "We compare our method with the following baseline: \n\n1. Weight Averaging. This approach averages the parameters across different instances of LoRA, resulting in a new composite LoRA defined as A \u2032 = 1 n n i=1 A i and B \u2032 = 1 n n i=1 B i , where A i and B i represent the parameters from the i-th instance of the original LoRA models, and n is the number of models being averaged. \n\n2. Ensemble. This method averages the outputs from each LoRA, simultaneously activating multiple LoRAs to compose a combined output. The specific calculation for the mixed output is defined as x \u2032 = 1 n n i=1 B j A j x i . 3. Task Arithmetic. This method is akin to weight averaging, but it differentiates by using weights derived from a hyper-parameter search to merge models. The calculations for this composite are A \u2032 = p n i=1 A i and B \u2032 = p n i=1 B i , where p represents the hyperparameter that scales the contributions of each model. 4. Ties-Merging. This method aims to resolve element-wise parameter interference by initially trimming the redundant parameters, retaining only the top-k% of values based on their magnitude. It then selects the sign vector for the merged model and finally performs a disjoint mean operation. Ties-Merging posits that the primary source of parameter interference arises from inconsistencies in the values of merged parameters, while potentially overlooking issues related to misalignment and knowledge conflict.",
            "score": 0.40302013941279163,
            "section_title": "C DETAILS OF BASELINE METHODS",
            "char_start_offset": 27375,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 53,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5126953125
        },
        {
            "corpus_id": "277313159",
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "text": "Model Merging. Model merging integrates existing models to handle multiple tasks without training [21,23,49], but it faces task interference challenges [36,48]. Simple averaging [44] results in great performance degradation. Methods like Fisher-Merging [33] and RegMean [23] determine merging coefficients using various matrices but incur high computational costs or data requirements, limiting their usage in edge deployment. Task Arithmetic [21] merges task vectors instead of weights, while Ties-Merging [48] resolves parameter conflicts, and AdaMerging [52] automates coefficient selection. However, these methods have requirements on data distribution and exhibit limited applicability. Techniques such as DARE [53] and PCB-Merging [12] analyze parameters to mitigate performance drops but still face task conflicts. EMR-Merging [20] and Twin-Merging [29] store task-specific knowledge but require large storage and overlook backbone optimization. Our method applies highpass filtering to reduce task interference and lightweight experts to enhance performance, achieving a balance between merging costs and effectiveness across various tasks. For further details, please refer to Appendix C.2. Model Ensemble. Model ensemble combines outputs from multiple models [11,15,22], but with large models, it faces storage and inference challenges. In contrast, Model merging enables a single model to solve multiple tasks, significantly reducing storage and inference costs. Multi-task Learning. Multi-task learning (MTL) aims to solve multiple tasks using a single model [4,28,41], but in the era of large models, it faces challenges like high training costs, data privacy issues, and expertise requirements [52]. In contrast, model merging uses existing open-source models to create a multi-task model with little or no training, significantly reducing deployment costs. b) The correlation between the merging performance of ViT-B-32 and the variance of frequency domain amplitude power. Normalized accuracy is defined as the ratio of merged model performance to fine-tuned model performance on the same task.",
            "score": 0.4025569433396218,
            "section_title": "Related Work",
            "char_start_offset": 5320,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 14
                },
                {
                    "start": 15,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1871
                },
                {
                    "start": 1872,
                    "end": 1988
                },
                {
                    "start": 1989,
                    "end": 2110
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 507,
                    "end": 511,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 557,
                    "end": 561,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 716,
                    "end": 720,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1269,
                    "end": 1273,
                    "matchedPaperCorpusId": "201667785"
                },
                {
                    "start": 1273,
                    "end": 1276,
                    "matchedPaperCorpusId": "233033495"
                },
                {
                    "start": 1276,
                    "end": 1279,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 1571,
                    "end": 1574,
                    "matchedPaperCorpusId": "237571793"
                },
                {
                    "start": 1574,
                    "end": 1577,
                    "matchedPaperCorpusId": "261080843"
                },
                {
                    "start": 1577,
                    "end": 1580,
                    "matchedPaperCorpusId": "221771219"
                },
                {
                    "start": 1708,
                    "end": 1712,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44970703125
        },
        {
            "corpus_id": "273098230",
            "title": "Parameter Competition Balancing for Model Merging",
            "text": "Previous studies found that when multiple models are fine-tuned from the same pre-trained initialization, averaging their weights can lead to improved performance on single tasks [20,86,13,29,92] different tasks [27] and out-of-distribution generalization [3,60]. Fisher Merging [46] goes beyond simple averaging to identify the importance of individual parameters using Fisher information matrix [17] and uses it to weigh the parameters in each model when merging. RegMean [30] proposed a closed-form solution for the merged model's parameters by solving a local linear regression problem for each individual linear layer in the model. However, both the Fisher Merging and RegMean methods are time-consuming and computationally intensive. \n\nTask Arithmetic [28] introduces the concept of task vectors, demonstrating their effectiveness and lightweight nature in facilitating cross tasks generalization. Expanding on this groundwork, PEM Composition [96] extends the task arithmetic framework to merge LoRA [24] models, while Ties-Merging [89] addresses task conflicts by resetting redundant parameters and resolving sign conflicts. However, these methods share a merging coefficient across all task vectors, limiting flexibility. In contrast, Lorahub [25] and AdaMerging [90] utilize different coefficients for enhanced adaptability, but Lorahub's performance is restricted as it only searches coefficients at the task level. AdaMerging also demands complex training and unlabeled test datasets and is applicable solely to classification problems. DARE [94] proposes drop and rescale as a preprocessing step when merging fine-tuned LLMs. Our approach mainly adopts the strategies of using drop to reduce interference and performing rescale at the parameter level, while simultaneously considering self-awareness and cross-model awareness.",
            "score": 0.40218946330396943,
            "section_title": "Merging fine-tuned models with same initialization",
            "char_start_offset": 5752,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1839
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 186,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "251493208"
                },
                {
                    "start": 256,
                    "end": 259,
                    "matchedPaperCorpusId": "239049452"
                },
                {
                    "start": 259,
                    "end": 262,
                    "matchedPaperCorpusId": "248887264"
                },
                {
                    "start": 474,
                    "end": 478,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 758,
                    "end": 762,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1039,
                    "end": 1043,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1272,
                    "end": 1276,
                    "matchedPaperCorpusId": "263620126"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "276776501",
            "title": "CABS: Conflict-Aware and Balanced Sparsification for Enhancing Model Merging",
            "text": "Model merging has become a vital strategy for combining multiple fine-tuned models into a single multitask model without requiring additional training. The simplest merging method is directly averaging the model parameters (Izmailov et al., 2018;Wortsman et al., 2022). However, this naive approach often fails to account for task-specific variations, leading to suboptimal performance. A more refined approach, Task Arithmetic (Ilharco et al., 2022), combines task vectors-differences between fine-tuned and pre-trained parameters-using weighted sums controlled by scaling coefficients \u03bb. These scaling coefficients allow precise control over the contribution of each task vector during merging, playing a critical role in balancing the influence of different tasks. However, it still struggles with parameter redundancy and sign conflicts. \n\nTo address these issues, TIES-Merging (Yadav et al., 2024) prunes low-magnitude parameters and resolves sign conflicts, reducing interference and preserving critical parameters during merging. DARE (Yu et al., 2024), a technique inspired by Dropout (Srivastava et al., 2014), reveals the high redundancy in task vectors by randomly dropping 90% of the parameters and rescaling the remaining ones. Using random pruning, DARE has been shown to outperform magnitude-based pruning methods in model merging. However, DARE does not fully explain the reasons for this improvement. Our analysis suggests that DARE helps mitigate some of the overlap and imbalance. However, the random nature of the approach can potentially sacrifice precision. \n\nModel pruning, particularly magnitude pruning (Zhu & Gupta, 2018), have been extensively studied for their role in optimizing model performance and reducing computational costs (Liu et al., 2019;Frankle & Carbin, 2018;Gale et al., 2019;Zhu & Gupta, 2018). Magnitude pruning retains parameters based on their magnitude, assuming that larger magnitudes correspond to more critical information (Ko-0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0  valeva et al., 2021;Puccetti et al., 2022;Yin et al., 2023).",
            "score": 0.40126294606120055,
            "section_title": "Related Work",
            "char_start_offset": 6974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 246,
                    "matchedPaperCorpusId": "3833416"
                },
                {
                    "start": 246,
                    "end": 268,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 882,
                    "end": 902,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1042,
                    "end": 1059,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1093,
                    "end": 1118,
                    "matchedPaperCorpusId": "6844431"
                },
                {
                    "start": 1628,
                    "end": 1647,
                    "matchedPaperCorpusId": "27494814"
                },
                {
                    "start": 1777,
                    "end": 1800,
                    "matchedPaperCorpusId": "53388625"
                },
                {
                    "start": 1818,
                    "end": 1836,
                    "matchedPaperCorpusId": "27494814"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7646484375
        },
        {
            "corpus_id": "273346635",
            "title": "Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs",
            "text": "Continual Learning is a well-established technique for updating existing machine learning models with the latest information and trends, allowing 7 https://huggingface.co/models?search=llama3 language models to adapt to new data while preserving the knowledge acquired during prior training (Caccia et al., 2020;Le Scao et al., 2023;Ibrahim et al., 2024) Yang et al. (2024c) adopt a strategy of continuous pre-training followed by instruction fine-tuning on domain-specific data to learn new domains. However, the reasons behind why this approach is the most effective for acquiring both new knowledge and instruction-following capabilities in large language models (LLMs) have not been thoroughly explored. Model Merging: Recent studies have demonstrated that specialized fine-tuned models can be merged to combine capabilities and generalize to new skills (Yu et al.;Yang et al., 2024b). Several techniques have been explored for merging the abilities of two or more models, including Task Arithmetic (Ilharco et al., 2022), TIES (Yadav et al., 2024), and Model Breadcrumbs (Davari and Belilovsky, 2023). Following Ilharco et al. (2022), we employ Task Arithmetic in our work to extract the instruction residual. Although different model merging techniques could affect the transfer of instruction-following capabilities, understanding the specific impact of these techniques is beyond the scope of this study.",
            "score": 0.4001101367085995,
            "section_title": "Related Work",
            "char_start_offset": 22373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 500
                },
                {
                    "start": 501,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1412
                }
            ],
            "ref_mentions": [
                {
                    "start": 291,
                    "end": 312,
                    "matchedPaperCorpusId": "212675315"
                },
                {
                    "start": 858,
                    "end": 869,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 1032,
                    "end": 1052,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.368408203125
        },
        {
            "corpus_id": "267061245",
            "title": "Knowledge Fusion of Large Language Models",
            "text": "With the continuous success of large language models (LLMs) such as GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023) series across a wide range of natural language processing (NLP) tasks, it has become a strategic imperative for corporations to create their own LLMs. However, the costs associated with LLM development are astronomical. In addition to requiring vast amounts of training data, advanced techniques, substantial computational resources, and skilled labor, the development process also exerts significant pressure on energy consumption and the environment (Rillig et al., 2023). While these LLMs exhibit structural and functional differences, they share similar capabilities across a spectrum of NLP tasks. Consequently, beyond the traditional approach of training an LLM from scratch, an alternative option is to combine existing LLMs into a new, more powerful one, which is termed knowledge fusion of LLMs in this paper. If successful, this fusion not only cuts the cost of initial training but also allows the integrated model to benefit from the strengths of all the LLMs. This new model can also be fine-tuned and adapted for various downstream tasks. Moreover, the fusion can also happen among fine-tuned LLMs that specialize in a specific task. \n\nThe endeavor to integrate the capabilities of multiple models has been a long-standing pursuit. For example, ensemble methods (Littlestone & Warmuth, 1994;Jiang et al., 2023) directly aggregate the outputs of different models to enhance prediction performance and robustness. However, this approach requires maintaining multiple trained models and executing each during inference, which is impractical for LLMs due to their substantial memory and inference time requirements. Likewise, this approach doesn't facilitate fine-tuning, which is essential for many LLMs. Another approach is to directly merge several neural networks into a single network through parameter-wise arithmetic operations (Wortsman et al., 2022;Jin et al., 2022). This approach typically assumes uniform network architectures and attempts to establish mappings between the weights of distinct neural net- works, which is often unattainable in the context of LLMs. Moreover, weight merging may lead to suboptimal results when substantial differences exist in the parameter space (Li et al., 2022).",
            "score": 0.3996240109913691,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1273
                },
                {
                    "start": 1276,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2212
                },
                {
                    "start": 2213,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 92,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 578,
                    "end": 599,
                    "matchedPaperCorpusId": "257098877"
                },
                {
                    "start": 1402,
                    "end": 1431,
                    "matchedPaperCorpusId": "12843330"
                },
                {
                    "start": 1971,
                    "end": 1994,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1994,
                    "end": 2011,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52783203125
        },
        {
            "corpus_id": "278501696",
            "title": "QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration",
            "text": "However, these approaches focus on performance of a single models and do not consider a multi-model multi-task system. \n\nMulti-Task Model Merging Model merging integrates multiple models into a single model by performing weight interpolation at the parameter level, serving as an efficient alternative (Singh & Jaggi, 2020;Yang et al., 2024b;2023). Existing model merging methods include weighted merging (Matena & Raffel, 2022;Wortsman et al., 2022;Jin et al., 2022), which assigns varying importance to different models, and subspace merging (Du et al., 2024;Yadav et al., 2023), which eliminates unimportant neurons to reduce task interference. However, these approaches use static merging strategies, limiting adaptability. Moreover, techniques such as activation matching, and permutation invariance help minimize discrepancies. Although these approaches demonstrate improved performance on considerably smaller models, their applicability for larger MoE architectures remains unexplored. Additionally, unlike previous methods that rely solely on static merging, our approach enhances both efficiency and effectiveness by integrating dynamic runtime reconfigurability with static merging.",
            "score": 0.39961172548496954,
            "section_title": "Related Work",
            "char_start_offset": 29470,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1193
                }
            ],
            "ref_mentions": [
                {
                    "start": 302,
                    "end": 323,
                    "matchedPaperCorpusId": "204512191"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "247362886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.307861328125
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "A significant subset of model merging methods focuses on combining model parameters through linear operations, often accompanied by pre-merging strategies to mitigate parameter interference. These methods consider only the model's parameters, making them highly computationally efficient compared to any LLM operation that involves data and inference. One of the pioneering methods in this space is Model Soups (Wortsman et al., 2022), which merges model weights via simple averaging. This approach first demonstrated the feasibility of weight-space merging for pre-trained LLMs and remains a reliable baseline due to its simplicity. However, despite its efficiency, the simplicity of Model Soup can lead to performance degradation caused by unresolved parameter conflicts. \n\nA more sophisticated alternative to linear interpolation is SLERP3 , which interpolates between two models along the curved path connecting them on the surface of a sphere, with the base model serving as the center. SLERP is effective and widely adopted but limited by the fact that it can only merge two models at a time. Methods such as TIES-Merging (Yadav et al., 2024) and DELLA-Merging (Deep et al., 2024) extend the basic principles of Model Soups by introducing refinements like trimming insignificant parameters, resolving sign conflicts, and pruning based on parameter magnitude. DARE (Yu et al., 2024) further enhances this process by sparsifying delta parameters before merging, reducing redundancy and improving efficiency. \n\nWhile these advanced techniques improve upon basic linear merging, they often require manual paramater tuning and are not fully automated. This lack of automation can hinder their scalability, particularly in complex multitask environments. Although their computational efficiency is a clear advantage, in scenarios where performance takes precedence over resource constraints, their inability to scale performance with available resources can be viewed as a limitation. \n\nIn practical applications, the open-source community frequently employs methods such as DARE-TIES (Yu et al., 2024), TIES (Yadav et al., 2024), and SLERP. While these methods require careful hyperparameter tuning (for instance, to balance the weighting towards each input model), the community has developed general rules of thumb to guide this process.",
            "score": 0.39895325599836196,
            "section_title": "Data-Free Merging",
            "char_start_offset": 5672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1511
                },
                {
                    "start": 1514,
                    "end": 1652
                },
                {
                    "start": 1653,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1984
                },
                {
                    "start": 1987,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 434,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1128,
                    "end": 1148,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1370,
                    "end": 1387,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2085,
                    "end": 2102,
                    "matchedPaperCorpusId": "265034087"
                },
                {
                    "start": 2109,
                    "end": 2129,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78857421875
        },
        {
            "corpus_id": "264426669",
            "title": "Merging Generated and Retrieved Knowledge for Open-Domain QA",
            "text": "Model. Here we aim to answer a question: How well can the reader model (i.e., FiD) pinpoint the correct answer when varying degrees of knowledge conflicts exist between LLM-generated texts and retrieved passages? Table 3 shows the performance of different methods when facing different rates of conflicting knowledge. We measure conflict rate as follows: \n\nN A refers to the number of retrieved passages that contain the gold answer string A and M \u2212 M A means the number of LLM-generated passages that do not contain the gold answer string. The conflicting rate indicates the percentage of conflicting pairs (i.e., rp i contains the answer while lp j does not) over all possible pairs. Table 3 suggests that when there is minimal conflicts between the two sources, both Direct Merging and COMBO can significantly improve over the Retrieved-passageonly model. However, when the conflict rate is high, only COMBO can maintain a consistent improvement over the Retrieved-passage-only baseline, suggesting its robustness. \n\nScaling with Number of Passages. We further evaluate the performance of COMBO with respect to different numbers of LLM-generated passages (M ) and retrieved passages (N ). Figure 5 shows the results for NaturalQuestions. Given a larger number of passages, we switch our reader model from FiD-large to FiD-base due to GPU memory limits. When M is smaller than N , we simply duplicate the LLM-generated passages to match the number of retrieved passages, so that every passage is included in the matching results (consistent with Section 3.3). We observe that given more passages from both sources, our framework generally achieves greater performance gain over the direct merging approach. Additional analysis in Appendix D.3 shows that more knowledge conflicts arise from the increased number of input passages, again highlighting the importance of compatibilityguided knowledge merging. Because we train our discriminators only on the top 10 passages, they may not generalize well when applied to the top 50 passages. This is possibly why our method seems less effective when provided with 50 retrieved passages as input. \n\nCase Study of Matching Results. Figure 6 shows an example from NaturalQuestions where the COMBO matching results of passages help rectify the prediction of the Direct Merging model. In the first compatible pair, both passages contain correct evidence that supports the ground-truth answer \"Don Shula\".",
            "score": 0.3988558528328763,
            "section_title": "Impact of Conflicting Contexts on the Reader",
            "char_start_offset": 24965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 6
                },
                {
                    "start": 7,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 354
                },
                {
                    "start": 357,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1907
                },
                {
                    "start": 1908,
                    "end": 2038
                },
                {
                    "start": 2039,
                    "end": 2142
                },
                {
                    "start": 2145,
                    "end": 2326
                },
                {
                    "start": 2327,
                    "end": 2446
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1251220703125
        },
        {
            "corpus_id": "278501405",
            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
            "text": "This section investigates the effectiveness of CAT Merging in mitigating knowledge conflict. Specifically, we consider the task vectors corresponding to Cars and RE-SISC45 and merge them using distinct scaling factors, i.e., 3 visualizes the knowledge conflict during merging under different merging coefficients. For Task Arithmetic, regions with minimal knowledge conflict are primarily concentrated where both \u03b1 C and \u03b1 R are close to 0. In contrast, CAT Merging effectively reduces knowledge conflict, enabling the use of larger \u03b1 values for model merging, which allows for better utilization of the task vectors.",
            "score": 0.39857759446045876,
            "section_title": "Analysis of Knowledge Conflict",
            "char_start_offset": 22516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 617
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4921875
        },
        {
            "corpus_id": "276236019",
            "title": "Mediator: Memory-efficient LLM Merging with Less Parameter Conflicts and Uncertainty Based Routing",
            "text": "Model merging aggregates Large Language Models (LLMs) finetuned on different tasks into a stronger one. However, parameter conflicts between models leads to performance degradation in averaging. While model routing addresses this issue by selecting individual models during inference, it imposes excessive storage and compute costs, and fails to leverage the common knowledge from different models. In this work, we observe that different layers exhibit varying levels of parameter conflicts. Building on this insight, we average layers with minimal parameter conflicts and use a novel task-level expert routing for layers with significant conflicts. To further reduce storage costs, inspired by task arithmetic sparsity, we decouple multiple fine-tuned experts into a dense expert and several sparse experts. Considering the out-of-distribution samples, we select and merge appropriate experts based on the task uncertainty of the input data. We conduct extensive experiments on both LLaMA and Qwen with varying parameter scales, and evaluate on real-world reasoning tasks. Results demonstrate that our method consistently achieves significant performance improvements while requiring less system cost compared to existing methods.",
            "score": 0.39857759446045876,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7998046875
        },
        {
            "corpus_id": "276449826",
            "title": "Transferring Textual Preferences to Vision-Language Understanding through Model Merging",
            "text": "Weighted Averaging Wortsman et al. (2022) showed that combining the weights of multiple models fine-tuned with varying hyperparameter settings often leads to improved accuracy and robustness. In this work, we employ a weighted averaging strategy as a straightforward method to merge a large vision-language model with a text-based reward model. The weighted averaging strategy is formally defined as: \n\nwhere \u03bb is a hyperparameter that determines the weight distribution between the two models. We explore \u03bb values in the range: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]. \n\nTask Arithmetic Ilharco et al. ( 2023) demonstrated that the task vector, obtained by subtracting the weights of a pre-trained model from those of the same model after fine-tuning for a specific task, defines the task direction. Utilizing this task vector can improve task performance. We also apply the task arithmetic approach to develop a visionlanguage reward model. The task arithmetic strategy is formally defined as: \n\nwhere \u03c4 LVLM denotes the task vector derived from instruction tuning, and \u03c4 RM refers to the task vector obtained from reward modeling. The hyperparameter \u03bb controls the relative contribution of task vectors. We explore \u03bb values in the range: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]. Yadav et al. (2024) consider the interference between parameters from different models during the model merging process. Their approach consists of three main steps. First, they prune task vector values based on magnitude, retaining only a proportion d of the task vector. Second, they resolve sign conflicts by calculating the total magnitude of parameter values in positive and negative directions and selecting the direction with the larger total magnitude. Only values that match the chosen sign are retained. Finally, they compute the mean of the retained values to determine the final parameter value. The TIES method can be simply expressed as:",
            "score": 0.39834957113617586,
            "section_title": "A Merging Details",
            "char_start_offset": 13249,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 400
                },
                {
                    "start": 403,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 585
                },
                {
                    "start": 588,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1827
                },
                {
                    "start": 1828,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 1965
                }
            ],
            "ref_mentions": [
                {
                    "start": 19,
                    "end": 41,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 1314,
                    "end": 1333,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54833984375
        },
        {
            "corpus_id": "278394735",
            "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging",
            "text": "In this section, we introduce model merging for transferring the reasoning abilities of textual LMs to VLMs. A typical VLM consists of three key components: a vision tower, a language Model, and a projector that bridges these two parts. The vision tower processes images, enabling the model to \"see\" visual content, while the language model serves as the reasoning engine, processing knowledge and generating responses. Therefore, we target the language model (\u03b8 vlm ) for merging while keeping the vision tower and projector unchanged. \n\nModel merging has emerged as a promising \"free lunch\" technique, enabling performance improvements by reusing existing models through simple arithmetic operations on their parameters, without requiring additional training. For simplicity, we adopt linear merging (Ilharco et al., 2023), a widely used and robust merging strategy, in our main experiments. We also experiment with TIES merging (Yadav et al., 2023) in some cases to compare both methods in Appendix C. \n\nThe core idea of model merging relies on task vectors, the modifications made during fine-tuning, which is usually the information necessary to do well on a given task. Given a base model \u03b8 base and a fine-tuned model \u03b8 ft , the corresponding task vector \u03c4 task is defined as: \n\nTask vectors provide an interpretable way to understand how fine-tuning adapts a model to a particular task. In the  (Yu et al., 2024a) Math Domain LLaMA-series MAmmoTH-1 (Yue et al., 2024a) Math Domain LLaMA-series MAmmoTH-2 (Yue et al., 2024b) General Domain LLaMA/Mistral-series Magpie-v0.3 (Xu et al., 2024) Math Domain LLaMA-series Deepseek-R1-Distill (DeepSeek-AI et al., 2025)  context of VLMs, we define the adaptation of the language model component as: \n\nwhere \u03c4 vlm captures the changes introduced when adapting the base LLM into the VLM. Similarly, for a reasoningspecialized LLM \u03b8 reason , we define its corresponding task vector: \n\nTo enhance the reasoning ability of the VLM, we merge its language model with a strong reasoning-specialized LLM by linear merging:",
            "score": 0.3975312887683164,
            "section_title": "Model Merging Across Modalities",
            "char_start_offset": 4104,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1748
                },
                {
                    "start": 1751,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1929
                },
                {
                    "start": 1932,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 802,
                    "end": 824,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 931,
                    "end": 951,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1457,
                    "end": 1476,
                    "matchedPaperCorpusId": "261696697"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51611328125
        },
        {
            "corpus_id": "275212824",
            "title": "Superposition in Transformers: A Novel Way of Building Mixture of Experts",
            "text": "To empirically demonstrate the advantages of our Superposition in Transformer technique, we conducted experiments comparing Superposition Merging with linear interpolation and task arithmetic techniques. The perplexity of the autoencoder-merged model (M-PPL=47.01) was markedly lower than that of the linearly interpolated model (I-PPL=60.29) and the task arithmetic model (I-PPL=61.30) during the last epoch of training, suggesting a higher confidence in predicting the next token. The merged model initially showed higher perplexity than both task arithmetic and linear interpolation models but then achieved lower values as training progresses (figure 1). Similarly, the next-token prediction accuracy was also improved with Superposition Merging (M-Acc=0.3270) compared to linear interpolation (I-Acc=0.3039) and task arithmetic (I-Acc=0.2957). Additionnaly, we examined the Jensen-Shannon Divergence JSD values over different epochs during training. The JSD drops sharply from M-JSD=82.9 to M-JSD=45.5 in the first two epochs. This drastic reduction indicates that the merged model's output distribution quickly diverges from the initial average of the base and fine-tuned models. After this initial drop, the JSD values fluctuated between approximately M-JSD=36.5 and M-JSD=44.2, suggesting that the merged model settles into a relatively stable output distribution that is distinct from the simple average. \n\nThese quantitative results strongly suggest that Superposition Merging, facilitated by the learning capabilities of autoencoders, offers a more robust and effective method for combining the strengths of independently trained models. Unlike traditional methods that can lead to a homogenization of expertise, our approach shows promising potential for achieving a genuine superposition, where the merged model can effectively leverage the unique skills of each expert model within their respective domains. This opens up exciting possibilities for creating more versatile and powerful AI systems by intelligently combining specialized knowledge.",
            "score": 0.3974983592406276,
            "section_title": "Overall Performance",
            "char_start_offset": 11324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "277313159",
            "title": "FREE-Merging: Fourier Transform for Efficient Model Merging",
            "text": "In the current era of deep learning, the pretrain-finetune paradigm has emerged as the standard methodology [9,37,40]. However, in numerous edge applications, users often lack powerful computing resources and large datasets, preferring ready-to-use models for specific scenarios. The recent growth of open-source platforms like Hugging-Face [43] has made this more feasible, allowing easy access to models for various tasks. Yet, practical demands are often not fully covered by a single existing model and are typically a union of several models [23,52]. However, running separate models for each subtask increases storage and inference costs, especially with large models. Model merging attempts to combine existing models to build a model capable of addressing all target tasks [21,33,48]. This method not only mitigates the training costs and data privacy issues associated with multi-task learning (MTL) [20,55] but also reduces deployment costs, garnering widespread attention. \n\nWhile model merging aims to reduce training costs, it often suffers from performance loss due to task interference [20]. To mitigate this issue, existing methods develop along three directions. The first direction computes merging coefficients based on weights or data [23,33]. The second uses the parameter differences before and after finetuning (i.e. task vectors) for merging [21,48,52,57]. The third method introduces independent knowledge for each task [20,29]. While the third way balances performance and costs, it still faces two critical limitations. First, it neglects backbone optimization during merging, resulting in subpar performance. Second, it requires high computation and storage, making it impractical for edge deployment. \n\nWe argue that an effective model merging method requires addressing both of the above issues. First, since the backbone constitutes the majority of parameters and thus determines performance, an efficient backbone merging method is essential to reduce performance loss. Then, given the no free lunch theorem, cost-free merging cannot avoid task interference. Therefore, introducing task-specific experts is necessary. However, they must remain lightweight to avoid storage and inference overhead. Thus, we propose FREE-Merging, a two-stage method that ensures an efficient backbone and lightweight experts as shown in Fig 1 .",
            "score": 0.39646613882235304,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 983
                },
                {
                    "start": 986,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1729
                },
                {
                    "start": 1732,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 108,
                    "end": 111,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 111,
                    "end": 114,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 551,
                    "end": 554,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 785,
                    "end": 788,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 788,
                    "end": 791,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 913,
                    "end": 916,
                    "matchedPaperCorpusId": "11311635"
                },
                {
                    "start": 1259,
                    "end": 1262,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1370,
                    "end": 1373,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1373,
                    "end": 1376,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1376,
                    "end": 1379,
                    "matchedPaperCorpusId": "270559703"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.626953125
        },
        {
            "corpus_id": "278394735",
            "title": "Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging",
            "text": "VLMs Large Vision-Language Models (VLMs) consist of three main components: a visual encoder for processing images, such as CLIP (Radford et al., 2021) or SigLip (Zhai et al., 2023); a language model (e.g., a LLaMA model (Dubey et al., 2024) or a Mistral model (Jiang et al., 2023)) for processing textual inputs and image features to generate responses; and a projector, typically implemented as multilayer perceptron (MLP), to bridge the gap between the visual and language components. This module maps features from the visual space to the language space, facilitating interaction between the two modalities. \n\nModel Merging Model merging offers a \"free lunch\" by repurposing fine-tuned models for downstream tasks (Wortsman et al., 2022;Ilharco et al., 2023;Zhang et al., 2023). It  N . Comparing before and after merging when applied masking out, we observe a larger drop in accuracy in math-targeted VQA tasks across all layers (highlighted in blue), suggesting that the contribution of all most all layers to math reasoning has increased. vations (Matena & Raffel, 2022;Jin et al., 2023). Some approaches initially sparsify the models to reduce conflicts across different functions (Yadav et al., 2023;Yu et al., 2024b). Recently, Layer Swapping (Bandarkar et al., 2024) was introduced, retains specific layers while merging others to enhance transfer learning. Despite their differences, simple averaging is often preferred for its simplicity and robustness. As model scales grow, performance gaps among merging techniques shrink (Yadav et al., 2024), making linear merging a natural choice. \n\nMerging for VLMs Although much prior research has focused on merging vision models for tasks such as image recognition and scene understanding (Ilharco et al., 2023;Yang et al., 2023), the composition for VLMs remain insufficiently explored, particularly in terms of transferring specialized skills across modalities.",
            "score": 0.396281148313889,
            "section_title": "Related Work",
            "char_start_offset": 21303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1367
                },
                {
                    "start": 1368,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1598
                },
                {
                    "start": 1601,
                    "end": 1918
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 150,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 161,
                    "end": 180,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 717,
                    "end": 740,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1053,
                    "end": 1076,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1076,
                    "end": 1093,
                    "matchedPaperCorpusId": "254877510"
                },
                {
                    "start": 1188,
                    "end": 1208,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 1208,
                    "end": 1225,
                    "matchedPaperCorpusId": "265034087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "273482450",
            "title": "Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace",
            "text": "Previous Multi-Task Merging Techniques. Denote the f \u03b8 as the CLIP-like pre-trained model f with weights \u03b8 and a set of datasets D = {D i } n i=1 for n downstream tasks. We can fine-tune the pre-trained model parameterized by \u03b8 pre to acquire n task-specific models parameterized by {\u03b8 i } n i=1 . Then, for each task i, the task vector can defined as the difference between \u03b8 pre and \u03b8 i , i.e., \u03c4 i = \u03b8 i \u2212 \u03b8 pre . Existing merging methods can be formulated as the optimization to acquire \u03b8 merged = \u03b8 pre + n i=1 {\u03bb i \u03c4 i \u2032 }, where \u2200\u03bb \u2208 [0, 1] refers to the merging coefficient and \u03d5(\u03c4 i ) = \u03c4 i \u2032 represents the revision for each task vector. The main difference among these methods exists in ways to acquire the \u03c4 i \u2032 and \u03bb i . For example, both Weight Average (Wortsman et al., 2022) and Task Arithmetic (Ilharco et al., 2022a) adopt the origin task vector \u03c4 i , with the \u03bb = 1 n adapted to the number of tasks and a fixed \u03bb = 0.3 respectively. Ties-Merging (Yadav et al., 2024) and Concrete (Tang et al., 2023) address the interference among tasks and replace the original task vector with \u03c4 i \u2032 . Moreover, RegMean (Jin et al., 2022) and AdaMerging (Yang et al., 2023) respectively formulate the optimization of \u03bb i according to the model's activations and the entropy on an unlabeled held-out dataset. However, these works share the same and single optimization objective to maximize the performance of the merged model on the clean test datasets as Eq.1 from the evaluation perspective, where A is a model merging algorithm associated with \u03d5(\u2022) and \u03bb. It is uncertain if current merging methods remain effective considering safety issues like backdoors, which introduces potential but important concerns for deploying merging algorithms to more scenarios. \n\nPerformance A(\u03b8 pre , \u03d5(\u03c4 i ), \u03bb i ), D test i . \n\n(1) \n\nMerging Considering the Existence of Backdoor.",
            "score": 0.3956826312921807,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 1486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1766
                },
                {
                    "start": 1769,
                    "end": 1817
                },
                {
                    "start": 1820,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 767,
                    "end": 790,
                    "matchedPaperCorpusId": "247362886"
                },
                {
                    "start": 965,
                    "end": 985,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376953125
        },
        {
            "corpus_id": "276575632",
            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
            "text": "We then demonstrate the robustness of merging methods to different training data sizes for both tasks. Fig. 4 shows the WER (top) and BLEU (bottom) scores for ASR and ST at different data scales, respectively. As the data scale increases, the performance of multi-lingual training does not always improve. This may be because the pretrained model already performs well, and the significant language interference and conflict in multi-lingual training hinder the effective improvement of multilanguage performance. Furthermore, the performance loss of model merging increases with data scale, compared to finetuned models. It can be explained by the fact that larger training data tends to increase the divergence in the optimisation trajectories of different finetuned models, resulting in the breakdown of linear mode connectivity, which leads to a greater performance loss. Moreover, LoRS-Merging still achieves obvious and stable improvement compared to TA.",
            "score": 0.3956501454817188,
            "section_title": "Effect of Language Data Scale",
            "char_start_offset": 22481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 960
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52880859375
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "To overcome catastrophic forgetting and maintain new learning ability, many works have proposed to merge models for old tasks and update models for new tasks [138,161,209]. Tangent Model Composition [117] proposes fine-tuning each task independently in the tangent space of the pre-trained model and then linearly fine-tuning these models to perform CL. This approach does not depend on the specific settings of CL and can be easily applied to task, class, and domain-incremental learning scenarios. In addition, ITA [143] emphasizes the necessity for the fine-tuned model to be in the same basin as the pre-trained model to ensure the composability of nonlinear models. It introduces a regularization term similar to EWC [94] in traditional CL to constrain the distance between the fine-tuned weights and the pre-trained weights when training the independent model. WARP [152] suggests linearly interpolating the pre-trained LLM's weights with its aligned weights via RLHF on a preference dataset, thus mitigating the forgetting of knowledge from the pre-trained LLM. BAM [7] continuously adapts LLMs to new languages by merging models while preserving general capabilities. Model Tailor [243] explores the problem of catastrophic forgetting during fine-tuning of MLLMs, and proposes to merge only the most important subset of parameters in the fine-tuned MLLM model into the pre-trained MLLM model, so as to retain the generalization ability of the pre-trained model as much as possible, while compensating the selected weights to reduce the performance of the finetuning task. MagMax [125] merges pruned task vectors to further alleviate parameter sign conflicts and old knowledge forgetting. Equifinality, PAINT [70] and LM-Cocktail [213] interpolate the weights of the finetuned model and the zero-shot model to improve accuracy on downstream tasks without degrading accuracy on supported/general tasks. \n\nIn contrast to merging full models, some research focuses on merging parameter-efficient modules. Chitale et al. [22] propose a CL method based on task arithmetic [71].",
            "score": 0.39533612793459016,
            "section_title": "Model Merging to Mitigate Catastrophic Forgetting",
            "char_start_offset": 54317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "256231189"
                },
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 199,
                    "end": 204,
                    "matchedPaperCorpusId": "259937354"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1587,
                    "end": 1592,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "251493208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.43408203125
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "One of the most straightforward approaches to model merging is to directly weighted average the parameters of multiple models [164,190], i.e., \u0398 (merge) = T t=1 1 T \u0398 (t) . However, the performance of simple weight averaging is generally unsatisfactory. Recently, Task Arithmetic [71] introduced the concept of \"task vector\" (in Figure 4(a)), which represents the model parameter \u0398 (t) fine-tuned on task t subtract the pre-trained  model parameter \u0398 (0) , i.e., \u03c4 t = \u0398 (t) \u2212 \u0398 (0) . In other words, task vectors are thought to steer the behavior of a neural network meaningfully. For example, multitask learning (MTL) can be accomplished by adding task vectors, forgetting can be achieved by subtracting task vectors, and task analogies can be performed using analogous task vectors. Specifically, when we want the pretrained model to perform MTL, we can add multiple task vectors {\u03c4 1 , . . . , \u03c4 T } to the pretrained model, i.e., \u0398 (merge) = \u0398 (0) + \u03bb \u2022 T t=1 \u03c4 t in Figure 4(b), where \u03bb is a hyperparameter. Conversely, when we want the pretrained model to forget a function t, we can subtract the corresponding task vector from pretrained model as Figure 4(c), i.e., \u0398 (merge) = \u0398 (0) \u2212 \u03c4 t . As shown in Figure 4(d), we can also implement task analogies by task vector analogies, thus enabling zeroshot learning of new tasks. Similarly, PEMs [234] combines Adapters with different capabilities by extending task arithmetic [71] to parameter-efficient fine-tuning settings. However, the performance of basic merging methods is not satisfactory most of the time, especially when the tasks interfere with each other.",
            "score": 0.3948362271991611,
            "section_title": "Basic Merging Methods",
            "char_start_offset": 17213,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 895
                },
                {
                    "start": 896,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1621
                }
            ],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 131,
                    "matchedPaperCorpusId": "11290566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.422607421875
        },
        {
            "corpus_id": "253244496",
            "title": "Data-Efficient Cross-Lingual Transfer with Language-Specific Subnetworks",
            "text": "Large multilingual language models typically share their parameters across all languages, which enables cross-lingual task transfer, but learning can also be hindered when training updates from different languages are in conflict. In this paper, we propose novel methods for using language-specific subnetworks, which control cross-lingual parameter sharing, to reduce conflicts and increase positive transfer during fine-tuning. We introduce dynamic subnetworks, which are jointly updated with the model, and we combine our methods with meta-learning, an established, but complementary, technique for improving cross-lingual transfer. Finally, we provide extensive analyses of how each of our methods affects the models.",
            "score": 0.39432739359488306,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74169921875
        },
        {
            "corpus_id": "276408756",
            "title": "LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging",
            "text": "Second, empirical evidence from Yadav et al. (2024) reveals that conflicting task vectors interactions could appear even when employing sparse estimation techniques. On the other hand, the sparsification process risks inadvertently eliminating essential task-specific features, thereby compromising the efficacy of the resultant merged model. These inherent constraints of sparse approximation methodologies underscore the necessity for developing alternative frameworks to estimate higherfidelity low-rank task vector representations. \n\nTo this end, we first empirically validate that task vectors exhibit a small number of dominant singular values, with the remaining singular values being significantly smaller in magnitude, as shown in Figure 1. Additionally, the dimension of the intersection of the images of two matrices is bounded by the minimum of their ranks. Therefore, we propose LORE-MERGING, a unified framework for model merging based on Low-Rank Estimation of task vectors, which eliminates the need for access to the base model. Specifically, given a set of FT models, we formulate the merging problem as an optimization problem whose goal is to simultaneously identify an approximate base model integrated with a set of low-rank task vectors that collectively approximate the behavior of the FT models. By leveraging low-rank estimations, task vectors become inherently less susceptible to interference, effectively addressing a fundamental challenge in model merging. We conduct extensive experiments on optimization modeling problems and math word problems to confirm the effectiveness of our method.",
            "score": 0.3938672308079316,
            "section_title": "Introduction",
            "char_start_offset": 2085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1620
                }
            ],
            "ref_mentions": [
                {
                    "start": 32,
                    "end": 51,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "273404154",
            "title": "LoRA Soups: Merging LoRAs for Practical Skill Composition Tasks",
            "text": "Large Language Models (LLMs) have demonstrated impressive capabilities in conversational tasks and general-purpose applications, such as writing emails or answering common questions. However, these general purpose LLMs may have restricted performance on tasks where specific skills and knowledge is required. We primarily focus on skill composition tasks, that necessitate the integration of multiple skills. \n\nMany industrial applications fit in this framework. Consider a company that manufactures ovens and is trying to design a chatbot to answer customer queries about its working and specifics. Directly using a frontier LLM (like gpt-4o) would fail since it lacks knowledge about the company's product. The ideal solution here would be to design an instruction dataset consisting of question-answer pairs about this product and fine-tuning an LLM on it. However, such a data collection and annotation procedure is expensive. Another possible solution is to fine-tune an LLM on a collection of product manuals and then impart it chat abilities by further fine-tuning on an instruction-tuning dataset like  Alpaca [55]. We refer to this method as DATA-MIX. Besides this approach being sequential, it suffers from catastrophic forgetting [28]. Whenever the company creates a new product, they need to redo fine-tuning on this data mixture or create a new question-answer dataset for the former method. \n\nIn this paper, we study model merging as an alternative approach. Given a model that is fine-tuned on the manuals and one that possesses question-answering capabilities, we optimally combine their weights to obtain a model that can answer product-specific questions. This approach is more efficient since we merge skill-specific fine-tuned models without any additional data collection or training from scratch. Among the multiple techniques to perform model merging, our framework specifically builds on LoRA [23] (a review of the LoRA method is in Appendix A), a fine-tuning technique that consists of adding a low-rank update to a few layers in the model. In this context, model merging consists of combining the LoRA weights from different models. \n\nPrior works [19,49,70] have investigated LoRA merging in computer vision where each skill is a visual concept or style and the objective is image generation.",
            "score": 0.3938117204022617,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1246
                },
                {
                    "start": 1247,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 2065
                },
                {
                    "start": 2066,
                    "end": 2158
                },
                {
                    "start": 2161,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "4704285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57373046875
        },
        {
            "corpus_id": "271957310",
            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
            "text": "Model merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our algorithm works in two steps: i) Localization: identify tiny ($1\\%$ of the total parameters) localized regions in the finetuned models containing essential skills for the downstream tasks, and ii) Stitching: reintegrate only these essential regions back into the pretrained model for task synergy. We demonstrate that our approach effectively locates sparse regions responsible for finetuned performance, and the localized regions could be treated as compact and interpretable representations of the finetuned models (tasks). Empirically, we evaluate our method on various vision and language benchmarks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond strong empirical performance, our algorithm also facilitates model compression and preserves pretrained knowledge, enabling flexible and continual skill composition from multiple finetuned models with minimal storage and computational overhead. Our code is available at https://github.com/uiuctml/Localize-and-Stitch.",
            "score": 0.39286560501247025,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "275906690",
            "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
            "text": "This section explores various techniques for integrating Large Language Models (LLMs) with knowledge bases, highlighting the potential solutions these integrations offer. By examining the methods used to combine LLMs with structured knowledge, we discuss how this synergy enhances the models' ability to provide more accurate, contextually relevant, and data-driven outputs. The integration strategies also open doors to improved reasoning capabilities, allowing LLMs to access and leverage external knowledge to address domain-specific queries and tasks more effective.",
            "score": 0.39257441645157265,
            "section_title": "Integrating LLMs with knowledge bases",
            "char_start_offset": 38064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 570
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.278076171875
        },
        {
            "corpus_id": "271903285",
            "title": "SMILE: Zero-Shot Sparse Mixture of Low-Rank Experts Construction From Pre-Trained Foundation Models",
            "text": "A primary concern is the potential interference between parameters of different models, which leads to suboptimal performance. Additionally, the lack of interpretability in the fusion process remains a significant hurdle, as current insights are largely confined to heuristic observations or simplified assumptions, such as linear mode connectivity, parameter signs or importance [Ainsworth et al., 2022, Stoica et al., 2023, Yadav et al., 2023, Yu et al., 2024]. Understanding how parameters are merged is crucial for building trust in the merged models and for further improving fusion techniques. These challenges are particularly pronounced in complex, high-dimensional, non-linear model architectures, where the interactions between parameters can be extremely intricate and non-intuitive. Instead of relying on heuristic methods or simplified assumptions, we propose a novel subspace perspective on understanding and addressing the parameter interference problem in this study. We first examine the fine-tuning process in linear layers through the lens of subspace analysis using matrix decomposition in Section 2. This allows us to decompose the prediction of a fine-tuned model into distinct components, encompassing the pre-trained knowledge and task-specific adaptation. This approach provides insights into how models adapt to downstream tasks while preserving pre-trained knowledge. Drawing from experimental observations, we build a more comprehensive understanding of fine-tuning, we further formulate parameter interference as an optimization problem in Section 3, providing a more rigorous and measurable perspective. \n\nBased on our insights, we introduce an innovative approach called zero-shot Sparse MIxture of Low-rank Experts (SMILE) construction, enhancing existing source models into a more versatile MoE model. The zero-shot aspect of our approach is particularly noteworthy, as it facilitates the immediate deployment of fused models in new environments or tasks, drastically minimizing the time and resources typically required for model adaptation. \n\nThe effectiveness of our proposed method is rooted in two key observations derived from our subspace analysis. Firstly, we found that the fine-tuning largely preserves the most important pre-trained weights and primarily utilizes less significant or previously unused dimensions of the parameter space to adapt to new tasks. This preservation ensures that the critical pre-training knowledge encoded in the original models is not lost during fine-tuning and implies that the parameter subspace required to accommodate new knowledge may vary from task to task.",
            "score": 0.39171689960160716,
            "section_title": "Introduction",
            "char_start_offset": 1879,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1633
                },
                {
                    "start": 1636,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2188
                },
                {
                    "start": 2189,
                    "end": 2402
                },
                {
                    "start": 2403,
                    "end": 2637
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58642578125
        },
        {
            "corpus_id": "275921352",
            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
            "text": "The entire merging process does not rely on any additional training process. \n\nMoreover, the techniques introduced in TATR selectively limit the merging process to a subset of model parameters, allowing it to function as a plug-and-play module that seamlessly integrates with a wide range of TA-based approaches, such as: \n\n\u2022 Ties-Merging & TATR: Ties-Merging (Yadav et al., 2023) partially reduces knowledge conflicts by pruning low-magnitude parameters and aligning the signs of task vectors. However, this approach overlooks conflicts that may arise from high-magnitude parameters. This bias can lead to knowledge conflicts, where some tasks dominate the model's behavior. The combination of TATR with Ties-Merging refines the process, as shown in the following formula: \n\nwhere \u03a6(.) indicates the TrIm, Elect Sign, and Merge operation of Ties-Merging. \n\n\u2022 AdaMerging & TATR: AdaMerging (Yang et al., 2024b) adaptively learns merging coefficients but does not inherently resolve knowledge conflicts between task vectors. This can lead to interference during coefficient learning, especially when tasks require opposing parameter adaptations. TATR addresses this by pre-filtering task vectors to retain only Published as a conference paper at ICLR 2025 those components within the trust region, ensuring that AdaMerging operates in a conflictreduced parameter space: \n\nwhere \u03bb 1 , . . . , \u03bb K represent the learnable coefficients for AdaMerging. \u2022 Surgery & TATR: Similarly, Surgery (Yang et al., 2024a) introduces additional modules to align task-specific features during merging. TATR complements Surgery by pre-selecting components of task vectors that reside in the trust region. The integrated approach is formalized as: \n\nwhere \u03b8 surgery denotes the additional parameters introduce by the Surgery module.",
            "score": 0.39161031555185843,
            "section_title": "TASK ARITHMETIC IN THE TRUST REGION",
            "char_start_offset": 17481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 79,
                    "end": 321
                },
                {
                    "start": 324,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1685
                },
                {
                    "start": 1686,
                    "end": 1727
                },
                {
                    "start": 1730,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 360,
                    "end": 380,
                    "matchedPaperCorpusId": "259064039"
                },
                {
                    "start": 890,
                    "end": 910,
                    "matchedPaperCorpusId": "263620126"
                },
                {
                    "start": 1485,
                    "end": 1505,
                    "matchedPaperCorpusId": "267412030"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8671875
        },
        {
            "corpus_id": "267750988",
            "title": "Discerning and Resolving Knowledge Conflicts through Adaptive Decoding with Contextual Information-Entropy Constraint",
            "text": "Characterized by the massive knowledge internalized into the parameters (Petroni et al., 2019;Geva et al., 2021b;Roberts et al., 2020), Large language models (LLMs) have pioneered numerous breakthroughs across various domains (Vaswani et al., 2017;Devlin et al., 2018;Brown et al., 2020;Chung et al., 2022;Touvron et al., 2023). Meanwhile, LLMs struggle with less popular factual knowledge (Mallen et al., 2023), are fundamentally incapable of adapting over time (Lazaridou et al., 2021;Kasai et al., 2022) and prone to hallucinations (Shuster et al., 2021). These challenges necessitate the incorporation of non-parametric knowledge sources, through retrieval (Shi et al., 2023b) or application of tools (Schick et al., 2023). How- ever, it has given rise to a sharp dilemma: knowledge conflicts, defined by Longpre et al. (2021), where the non-parametric contextual knowledge conflicts with internal parametric knowledge. Prior works (Longpre et al., 2021;Chen et al., 2022;Li et al., 2023a;Zhou et al., 2023;Wang et al., 2023c) have flagged that when confronting conflicts, larger models have a greater tendency to ignore the given context when it contradicts with model's parametric knowledge. As shown in the Figure 1, due to the model's bias towards its parametric knowledge, it fails to ground its answer in the conflicting context. \n\nEarly attempts on knowledge conflict-resolving methods resort to fine-tuning a small-scale model like T5 (Raffel et al., 2020) by data augmentation, such as KAFT (Li et al., 2023a) and Dis-entQA (Neeman et al., 2023). Those fine-tuning methods bear the risk of undermining the intrinsic linguistic capabilities of the models (Dong et al., 2023). Another line of works employ various decoding strategies during inference.",
            "score": 0.3908534054751986,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1687
                },
                {
                    "start": 1688,
                    "end": 1762
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 94,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 94,
                    "end": 113,
                    "matchedPaperCorpusId": "229923720"
                },
                {
                    "start": 113,
                    "end": 134,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 226,
                    "end": 248,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 268,
                    "end": 287,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 463,
                    "end": 487,
                    "matchedPaperCorpusId": "239886013"
                },
                {
                    "start": 535,
                    "end": 557,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 809,
                    "end": 830,
                    "matchedPaperCorpusId": "237491581"
                },
                {
                    "start": 936,
                    "end": 958,
                    "matchedPaperCorpusId": "237491581"
                },
                {
                    "start": 958,
                    "end": 976,
                    "matchedPaperCorpusId": "253107178"
                },
                {
                    "start": 976,
                    "end": 993,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 1447,
                    "end": 1468,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 1504,
                    "end": 1522,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 1537,
                    "end": 1558,
                    "matchedPaperCorpusId": "253447228"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07965087890625
        },
        {
            "corpus_id": "273798488",
            "title": "MoD: A Distribution-Based Approach for Merging Large Language Models",
            "text": "While MoD demonstrates superior performance compared to existing methods, we acknowledge some limitations in our current study. First, our experimental validation is primarily confined to the mathematical domain, which, while comprehensive, may not fully represent the method's generalizability across other specialized fields. Second, our current approach employs a simplified strategy for determining mixture weights, which may not capture optimal combinations for all scenarios. \n\nThese limitations suggest several promising directions for future research. First, extending the evaluation of MoD to diverse domains beyond mathematics would provide valuable insights into the method's robustness and general applicability. Second, developing more sophisticated approaches for determining optimal mixture weights could potentially enhance the method's performance further. Additionally, investigating the theoretical foundations of distribution-based merging approaches could lead to more principled strategies for model combination and integration. These directions would contribute to a deeper understanding of model merging techniques and their applications in developing more capable language models.",
            "score": 0.3908534054751986,
            "section_title": "Limitations and Future Work",
            "char_start_offset": 15911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1205
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7333984375
        },
        {
            "corpus_id": "271903078",
            "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
            "text": "The issue of catastrophic forgetting [16] is another concern when finetuning a language model. Additionally, fully fine-tuning LLMs [17] becomes inefficient as multiple instances of the model for different tasks should be trained and deployed. \n\nTo address the above-mentioned issues, recent research shows the efficiency of merging models. Merging models enhances their performance compared to task-specific models [9], [10]. Merging models combines the parameters of individual task-specific models into a single model in order to leverage the knowledge of other models [9]. Note that merging models is intrinsically different from Multi-Task Learning (MTL) [18], [19]. In MTL, one model is jointly trained on two or more tasks; while in merging models, there exist multiple models, each of them trained separately on distinct tasks. Then, all of these models are merged together without additional training. \n\nSimilar techniques are developed for merging adapters. \n\nThere are studies that merge multiple models [20], [21], [22] or adapters in natural language processing (NLP) domain [23], [24]. Adapters were also used for Multi-Task Learning [25]. Others adopted the idea of mixture-of-experts to inject a set of expert adapter layers in the transformer-based models in order to train multi-task models [26]. However, the research on merging adapters is scarce, and there is a research gap in investigating the merging ability of adapters for code-related tasks. This is specially important with the advent of Code LLMs. Though these models have shown promising results for many SE tasks, their computational cost is not negligible for companies and researchers. Thus, it is beneficial to reuse the trained models/adapters for new tasks by merging them, without additional training. There is no research that investigates if the current approaches affect the code-related tasks in the same way as in NLP and whether they would improve the performance of each task in the merged model. \n\nThe main goal of this study is to explore the performance of merged task-specific adapters and continual merging, in the context of Automated Program Repair (APR) task. We plan to investigate the idea of merging adapters for different SE tasks, using our framework, MergeRepair.",
            "score": 0.3904721630615464,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 2112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 243
                },
                {
                    "start": 246,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 967
                },
                {
                    "start": 970,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1990
                },
                {
                    "start": 1993,
                    "end": 2161
                },
                {
                    "start": 2162,
                    "end": 2271
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "258556987"
                },
                {
                    "start": 132,
                    "end": 136,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 421,
                    "end": 425,
                    "matchedPaperCorpusId": "258480011"
                },
                {
                    "start": 660,
                    "end": 664,
                    "matchedPaperCorpusId": "90063862"
                },
                {
                    "start": 666,
                    "end": 670,
                    "matchedPaperCorpusId": "11754890"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "252199400"
                },
                {
                    "start": 1088,
                    "end": 1092,
                    "matchedPaperCorpusId": "256846453"
                },
                {
                    "start": 1148,
                    "end": 1152,
                    "matchedPaperCorpusId": "218470208"
                },
                {
                    "start": 1309,
                    "end": 1313,
                    "matchedPaperCorpusId": "253153886"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59423828125
        },
        {
            "corpus_id": "276937513",
            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
            "text": "By doing so, DTM effectively distributes and mitigates dataset-specific biases, maintaining the model's instruction-following capacity while reducing the detrimental effects of such biases on knowledge and reasoning benchmarks. This simple yet efficient approach demonstrates the promise of task-level separation and targeted training strategies to enhance model performance in multi-task settings. \n\nIn addition to operations on weights and activations, future research could benefit from a deeper exploration of network architecture design as a pivotal factor in enhancing model performance during merging and continual learning. As highlighted in recent work, different neural network structures exhibit significantly varying degrees of resilience to catastrophic forgetting, a core challenge in continual learning [Lu et al., 2024a]. Tailoring merging methods to align with these structural features could lead to more effective adaptation and knowledge retention across tasks. By leveraging insights into the unique structural properties of models, it becomes possible to optimize merging strategies not just for parameter efficiency, but also for preserving task-specific knowledge in multi-task settings. \n\nThis direction underscores the importance of designing architecture-aware methods in model merging, where decisions about how and where to merge layers or weights could be dynamically adjusted based on architectural traits. Such an approach could unlock new possibilities for mitigating catastrophic forgetting while maintaining model scalability and generalization. \n\nIn addition to the integration of model compression and model merging as a significant future trend, several other methodologies hold substantial development potential. These include the application of dual-space constraints-encompassing both weight space and activation space-to enhance model performance and generalization. Furthermore, decomposing model gradients into their directional and magnitude components can provide deeper insights into optimization processes. Lastly, employing clustering techniques to select appropriate experts within models can lead to more efficient and specialized architectures. Collectively, these approaches offer promising avenues for advancing model efficiency and effectiveness.",
            "score": 0.38999671749770703,
            "section_title": "Challenges and Future Perspectives of Model Merging",
            "char_start_offset": 31801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1211
                },
                {
                    "start": 1214,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1580
                },
                {
                    "start": 1583,
                    "end": 1751
                },
                {
                    "start": 1752,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2196
                },
                {
                    "start": 2197,
                    "end": 2301
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "273323680",
            "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation",
            "text": "Recognizing the limitations of manual parameter tuning, researchers have explored automated model merging techniques. These techniques leverage representative data during inference to better balance the trade-offs between the input models being merged. By utilizing the additional insights gained during inference, they facilitate the automatic selection of hyperparameters that would otherwise need to be approximated in data-free methods. Given these methods are fully automated, they can opti-mize more granular hyperparameters than the datafree methods, such as per-layer or even per-feature weighting parameters. \n\nAdaMerging (Yang et al., 2023) is an advanced technique that automates model merging by focusing on minimizing the entropy associated with both the input models and their respective datasets. The process begins by calculating task vectors, which represent the difference in weights between the input models and the base model. Coefficients are then assigned and optimized for each task vector, determining the impact each input model has on the final merged model. AdaMerging adjusts these coefficients in an unsupervised manner, minimizing entropy on multi-task, unlabeled test data, which serves as a surrogate objective for minimizing loss on the full dataset. This optimization not only balances task vectors but also ensures the merged model's predictions are more deterministic and robust. While AdaMerging has shown success in image classification tasks using transformer models, our work focuses on adapting this concept to merge LLMs for a broader range of tasks, including multilingual capabilities and domain-specific knowledge. \n\nZipLoRA (Shah et al., 2023) is a technique developed to merge independently trained Low-Rank Adaptations (LoRAs) (Hu et al., 2021) by optimizing coefficients to reduce interference between content and style representations. They demonstrate that orthogonality between the columns of the Lo-RAs is strongly correlated with successful merging that minimizes interference. Primarily applied in the image generation domain with diffusion models, ZipLoRA preserves subject fidelity and style accuracy without requiring extensive manual adjustments. \n\nEvolutionary Model Merging (Akiba et al., 2024) leverages evolutionary algorithms to optimize merging parameters across both the parameter space and the data flow (layer) space.",
            "score": 0.3896853421000114,
            "section_title": "Data-Informed Merging",
            "char_start_offset": 8720,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 617
                },
                {
                    "start": 620,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1885
                },
                {
                    "start": 1886,
                    "end": 2031
                },
                {
                    "start": 2032,
                    "end": 2205
                },
                {
                    "start": 2208,
                    "end": 2385
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7880859375
        },
        {
            "corpus_id": "273162841",
            "title": "What Matters for Model Merging at Scale?",
            "text": "Model merging has emerged as a cost-effective method for developing improved models. Two common use cases of merging are: (1) combining model checkpoints from different data versions, hyperparameters, or training stages to enhance distributional robustness (Team et al., 2024;Dubey et al., 2024), and (2) combining multiple expert models trained on different datasets to leverage their complementary capabilities. In both scenarios, the expert models generally share a common architecture and a base model from which the expert models are created via fine-tuning. \n\nThis work focuses on merging specialized, fine-tuned versions (experts) of a single base model to enhance its capabilities. Each expert model is trained on distinct datasets covering different tasks, domains, and/or capabilities. We refer to the tasks/datasets used for training the expert models as \"held-in\", while those that are new and unseen are called \"held-out\". Our goal is to create a unified model that retains the individual expert models' capabilities on held-in tasks while improving zeroshot generalization on held-out tasks. This merging approach provides a flexible, modular method for post-training large language models, facilitating the addition of new features and capabilities to top-performing models.",
            "score": 0.3893964133604661,
            "section_title": "BACKGROUND",
            "char_start_offset": 6956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1289
                }
            ],
            "ref_mentions": [
                {
                    "start": 257,
                    "end": 276,
                    "matchedPaperCorpusId": "270843326"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74658203125
        },
        {
            "paperId": "111d019bc2559f43c6ca627704d59673adea7efb",
            "corpusId": 275570290,
            "title": "Merging Models on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 7,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.09522, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "151497321",
                    "name": "Enneng Yang"
                },
                {
                    "authorId": "2327007623",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2279402395",
                    "name": "Yong Luo"
                },
                {
                    "authorId": "2247556302",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2338825263",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2135519749",
                    "name": "Dacheng Tao"
                }
            ],
            "abstract": "Deep model merging represents an emerging research direction that combines multiple fine-tuned models to harness their specialized capabilities across different tasks and domains. Current model merging techniques focus on merging all available models simultaneously, with weight interpolation-based methods being the predominant approaches. However, these conventional approaches are not well-suited for scenarios where models become available sequentially, and they often suffer from high memory requirements and potential interference between tasks. In this study, we propose a training-free projection-based continual merging method that processes models sequentially through orthogonal projections of weight matrices and adaptive scaling mechanisms. Our method operates by projecting new parameter updates onto subspaces orthogonal to existing merged parameter updates while using an adaptive scaling mechanism to maintain stable parameter distances, enabling efficient sequential integration of task-specific knowledge. Our approach maintains constant memory complexity to the number of models, minimizes interference between tasks through orthogonal projections, and retains the performance of previously merged models through adaptive task vector scaling. Extensive experiments on CLIP-ViT models demonstrate that our method achieves a 5-8% average accuracy improvement while maintaining robust performance in different task orderings.",
            "corpus_id": "275570290",
            "text": "Deep model merging represents an emerging research direction that combines multiple fine-tuned models to harness their specialized capabilities across different tasks and domains. Current model merging techniques focus on merging all available models simultaneously, with weight interpolation-based methods being the predominant approaches. However, these conventional approaches are not well-suited for scenarios where models become available sequentially, and they often suffer from high memory requirements and potential interference between tasks. In this study, we propose a training-free projection-based continual merging method that processes models sequentially through orthogonal projections of weight matrices and adaptive scaling mechanisms. Our method operates by projecting new parameter updates onto subspaces orthogonal to existing merged parameter updates while using an adaptive scaling mechanism to maintain stable parameter distances, enabling efficient sequential integration of task-specific knowledge. Our approach maintains constant memory complexity to the number of models, minimizes interference between tasks through orthogonal projections, and retains the performance of previously merged models through adaptive task vector scaling. Extensive experiments on CLIP-ViT models demonstrate that our method achieves a 5-8% average accuracy improvement while maintaining robust performance in different task orderings.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.91650390625
        },
        {
            "paperId": "7a4dc7b9f167f7ea0b9145fd6a165dfb0ce2337d",
            "corpusId": 277043142,
            "title": "Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning",
            "venue": "",
            "year": 2024,
            "referenceCount": 54,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.06664, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326912401",
                    "name": "Qianli Ma"
                },
                {
                    "authorId": "2325009144",
                    "name": "Xuefei Ning"
                },
                {
                    "authorId": "2350508615",
                    "name": "Dongrui Liu"
                },
                {
                    "authorId": "2325066369",
                    "name": "Li Niu"
                },
                {
                    "authorId": "2306069352",
                    "name": "Linfeng Zhang"
                }
            ],
            "abstract": "Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a \\textbf{De}couple-then-\\textbf{Me}rge (\\textbf{DeMe}) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is available at \\href{https://github.com/MqLeet/DeMe}{GitHub}.",
            "corpus_id": "277043142",
            "text": "Diffusion models are trained by learning a sequence of models that reverse each step of noise corruption. Typically, the model parameters are fully shared across multiple timesteps to enhance training efficiency. However, since the denoising tasks differ at each timestep, the gradients computed at different timesteps may conflict, potentially degrading the overall performance of image generation. To solve this issue, this work proposes a \\textbf{De}couple-then-\\textbf{Me}rge (\\textbf{DeMe}) framework, which begins with a pretrained model and finetunes separate models tailored to specific timesteps. We introduce several improved techniques during the finetuning stage to promote effective knowledge sharing while minimizing training interference across timesteps. Finally, after finetuning, these separate models can be merged into a single model in the parameter space, ensuring efficient and practical inference. Experimental results show significant generation quality improvements upon 6 benchmarks including Stable Diffusion on COCO30K, ImageNet1K, PartiPrompts, and DDPM on LSUN Church, LSUN Bedroom, and CIFAR10. Code is available at \\href{https://github.com/MqLeet/DeMe}{GitHub}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1163330078125
        },
        {
            "paperId": "c7507e2446e054a58c4ec11ae5fc72b01625d158",
            "corpusId": 279402179,
            "title": "Training-free LLM Merging for Multi-task Learning",
            "venue": "",
            "year": 2025,
            "referenceCount": 43,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.12379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275537250",
                    "name": "Zichuan Fu"
                },
                {
                    "authorId": "2277462592",
                    "name": "Xian Wu"
                },
                {
                    "authorId": "2162455919",
                    "name": "Yejing Wang"
                },
                {
                    "authorId": "2211473272",
                    "name": "Wanyu Wang"
                },
                {
                    "authorId": "2367227017",
                    "name": "Shanshan Ye"
                },
                {
                    "authorId": "2282271789",
                    "name": "Hongzhi Yin"
                },
                {
                    "authorId": "2316790244",
                    "name": "Yi Chang"
                },
                {
                    "authorId": "2237585282",
                    "name": "Yefeng Zheng"
                },
                {
                    "authorId": "2281902096",
                    "name": "Xiangyu Zhao"
                }
            ],
            "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.",
            "corpus_id": "279402179",
            "text": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse natural language processing (NLP) tasks. The release of open-source LLMs like LLaMA and Qwen has triggered the development of numerous fine-tuned models tailored for various tasks and languages. In this paper, we explore an important question: is it possible to combine these specialized models to create a unified model with multi-task capabilities. We introduces Hierarchical Iterative Merging (Hi-Merging), a training-free method for unifying different specialized LLMs into a single model. Specifically, Hi-Merging employs model-wise and layer-wise pruning and scaling, guided by contribution analysis, to mitigate parameter conflicts. Extensive experiments on multiple-choice and question-answering tasks in both Chinese and English validate Hi-Merging's ability for multi-task learning. The results demonstrate that Hi-Merging consistently outperforms existing merging techniques and surpasses the performance of models fine-tuned on combined datasets in most scenarios. Code is available at: https://github.com/Applied-Machine-Learning-Lab/Hi-Merging.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.953125
        },
        {
            "paperId": "d52f643319e3e6bb8e3e3ae8dceba7cd85b89c1e",
            "corpusId": 278497689,
            "title": "RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality Adaptation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 51,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.48550/arXiv.2404.02424?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.48550/arXiv.2404.02424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2361011095",
                    "name": "Shwai He"
                },
                {
                    "authorId": "2295299539",
                    "name": "Tianlong Chen"
                }
            ],
            "abstract": "Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair. Moreover, while parameter-efficient LoRA finetuning has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models. To tackle these challenges, we propose to Repair Sparse Vision-Language Models via Sparse Cross-modality Adaptation (RESSA). RESSA utilizes cross-modality finetuning to enhance task-specific performance and facilitate knowledge distillation from original dense models. Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models. Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3% improvement under 2:4 sparsity and a remarkable 47.6% enhancement under unstructured 70% sparsity. Code will be released at: https://github.com",
            "corpus_id": "278497689",
            "text": "Vision-Language Models (VLMs), integrating diverse information from multiple modalities, have shown remarkable success across various tasks. However, deploying VLMs, comprising large-scale vision and language models poses challenges in resource-constrained scenarios. While pruning followed by finetuning offers a potential solution to maintain performance with smaller model sizes, its application to VLMs remains relatively unexplored, presenting two main questions: how to distribute sparsity across different modality-specific models, and how to repair the performance of pruned sparse VLMs. To answer the first question, we conducted preliminary studies on VLM pruning and found that pruning vision models and language models with the same sparsity ratios contribute to nearly optimal performance. For the second question, unlike finetuning unimodal sparse models, sparse VLMs involve cross-modality interactions, requiring specialized techniques for post-pruning performance repair. Moreover, while parameter-efficient LoRA finetuning has been proposed to repair the performance of sparse models, a significant challenge of weights merging arises due to the incompatibility of dense LoRA modules with sparse models that destroy the sparsity of pruned models. To tackle these challenges, we propose to Repair Sparse Vision-Language Models via Sparse Cross-modality Adaptation (RESSA). RESSA utilizes cross-modality finetuning to enhance task-specific performance and facilitate knowledge distillation from original dense models. Additionally, we introduce SparseLoRA, which applies sparsity directly to LoRA weights, enabling seamless integration with sparse models. Our experimental results validate the effectiveness of RESSA, showcasing significant enhancements, such as an 11.3% improvement under 2:4 sparsity and a remarkable 47.6% enhancement under unstructured 70% sparsity. Code will be released at: https://github.com",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2445068359375
        },
        {
            "paperId": "fd1f8f090baac703801b9796d4bedac3575c9151",
            "corpusId": 278886444,
            "title": "AstroMLab 4: Benchmark-Topping Performance in Astronomy Q&A with a 70B-Parameter Domain-Specialized Reasoning Model",
            "venue": "",
            "year": 2025,
            "referenceCount": 10,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.17592, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310232907",
                    "name": "Tijmen de Haan"
                },
                {
                    "authorId": "2260337474",
                    "name": "Y.-S. Ting"
                },
                {
                    "authorId": "2301583035",
                    "name": "Tirthankar Ghosal"
                },
                {
                    "authorId": "2323501341",
                    "name": "Tuan Dung Nguyen"
                },
                {
                    "authorId": "2311506761",
                    "name": "Alberto Accomazzi"
                },
                {
                    "authorId": "2329737480",
                    "name": "Emily Herron"
                },
                {
                    "authorId": "2329739232",
                    "name": "Vanessa Lama"
                },
                {
                    "authorId": "2277742348",
                    "name": "Rui Pan"
                },
                {
                    "authorId": "2311510072",
                    "name": "Azton Wells"
                },
                {
                    "authorId": "71349489",
                    "name": "Nesar Ramachandra"
                }
            ],
            "abstract": "General-purpose large language models, despite their broad capabilities, often struggle with specialized domain knowledge, a limitation particularly pronounced in more accessible, lower-parameter versions. This gap hinders their deployment as effective agents in demanding fields such as astronomy. Building on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a significantly larger and more advanced domain-specialized natural-language AI assistant. It is designed for research and education across astronomy, astrophysics, space science, astroparticle physics, cosmology, and astronomical instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B underwent extensive continued pre-training on a vast corpus of astronomical literature, followed by supervised fine-tuning and model merging. Beyond its 70-billion parameter scale, this model incorporates refined datasets, judiciously chosen learning hyperparameters, and improved training procedures, achieving state-of-the-art performance on complex astronomical tasks. Notably, we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to either answer the user query immediately, or first emit a human-readable thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425 questions from literature withheld during training -- AstroSage-70B achieves state-of-the-art performance. It surpasses all other tested open-weight and proprietary models, including leading systems like o3, Gemini-2.5-Pro, Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two orders of magnitude higher. This work demonstrates that domain specialization, when applied to large-scale models, can enable them to outperform generalist counterparts in specialized knowledge areas like astronomy, thereby advancing the frontier of AI capabilities in the field.",
            "corpus_id": "278886444",
            "text": "General-purpose large language models, despite their broad capabilities, often struggle with specialized domain knowledge, a limitation particularly pronounced in more accessible, lower-parameter versions. This gap hinders their deployment as effective agents in demanding fields such as astronomy. Building on our prior work with AstroSage-8B, this study introduces AstroSage-70B, a significantly larger and more advanced domain-specialized natural-language AI assistant. It is designed for research and education across astronomy, astrophysics, space science, astroparticle physics, cosmology, and astronomical instrumentation. Developed from the Llama-3.1-70B foundation, AstroSage-70B underwent extensive continued pre-training on a vast corpus of astronomical literature, followed by supervised fine-tuning and model merging. Beyond its 70-billion parameter scale, this model incorporates refined datasets, judiciously chosen learning hyperparameters, and improved training procedures, achieving state-of-the-art performance on complex astronomical tasks. Notably, we integrated reasoning chains into the SFT dataset, enabling AstroSage-70B to either answer the user query immediately, or first emit a human-readable thought process. Evaluated on the AstroMLab-1 benchmark -- comprising 4,425 questions from literature withheld during training -- AstroSage-70B achieves state-of-the-art performance. It surpasses all other tested open-weight and proprietary models, including leading systems like o3, Gemini-2.5-Pro, Claude-3.7-Sonnet, Deepseek-R1, and Qwen-3-235B, even those with API costs two orders of magnitude higher. This work demonstrates that domain specialization, when applied to large-scale models, can enable them to outperform generalist counterparts in specialized knowledge areas like astronomy, thereby advancing the frontier of AI capabilities in the field.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1763916015625
        },
        {
            "paperId": "192028b7f0b920ce19cedc2dadc5992d71bc8ff6",
            "corpusId": 266163240,
            "title": "Concrete Subspace Learning based Interference Elimination for Multi-task Model Fusion",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 53,
            "citationCount": 22,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.06173, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2178366354",
                    "name": "A. Tang"
                },
                {
                    "authorId": "2248152216",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2150649639",
                    "name": "Yong Luo"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2247556302",
                    "name": "Han Hu"
                },
                {
                    "authorId": "2212029373",
                    "name": "Bo Du"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Merging models fine-tuned from a common, extensively pre-trained large model but specialized for different tasks has been demonstrated as a cheap and scalable strategy to construct a multi-task model that performs well across diverse tasks. Recent research, exemplified by task arithmetic, highlights that this multi-task model can be derived through arithmetic operations on task vectors. Nevertheless, current merging techniques frequently resolve potential conflicts among parameters from task-specific models by evaluating individual attributes, such as the parameters' magnitude or sign, overlooking their collective impact on the overall functionality of the model. In this work, we propose the CONtinuous relaxation of disCRETE (Concrete) subspace learning method to identify a common low-dimensional subspace and utilize its shared information to track the interference problem without sacrificing much performance. Specifically, we model the problem as a bi-level optimization problem and introduce a meta-learning framework to find the Concrete subspace mask through gradient-based techniques. At the upper level, we focus on learning a shared Concrete mask to identify the subspace, while at the inner level, model merging is performed to maximize the performance of the merged model. We conduct extensive experiments on both vision domain and language domain, and the results demonstrate the effectiveness of our method. The code is available at https://github.com/tanganke/subspace_fusion",
            "corpus_id": "266163240",
            "text": "Merging models fine-tuned from a common, extensively pre-trained large model but specialized for different tasks has been demonstrated as a cheap and scalable strategy to construct a multi-task model that performs well across diverse tasks. Recent research, exemplified by task arithmetic, highlights that this multi-task model can be derived through arithmetic operations on task vectors. Nevertheless, current merging techniques frequently resolve potential conflicts among parameters from task-specific models by evaluating individual attributes, such as the parameters' magnitude or sign, overlooking their collective impact on the overall functionality of the model. In this work, we propose the CONtinuous relaxation of disCRETE (Concrete) subspace learning method to identify a common low-dimensional subspace and utilize its shared information to track the interference problem without sacrificing much performance. Specifically, we model the problem as a bi-level optimization problem and introduce a meta-learning framework to find the Concrete subspace mask through gradient-based techniques. At the upper level, we focus on learning a shared Concrete mask to identify the subspace, while at the inner level, model merging is performed to maximize the performance of the merged model. We conduct extensive experiments on both vision domain and language domain, and the results demonstrate the effectiveness of our method. The code is available at https://github.com/tanganke/subspace_fusion",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.92822265625
        },
        {
            "paperId": "4d0c292e5c5338cbeeacca7e8c4d7bf1c6c2f423",
            "corpusId": 273346716,
            "title": "MoTE: Reconciling Generalization with Specialization for Visual-Language to Video Knowledge Transfer",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "referenceCount": 59,
            "citationCount": 1,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.10589, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2237478018",
                    "name": "Minghao Zhu"
                },
                {
                    "authorId": "2326262142",
                    "name": "Zhengpu Wang"
                },
                {
                    "authorId": "2282221443",
                    "name": "Mengxian Hu"
                },
                {
                    "authorId": "2131077260",
                    "name": "Ronghao Dang"
                },
                {
                    "authorId": "2237435197",
                    "name": "Xiao Lin"
                },
                {
                    "authorId": "2282035862",
                    "name": "Xun Zhou"
                },
                {
                    "authorId": "2920326",
                    "name": "Chengju Liu"
                },
                {
                    "authorId": "145844275",
                    "name": "Qi Chen"
                }
            ],
            "abstract": "Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose \\emph{Weight Merging Regularization}, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain state-of-the-art or competitive results on various datasets, including Kinetics-400 \\&600, UCF, and HMDB. Code is available at \\url{https://github.com/ZMHH-H/MoTE}.",
            "corpus_id": "273346716",
            "text": "Transferring visual-language knowledge from large-scale foundation models for video recognition has proved to be effective. To bridge the domain gap, additional parametric modules are added to capture the temporal information. However, zero-shot generalization diminishes with the increase in the number of specialized parameters, making existing works a trade-off between zero-shot and close-set performance. In this paper, we present MoTE, a novel framework that enables generalization and specialization to be balanced in one unified model. Our approach tunes a mixture of temporal experts to learn multiple task views with various degrees of data fitting. To maximally preserve the knowledge of each expert, we propose \\emph{Weight Merging Regularization}, which regularizes the merging process of experts in weight space. Additionally with temporal feature modulation to regularize the contribution of temporal feature during test. We achieve a sound balance between zero-shot and close-set video recognition tasks and obtain state-of-the-art or competitive results on various datasets, including Kinetics-400 \\&600, UCF, and HMDB. Code is available at \\url{https://github.com/ZMHH-H/MoTE}.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.25244140625
        },
        {
            "paperId": "c0ebf1852b82d4ca52ab8429003be4bff884bc5b",
            "corpusId": 273507837,
            "title": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 114,
            "citationCount": 13,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257324343",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "153670517",
                    "name": "Nikolaos Dimitriadis"
                },
                {
                    "authorId": "2088881127",
                    "name": "Alessandro Favero"
                },
                {
                    "authorId": "2257000703",
                    "name": "Guillermo Ortiz-Jim\u00e9nez"
                },
                {
                    "authorId": "2721983",
                    "name": "F. Fleuret"
                },
                {
                    "authorId": "2256985187",
                    "name": "Pascal Frossard"
                }
            ],
            "abstract": "Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, \\textit{(i)} fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and \\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to significant performance loss. To address these challenges, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Our method is simple to implement, computationally efficient and complementary to many existing techniques. Our source code is available at https://github.com/wang-kee/LiNeS",
            "corpus_id": "273507837",
            "text": "Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, \\textit{(i)} fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and \\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to significant performance loss. To address these challenges, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Our method is simple to implement, computationally efficient and complementary to many existing techniques. Our source code is available at https://github.com/wang-kee/LiNeS",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.81640625
        },
        {
            "paperId": "7432f383568c98391ae6cb5727bf0c8c33d05321",
            "corpusId": 279118713,
            "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging",
            "venue": "",
            "year": 2025,
            "referenceCount": 33,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.02478, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2239036328",
                    "name": "Zijian Li"
                },
                {
                    "authorId": "2674998",
                    "name": "Xiaocheng Feng"
                },
                {
                    "authorId": "2365043614",
                    "name": "Huixin Liu"
                },
                {
                    "authorId": "2118138548",
                    "name": "Yi-Chong Huang"
                },
                {
                    "authorId": "2274093523",
                    "name": "Ting Liu"
                },
                {
                    "authorId": "2257004102",
                    "name": "Bing Qin"
                }
            ],
            "abstract": "With the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.",
            "corpus_id": "279118713",
            "text": "With the development of large language models, fine-tuning has emerged as an effective method to enhance performance in specific scenarios by injecting domain-specific knowledge. In this context, model merging techniques provide a solution for fusing knowledge from multiple fine-tuning models by combining their parameters. However, traditional methods often encounter task interference when merging full fine-tuning models, and this problem becomes even more evident in parameter-efficient fine-tuning scenarios. In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.888671875
        }
    ],
    "quotes": {
        "cost": 0.20204099999999997,
        "quotes": [
            {
                "idx": 0,
                "key": "[259064039 | Yadav et al. | 2023 | Citations: 317]",
                "snippets": "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Original Values",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 761,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 446
                            },
                            {
                                "start": 447,
                                "end": 761
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[266053657 | Tam et al. | 2023 | Citations: 12]",
                "snippets": "Fisher merging (Matena et al., 2021), RegMean (Jin et al., 2022), and simple parameter averaging (McMahan et al., 2016)Stich, 2018) all perform merging in this way and differ only in their choice of task parameter subspace. Concurrently, other works have focused on inaccuracies in model merging stemming from gradient mismatches in different models, and use these insights to connect diagonal Fisher merging and Task Arithmetic (Daheim et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[14955348 | McMahan et al. | 2016 | Citations: 17603]": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. \nWe present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.",
                    "[244345933 | Matena et al. | 2021 | Citations: 402]": "Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 450,
                        "end": 903,
                        "sentence_offsets": [
                            {
                                "start": 423,
                                "end": 675
                            },
                            {
                                "start": 676,
                                "end": 903
                            }
                        ],
                        "ref_mentions": [
                            "244345933",
                            "14955348"
                        ],
                        "quote": "Fisher merging (Matena et al., 2021), RegMean (Jin et al., 2022), and simple parameter averaging (McMahan et al., 2016)Stich, 2018) all perform merging in this way and differ only in their choice of task parameter subspace. Concurrently, other works have focused on inaccuracies in model merging stemming from gradient mismatches in different models, and use these insights to connect diagonal Fisher merging and Task Arithmetic (Daheim et al., 2023)."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[268537290 | Akiba et al. | 2024 | Citations: 125]",
                "snippets": "A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                },
                "metadata": [
                    {
                        "section_title": "Merging Language Models",
                        "pdf_hash": "",
                        "start": 464,
                        "end": 1325,
                        "sentence_offsets": [
                            {
                                "start": 350,
                                "end": 562
                            },
                            {
                                "start": 564,
                                "end": 696
                            },
                            {
                                "start": 696,
                                "end": 923
                            },
                            {
                                "start": 923,
                                "end": 1140
                            },
                            {
                                "start": 1142,
                                "end": 1329
                            }
                        ],
                        "ref_mentions": [
                            "259064039"
                        ],
                        "quote": "A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[271039884 | Tao et al. | 2024 | Citations: 5]",
                "snippets": "In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;(Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."
                },
                "metadata": [
                    {
                        "section_title": "Preliminary: Model Merging",
                        "pdf_hash": "",
                        "start": 341,
                        "end": 1750,
                        "sentence_offsets": [
                            {
                                "start": 341,
                                "end": 508
                            },
                            {
                                "start": 509,
                                "end": 559
                            },
                            {
                                "start": 562,
                                "end": 672
                            },
                            {
                                "start": 675,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 894
                            },
                            {
                                "start": 895,
                                "end": 995
                            },
                            {
                                "start": 996,
                                "end": 1085
                            },
                            {
                                "start": 1088,
                                "end": 1283
                            },
                            {
                                "start": 1286,
                                "end": 1434
                            },
                            {
                                "start": 1437,
                                "end": 1609
                            },
                            {
                                "start": 1610,
                                "end": 1750
                            }
                        ],
                        "ref_mentions": [
                            "247362886"
                        ],
                        "quote": "In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;(Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[271050386 | Lu et al. | 2024 | Citations: 27]",
                "snippets": "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[271329267 | Hagos et al. | 2024 | Citations: 26]",
                "snippets": "Techniques such as parameter averaging (Matena et al., 2021) or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[244345933 | Matena et al. | 2021 | Citations: 402]": "Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."
                },
                "metadata": [
                    {
                        "section_title": "E. Model Merging",
                        "pdf_hash": "",
                        "start": 288,
                        "end": 708,
                        "sentence_offsets": [
                            {
                                "start": 288,
                                "end": 432
                            },
                            {
                                "start": 433,
                                "end": 708
                            }
                        ],
                        "ref_mentions": [
                            "244345933"
                        ],
                        "quote": "Techniques such as parameter averaging (Matena et al., 2021) or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[271843401 | Shi et al. | 2024 | Citations: 3]",
                "snippets": "Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"
                },
                "metadata": [
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 598,
                        "end": 1534,
                        "sentence_offsets": [
                            {
                                "start": 598,
                                "end": 760
                            },
                            {
                                "start": 761,
                                "end": 895
                            },
                            {
                                "start": 896,
                                "end": 1084
                            },
                            {
                                "start": 1085,
                                "end": 1224
                            },
                            {
                                "start": 1225,
                                "end": 1325
                            },
                            {
                                "start": 1326,
                                "end": 1534
                            }
                        ],
                        "ref_mentions": [
                            "259064039"
                        ],
                        "quote": "Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[271903078 | Dehghan et al. | 2024 | Citations: 2]",
                "snippets": "In this paper, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[271957310 | He et al. | 2024 | Citations: 25]",
                "snippets": "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation...However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation",
                        "pdf_hash": ""
                    },
                    {
                        "section_title": "Motivation and objectives",
                        "pdf_hash": "",
                        "start": 735,
                        "end": 1141,
                        "sentence_offsets": [
                            {
                                "start": 735,
                                "end": 952
                            },
                            {
                                "start": 953,
                                "end": 1140
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[274423281 | Xiong et al. | 2024 | Citations: 8]",
                "snippets": "Recently, Ilharco et al. (Ilharco et al., 2022) demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging (Yadav et al., 2023) resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE (Yu et al., 2023) mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging (Wang et al., 2024) eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
                    "[254408495 | Ilharco et al. | 2022 | Citations: 520]": "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models.",
                    "[265034087 | Yu et al. | 2023 | Citations: 335]": "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard.",
                    "[269757600 | Wang et al. | 2024 | Citations: 60]": "Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve>99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 695,
                        "end": 1614,
                        "sentence_offsets": [
                            {
                                "start": 695,
                                "end": 845
                            },
                            {
                                "start": 846,
                                "end": 905
                            },
                            {
                                "start": 906,
                                "end": 1079
                            },
                            {
                                "start": 1080,
                                "end": 1221
                            },
                            {
                                "start": 1224,
                                "end": 1411
                            },
                            {
                                "start": 1412,
                                "end": 1614
                            }
                        ],
                        "ref_mentions": [
                            "254408495",
                            "259064039",
                            "265034087",
                            "269757600"
                        ],
                        "quote": "Recently, Ilharco et al. (Ilharco et al., 2022) demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging (Yadav et al., 2023) resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE (Yu et al., 2023) mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging (Wang et al., 2024) eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[274788575 | Choi et al. | 2024 | Citations: 5]",
                "snippets": "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 744,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[275921352 | Sun et al. | 2025 | Citations: 5]",
                "snippets": "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[276409347 | Liu et al. | 2025 | Citations: 2]",
                "snippets": "We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 335,
                        "end": 735,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276422064 | Liu et al. | 2025 | Citations: 3]",
                "snippets": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[276575632 | Zhao et al. | 2025 | Citations: 1]",
                "snippets": "We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models.\n\nLoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1051,
                        "end": 1569,
                        "sentence_offsets": [
                            {
                                "start": 1049,
                                "end": 1216
                            },
                            {
                                "start": 1217,
                                "end": 1282
                            },
                            {
                                "start": 1285,
                                "end": 1400
                            },
                            {
                                "start": 1401,
                                "end": 1508
                            },
                            {
                                "start": 1511,
                                "end": 1823
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models.\n\nLoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[276937513 | Ruan et al. | 2025 | Citations: 0]",
                "snippets": "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a].",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[276961298 | Fang et al. | 2025 | Citations: 0]",
                "snippets": "Rather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 402,
                        "end": 1333,
                        "sentence_offsets": [
                            {
                                "start": 402,
                                "end": 622
                            },
                            {
                                "start": 623,
                                "end": 731
                            },
                            {
                                "start": 732,
                                "end": 794
                            },
                            {
                                "start": 797,
                                "end": 932
                            },
                            {
                                "start": 933,
                                "end": 1095
                            },
                            {
                                "start": 1098,
                                "end": 1195
                            },
                            {
                                "start": 1196,
                                "end": 1333
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Rather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[277065877 | Chen et al. | 2025 | Citations: 1]",
                "snippets": "Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration. In contrast to the data-free methods mentioned above, data-informed methods [56,69,70] optimize merging coefficients using additional data. Model merging is impactful for LLMs, enabling efficient knowledge integration without full retraining, facilitating distributed finetuning [62], multi-task learning [49], and cost-effective model adaptation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 571,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 223
                            },
                            {
                                "start": 224,
                                "end": 363
                            },
                            {
                                "start": 364,
                                "end": 571
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Task Arithmetic [44] enables weight-space operations to combine task-specific behaviors in language models, while Diversity-Aware Merging, such as DARE [33], leverages model diversity to improve sparse-to-dense integration. In contrast to the data-free methods mentioned above, data-informed methods [56,69,70] optimize merging coefficients using additional data. Model merging is impactful for LLMs, enabling efficient knowledge integration without full retraining, facilitating distributed finetuning [62], multi-task learning [49], and cost-effective model adaptation."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[277322544 | Wu et al. | 2025 | Citations: 14]",
                "snippets": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups.",
                    "[254408495 | Ilharco et al. | 2022 | Citations: 520]": "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."
                },
                "metadata": [
                    {
                        "section_title": "MODEL MERGING",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 2281,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 180
                            },
                            {
                                "start": 181,
                                "end": 235
                            },
                            {
                                "start": 236,
                                "end": 342
                            },
                            {
                                "start": 345,
                                "end": 522
                            },
                            {
                                "start": 523,
                                "end": 632
                            },
                            {
                                "start": 633,
                                "end": 774
                            },
                            {
                                "start": 777,
                                "end": 998
                            },
                            {
                                "start": 999,
                                "end": 1131
                            },
                            {
                                "start": 1132,
                                "end": 1225
                            },
                            {
                                "start": 1228,
                                "end": 1357
                            },
                            {
                                "start": 1360,
                                "end": 1518
                            },
                            {
                                "start": 1519,
                                "end": 1762
                            },
                            {
                                "start": 1763,
                                "end": 1910
                            },
                            {
                                "start": 1911,
                                "end": 2000
                            },
                            {
                                "start": 2003,
                                "end": 2285
                            }
                        ],
                        "ref_mentions": [
                            "247362886",
                            "254408495"
                        ],
                        "quote": "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[277634330 | Nayak et al. | 2025 | Citations: 0]",
                "snippets": "On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
                    "[251493208 | Ilharco et al. | 2022 | Citations: 176]": "Open-vocabulary models like CLIP achieve high accuracy across many image classification tasks. However, there are still settings where their zero-shot performance is far from optimal. We study model patching, where the goal is to improve accuracy on specific tasks without degrading accuracy on tasks where performance is already adequate. Towards this goal, we introduce PAINT, a patching method that uses interpolations between the weights of a model before fine-tuning and the weights after fine-tuning on a task to be patched. On nine tasks where zero-shot CLIP performs poorly, PAINT increases accuracy by 15 to 60 percentage points while preserving accuracy on ImageNet within one percentage point of the zero-shot model. PAINT also allows a single model to be patched on multiple tasks and improves with model scale. Furthermore, we identify cases of broad transfer, where patching on one task increases accuracy on other tasks even when the tasks have disjoint classes. Finally, we investigate applications beyond common benchmarks such as counting or reducing the impact of typographic attacks on CLIP. Our findings demonstrate that it is possible to expand the set of tasks on which open-vocabulary models achieve high accuracy without re-training them from scratch."
                },
                "metadata": [
                    {
                        "section_title": "Full-Model Fine-Tuning and Model Merging:",
                        "pdf_hash": "",
                        "start": 227,
                        "end": 637,
                        "sentence_offsets": [
                            {
                                "start": 227,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 637
                            }
                        ],
                        "ref_mentions": [
                            "251493208",
                            "259064039"
                        ],
                        "quote": "On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[278328055 | Ociepa et al. | 2025 | Citations: 0]",
                "snippets": "To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) (Wortsman et al., 2022), TIES (TIES-Merging: Resolving Interference When Merging Models) (Yadav et al., 2023), Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[259064039 | Yadav et al. | 2023 | Citations: 317]": "Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging",
                    "[247362886 | Wortsman et al. | 2022 | Citations: 1011]": "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."
                },
                "metadata": [
                    {
                        "section_title": "Model Merging",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 972,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 333
                            },
                            {
                                "start": 336,
                                "end": 521
                            },
                            {
                                "start": 524,
                                "end": 618
                            },
                            {
                                "start": 619,
                                "end": 767
                            },
                            {
                                "start": 768,
                                "end": 972
                            }
                        ],
                        "ref_mentions": [
                            "247362886",
                            "259064039"
                        ],
                        "quote": "To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) (Wortsman et al., 2022), TIES (TIES-Merging: Resolving Interference When Merging Models) (Yadav et al., 2023), Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[278501405 | Sun et al. | 2025 | Citations: 2]",
                "snippets": "Motivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1034,
                        "end": 1749,
                        "sentence_offsets": [
                            {
                                "start": 1034,
                                "end": 1255
                            },
                            {
                                "start": 1256,
                                "end": 1357
                            },
                            {
                                "start": 1358,
                                "end": 1564
                            },
                            {
                                "start": 1565,
                                "end": 1749
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Motivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[279118713 | Li et al. | 2025 | Citations: 0]",
                "snippets": "In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.031737,
        "cot": "To effectively structure a response about parameter-space merging techniques for combining specialized language models, I need to organize the quotes into logical dimensions that directly address the user query.\n\nThe user is asking about:\n1. The most effective parameter-space merging techniques for combining specialized language models\n2. How these techniques address knowledge conflict and task interference\n\nLooking at the quotes, I can see several major themes:\n- Different merging techniques and their approaches\n- Sources of interference in model merging\n- Methods to address knowledge conflicts\n- Comparative performance of different merging techniques\n- Applications and implementations\n\nBased on these themes and the user query, I'll organize the quotes into the following dimensions:\n\n1. \"Introduction to Parameter-Space Model Merging\" - This will provide necessary background on what model merging is and why it's important. This should be in synthesis format to give readers a clear understanding of the concept.\n\n2. \"Common Sources of Interference in Model Merging\" - This will explain the key problems that need to be addressed in model merging. Synthesis format is appropriate here to clearly explain the concepts.\n\n3. \"TIES-Merging: A Leading Parameter-Space Merging Technique\" - TIES-Merging appears frequently in the quotes as a significant technique. This deserves its own section in synthesis format to thoroughly explain this important method.\n\n4. \"Other Effective Parameter-Space Merging Techniques\" - This will cover other prominent techniques mentioned in the quotes. List format is appropriate here to clearly enumerate and describe each technique.\n\n5. \"Comparative Performance and Effectiveness\" - This will provide insights into how these techniques compare and which ones are most effective for different scenarios. Synthesis format will allow for clear comparison and analysis.\n\n6. \"Recent Advancements in Addressing Knowledge Conflicts\" - This will focus specifically on newer approaches mentioned that directly address knowledge conflicts. Synthesis format will help explain these complex approaches.\n\nThis organization allows me to directly address the user's query about the most effective techniques and how they address knowledge conflict, while providing a comprehensive view of the topic.",
        "plan": {
            "Introduction to Parameter-Space Model Merging (synthesis)": [],
            "Common Sources of Interference in Model Merging (synthesis)": [
                0,
                2,
                4,
                8,
                10,
                11,
                15,
                21
            ],
            "TIES-Merging: A Leading Parameter-Space Merging Technique (synthesis)": [
                0,
                2,
                3,
                4,
                6,
                9,
                11,
                15,
                16,
                18,
                20
            ],
            "Other Effective Parameter-Space Merging Techniques (list)": [
                1,
                5,
                6,
                7,
                9,
                11,
                12,
                13,
                14,
                15,
                17,
                18,
                19,
                22
            ],
            "Comparative Performance and Effectiveness (synthesis)": [
                2,
                4,
                8,
                10,
                13,
                15,
                20
            ],
            "Recent Advancements in Addressing Knowledge Conflicts (synthesis)": [
                8,
                9,
                11,
                13,
                14,
                15,
                21
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Parameter-Space Model Merging",
                "tldr": "Parameter-space model merging combines multiple language models by directly manipulating their weights to create a single model that preserves the capabilities of all constituent models. This approach offers a computationally efficient alternative to ensemble methods, allowing for knowledge integration without increasing inference costs. (LLM Memory)",
                "text": "\nParameter-space model merging is a technique that combines multiple pre-trained language models (LMs) at the weight level, rather than at the output level as in ensemble methods. This approach has gained significant attention as an efficient way to integrate specialized knowledge and capabilities from different models into a single model architecture. The fundamental idea is to combine the weights of multiple models that share the same architecture but have been trained on different tasks or domains. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe simplest form of parameter-space merging is linear interpolation, where the weights of the merged model are calculated as a weighted average of the constituent models' parameters. However, this naive approach often leads to interference between different knowledge sources, resulting in performance degradation. More sophisticated techniques have been developed to address these challenges, focusing on identifying and preserving the most important parameters from each source model. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe primary advantages of parameter-space merging include computational efficiency (as only one model needs to be deployed at inference time), the preservation of knowledge from multiple sources, and the potential to mitigate catastrophic forgetting when models are fine-tuned on new tasks. Additionally, this approach allows for the creation of generalist models that perform well across multiple domains without requiring retraining from scratch. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research has shown that effective parameter-space merging can not only combine models without significant performance degradation but can sometimes lead to emergent capabilities not present in any of the constituent models. This has made model merging an important area of study for developing more capable and efficient language models. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Common Sources of Interference in Model Merging",
                "tldr": "Parameter-space model merging often suffers from two major sources of interference: redundant parameter values that contribute little to task performance, and sign conflicts where parameter updates disagree in direction across different models. These interference sources can significantly degrade performance when merging multiple specialized models. (8 sources)",
                "text": "\nParameter-space model merging techniques, while promising for creating multi-task models without additional training, frequently encounter performance degradation due to parameter interference. Research has identified several key sources of this interference:\n\n## Redundant Parameter Values\nWhen models are fine-tuned for specific tasks, many parameter changes are minimal or redundant, contributing little to task performance but potentially causing interference during merging. These small, non-essential parameter changes can accumulate and disrupt the knowledge encoded in more important parameters <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. Techniques like TIES-Merging address this by \"trimming\" these redundant values, effectively resetting them to their pre-trained values <Paper corpusId=\"268537290\" paperTitle=\"(Akiba et al., 2024)\" isShortName></Paper>.\n\n## Sign Conflicts\nPerhaps the most significant source of interference occurs when the same parameter position shows opposite directional changes (sign conflicts) across different fine-tuned models. This indicates fundamental disagreement about how a parameter should be adjusted for different tasks <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. When these conflicting parameters are naively averaged, they can cancel each other out, resulting in significant performance degradation - sometimes up to 10% performance drops with simple averaging <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\n## Task Vector Conflicts\nThe task vector paradigm, which represents the difference between fine-tuned and pre-trained models, has revealed that conflicts in parameter space arise from divergent adaptation directions for different tasks <Paper corpusId=\"275921352\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. Visualizing these task vectors demonstrates how different models pull parameters in opposing directions, creating interference when merged <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\n## Global vs. Localized Conflicts\nRecent research has found that globally merging models often leads to unnecessary task interference due to parameter redundancy in task vectors. Studies show that much of a model's task-specific knowledge is concentrated in small, localized regions of the parameter space <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. When multiple tasks share highly overlapping localized regions, they compete for parameter control, leading to performance degradation <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\n## Feature-Level Conflicts\nInterference also manifests at the feature level, where task-specific adaptations can conflict. Feature projection weights, normalization scalers, and shifts often require different strategies to mitigate knowledge conflicts <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>. Analysis shows that centering task vectors and applying low-rank approximation can significantly reduce task interference by isolating task-specific knowledge in the top singular vectors <Paper corpusId=\"274788575\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>.\n\nTo address these interference sources, researchers have developed various approaches, including parameter selection based on magnitude (dropping insignificant changes), sign conflict resolution by selecting dominant directions, and focusing merging on specific parameter subsets rather than the entire model <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>. These strategies aim to preserve the most valuable task-specific knowledge while minimizing destructive interference during the merging process.",
                "citations": [
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Akiba et al., 2024)",
                        "snippets": [
                            "A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."
                        ],
                        "paper": {
                            "corpus_id": 268537290,
                            "title": "Evolutionary Optimization of Model Merging Recipes",
                            "authors": [
                                {
                                    "authorId": "2292261829",
                                    "name": "Takuya Akiba"
                                },
                                {
                                    "authorId": "2205654861",
                                    "name": "Makoto Shing"
                                },
                                {
                                    "authorId": "2244138850",
                                    "name": "Yujin Tang"
                                },
                                {
                                    "authorId": "2326487319",
                                    "name": "Qi Sun"
                                },
                                {
                                    "authorId": "2292260083",
                                    "name": "David Ha"
                                }
                            ],
                            "year": 2024,
                            "venue": "Nat. Mac. Intell.",
                            "n_citations": 125
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC."
                        ],
                        "paper": {
                            "corpus_id": 271050386,
                            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "81758928",
                                    "name": "Jinliang Lu"
                                },
                                {
                                    "authorId": "2310334594",
                                    "name": "Ziliang Pang"
                                },
                                {
                                    "authorId": "2310342808",
                                    "name": "Min Xiao"
                                },
                                {
                                    "authorId": "2290018211",
                                    "name": "Yaochen Zhu"
                                },
                                {
                                    "authorId": "2258711381",
                                    "name": "Rui Xia"
                                },
                                {
                                    "authorId": "2290006077",
                                    "name": "Jiajun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 27
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors."
                        ],
                        "paper": {
                            "corpus_id": 275921352,
                            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "2119293226",
                                    "name": "Wen Wang"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation",
                            "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."
                        ],
                        "paper": {
                            "corpus_id": 271957310,
                            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "2182670937",
                                    "name": "Yifei He"
                                },
                                {
                                    "authorId": "2317078449",
                                    "name": "Yuzheng Hu"
                                },
                                {
                                    "authorId": "2292270783",
                                    "name": "Yong Lin"
                                },
                                {
                                    "authorId": "2306841244",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "2283183420",
                                    "name": "Han Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 25
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Sun et al._1, 2025)",
                        "snippets": [
                            "Motivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars."
                        ],
                        "paper": {
                            "corpus_id": 278501405,
                            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Choi et al., 2024)",
                        "snippets": [
                            "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors."
                        ],
                        "paper": {
                            "corpus_id": 274788575,
                            "title": "Revisiting Weight Averaging for Model Merging",
                            "authors": [
                                {
                                    "authorId": "2336394540",
                                    "name": "Jiho Choi"
                                },
                                {
                                    "authorId": "122204266",
                                    "name": "Donggyun Kim"
                                },
                                {
                                    "authorId": "2335812409",
                                    "name": "Chanhyuk Lee"
                                },
                                {
                                    "authorId": "2335816260",
                                    "name": "Seunghoon Hong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91650390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "TIES-Merging: A Leading Parameter-Space Merging Technique",
                "tldr": "TIES-Merging addresses parameter interference through a three-step approach: trimming redundant parameters, resolving sign conflicts, and merging only aligned parameters. This technique consistently outperforms other merging methods across diverse models, tasks, and domains by specifically targeting the two major sources of interference in model merging. (11 sources)",
                "text": "\nTIES-Merging (TRIM, ELECT SIGN & MERGE) has emerged as one of the most effective parameter-space model merging techniques, specifically designed to address the fundamental challenges of interference when combining multiple fine-tuned models. Developed by Yadav et al., TIES-Merging implements a three-step approach to mitigate the problems of redundant parameter values and sign conflicts identified in previous sections <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>.\n\nThe first step, \"Trim,\" focuses on removing redundant parameters by identifying and retaining only the most influential parameter values in each task vector (defined as the difference between fine-tuned and pre-trained model weights). This is accomplished by setting parameters with minimal changes back to their pre-trained values, effectively pruning noise that contributes little to task performance <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This trimming process can be conceptualized as keeping only the top-k% of parameter values based on magnitude, where k is a tunable hyperparameter <Paper corpusId=\"271039884\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper>.\n\nThe second step, \"Elect Sign,\" addresses sign conflicts that persist even after trimming. For each parameter position where task vectors disagree on the direction of change, TIES-Merging elects the dominant sign by selecting the direction with the largest total magnitude across all models <Paper corpusId=\"271039884\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper>. This majority voting approach ensures that parameter updates move consistently in the most significant direction.\n\nFinally, the \"Merge\" step selectively combines only those parameters whose signs align with the elected sign direction <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This disjoint merging process creates a mean value using only parameters that agree with the elected sign, preventing destructive interference from conflicting parameter updates <Paper corpusId=\"271039884\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276961298\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n\nThe effectiveness of TIES-Merging stems from its targeted approach to the two primary sources of interference identified in parameter-space model merging. Unlike simpler methods like weighted averaging that often suffer from up to 10% performance drops when merging multiple models <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>, TIES-Merging explicitly aligns model representations while pruning redundant or conflicting parameters <Paper corpusId=\"276961298\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n\nTIES-Merging has been extensively evaluated across diverse settings, including various model architectures, sizes, domains, and numbers of tasks, consistently outperforming other merging techniques <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. Its effectiveness has made it a foundation for numerous subsequent merging techniques, and it is often used in combination with other approaches like DARE, which further enhances performance by randomly dropping additional parameters and rescaling the remaining ones <Paper corpusId=\"268537290\" paperTitle=\"(Akiba et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\nThe impact of TIES-Merging extends beyond its original implementation, with numerous subsequent approaches building upon its insights about parameter interference. For example, WIDEN (Weight Disentanglement) extends model merging techniques by disentangling model weights into magnitude and direction components <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>, while EMR-Merging applies task-specific masks and rescalers to further reduce interference <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>. Additionally, Model Breadcrumbs employs a dual masking strategy that removes both large outliers and small perturbations in task vectors <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nDespite its advantages, it's worth noting that in some specific applications, simpler approaches like Linear merging (equal weight averaging) have occasionally produced better results <Paper corpusId=\"278328055\" paperTitle=\"(Ociepa et al., 2025)\" isShortName></Paper> <Paper corpusId=\"247362886\" paperTitle=\"(Wortsman et al., 2022)\" isShortName></Paper>. However, TIES-Merging remains particularly valuable when combining models with potentially conflicting knowledge representations, especially when merging across diverse tasks or domains <Paper corpusId=\"271843401\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275921352\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Tao et al., 2024)",
                        "snippets": [
                            "In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;(Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}."
                        ],
                        "paper": {
                            "corpus_id": 271039884,
                            "title": "Unlocking the Potential of Model Merging for Low-Resource Languages",
                            "authors": [
                                {
                                    "authorId": "2053641126",
                                    "name": "Mingxu Tao"
                                },
                                {
                                    "authorId": "2111574159",
                                    "name": "Chen Zhang"
                                },
                                {
                                    "authorId": "2007771781",
                                    "name": "Quzhe Huang"
                                },
                                {
                                    "authorId": "2310297716",
                                    "name": "Tianyao Ma"
                                },
                                {
                                    "authorId": "2290972467",
                                    "name": "Songfang Huang"
                                },
                                {
                                    "authorId": "2258097237",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "2115387922",
                                    "name": "Yansong Feng"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 5
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Fang et al., 2025)",
                        "snippets": [
                            "Rather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference."
                        ],
                        "paper": {
                            "corpus_id": 276961298,
                            "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment",
                            "authors": [
                                {
                                    "authorId": "2263268557",
                                    "name": "Luyang Fang"
                                },
                                {
                                    "authorId": "2258714804",
                                    "name": "Ehsan Latif"
                                },
                                {
                                    "authorId": "2298332870",
                                    "name": "Haoran Lu"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2276478007",
                                    "name": "Ping Ma"
                                },
                                {
                                    "authorId": "2262445470",
                                    "name": "Xiaoming Zhai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.94189453125
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC."
                        ],
                        "paper": {
                            "corpus_id": 271050386,
                            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "81758928",
                                    "name": "Jinliang Lu"
                                },
                                {
                                    "authorId": "2310334594",
                                    "name": "Ziliang Pang"
                                },
                                {
                                    "authorId": "2310342808",
                                    "name": "Min Xiao"
                                },
                                {
                                    "authorId": "2290018211",
                                    "name": "Yaochen Zhu"
                                },
                                {
                                    "authorId": "2258711381",
                                    "name": "Rui Xia"
                                },
                                {
                                    "authorId": "2290006077",
                                    "name": "Jiajun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 27
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Akiba et al., 2024)",
                        "snippets": [
                            "A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."
                        ],
                        "paper": {
                            "corpus_id": 268537290,
                            "title": "Evolutionary Optimization of Model Merging Recipes",
                            "authors": [
                                {
                                    "authorId": "2292261829",
                                    "name": "Takuya Akiba"
                                },
                                {
                                    "authorId": "2205654861",
                                    "name": "Makoto Shing"
                                },
                                {
                                    "authorId": "2244138850",
                                    "name": "Yujin Tang"
                                },
                                {
                                    "authorId": "2326487319",
                                    "name": "Qi Sun"
                                },
                                {
                                    "authorId": "2292260083",
                                    "name": "David Ha"
                                }
                            ],
                            "year": 2024,
                            "venue": "Nat. Mac. Intell.",
                            "n_citations": 125
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                        ],
                        "paper": {
                            "corpus_id": 265034087,
                            "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
                            "authors": [
                                {
                                    "authorId": "2265527327",
                                    "name": "Le Yu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "46493167",
                                    "name": "Haiyang Yu"
                                },
                                {
                                    "authorId": "2257407873",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "1527090216",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 335
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Ociepa et al., 2025)",
                        "snippets": [
                            "To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) (Wortsman et al., 2022), TIES (TIES-Merging: Resolving Interference When Merging Models) (Yadav et al., 2023), Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training."
                        ],
                        "paper": {
                            "corpus_id": 278328055,
                            "title": "Bielik 11B v2 Technical Report",
                            "authors": [
                                {
                                    "authorId": "2327339379",
                                    "name": "Krzysztof Ociepa"
                                },
                                {
                                    "authorId": "2327338025",
                                    "name": "Lukasz Flis"
                                },
                                {
                                    "authorId": "2327337845",
                                    "name": "Krzysztof Wr'obel"
                                },
                                {
                                    "authorId": "2327337329",
                                    "name": "Adrian Gwo'zdziej"
                                },
                                {
                                    "authorId": "2327339098",
                                    "name": "Remigiusz Kinas"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.87060546875
                    },
                    {
                        "id": "(Wortsman et al., 2022)",
                        "snippets": [
                            "The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."
                        ],
                        "paper": {
                            "corpus_id": 247362886,
                            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
                            "authors": [
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "1387466862",
                                    "name": "S. Gadre"
                                },
                                {
                                    "authorId": "40458654",
                                    "name": "R. Roelofs"
                                },
                                {
                                    "authorId": "2158366935",
                                    "name": "Raphael Gontijo-Lopes"
                                },
                                {
                                    "authorId": "4690624",
                                    "name": "Ari S. Morcos"
                                },
                                {
                                    "authorId": "40281109",
                                    "name": "Hongseok Namkoong"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                },
                                {
                                    "authorId": "40464924",
                                    "name": "Simon Kornblith"
                                },
                                {
                                    "authorId": "152772922",
                                    "name": "Ludwig Schmidt"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1011
                        },
                        "score": 0
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency."
                        ],
                        "paper": {
                            "corpus_id": 271843401,
                            "title": "ProFuser: Progressive Fusion of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2257014771",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2217614543",
                                    "name": "Fanqi Wan"
                                },
                                {
                                    "authorId": "2258677979",
                                    "name": "Canbin Huang"
                                },
                                {
                                    "authorId": "2261363209",
                                    "name": "Xiaojun Quan"
                                },
                                {
                                    "authorId": "143971529",
                                    "name": "Chenliang Li"
                                },
                                {
                                    "authorId": "2114009661",
                                    "name": "Mingshi Yan"
                                },
                                {
                                    "authorId": "2281901929",
                                    "name": "Ji Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors."
                        ],
                        "paper": {
                            "corpus_id": 275921352,
                            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "2119293226",
                                    "name": "Wen Wang"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.91943359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Other Effective Parameter-Space Merging Techniques",
                "tldr": "Beyond TIES-Merging, several other effective parameter-space merging techniques have emerged, each with unique approaches to addressing knowledge conflicts. These include Fisher Merging, DARE, Task Arithmetic, Model Breadcrumbs, and other specialized techniques that leverage different mechanisms to reduce interference while preserving task-specific knowledge. (15 sources)",
                "text": "\n## Task Arithmetic (TA)\nTask Arithmetic introduced the concept of \"task vectors,\" defined as the difference between a fine-tuned model and its pre-trained version in parameter space. By combining these vectors through arithmetic operations, TA enables efficient knowledge transfer and capability steering. This approach allows models to inherit multiple task capabilities simultaneously by adding weighted task vectors to the pre-trained model parameters <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper>. Task Arithmetic provides a foundational framework for many subsequent merging techniques, demonstrating that model behaviors can be effectively steered through weight-space operations <Paper corpusId=\"277322544\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\n## Fisher Merging\nFisher Merging leverages the Fisher information matrix to estimate parameter importance during the merging process. Rather than treating all parameters equally, this technique approximates each model's posterior as a Gaussian distribution with a precision matrix corresponding to its Fisher information <Paper corpusId=\"244345933\" paperTitle=\"(Matena et al., 2021)\" isShortName></Paper>. This approach has shown performance improvements in robust fine-tuning and model ensembling scenarios, offering a more principled alternative to simple parameter averaging <Paper corpusId=\"271329267\" paperTitle=\"(Hagos et al., 2024)\" isShortName></Paper>. However, computing the Fisher information matrix becomes computationally and memory-intensive as model size increases <Paper corpusId=\"271843401\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>.\n\n## DARE (Drop And REscale)\nDARE addresses parameter redundancy by randomly dropping delta parameters (the difference between fine-tuned and pre-trained weights) and rescaling the remaining ones. This technique randomly zeros out a large percentage (up to 90-99%) of parameter changes and compensates by rescaling the remaining parameters to approximate the original embeddings <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>. DARE effectively mitigates merging conflicts while preserving model performance through essential unscaling operations <Paper corpusId=\"274423281\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>. It is particularly effective for large language models, where merged models can sometimes exceed the performance of any source model <Paper corpusId=\"271903078\" paperTitle=\"(Dehghan et al., 2024)\" isShortName></Paper>.\n\n## Model Breadcrumbs\nUnlike TIES-Merging which focuses on sign conflicts and low-magnitude parameters, Model Breadcrumbs employs a dual masking strategy that simultaneously removes both large outliers and small perturbations in task vectors. This balanced approach provides more noise-resistant parameter selection, ensuring that neither extreme values nor insignificant changes interfere with the merging process <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\n## Consensus Merging\nConsensus Merging identifies and eliminates \"selfish\" and \"catastrophic\" weights\u2014parameters that are important exclusively to one task but detrimental to multi-task fusion. This technique improves the general performance of existing model merging approaches while simultaneously enabling efficient model compression <Paper corpusId=\"274423281\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269757600\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## EMR-Merging\nEMR-Merging (Efficient Model Routing) enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction. It applies task-specific masks to filter conflicting signs and rescalers to adjust parameter magnitudes, adapting the merged model to different tasks during inference without additional training <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\n## WIDEN (Weight Disentanglement)\nWIDEN extends model merging techniques by disentangling model weights into magnitude and direction components. By quantifying the divergence of these components from the backbone model, WIDEN automatically determines the importance of each model in the merging process, eliminating the need for manually assigned scaling factors <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\n## RegMean and FroM\nRegMean transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model <Paper corpusId=\"271843401\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. Building upon this, FroM (Frobenius norm-based Merging) directly measures model parameters using the Frobenius norm without requiring training data, introducing an additional hyperparameter for controlling the merging process to alleviate task interference <Paper corpusId=\"279118713\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\n## Specialized Merging Techniques\nSeveral specialized techniques have emerged to address specific merging scenarios:\n- **Sens-Merging**: Analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n- **LoRS-Merging**: Combines low-rank structure and sparsity for multilingual speech models, minimizing parameter redundancy and conflicts <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n- **1bit-Merging**: Integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency, leveraging the observation that different tasks store knowledge in distinct model layers <Paper corpusId=\"276409347\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nMost of these techniques can be implemented as plug-and-play enhancements to existing merging methods, offering flexible approaches to address the knowledge conflict and task interference challenges inherent in parameter-space model merging <Paper corpusId=\"271329267\" paperTitle=\"(Hagos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277634330\" paperTitle=\"(Nayak et al., 2025)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Ilharco et al., 2022)",
                        "snippets": [
                            "Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."
                        ],
                        "paper": {
                            "corpus_id": 254408495,
                            "title": "Editing Models with Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "1387994137",
                                    "name": "Gabriel Ilharco"
                                },
                                {
                                    "authorId": "78846919",
                                    "name": "Marco Tulio Ribeiro"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "40895369",
                                    "name": "Suchin Gururangan"
                                },
                                {
                                    "authorId": "152772922",
                                    "name": "Ludwig Schmidt"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                },
                                {
                                    "authorId": "143787583",
                                    "name": "Ali Farhadi"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 520
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wu et al., 2025)",
                        "snippets": [
                            "Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge."
                        ],
                        "paper": {
                            "corpus_id": 277322544,
                            "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging",
                            "authors": [
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2345985527",
                                    "name": "Yuxuan Yao"
                                },
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2333317068",
                                    "name": "Zehua Liu"
                                },
                                {
                                    "authorId": "2221337060",
                                    "name": "Xiaojin Fu"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2344902525",
                                    "name": "Xing Li"
                                },
                                {
                                    "authorId": "2267558779",
                                    "name": "Hui-Ling Zhen"
                                },
                                {
                                    "authorId": "2332348570",
                                    "name": "Tao Zhong"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 14
                        },
                        "score": 0.8720703125
                    },
                    {
                        "id": "(Matena et al., 2021)",
                        "snippets": [
                            "Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."
                        ],
                        "paper": {
                            "corpus_id": 244345933,
                            "title": "Merging Models with Fisher-Weighted Averaging",
                            "authors": [
                                {
                                    "authorId": "1380243217",
                                    "name": "Michael Matena"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                }
                            ],
                            "year": 2021,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 402
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hagos et al., 2024)",
                        "snippets": [
                            "Techniques such as parameter averaging (Matena et al., 2021) or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks."
                        ],
                        "paper": {
                            "corpus_id": 271329267,
                            "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives",
                            "authors": [
                                {
                                    "authorId": "1930733",
                                    "name": "D. Hagos"
                                },
                                {
                                    "authorId": "2312327093",
                                    "name": "Rick Battle"
                                },
                                {
                                    "authorId": "2260694752",
                                    "name": "Danda B. Rawat"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE Transactions on Artificial Intelligence",
                            "n_citations": 26
                        },
                        "score": 0.9287109375
                    },
                    {
                        "id": "(Shi et al., 2024)",
                        "snippets": [
                            "Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency."
                        ],
                        "paper": {
                            "corpus_id": 271843401,
                            "title": "ProFuser: Progressive Fusion of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2257014771",
                                    "name": "Tianyuan Shi"
                                },
                                {
                                    "authorId": "2217614543",
                                    "name": "Fanqi Wan"
                                },
                                {
                                    "authorId": "2258677979",
                                    "name": "Canbin Huang"
                                },
                                {
                                    "authorId": "2261363209",
                                    "name": "Xiaojun Quan"
                                },
                                {
                                    "authorId": "143971529",
                                    "name": "Chenliang Li"
                                },
                                {
                                    "authorId": "2114009661",
                                    "name": "Mingshi Yan"
                                },
                                {
                                    "authorId": "2281901929",
                                    "name": "Ji Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.93359375
                    },
                    {
                        "id": "(Yu et al., 2023)",
                        "snippets": [
                            "In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."
                        ],
                        "paper": {
                            "corpus_id": 265034087,
                            "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch",
                            "authors": [
                                {
                                    "authorId": "2265527327",
                                    "name": "Le Yu"
                                },
                                {
                                    "authorId": "48613402",
                                    "name": "Yu Bowen"
                                },
                                {
                                    "authorId": "46493167",
                                    "name": "Haiyang Yu"
                                },
                                {
                                    "authorId": "2257407873",
                                    "name": "Fei Huang"
                                },
                                {
                                    "authorId": "1527090216",
                                    "name": "Yongbin Li"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 335
                        },
                        "score": 0
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Recently, Ilharco et al. (Ilharco et al., 2022) demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging (Yadav et al., 2023) resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE (Yu et al., 2023) mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging (Wang et al., 2024) eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality."
                        ],
                        "paper": {
                            "corpus_id": 274423281,
                            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
                            "authors": [
                                {
                                    "authorId": "2310819377",
                                    "name": "Feng Xiong"
                                },
                                {
                                    "authorId": "2315308131",
                                    "name": "Runxi Cheng"
                                },
                                {
                                    "authorId": "2333239207",
                                    "name": "Wang Chen"
                                },
                                {
                                    "authorId": "2316516702",
                                    "name": "Zhanqiu Zhang"
                                },
                                {
                                    "authorId": "2326995130",
                                    "name": "Yiwen Guo"
                                },
                                {
                                    "authorId": "2333235284",
                                    "name": "Chun Yuan"
                                },
                                {
                                    "authorId": "2333328707",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.87353515625
                    },
                    {
                        "id": "(Dehghan et al., 2024)",
                        "snippets": [
                            "In this paper, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods."
                        ],
                        "paper": {
                            "corpus_id": 271903078,
                            "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair",
                            "authors": [
                                {
                                    "authorId": "2203935492",
                                    "name": "Meghdad Dehghan"
                                },
                                {
                                    "authorId": "2304517862",
                                    "name": "Jie Jw Wu"
                                },
                                {
                                    "authorId": "2304472005",
                                    "name": "Fatemeh H. Fard"
                                },
                                {
                                    "authorId": "1994554",
                                    "name": "Ali Ouni"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.873046875
                    },
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve>99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."
                        ],
                        "paper": {
                            "corpus_id": 269757600,
                            "title": "Localizing Task Information for Improved Model Merging and Compression",
                            "authors": [
                                {
                                    "authorId": "2257324343",
                                    "name": "Ke Wang"
                                },
                                {
                                    "authorId": "153670517",
                                    "name": "Nikolaos Dimitriadis"
                                },
                                {
                                    "authorId": "2257000703",
                                    "name": "Guillermo Ortiz-Jim\u00e9nez"
                                },
                                {
                                    "authorId": "116272138",
                                    "name": "Franccois Fleuret"
                                },
                                {
                                    "authorId": "2256985187",
                                    "name": "Pascal Frossard"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 60
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al., 2025)",
                        "snippets": [
                            "In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem."
                        ],
                        "paper": {
                            "corpus_id": 279118713,
                            "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging",
                            "authors": [
                                {
                                    "authorId": "2239036328",
                                    "name": "Zijian Li"
                                },
                                {
                                    "authorId": "2674998",
                                    "name": "Xiaocheng Feng"
                                },
                                {
                                    "authorId": "2365043614",
                                    "name": "Huixin Liu"
                                },
                                {
                                    "authorId": "2118138548",
                                    "name": "Yi-Chong Huang"
                                },
                                {
                                    "authorId": "2274093523",
                                    "name": "Ting Liu"
                                },
                                {
                                    "authorId": "2257004102",
                                    "name": "Bing Qin"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.888671875
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."
                        ],
                        "paper": {
                            "corpus_id": 276422064,
                            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models.\n\nLoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model."
                        ],
                        "paper": {
                            "corpus_id": 276575632,
                            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
                            "authors": [
                                {
                                    "authorId": "2241702793",
                                    "name": "Qiuming Zhao"
                                },
                                {
                                    "authorId": "2107310187",
                                    "name": "Guangzhi Sun"
                                },
                                {
                                    "authorId": "2256775692",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "2241950375",
                                    "name": "Mingxing Xu"
                                },
                                {
                                    "authorId": "2241350908",
                                    "name": "Thomas Fang Zheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Liu et al._1, 2025)",
                        "snippets": [
                            "We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies."
                        ],
                        "paper": {
                            "corpus_id": 276409347,
                            "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2333317068",
                                    "name": "Zehua Liu"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9404296875
                    },
                    {
                        "id": "(Nayak et al., 2025)",
                        "snippets": [
                            "On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging."
                        ],
                        "paper": {
                            "corpus_id": 277634330,
                            "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
                            "authors": [
                                {
                                    "authorId": "2353385023",
                                    "name": "Nikhil Shivakumar Nayak"
                                },
                                {
                                    "authorId": "2273669923",
                                    "name": "Krishnateja Killamsetty"
                                },
                                {
                                    "authorId": "2249472971",
                                    "name": "Ligong Han"
                                },
                                {
                                    "authorId": "150894502",
                                    "name": "Abhishek Bhandwaldar"
                                },
                                {
                                    "authorId": "2290484410",
                                    "name": "Prateek Chanda"
                                },
                                {
                                    "authorId": "2261102120",
                                    "name": "Kai Xu"
                                },
                                {
                                    "authorId": "2336343895",
                                    "name": "Hao Wang"
                                },
                                {
                                    "authorId": "2288530668",
                                    "name": "Aldo Pareja"
                                },
                                {
                                    "authorId": "2354335460",
                                    "name": "Oleg Silkin"
                                },
                                {
                                    "authorId": "2166051120",
                                    "name": "Mustafa Eyceoz"
                                },
                                {
                                    "authorId": "2265502573",
                                    "name": "Akash Srivastava"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.93896484375
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Comparative Performance and Effectiveness",
                "tldr": "Empirical evaluations show that parameter-space merging techniques vary significantly in effectiveness across different contexts, with TIES-Merging and its variants generally outperforming simple averaging for multiple tasks, though simpler methods like Linear merging can sometimes excel in specific scenarios. (8 sources)",
                "text": "\nComparative analyses of parameter-space merging techniques reveal significant performance differences based on merging context, task diversity, and model architectures. Simple averaging methods, while straightforward to implement, often suffer from substantial performance degradation due to parameter interference, with studies showing performance drops of up to 10% when merging multiple models <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This performance degradation occurs primarily because weighted averaging approaches emphasize parameter importance but overlook task-specific properties of those parameters.\n\nTIES-Merging consistently demonstrates superior performance across diverse settings compared to simple averaging techniques. Its effectiveness stems from directly addressing the two primary sources of interference: redundant parameter values and sign conflicts <Paper corpusId=\"268537290\" paperTitle=\"(Akiba et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. Empirical evaluations show TIES-Merging outperforms earlier methods across multiple modalities, domains, task quantities, model sizes, and architectures <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nThe performance benefits of advanced merging techniques appear most pronounced when combining models from diverse tasks with potentially conflicting knowledge representations. The Localize-and-Stitch approach, which performs localized merging via sparse task arithmetic, demonstrates that much of the task-specific knowledge is concentrated in small regions of the parameter space. By stitching only these essential regions back onto the pretrained model, this approach significantly reduces task interference compared to global merging methods <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\nRecent analysis reveals that centering task vectors around their weight average implicitly reduces task interference, with most task-specific knowledge concentrated in the top singular vectors. Applying low-rank approximation to these centered task vectors significantly improves merging performance by further reducing conflicts <Paper corpusId=\"274788575\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>.\n\nSensitivity-guided approaches like Sens-Merging enhance existing task vector-based merging techniques by moving beyond uniform coefficients. By analyzing parameter sensitivity both within individual tasks and across different tasks, these methods determine optimal merging coefficients that account for varying parameter importance, leading to more effective knowledge integration <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nInterestingly, despite the theoretical advantages of sophisticated merging techniques, simpler approaches can sometimes yield superior results in specific contexts. For instance, when merging models obtained from different phases of the same training process, Linear merging with equal weights has produced the most favorable outcomes in some Polish language benchmarks <Paper corpusId=\"278328055\" paperTitle=\"(Ociepa et al., 2025)\" isShortName></Paper>. This suggests that the optimal merging strategy may depend on the specific characteristics of the models being combined, including their training history and the degree of task diversity.\n\nThe comparative effectiveness of different merging techniques also varies based on inference requirements. Methods like EMR-Merging enable tuning-free model adaptation across multiple tasks during inference by applying task-specific masks and rescalers, achieving high accuracy across vision, NLP, and multi-modal models without additional training <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nWhen considering the trade-off between performance and implementation complexity, TIES-Merging and its derivatives (such as when combined with DARE) offer an attractive balance for most general merging scenarios. However, the optimal choice ultimately depends on specific application requirements, the diversity of tasks being merged, and computational constraints.",
                "citations": [
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC."
                        ],
                        "paper": {
                            "corpus_id": 271050386,
                            "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models",
                            "authors": [
                                {
                                    "authorId": "81758928",
                                    "name": "Jinliang Lu"
                                },
                                {
                                    "authorId": "2310334594",
                                    "name": "Ziliang Pang"
                                },
                                {
                                    "authorId": "2310342808",
                                    "name": "Min Xiao"
                                },
                                {
                                    "authorId": "2290018211",
                                    "name": "Yaochen Zhu"
                                },
                                {
                                    "authorId": "2258711381",
                                    "name": "Rui Xia"
                                },
                                {
                                    "authorId": "2290006077",
                                    "name": "Jiajun Zhang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 27
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Akiba et al., 2024)",
                        "snippets": [
                            "A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."
                        ],
                        "paper": {
                            "corpus_id": 268537290,
                            "title": "Evolutionary Optimization of Model Merging Recipes",
                            "authors": [
                                {
                                    "authorId": "2292261829",
                                    "name": "Takuya Akiba"
                                },
                                {
                                    "authorId": "2205654861",
                                    "name": "Makoto Shing"
                                },
                                {
                                    "authorId": "2244138850",
                                    "name": "Yujin Tang"
                                },
                                {
                                    "authorId": "2326487319",
                                    "name": "Qi Sun"
                                },
                                {
                                    "authorId": "2292260083",
                                    "name": "David Ha"
                                }
                            ],
                            "year": 2024,
                            "venue": "Nat. Mac. Intell.",
                            "n_citations": 125
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Yadav et al., 2023)",
                        "snippets": [
                            "To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."
                        ],
                        "paper": {
                            "corpus_id": 259064039,
                            "title": "TIES-Merging: Resolving Interference When Merging Models",
                            "authors": [
                                {
                                    "authorId": "46841632",
                                    "name": "Prateek Yadav"
                                },
                                {
                                    "authorId": "1390031652",
                                    "name": "Derek Tam"
                                },
                                {
                                    "authorId": "41019330",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2402716",
                                    "name": "Colin Raffel"
                                },
                                {
                                    "authorId": "143977268",
                                    "name": "Mohit Bansal"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 317
                        },
                        "score": 0.939453125
                    },
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation",
                            "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."
                        ],
                        "paper": {
                            "corpus_id": 271957310,
                            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "2182670937",
                                    "name": "Yifei He"
                                },
                                {
                                    "authorId": "2317078449",
                                    "name": "Yuzheng Hu"
                                },
                                {
                                    "authorId": "2292270783",
                                    "name": "Yong Lin"
                                },
                                {
                                    "authorId": "2306841244",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "2283183420",
                                    "name": "Han Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 25
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Choi et al., 2024)",
                        "snippets": [
                            "Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors."
                        ],
                        "paper": {
                            "corpus_id": 274788575,
                            "title": "Revisiting Weight Averaging for Model Merging",
                            "authors": [
                                {
                                    "authorId": "2336394540",
                                    "name": "Jiho Choi"
                                },
                                {
                                    "authorId": "122204266",
                                    "name": "Donggyun Kim"
                                },
                                {
                                    "authorId": "2335812409",
                                    "name": "Chanhyuk Lee"
                                },
                                {
                                    "authorId": "2335816260",
                                    "name": "Seunghoon Hong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.88916015625
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."
                        ],
                        "paper": {
                            "corpus_id": 276422064,
                            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Ociepa et al., 2025)",
                        "snippets": [
                            "To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) (Wortsman et al., 2022), TIES (TIES-Merging: Resolving Interference When Merging Models) (Yadav et al., 2023), Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training."
                        ],
                        "paper": {
                            "corpus_id": 278328055,
                            "title": "Bielik 11B v2 Technical Report",
                            "authors": [
                                {
                                    "authorId": "2327339379",
                                    "name": "Krzysztof Ociepa"
                                },
                                {
                                    "authorId": "2327338025",
                                    "name": "Lukasz Flis"
                                },
                                {
                                    "authorId": "2327337845",
                                    "name": "Krzysztof Wr'obel"
                                },
                                {
                                    "authorId": "2327337329",
                                    "name": "Adrian Gwo'zdziej"
                                },
                                {
                                    "authorId": "2327339098",
                                    "name": "Remigiusz Kinas"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.87060546875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Recent Advancements in Addressing Knowledge Conflicts",
                "tldr": "Recent advancements in parameter-space merging focus on identifying and mitigating specific sources of knowledge conflicts through techniques like localized merging, sensitivity-guided coefficient adjustment, and feature-centric conflict resolution. These approaches move beyond uniform coefficient strategies to address task interference at different granularity levels, from parameter-specific to feature-level conflicts. (8 sources)",
                "text": "\nRecent research in parameter-space model merging has shifted toward more targeted approaches for addressing knowledge conflicts and task interference. These advancements build upon foundational techniques like TIES-Merging and Task Arithmetic while introducing more sophisticated mechanisms for conflict resolution.\n\n## Localized Merging for Reduced Task Interference\n\nOne significant advancement comes from the Localize-and-Stitch approach, which challenges the practice of global merging by recognizing that task-specific knowledge is often concentrated in small regions of parameter space. This method first identifies tiny regions in fine-tuned models that contain essential skills acquired during fine-tuning and stitches only those regions back onto the pre-trained model. By reducing the overlap between localized regions, this approach decreases task conflicts and enhances overall performance. The key insight is that \"including more parameters than necessary only introduces additional task interference\" as long as the localized region contains sufficient task-specific knowledge <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\n## Parameter Selection Based on Consensus\n\nConsensus Merging represents another significant advancement that identifies and eliminates \"selfish\" and \"catastrophic\" weights\u2014parameters that are important exclusively to one task but detrimental to multi-task fusion. This approach reveals that \"different tasks mostly use non-overlapping sets of weights,\" suggesting that multi-task performance degradation stems from specific problematic parameters rather than general incompatibility <Paper corpusId=\"274423281\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269757600\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. By removing these problematic weights, Consensus Merging consistently improves existing model merging approaches while simultaneously enabling model compression.\n\n## Sensitivity-Guided Coefficient Adjustment\n\nTraditional merging techniques often apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. Sens-Merging addresses this limitation by analyzing parameter sensitivity at two levels:\n\n1. Within individual tasks: Identifying critical layers that significantly impact task performance\n2. Across different tasks: Prioritizing models that enhance the performance of others\n\nThis sensitivity-guided approach provides \"fine-grained control over parameter importance\" during merging, enhancing existing task vector-based methods by making them more adaptive to the specific importance of different parameters <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\n## Modality-Specific Merging Techniques\n\nRecent work has also extended parameter-space merging to new modalities. LoRS-Merging represents the first application of model merging techniques to speech models, exploiting the combination of low-rank structure and sparsity of language-specific and task-specific weights. This approach minimizes parameter redundancy and conflicts while providing an efficient way to incorporate new knowledge from task or language-specialized models <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\n## Feature-Centric Conflict Resolution\n\nMoving beyond parameter-level interventions, Conflict-Aware Task Merging (CAT Merging) introduces a feature-centric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. This approach analyzes task vector components layer by layer and applies tailored operations for different parameter types: feature projection for linear weights and masking for normalization scalers and shifts. This strategy effectively mitigates knowledge conflicts in a training-free manner, requiring only a lightweight forward pass with a few unlabeled examples <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>.\n\n## Enhanced Parameter Representation\n\nWeight Disentanglement (WIDEN) extends model merging techniques by separating model weights into magnitude and direction components. By quantifying the divergence of these components from the backbone model, WIDEN automatically determines each model's importance in the merging process, eliminating the need for manually assigned scaling factors <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>. This approach demonstrates that the orthogonality among task vectors is key to improving performance in model merging <Paper corpusId=\"275921352\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\n## Inference-Time Adaptation\n\nEMR-Merging enables tuning-free model adaptation across multiple tasks during inference by selecting the maximum absolute value of each parameter while preserving the dominant sign direction. This method applies task-specific masks to filter conflicting signs and rescalers to adjust parameter magnitudes, adapting the merged model to different tasks during inference without additional training <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nThese recent advancements collectively demonstrate a shift toward more nuanced, targeted approaches to knowledge conflict resolution in parameter-space model merging. By addressing conflicts at different levels of granularity\u2014from parameter-specific to feature-level conflicts\u2014these techniques provide more effective solutions for combining specialized language models while preserving their distinct capabilities.",
                "citations": [
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation",
                            "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."
                        ],
                        "paper": {
                            "corpus_id": 271957310,
                            "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
                            "authors": [
                                {
                                    "authorId": "2182670937",
                                    "name": "Yifei He"
                                },
                                {
                                    "authorId": "2317078449",
                                    "name": "Yuzheng Hu"
                                },
                                {
                                    "authorId": "2292270783",
                                    "name": "Yong Lin"
                                },
                                {
                                    "authorId": "2306841244",
                                    "name": "Tong Zhang"
                                },
                                {
                                    "authorId": "2283183420",
                                    "name": "Han Zhao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 25
                        },
                        "score": 0.92138671875
                    },
                    {
                        "id": "(Xiong et al., 2024)",
                        "snippets": [
                            "Recently, Ilharco et al. (Ilharco et al., 2022) demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging (Yadav et al., 2023) resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE (Yu et al., 2023) mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging (Wang et al., 2024) eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality."
                        ],
                        "paper": {
                            "corpus_id": 274423281,
                            "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement",
                            "authors": [
                                {
                                    "authorId": "2310819377",
                                    "name": "Feng Xiong"
                                },
                                {
                                    "authorId": "2315308131",
                                    "name": "Runxi Cheng"
                                },
                                {
                                    "authorId": "2333239207",
                                    "name": "Wang Chen"
                                },
                                {
                                    "authorId": "2316516702",
                                    "name": "Zhanqiu Zhang"
                                },
                                {
                                    "authorId": "2326995130",
                                    "name": "Yiwen Guo"
                                },
                                {
                                    "authorId": "2333235284",
                                    "name": "Chun Yuan"
                                },
                                {
                                    "authorId": "2333328707",
                                    "name": "Ruifeng Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.87353515625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve>99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."
                        ],
                        "paper": {
                            "corpus_id": 269757600,
                            "title": "Localizing Task Information for Improved Model Merging and Compression",
                            "authors": [
                                {
                                    "authorId": "2257324343",
                                    "name": "Ke Wang"
                                },
                                {
                                    "authorId": "153670517",
                                    "name": "Nikolaos Dimitriadis"
                                },
                                {
                                    "authorId": "2257000703",
                                    "name": "Guillermo Ortiz-Jim\u00e9nez"
                                },
                                {
                                    "authorId": "116272138",
                                    "name": "Franccois Fleuret"
                                },
                                {
                                    "authorId": "2256985187",
                                    "name": "Pascal Frossard"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 60
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."
                        ],
                        "paper": {
                            "corpus_id": 276422064,
                            "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2305720492",
                                    "name": "Shuqi Liu"
                                },
                                {
                                    "authorId": "2346255376",
                                    "name": "Han Wu"
                                },
                                {
                                    "authorId": "2276605422",
                                    "name": "Bowei He"
                                },
                                {
                                    "authorId": "2148635550",
                                    "name": "Xiongwei Han"
                                },
                                {
                                    "authorId": "2347282055",
                                    "name": "Mingxuan Yuan"
                                },
                                {
                                    "authorId": "2257556686",
                                    "name": "Linqi Song"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.98046875
                    },
                    {
                        "id": "(Zhao et al., 2025)",
                        "snippets": [
                            "We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models.\n\nLoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model."
                        ],
                        "paper": {
                            "corpus_id": 276575632,
                            "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation",
                            "authors": [
                                {
                                    "authorId": "2241702793",
                                    "name": "Qiuming Zhao"
                                },
                                {
                                    "authorId": "2107310187",
                                    "name": "Guangzhi Sun"
                                },
                                {
                                    "authorId": "2256775692",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "2241950375",
                                    "name": "Mingxing Xu"
                                },
                                {
                                    "authorId": "2241350908",
                                    "name": "Thomas Fang Zheng"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Sun et al._1, 2025)",
                        "snippets": [
                            "Motivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars."
                        ],
                        "paper": {
                            "corpus_id": 278501405,
                            "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.91552734375
                    },
                    {
                        "id": "(Ruan et al., 2025)",
                        "snippets": [
                            "TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."
                        ],
                        "paper": {
                            "corpus_id": 276937513,
                            "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches",
                            "authors": [
                                {
                                    "authorId": "2321405806",
                                    "name": "Wei Ruan"
                                },
                                {
                                    "authorId": "2263682353",
                                    "name": "Tianze Yang"
                                },
                                {
                                    "authorId": "2325891087",
                                    "name": "Yifan Zhou"
                                },
                                {
                                    "authorId": "2349736445",
                                    "name": "Tianming Liu"
                                },
                                {
                                    "authorId": "2331910055",
                                    "name": "Jin Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.91650390625
                    },
                    {
                        "id": "(Sun et al., 2025)",
                        "snippets": [
                            "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors."
                        ],
                        "paper": {
                            "corpus_id": 275921352,
                            "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
                            "authors": [
                                {
                                    "authorId": "35640834",
                                    "name": "Wenju Sun"
                                },
                                {
                                    "authorId": "2262408434",
                                    "name": "Qingyong Li"
                                },
                                {
                                    "authorId": "2119293226",
                                    "name": "Wen Wang"
                                },
                                {
                                    "authorId": "8010931",
                                    "name": "Yangli-ao Geng"
                                },
                                {
                                    "authorId": "2342563128",
                                    "name": "Boyang Li"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.91943359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.214122
    }
}