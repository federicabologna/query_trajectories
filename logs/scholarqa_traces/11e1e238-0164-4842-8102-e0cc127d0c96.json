{
    "query": "What empirical evidence exists on catastrophic forgetting or performance degradation in LLMs after fine-tuning for alignment or factuality tasks?",
    "user_id": "lib_user",
    "task_id": "11e1e238-0164-4842-8102-e0cc127d0c96",
    "timestamp": "2025-06-23T21:20:40.877930",
    "n_retrieval": 256,
    "n_retrieved": 269,
    "n_candidates": 31,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.374382,
    "decomposed_query": {
        "rewritten_query": "Empirical evidence on catastrophic forgetting or performance degradation in LLMs after fine-tuning for alignment or factuality tasks.",
        "keyword_query": "catastrophic forgetting performance degradation LLMs fine-tuning alignment factuality",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009921,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "venue": "arXiv.org",
            "year": 2023,
            "reference_count": 41,
            "citation_count": 318,
            "influential_citation_count": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2308.08747",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2308.08747, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2181146702",
                    "name": "Yun Luo"
                },
                {
                    "authorId": "2149231975",
                    "name": "Zhen Yang"
                },
                {
                    "authorId": "33427918",
                    "name": "Fandong Meng"
                },
                {
                    "authorId": "2110450452",
                    "name": "Yafu Li"
                },
                {
                    "authorId": "48128428",
                    "name": "Jie Zhou"
                },
                {
                    "authorId": "2167740183",
                    "name": "Yue Zhang"
                }
            ],
            "abstract": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.",
            "corpus_id": 261031244,
            "sentences": [
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "In this study, we conducted an empirical investigation into the catastrophic forgetting (CF) phenomenon experienced by large language models (LLMs) during continual instruction tuning. Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills. Furthermore, our comparative analysis showed that the decoder-only model, BLOOMZ, demonstrates a superior ability to retain knowledge and skills during continual fine-tuning when compared to the encoder-decoder model, mT0. Additionally, we discovered that employing general instruction tuning techniques may help alleviate the CF problem in LLMs. Our empirical study suggests that exploring more effective methods to mitigate CF in LLMs during continual fine-tuning is a promising research direction. When applying LLMs, practitioners should remain vigilant and pay close attention to the issue of knowledge forgetting that may occur after instruction tuning. Addressing this challenge is crucial to ensure the reliable and consistent performance of LLMs in real-world applications.",
                    "score": 0.7275733955580117,
                    "section_title": "Conclusion",
                    "char_start_offset": 22052,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 295
                        },
                        {
                            "start": 296,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 682
                        },
                        {
                            "start": 683,
                            "end": 806
                        },
                        {
                            "start": 807,
                            "end": 960
                        },
                        {
                            "start": 961,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1242
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93212890625
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Scialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning. However, their analysis primarily focuses on the forgetting phenomenon observed in downstream tasks, such as summarization and style transfer. The evolution of general knowledge stored in the original pre-trained LLMs during instruction tuning remains unexplored. Luo et al. (2023b) conduct an analysis using probing methods on pre-trained language models to examine the problem of generality destruction and general knowledge forgetting during continual fine-tuning. Nevertheless, their study is restricted to encoderonly models and classification tasks. In this work, we draw attention to the following fundamental questions regarding forgetting in generative LLMs: \n\n1. Are the general knowledge stored in LLMs forgotten during continual instruction tuning? 2. What types of general knowledge are saliently forgotten? 3. What are the effects of model scales, model architectures, and general instruction tuning in the forgetting problem? \n\nTo address these questions, we conduct an empirical study on various LLLMs, such BLOOMZ, mT0 (Muennighoff et al., 2022), LLAMA Touvron et al. (2023), and ALPACA Taori et al. (2023) to analyze the catastrophic forgetting (CF) problem during continual instruction tuning. We continually train the original LLMs with five instruction tasks and evaluate the retention of the general knowledge within the model from three perspectives: domain knowledge, reasoning, and reading comprehension. Furthermore, we investigate the evolution of bias in LLMs throughout the tuning process. To gain insights into the effect of model architecture, we compare the performance of BLOOMZ with that of mT0 (Muennighoff et al., 2022) (an encoder-decoder model), which is fine-tuned using similar datasets. We also investigate the impact of general instruction tuning on the CF problem by comparing the performance of the initial model with the instruction-tuned version such as (BLOOM, BLOOMZ) and (LLAMA (Touvron et al., 2023), ALPACA (Taori et al., 2023)). \n\nOur findings reveal that the forgetting problem is generally present in LLMs.",
                    "score": 0.6788799324231951,
                    "section_title": "Introduction",
                    "char_start_offset": 2002,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 97
                        },
                        {
                            "start": 98,
                            "end": 240
                        },
                        {
                            "start": 241,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 653
                        },
                        {
                            "start": 654,
                            "end": 765
                        },
                        {
                            "start": 768,
                            "end": 918
                        },
                        {
                            "start": 919,
                            "end": 1038
                        },
                        {
                            "start": 1041,
                            "end": 1310
                        },
                        {
                            "start": 1311,
                            "end": 1527
                        },
                        {
                            "start": 1528,
                            "end": 1616
                        },
                        {
                            "start": 1617,
                            "end": 1825
                        },
                        {
                            "start": 1826,
                            "end": 2078
                        },
                        {
                            "start": 2081,
                            "end": 2158
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.86865234375
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning. Additionally, we observe that the bias in LLMs is mitigated throughout the continual instruction tuning process. When comparing BLOOMZ with mT0 at comparable model scale, we find that BLOOMZ experiences a relatively milder forgetting problem, suggesting that the decoder-only architecture may be better at retaining information during continual instruction tuning. Lastly, empirical results on LLAMA and its instruction-tuned version (i.e., ALPACA) indicate that diverse instruction tuning can help alleviate the CF phenomenon for LLMs in further continual fine-tuning. \n\nThe contribution of our paper can be summarized as follows: \n\n1. We take an initial step to analyze the catastrophic forgetting (CF) problem during continual instruction tuning by an empirically study, where a specific evaluation setting is designed from the perspective of general knowledge such as domain knowledge, reasoning, reading comprehension and the bias problem. 2. We provide an initial research evidence that the CF problem generally exists in the continual instruction tuning process for different models such as BLOOMZ, mT0, LLAMA and ALPACA. We also show that the model architecture, model scale and general instruction tuning have different effects on the CF problem.",
                    "score": 0.6429140039115573,
                    "section_title": "Introduction",
                    "char_start_offset": 4083,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 77
                        },
                        {
                            "start": 78,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 408
                        },
                        {
                            "start": 409,
                            "end": 521
                        },
                        {
                            "start": 522,
                            "end": 773
                        },
                        {
                            "start": 774,
                            "end": 978
                        },
                        {
                            "start": 981,
                            "end": 1040
                        },
                        {
                            "start": 1043,
                            "end": 1353
                        },
                        {
                            "start": 1354,
                            "end": 1537
                        },
                        {
                            "start": 1538,
                            "end": 1664
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.865234375
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.",
                    "score": 0.7292896464464648,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8603515625
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Firstly, Figure 3 shows the performance of the continually trained BLOOMZ-7.1b model on the instruction tasks compared to the initial model's performance. For example, after being trained on the Simp training data, the BLOOMZ model achieves a SARI score of 47.24 on the Simp test set, which is 1.98 higher than the initial model's score. Subsequently, when we continually train the model on the Emdg dataset, the obtained model achieves a BLEU score of 53.3, surpassing the initial model's performance by 3.62. These improvements demonstrate that the model can benefit from the instruction tuning process and achieve significantly better performance on the instruction tasks. Additional results for other LLMs are presented in Appendix B. \n\nNext, Figure 2 displays the FG values of BLOOMZ-1.1b and BLOOMZ-7.1B. As we can observe, the performance gradually decreases as we continually tune the model with instruction tasks. For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting. We can also notice that the performance of the BLOOMZ-7.1b model drops more drastically in these evaluation tasks, The main results of forgetting are reported in Table 3. We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively. Interestingly, we observe that the FG values for bias (Table 4) are mostly above zero in the experiments, which suggests that model biases, such as those related to race, color, gender, and so on, are mitigated during continual instruction tuning. For instance, in sentences describing physical appearance, BLOOMZ-7.1b",
                    "score": 0.5161712409673523,
                    "section_title": "Main Results",
                    "char_start_offset": 15167,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 79,
                            "end": 154
                        },
                        {
                            "start": 155,
                            "end": 337
                        },
                        {
                            "start": 338,
                            "end": 510
                        },
                        {
                            "start": 511,
                            "end": 675
                        },
                        {
                            "start": 676,
                            "end": 738
                        },
                        {
                            "start": 741,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 810
                        },
                        {
                            "start": 811,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1055
                        },
                        {
                            "start": 1056,
                            "end": 1205
                        },
                        {
                            "start": 1206,
                            "end": 1316
                        },
                        {
                            "start": 1317,
                            "end": 1375
                        },
                        {
                            "start": 1376,
                            "end": 1487
                        },
                        {
                            "start": 1488,
                            "end": 1680
                        },
                        {
                            "start": 1681,
                            "end": 1781
                        },
                        {
                            "start": 1782,
                            "end": 1823
                        },
                        {
                            "start": 1824,
                            "end": 1835
                        },
                        {
                            "start": 1836,
                            "end": 1927
                        },
                        {
                            "start": 1928,
                            "end": 2175
                        },
                        {
                            "start": 2176,
                            "end": 2246
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.765625
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Instruction Tuning. Instruction tuning has proven to be effective in aligning responses from pre-trained language models with human intents or preferences (Ouyang et al., 2022;Stiennon et al., 2020;Min et al., 2021). This technique refines a model's ability to predict a specific response to a given prompt, which may optionally include an instruction that outlines a task for the model. Examples of such models include T0 (Sanh et al., 2021), mT0 (Muennighoff et al., 2022), and BLOOMZ (Muennighoff et al., 2022). It has been demonstrated that instruction tuning can enhance the ability of language models to generalize to unseen tasks without prior exposure (Wei et al., 2021;Sanh et al., 2021). In this work, we focus on fine-tuning LLMs in a continual manner and analyze the catastrophic forgetting (CF) phenomenon during training. Specifically, instructions for a particular type of task (such as generating headlines) are used to tune the LLMs in each training phase, and the model does not have access to previously learned tasks. \n\nEvaluation of CF in Continual Learning. Various training strategies have been proposed to address the problem of catastrophic forgetting (CF) in continual learning (Riemer et al., 2019;Buzzega et al., 2020;Ke et al., 2022;Chen et al., 2022;Luo et al., 2023a). Previous studies have primarily measured CF by evaluating the performance decrease in previously learned tasks during continual learning or the average performance of learned tasks at the end of training. However, Davari et al. (2022) discovered that even when the model performance on previously learned tasks is preserved, the representations still suffer from significant drift due to parameter updates. As a result, they propose using an optimal linear classifier of learned tasks to measure performance, with changes considered as a surrogate to quantify CF. Similarly, Wu et al. (2022) employs layer-wise and task-wise probing to analyze CF in each layer for previously learned tasks. Luo et al. (2023b) propose using a series of probing tasks to evaluate the knowledge stored in LLMs and analyze the generality of the models.",
                    "score": 0.5650253918205671,
                    "section_title": "Related Work",
                    "char_start_offset": 5764,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 19
                        },
                        {
                            "start": 20,
                            "end": 216
                        },
                        {
                            "start": 217,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 514
                        },
                        {
                            "start": 515,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 835
                        },
                        {
                            "start": 836,
                            "end": 1037
                        },
                        {
                            "start": 1040,
                            "end": 1079
                        },
                        {
                            "start": 1080,
                            "end": 1299
                        },
                        {
                            "start": 1300,
                            "end": 1504
                        },
                        {
                            "start": 1505,
                            "end": 1706
                        },
                        {
                            "start": 1707,
                            "end": 1863
                        },
                        {
                            "start": 1864,
                            "end": 1990
                        },
                        {
                            "start": 1991,
                            "end": 2132
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 155,
                            "end": 176,
                            "matchedPaperCorpusId": "246426909"
                        },
                        {
                            "start": 176,
                            "end": 198,
                            "matchedPaperCorpusId": "221665105"
                        },
                        {
                            "start": 1204,
                            "end": 1225,
                            "matchedPaperCorpusId": "53100211"
                        },
                        {
                            "start": 1514,
                            "end": 1534,
                            "matchedPaperCorpusId": "247748982"
                        },
                        {
                            "start": 1875,
                            "end": 1891,
                            "matchedPaperCorpusId": "247717213"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7119140625
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Initial Simp Emdg InqQG Exp HGen Initial Simp Emdg InqQG Exp HGen Initial Simp Emdg InqQG Exp HGen significant forgetting in the first instruction tuning, which suggests that models without general instruction tuning may have less ability to retain knowledge during continual fine-tuning. The better retention of knowledge implies that general instruction tuning can mitigate catastrophic forgetting in LLMs during further continual fine-tuning. This finding highlights the importance of general instruction tuning in preserving the acquired knowledge and skills of LLMs when they undergo subsequent task-specific fine-tuning. \n\nTo further demonstrate the effect of general instruction tuning, we mix 10,000 general instruction data samples from ALPACA (Taori et al., 2023) with the continual instruction tasks to train the LLAMA-7b model. For the sake of brevity, we present the performance of one data split from each evaluation set (MMLU-human, Hellaswag, and Race-middle) to illustrate the effect in Figure 6. The results clearly show that the forgetting during continual instruction tuning can be mitigated to a certain extent by incorporating general instruction data. For instance, the performance of MMLU-human in the initial LLAMA-7b model is 34.72%, but it decreases to 26.8% when trained solely on the instruction data. However, when trained on the mixed data, the performance becomes 30%. These findings further further show that general instruction tuning can help alleviate the CF problem encountered during continual instruction tuning.",
                    "score": 0.5380658725166665,
                    "section_title": "ALPACA-7B",
                    "char_start_offset": 20486,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 288
                        },
                        {
                            "start": 289,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 626
                        },
                        {
                            "start": 629,
                            "end": 839
                        },
                        {
                            "start": 840,
                            "end": 1013
                        },
                        {
                            "start": 1014,
                            "end": 1174
                        },
                        {
                            "start": 1175,
                            "end": 1330
                        },
                        {
                            "start": 1331,
                            "end": 1400
                        },
                        {
                            "start": 1401,
                            "end": 1551
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.693359375
                },
                {
                    "corpus_id": "261031244",
                    "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                    "text": "Large language models (LLMs) have demonstrated impressive general capabilities in tackling a wide range of tasks (Bubeck et al., 2023). However, when it comes to real-world applications, users often find that certain specific abilities need enhancement. In such cases, relevant task-specific data are adopted to fine-tune the model in instructions to improve its performance in particular tasks (Touvron et al., 2023;Scialom et al., 2022). However, the widely used LLMs such as LLAMA-Chat (Touvron et al., 2023), ChatGPT1 , and Claude-22 have been fine-tuned, and the continual instruction tuning in specific tasks may affect the general knowledge stored in LLMs since the original training data are inaccessible. The study of continual instruction tuning is crucial for the practical application of LLMs. For example, in Table 1, an LLM is first fine-tuned for the task of title generation and then learns to generate an explanation. This scenario is related to the continual learning paradigm in machine learning, where a model is trained on a sequence of tasks (Lopez-Paz & Ranzato, 2017;Wu et al., 2022;Ke & Liu, 2022). A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge due to parameter updates during the learning of new tasks, leading to decreased performance on earlier tasks (Li & Hoiem, 2017;Lopez-Paz & Ranzato, 2017). \n\nInstruction: Make a title for this article with 'computer': japan's NEC corp. and UNK computer corp. of the United States said Wednesday they had agreed to join forces ... Output: NEC UNK in computer sales tie-up. Instruction: Explain why the two following sentences are unrelated: Sentence 1: 'This church choir sings to the masses as they sing joyous songs from the book at a church'; Sentence 2: 'The church has cracks in the ceiling.' Output: There is no indication that there are cracks in the ceiling of the church. \n\nTable 1: Fine-tuning examples in an instruction form. Scialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning.",
                    "score": 0.5961608561003537,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 253
                        },
                        {
                            "start": 254,
                            "end": 439
                        },
                        {
                            "start": 440,
                            "end": 713
                        },
                        {
                            "start": 714,
                            "end": 805
                        },
                        {
                            "start": 806,
                            "end": 934
                        },
                        {
                            "start": 935,
                            "end": 1123
                        },
                        {
                            "start": 1124,
                            "end": 1406
                        },
                        {
                            "start": 1409,
                            "end": 1622
                        },
                        {
                            "start": 1623,
                            "end": 1847
                        },
                        {
                            "start": 1848,
                            "end": 1930
                        },
                        {
                            "start": 1933,
                            "end": 1986
                        },
                        {
                            "start": 1987,
                            "end": 2084
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 417,
                            "end": 438,
                            "matchedPaperCorpusId": "252815378"
                        },
                        {
                            "start": 1091,
                            "end": 1107,
                            "matchedPaperCorpusId": "247717213"
                        },
                        {
                            "start": 1361,
                            "end": 1379,
                            "matchedPaperCorpusId": "4853851"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6748046875
                }
            ],
            "relevance_judgement": 0.93212890625,
            "relevance_judgment_input_expanded": "# Title: An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning\n# Venue: arXiv.org\n# Authors: Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, Yue Zhang\n## Abstract\nCatastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.\n## Introduction\nLarge language models (LLMs) have demonstrated impressive general capabilities in tackling a wide range of tasks (Bubeck et al., 2023). However, when it comes to real-world applications, users often find that certain specific abilities need enhancement. In such cases, relevant task-specific data are adopted to fine-tune the model in instructions to improve its performance in particular tasks (Touvron et al., 2023;Scialom et al., 2022). However, the widely used LLMs such as LLAMA-Chat (Touvron et al., 2023), ChatGPT1 , and Claude-22 have been fine-tuned, and the continual instruction tuning in specific tasks may affect the general knowledge stored in LLMs since the original training data are inaccessible. The study of continual instruction tuning is crucial for the practical application of LLMs. For example, in Table 1, an LLM is first fine-tuned for the task of title generation and then learns to generate an explanation. This scenario is related to the continual learning paradigm in machine learning, where a model is trained on a sequence of tasks (Lopez-Paz & Ranzato, 2017;Wu et al., 2022;Ke & Liu, 2022). A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge due to parameter updates during the learning of new tasks, leading to decreased performance on earlier tasks (Li & Hoiem, 2017;Lopez-Paz & Ranzato, 2017). \n\nInstruction: Make a title for this article with 'computer': japan's NEC corp. and UNK computer corp. of the United States said Wednesday they had agreed to join forces ... Output: NEC UNK in computer sales tie-up. Instruction: Explain why the two following sentences are unrelated: Sentence 1: 'This church choir sings to the masses as they sing joyous songs from the book at a church'; Sentence 2: 'The church has cracks in the ceiling.' Output: There is no indication that there are cracks in the ceiling of the church. \n\nTable 1: Fine-tuning examples in an instruction form. Scialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning.\n...\nScialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning. However, their analysis primarily focuses on the forgetting phenomenon observed in downstream tasks, such as summarization and style transfer. The evolution of general knowledge stored in the original pre-trained LLMs during instruction tuning remains unexplored. Luo et al. (2023b) conduct an analysis using probing methods on pre-trained language models to examine the problem of generality destruction and general knowledge forgetting during continual fine-tuning. Nevertheless, their study is restricted to encoderonly models and classification tasks. In this work, we draw attention to the following fundamental questions regarding forgetting in generative LLMs: \n\n1. Are the general knowledge stored in LLMs forgotten during continual instruction tuning? 2. What types of general knowledge are saliently forgotten? 3. What are the effects of model scales, model architectures, and general instruction tuning in the forgetting problem? \n\nTo address these questions, we conduct an empirical study on various LLLMs, such BLOOMZ, mT0 (Muennighoff et al., 2022), LLAMA Touvron et al. (2023), and ALPACA Taori et al. (2023) to analyze the catastrophic forgetting (CF) problem during continual instruction tuning. We continually train the original LLMs with five instruction tasks and evaluate the retention of the general knowledge within the model from three perspectives: domain knowledge, reasoning, and reading comprehension. Furthermore, we investigate the evolution of bias in LLMs throughout the tuning process. To gain insights into the effect of model architecture, we compare the performance of BLOOMZ with that of mT0 (Muennighoff et al., 2022) (an encoder-decoder model), which is fine-tuned using similar datasets. We also investigate the impact of general instruction tuning on the CF problem by comparing the performance of the initial model with the instruction-tuned version such as (BLOOM, BLOOMZ) and (LLAMA (Touvron et al., 2023), ALPACA (Taori et al., 2023)). \n\nOur findings reveal that the forgetting problem is generally present in LLMs.\n...\nOur findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning. Additionally, we observe that the bias in LLMs is mitigated throughout the continual instruction tuning process. When comparing BLOOMZ with mT0 at comparable model scale, we find that BLOOMZ experiences a relatively milder forgetting problem, suggesting that the decoder-only architecture may be better at retaining information during continual instruction tuning. Lastly, empirical results on LLAMA and its instruction-tuned version (i.e., ALPACA) indicate that diverse instruction tuning can help alleviate the CF phenomenon for LLMs in further continual fine-tuning. \n\nThe contribution of our paper can be summarized as follows: \n\n1. We take an initial step to analyze the catastrophic forgetting (CF) problem during continual instruction tuning by an empirically study, where a specific evaluation setting is designed from the perspective of general knowledge such as domain knowledge, reasoning, reading comprehension and the bias problem. 2. We provide an initial research evidence that the CF problem generally exists in the continual instruction tuning process for different models such as BLOOMZ, mT0, LLAMA and ALPACA. We also show that the model architecture, model scale and general instruction tuning have different effects on the CF problem.\n\n## Related Work\nInstruction Tuning. Instruction tuning has proven to be effective in aligning responses from pre-trained language models with human intents or preferences (Ouyang et al., 2022;Stiennon et al., 2020;Min et al., 2021). This technique refines a model's ability to predict a specific response to a given prompt, which may optionally include an instruction that outlines a task for the model. Examples of such models include T0 (Sanh et al., 2021), mT0 (Muennighoff et al., 2022), and BLOOMZ (Muennighoff et al., 2022). It has been demonstrated that instruction tuning can enhance the ability of language models to generalize to unseen tasks without prior exposure (Wei et al., 2021;Sanh et al., 2021). In this work, we focus on fine-tuning LLMs in a continual manner and analyze the catastrophic forgetting (CF) phenomenon during training. Specifically, instructions for a particular type of task (such as generating headlines) are used to tune the LLMs in each training phase, and the model does not have access to previously learned tasks. \n\nEvaluation of CF in Continual Learning. Various training strategies have been proposed to address the problem of catastrophic forgetting (CF) in continual learning (Riemer et al., 2019;Buzzega et al., 2020;Ke et al., 2022;Chen et al., 2022;Luo et al., 2023a). Previous studies have primarily measured CF by evaluating the performance decrease in previously learned tasks during continual learning or the average performance of learned tasks at the end of training. However, Davari et al. (2022) discovered that even when the model performance on previously learned tasks is preserved, the representations still suffer from significant drift due to parameter updates. As a result, they propose using an optimal linear classifier of learned tasks to measure performance, with changes considered as a surrogate to quantify CF. Similarly, Wu et al. (2022) employs layer-wise and task-wise probing to analyze CF in each layer for previously learned tasks. Luo et al. (2023b) propose using a series of probing tasks to evaluate the knowledge stored in LLMs and analyze the generality of the models.\n\n## Main Results\nFirstly, Figure 3 shows the performance of the continually trained BLOOMZ-7.1b model on the instruction tasks compared to the initial model's performance. For example, after being trained on the Simp training data, the BLOOMZ model achieves a SARI score of 47.24 on the Simp test set, which is 1.98 higher than the initial model's score. Subsequently, when we continually train the model on the Emdg dataset, the obtained model achieves a BLEU score of 53.3, surpassing the initial model's performance by 3.62. These improvements demonstrate that the model can benefit from the instruction tuning process and achieve significantly better performance on the instruction tasks. Additional results for other LLMs are presented in Appendix B. \n\nNext, Figure 2 displays the FG values of BLOOMZ-1.1b and BLOOMZ-7.1B. As we can observe, the performance gradually decreases as we continually tune the model with instruction tasks. For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting. We can also notice that the performance of the BLOOMZ-7.1b model drops more drastically in these evaluation tasks, The main results of forgetting are reported in Table 3. We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively. Interestingly, we observe that the FG values for bias (Table 4) are mostly above zero in the experiments, which suggests that model biases, such as those related to race, color, gender, and so on, are mitigated during continual instruction tuning. For instance, in sentences describing physical appearance, BLOOMZ-7.1b\n\n## ALPACA-7B\nInitial Simp Emdg InqQG Exp HGen Initial Simp Emdg InqQG Exp HGen Initial Simp Emdg InqQG Exp HGen significant forgetting in the first instruction tuning, which suggests that models without general instruction tuning may have less ability to retain knowledge during continual fine-tuning. The better retention of knowledge implies that general instruction tuning can mitigate catastrophic forgetting in LLMs during further continual fine-tuning. This finding highlights the importance of general instruction tuning in preserving the acquired knowledge and skills of LLMs when they undergo subsequent task-specific fine-tuning. \n\nTo further demonstrate the effect of general instruction tuning, we mix 10,000 general instruction data samples from ALPACA (Taori et al., 2023) with the continual instruction tasks to train the LLAMA-7b model. For the sake of brevity, we present the performance of one data split from each evaluation set (MMLU-human, Hellaswag, and Race-middle) to illustrate the effect in Figure 6. The results clearly show that the forgetting during continual instruction tuning can be mitigated to a certain extent by incorporating general instruction data. For instance, the performance of MMLU-human in the initial LLAMA-7b model is 34.72%, but it decreases to 26.8% when trained solely on the instruction data. However, when trained on the mixed data, the performance becomes 30%. These findings further further show that general instruction tuning can help alleviate the CF problem encountered during continual instruction tuning.\n\n## Conclusion\nIn this study, we conducted an empirical investigation into the catastrophic forgetting (CF) phenomenon experienced by large language models (LLMs) during continual instruction tuning. Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills. Furthermore, our comparative analysis showed that the decoder-only model, BLOOMZ, demonstrates a superior ability to retain knowledge and skills during continual fine-tuning when compared to the encoder-decoder model, mT0. Additionally, we discovered that employing general instruction tuning techniques may help alleviate the CF problem in LLMs. Our empirical study suggests that exploring more effective methods to mitigate CF in LLMs during continual fine-tuning is a promising research direction. When applying LLMs, practitioners should remain vigilant and pay close attention to the issue of knowledge forgetting that may occur after instruction tuning. Addressing this challenge is crucial to ensure the reliable and consistent performance of LLMs in real-world applications.",
            "reference_string": "[261031244 | Luo et al. | 2023 | Citations: 318]"
        },
        {
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.15382, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332098876",
                    "name": "Elita Lobo"
                },
                {
                    "authorId": "40228633",
                    "name": "Chirag Agarwal"
                },
                {
                    "authorId": "1892673",
                    "name": "Himabindu Lakkaraju"
                }
            ],
            "abstract": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
            "corpus_id": 274234789,
            "sentences": [
                {
                    "corpus_id": "274234789",
                    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                    "text": "Notably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024). Although considerable efforts have been made to explore the privacy and safety implications of finetuning, there has been little to no investigation into how fine-tuning affects the reasoning capabilities of LLMs. If fine-tuning LLMs diminishes their reasoning abilities, LLMs may lose their core appeal to users (Brown et al., 2020;Wei et al., 2022b) A key method for eliciting reasoning in LLMs is the Chain-of-Thought reasoning approach Wei et al. (2022b). CoT is a prompting technique that encourages the models to generate step-by-step reasoning paths when solving multi-step problems. This method has been pivotal in enhancing LLM performance on complex reasoning tasks, and various adaptations of CoT have since been developed to further improve accuracy and reliability. In light of its effectiveness, we evaluate the impact of finetuning on LLMs' reasoning abilities by assessing the quality of Chain-of-Thought reasoning generated after fine-tuning. Our work. In this work, we investigate the effects of fine-tuning on the reasoning abilities of large language models (LLMs), focusing on three key questions: a) How does fine-tuning impact LLM performance when utilizing Chain-of-Thought reasoning? b) Does fine-tuning affect the faithfulness of CoT reasoning? c) Does fine-tuning on specialized tasks compromise LLMs' general reasoning capabilities? Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.",
                    "score": 0.6296302693449948,
                    "section_title": "Introduction",
                    "char_start_offset": 1590,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 463
                        },
                        {
                            "start": 464,
                            "end": 677
                        },
                        {
                            "start": 678,
                            "end": 923
                        },
                        {
                            "start": 924,
                            "end": 1054
                        },
                        {
                            "start": 1055,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1423
                        },
                        {
                            "start": 1424,
                            "end": 1433
                        },
                        {
                            "start": 1434,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1734
                        },
                        {
                            "start": 1735,
                            "end": 1824
                        },
                        {
                            "start": 1825,
                            "end": 2019
                        },
                        {
                            "start": 2020,
                            "end": 2201
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 424,
                            "end": 444,
                            "matchedPaperCorpusId": "267149066"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.890625
                },
                {
                    "corpus_id": "274234789",
                    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                    "text": "We begin by discussing the prior works that study fine-tuning methods in LLMs. \n\nFine-Tuning LLMs and limitations: Various fine-tuning techniques have been proposed to more effectively adapt LLMs to new tasks. Among these techniques, the most widely used are LoRA and QLoRA, which improve efficiency by decomposing gradient matrices into low-rank matrices during fine-tuning. In this work, we use the QLoRA finetuning technique in conjunction with the supervised fine-tuning method (SFT) to efficiently fine-tune LLMs. Although fine-tuning is crucial for improving LLM performance in specific tasks, it can also lead to unintended side effects. For instance, Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks. Similarly, Singh et al. (2024) and Zeng et al. (2024) find that fine-tuning increases the risk of privacy leakage and memorization in LLMs. Moreover, fine-tuning can sometimes reverse previously established safety mechanisms, such as undoing learned toxicity filters (Kumar et al., 2024). Other studies, such as Navigli et al. (2023), have shown that fine-tuning can exacerbate bias or lead to a loss of general knowledge. Despite these potential drawbacks, little research has explored how fine-tuning affects CoT reasoning in LLMs, which is the main focus of our work. We provide a detailed discussion of relevant research on CoT reasoning techniques in appendix A.",
                    "score": 0.5751567168248836,
                    "section_title": "Related Work",
                    "char_start_offset": 3808,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 78
                        },
                        {
                            "start": 81,
                            "end": 209
                        },
                        {
                            "start": 210,
                            "end": 375
                        },
                        {
                            "start": 376,
                            "end": 518
                        },
                        {
                            "start": 519,
                            "end": 644
                        },
                        {
                            "start": 645,
                            "end": 824
                        },
                        {
                            "start": 825,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1113
                        },
                        {
                            "start": 1114,
                            "end": 1247
                        },
                        {
                            "start": 1248,
                            "end": 1395
                        },
                        {
                            "start": 1396,
                            "end": 1492
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 836,
                            "end": 855,
                            "matchedPaperCorpusId": "267149066"
                        },
                        {
                            "start": 1137,
                            "end": 1158,
                            "matchedPaperCorpusId": "258688053"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.81884765625
                },
                {
                    "corpus_id": "274234789",
                    "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                    "text": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
                    "score": 0.5967443267920667,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.76318359375
                }
            ],
            "relevance_judgement": 0.890625,
            "relevance_judgment_input_expanded": "# Title: On the Impact of Fine-Tuning on Chain-of-Thought Reasoning\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Elita Lobo, Chirag Agarwal, Himabindu Lakkaraju\n## Abstract\nLarge language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.\n## Introduction\nNotably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024). Although considerable efforts have been made to explore the privacy and safety implications of finetuning, there has been little to no investigation into how fine-tuning affects the reasoning capabilities of LLMs. If fine-tuning LLMs diminishes their reasoning abilities, LLMs may lose their core appeal to users (Brown et al., 2020;Wei et al., 2022b) A key method for eliciting reasoning in LLMs is the Chain-of-Thought reasoning approach Wei et al. (2022b). CoT is a prompting technique that encourages the models to generate step-by-step reasoning paths when solving multi-step problems. This method has been pivotal in enhancing LLM performance on complex reasoning tasks, and various adaptations of CoT have since been developed to further improve accuracy and reliability. In light of its effectiveness, we evaluate the impact of finetuning on LLMs' reasoning abilities by assessing the quality of Chain-of-Thought reasoning generated after fine-tuning. Our work. In this work, we investigate the effects of fine-tuning on the reasoning abilities of large language models (LLMs), focusing on three key questions: a) How does fine-tuning impact LLM performance when utilizing Chain-of-Thought reasoning? b) Does fine-tuning affect the faithfulness of CoT reasoning? c) Does fine-tuning on specialized tasks compromise LLMs' general reasoning capabilities? Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.\n\n## Related Work\nWe begin by discussing the prior works that study fine-tuning methods in LLMs. \n\nFine-Tuning LLMs and limitations: Various fine-tuning techniques have been proposed to more effectively adapt LLMs to new tasks. Among these techniques, the most widely used are LoRA and QLoRA, which improve efficiency by decomposing gradient matrices into low-rank matrices during fine-tuning. In this work, we use the QLoRA finetuning technique in conjunction with the supervised fine-tuning method (SFT) to efficiently fine-tune LLMs. Although fine-tuning is crucial for improving LLM performance in specific tasks, it can also lead to unintended side effects. For instance, Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks. Similarly, Singh et al. (2024) and Zeng et al. (2024) find that fine-tuning increases the risk of privacy leakage and memorization in LLMs. Moreover, fine-tuning can sometimes reverse previously established safety mechanisms, such as undoing learned toxicity filters (Kumar et al., 2024). Other studies, such as Navigli et al. (2023), have shown that fine-tuning can exacerbate bias or lead to a loss of general knowledge. Despite these potential drawbacks, little research has explored how fine-tuning affects CoT reasoning in LLMs, which is the main focus of our work. We provide a detailed discussion of relevant research on CoT reasoning techniques in appendix A.",
            "reference_string": "[274234789 | Lobo et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Chained Tuning Leads to Biased Forgetting",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 43,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.16469, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268761197",
                    "name": "Megan Ung"
                },
                {
                    "authorId": "2336870239",
                    "name": "Alicia Sun"
                },
                {
                    "authorId": "2330399051",
                    "name": "Samuel J. Bell"
                },
                {
                    "authorId": "2219919981",
                    "name": "Bhaktipriya Radharapu"
                },
                {
                    "authorId": "2253337895",
                    "name": "Levent Sagun"
                },
                {
                    "authorId": "2330433992",
                    "name": "Adina Williams"
                }
            ],
            "abstract": "Large language models (LLMs) are often fine-tuned for use on downstream tasks, though this can degrade capabilities learned during previous training. This phenomenon, often referred to as catastrophic forgetting, has important potential implications for the safety of deployed models. In this work, we first show that models trained on downstream tasks forget their safety tuning to a greater extent than models trained in the opposite order. Second, we show that forgetting disproportionately impacts safety information about certain groups. To quantify this phenomenon, we define a new metric we term biased forgetting. We conduct a systematic evaluation of the effects of task ordering on forgetting and apply mitigations that can help the model recover from the forgetting observed. We hope our findings can better inform methods for chaining the finetuning of LLMs in continual learning settings to enable training of safer and less toxic models.",
            "corpus_id": 274982256,
            "sentences": [
                {
                    "corpus_id": "274982256",
                    "title": "Chained Tuning Leads to Biased Forgetting",
                    "text": "Catastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2014;Kirkpatrick et al., 2016;Kemker et al., 2018). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;Razdaibiedina et al., 2023;Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023). \n\nDespite empirical observations of catastrophic forgetting, it remains unclear which post-training recipes can lead to forgetting. In this paper, we study how different finetuning regimes, including task ordering and learning rate, influence the severity of catastrophic forgetting. Specifically, we investigate a new sub-phenomenon of catastrophic forgetting that we call biased forgetting, whereby model performance degrades disproportionately for safety-related tasks, or for specific demographic groups, and propose a simple metric to quantify it. Empirically, we found that the order of finetuning on different tasks matters: safety/bias task are more prone to forgetting when subsequently finetuned with a capability task as compared to the other order. We hypothesized that certain tasks may be more prone to forgetting due to the width of the minima obtained the end of training, and empirically observed that safety tasks converged to sharper minima. Based on these observations, we explored two approaches for mitigating biased forgetting: determining task ordering using first task loss curvature, and retraining using a small portion of the forgotten task.",
                    "score": 0.6659202965854761,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 505
                        },
                        {
                            "start": 506,
                            "end": 782
                        },
                        {
                            "start": 783,
                            "end": 1034
                        },
                        {
                            "start": 1037,
                            "end": 1166
                        },
                        {
                            "start": 1167,
                            "end": 1318
                        },
                        {
                            "start": 1319,
                            "end": 1587
                        },
                        {
                            "start": 1588,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1995
                        },
                        {
                            "start": 1996,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 163,
                            "end": 178,
                            "matchedPaperCorpusId": "18556305"
                        },
                        {
                            "start": 239,
                            "end": 264,
                            "matchedPaperCorpusId": "12730344"
                        },
                        {
                            "start": 289,
                            "end": 309,
                            "matchedPaperCorpusId": "22910766"
                        },
                        {
                            "start": 460,
                            "end": 487,
                            "matchedPaperCorpusId": "256390383"
                        },
                        {
                            "start": 889,
                            "end": 909,
                            "matchedPaperCorpusId": "246634238"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.85400390625
                },
                {
                    "corpus_id": "274982256",
                    "title": "Chained Tuning Leads to Biased Forgetting",
                    "text": "Large language models (LLMs) are often fine-tuned for use on downstream tasks, though this can degrade capabilities learned during previous training. This phenomenon, often referred to as catastrophic forgetting, has important potential implications for the safety of deployed models. In this work, we first show that models trained on downstream tasks forget their safety tuning to a greater extent than models trained in the opposite order. Second, we show that forgetting disproportionately impacts safety information about certain groups. To quantify this phenomenon, we define a new metric we term biased forgetting. We conduct a systematic evaluation of the effects of task ordering on forgetting and apply mitigations that can help the model recover from the forgetting observed. We hope our findings can better inform methods for chaining the finetuning of LLMs in continual learning settings to enable training of safer and less toxic models.",
                    "score": 0.6078527361454482,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.78759765625
                },
                {
                    "corpus_id": "274982256",
                    "title": "Chained Tuning Leads to Biased Forgetting",
                    "text": "Catastrophic Forgetting in LLMs. Catastrophic forgetting and continual learning has been long studied in machine learning (Goodfellow et al., 2014;Ramasesh et al., 2020). Proposed mitigations for catastrophic forgetting include weight regularization on subsequent tasks (Kirkpatrick et al., 2017;Zenke et al., 2017), and replay-based methods by injecting samples from previous tasks (Chaudhry et al., 2018;Lopez-Paz and Ranzato, 2017;Chaudhry et al., 2019). There is also work studying the relationship between loss pass and curvature and forgetting (Mirzadeh et al., 2020a). \n\nRecent work has shown that large language models are susceptible to catastrophic forgetting and that forgetting can increase as model size increases (Luo et al., 2023). Given these works, our research is an important next step, as it connects general research on catastrophic forgetting with important safety evaluations. Previous studies (Bianchi et al., 2023)  incorporation of a mixture of safety data during the capability fine-tuning stage (Qi et al., 2023;Jan et al., 2024). However, our study is the first to investigate the effects of data rehearsal techniques in a chained fine-tuning context. \n\nBiases and Safety of LLMs. Language models pre-trained on large corpora can contain cultural biases (Blodgett et al., 2020;Sun et al., 2019;Smith et al., 2022) and produce harmful output and contents (Gehman et al., 2020;Weidinger et al., 2021). \n\nAlthough LLMs are increasingly subsequently finetuned on safety and/or alignment datasets, such guardrails can be undermined through adversarial attack (Perez et al., 2022), in a continual learning setting (Qi et al., 2023), or by altering prompt and data training mix during safety tuning (Lyu et al., 2024). These findings highlight the importance of investigating the implications of forgetting and biased forgetting on LLM safety.",
                    "score": 0.6800236001550577,
                    "section_title": "Related Works",
                    "char_start_offset": 20669,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 32
                        },
                        {
                            "start": 33,
                            "end": 170
                        },
                        {
                            "start": 171,
                            "end": 457
                        },
                        {
                            "start": 458,
                            "end": 575
                        },
                        {
                            "start": 578,
                            "end": 746
                        },
                        {
                            "start": 747,
                            "end": 899
                        },
                        {
                            "start": 900,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1180
                        },
                        {
                            "start": 1183,
                            "end": 1209
                        },
                        {
                            "start": 1210,
                            "end": 1428
                        },
                        {
                            "start": 1431,
                            "end": 1740
                        },
                        {
                            "start": 1741,
                            "end": 1865
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 122,
                            "end": 147,
                            "matchedPaperCorpusId": "12730344"
                        },
                        {
                            "start": 296,
                            "end": 315,
                            "matchedPaperCorpusId": "10409742"
                        },
                        {
                            "start": 406,
                            "end": 434,
                            "matchedPaperCorpusId": "37308416"
                        },
                        {
                            "start": 1383,
                            "end": 1404,
                            "matchedPaperCorpusId": "221878771"
                        },
                        {
                            "start": 1583,
                            "end": 1603,
                            "matchedPaperCorpusId": "246634238"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.671875
                }
            ],
            "relevance_judgement": 0.85400390625,
            "relevance_judgment_input_expanded": "# Title: Chained Tuning Leads to Biased Forgetting\n# Venue: arXiv.org\n# Authors: Megan Ung, Alicia Sun, Samuel J. Bell, Bhaktipriya Radharapu, Levent Sagun, Adina Williams\n## Abstract\nLarge language models (LLMs) are often fine-tuned for use on downstream tasks, though this can degrade capabilities learned during previous training. This phenomenon, often referred to as catastrophic forgetting, has important potential implications for the safety of deployed models. In this work, we first show that models trained on downstream tasks forget their safety tuning to a greater extent than models trained in the opposite order. Second, we show that forgetting disproportionately impacts safety information about certain groups. To quantify this phenomenon, we define a new metric we term biased forgetting. We conduct a systematic evaluation of the effects of task ordering on forgetting and apply mitigations that can help the model recover from the forgetting observed. We hope our findings can better inform methods for chaining the finetuning of LLMs in continual learning settings to enable training of safer and less toxic models.\n## Introduction\nCatastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2014;Kirkpatrick et al., 2016;Kemker et al., 2018). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;Razdaibiedina et al., 2023;Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023). \n\nDespite empirical observations of catastrophic forgetting, it remains unclear which post-training recipes can lead to forgetting. In this paper, we study how different finetuning regimes, including task ordering and learning rate, influence the severity of catastrophic forgetting. Specifically, we investigate a new sub-phenomenon of catastrophic forgetting that we call biased forgetting, whereby model performance degrades disproportionately for safety-related tasks, or for specific demographic groups, and propose a simple metric to quantify it. Empirically, we found that the order of finetuning on different tasks matters: safety/bias task are more prone to forgetting when subsequently finetuned with a capability task as compared to the other order. We hypothesized that certain tasks may be more prone to forgetting due to the width of the minima obtained the end of training, and empirically observed that safety tasks converged to sharper minima. Based on these observations, we explored two approaches for mitigating biased forgetting: determining task ordering using first task loss curvature, and retraining using a small portion of the forgotten task.\n\n## Related Works\nCatastrophic Forgetting in LLMs. Catastrophic forgetting and continual learning has been long studied in machine learning (Goodfellow et al., 2014;Ramasesh et al., 2020). Proposed mitigations for catastrophic forgetting include weight regularization on subsequent tasks (Kirkpatrick et al., 2017;Zenke et al., 2017), and replay-based methods by injecting samples from previous tasks (Chaudhry et al., 2018;Lopez-Paz and Ranzato, 2017;Chaudhry et al., 2019). There is also work studying the relationship between loss pass and curvature and forgetting (Mirzadeh et al., 2020a). \n\nRecent work has shown that large language models are susceptible to catastrophic forgetting and that forgetting can increase as model size increases (Luo et al., 2023). Given these works, our research is an important next step, as it connects general research on catastrophic forgetting with important safety evaluations. Previous studies (Bianchi et al., 2023)  incorporation of a mixture of safety data during the capability fine-tuning stage (Qi et al., 2023;Jan et al., 2024). However, our study is the first to investigate the effects of data rehearsal techniques in a chained fine-tuning context. \n\nBiases and Safety of LLMs. Language models pre-trained on large corpora can contain cultural biases (Blodgett et al., 2020;Sun et al., 2019;Smith et al., 2022) and produce harmful output and contents (Gehman et al., 2020;Weidinger et al., 2021). \n\nAlthough LLMs are increasingly subsequently finetuned on safety and/or alignment datasets, such guardrails can be undermined through adversarial attack (Perez et al., 2022), in a continual learning setting (Qi et al., 2023), or by altering prompt and data training mix during safety tuning (Lyu et al., 2024). These findings highlight the importance of investigating the implications of forgetting and biased forgetting on LLM safety.",
            "reference_string": "[274982256 | Ung et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 36,
            "citation_count": 39,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.18865, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "22500310",
                    "name": "Weijieying Ren"
                },
                {
                    "authorId": "2288186960",
                    "name": "Xinlong Li"
                },
                {
                    "authorId": "2288224352",
                    "name": "Lei Wang"
                },
                {
                    "authorId": "2238191223",
                    "name": "Tianxiang Zhao"
                },
                {
                    "authorId": "2258070286",
                    "name": "Wei Qin"
                }
            ],
            "abstract": "Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \\url{https://github.com/which47/LLMCL}.",
            "corpus_id": 268063771,
            "sentences": [
                {
                    "corpus_id": "268063771",
                    "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
                    "text": "Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \\url{https://github.com/which47/LLMCL}.",
                    "score": 0.5101006555860417,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.80615234375
                }
            ],
            "relevance_judgement": 0.80615234375,
            "relevance_judgment_input_expanded": "# Title: Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning\n# Venue: arXiv.org\n# Authors: Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, Wei Qin\n## Abstract\nExisting research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \\url{https://github.com/which47/LLMCL}.\n",
            "reference_string": "[268063771 | Ren et al. | 2024 | Citations: 39]"
        },
        {
            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.03129, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186278878",
                    "name": "Chen-An Li"
                },
                {
                    "authorId": "2278588523",
                    "name": "Hung-yi Lee"
                }
            ],
            "abstract": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.",
            "corpus_id": 266844262,
            "sentences": [
                {
                    "corpus_id": "266844262",
                    "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
                    "text": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.",
                    "score": 0.5959711563541887,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7939453125
                }
            ],
            "relevance_judgement": 0.7939453125,
            "relevance_judgment_input_expanded": "# Title: Examining Forgetting in Continual Pre-training of Aligned Large Language Models\n# Venue: arXiv.org\n# Authors: Chen-An Li, Hung-yi Lee\n## Abstract\nRecent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.\n",
            "reference_string": "[266844262 | Li et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 45,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.05605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3086897",
                    "name": "Damjan Kalajdzievski"
                }
            ],
            "abstract": "We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting",
            "corpus_id": 266933290,
            "sentences": [
                {
                    "corpus_id": "266933290",
                    "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
                    "text": "We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting",
                    "score": 0.5788587423326661,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.79150390625
                },
                {
                    "corpus_id": "266933290",
                    "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
                    "text": "For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts. \n\nThe works (Lee et al., 2019;Wiese et al., 2017) focus on using techniques that mitigate forgetting by regularizing outputs or parameters to remain close to the pre-trained model, to actually improve fine-tuning performance and stability. This can be reasoned to entail that forgetting while fine-tuning degrades the ability of the model to adapt and be fine-tuned, and forgetting occurs during the process of fine-tuning on a single task. \n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model.",
                    "score": 0.7095603065851567,
                    "section_title": "Catastrophic Forgetting",
                    "char_start_offset": 5604,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 196
                        },
                        {
                            "start": 197,
                            "end": 419
                        },
                        {
                            "start": 422,
                            "end": 659
                        },
                        {
                            "start": 660,
                            "end": 860
                        },
                        {
                            "start": 863,
                            "end": 966
                        },
                        {
                            "start": 967,
                            "end": 1169
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.736328125
                },
                {
                    "corpus_id": "266933290",
                    "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
                    "text": "In conclusion, using a LoRA fine-tuning setup, we empirically demonstrated that during fine-tuning on a downstream task forgetting is strongly predicted by a linear function of fine-tuning loss, and a shifted power law in the number of non-embedding parameters fine-tuned and the number of update steps. In addition, we used our setup to identify that similar scaling laws to those for pre-training LLMs identified by (Kaplan et al., 2020;Henighan et al., 2020) held for fine-tuning. In particular, the fine-tuning loss was also fit by a shifted power law function. We then examined the forgetting behaviour in model generation, and showed that both model safety and reasoning benchmark performance suffer from forgetting. \n\nTo quantify forgetting consistently, we used the cross-entropy loss between the fine-tuned model and the base model's predictions. We reasoned in section 3.1 why this metric is the most appropriate, and why usual metrics for loss may be inadequate. \n\nIn light of our laws for forgetting, which show that forgetting is a consequence of fine-tuning performance, we underscore the need for techniques to mitigate forgetting in LLMs. As such, an avenue for future work would be to develop and evaluate techniques for mitigating forgetting in our setup, and compare precisely how much the functional relationships may change to be more favourable to fine-tuning without forgetting.",
                    "score": 0.6585829851785138,
                    "section_title": "Conclusion",
                    "char_start_offset": 26998,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 303
                        },
                        {
                            "start": 304,
                            "end": 483
                        },
                        {
                            "start": 484,
                            "end": 565
                        },
                        {
                            "start": 566,
                            "end": 722
                        },
                        {
                            "start": 725,
                            "end": 855
                        },
                        {
                            "start": 856,
                            "end": 973
                        },
                        {
                            "start": 976,
                            "end": 1154
                        },
                        {
                            "start": 1155,
                            "end": 1401
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6552734375
                },
                {
                    "corpus_id": "266933290",
                    "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
                    "text": "The phenomenon of a neural network forgetting a previously learned task when trained on a new one, continues to pose a key challenge in deep learning. The problem was first brought to attention for early neural networks in (McCloskey & Cohen, 1989;Ratcliff, 1990), in a task incremental setup. These works showed that when a neural network trained on a task was further trained to learn a new task, performance on the first task catastrophically degraded. This forgetting was given the moniker \"Catastrophic Forgetting\". Forgetting has continued to pose a serious problem in the domain of task sequential learning or continual learning (Kemker et al., 2018;Goodfellow et al., 2014;De Lange et al., 2022;Parisi et al., 2019). \n\nApproaches to mitigate forgetting can be broadly categorized (De Lange et al., 2022) into (i) regularization methods (Kirkpatrick et al., 2017;Ahn et al., 2019), (ii) ensembling and parameter isolation methods (Xu & Zhu, 2018;Rusu et al., 2016;Aljundi et al., 2017), and (iii) experience replay/rehearsal (ROBINS, 1995;Rebuffi et al., 2017;Isele & Cosgun, 2018;Lopez-Paz & Ranzato, 2017;Gepperth & Karaoguz, 2016;Chaudhry et al., 2019;Rolnick et al., 2019;Purushwalkam et al., 2022). \n\nRecently, it has been demonstrated that LLMs struggle with forgetting when fine-tuning, especially when adapting to a small fine-tuning dataset. In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training.",
                    "score": 0.6315978914291953,
                    "section_title": "Catastrophic Forgetting",
                    "char_start_offset": 3928,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 293
                        },
                        {
                            "start": 294,
                            "end": 455
                        },
                        {
                            "start": 456,
                            "end": 520
                        },
                        {
                            "start": 521,
                            "end": 724
                        },
                        {
                            "start": 727,
                            "end": 1210
                        },
                        {
                            "start": 1213,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1568
                        },
                        {
                            "start": 1569,
                            "end": 1675
                        },
                        {
                            "start": 1676,
                            "end": 1872
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 223,
                            "end": 248,
                            "matchedPaperCorpusId": "61019113"
                        },
                        {
                            "start": 248,
                            "end": 263,
                            "matchedPaperCorpusId": "18556305"
                        },
                        {
                            "start": 636,
                            "end": 657,
                            "matchedPaperCorpusId": "22910766"
                        },
                        {
                            "start": 657,
                            "end": 681,
                            "matchedPaperCorpusId": "12730344"
                        },
                        {
                            "start": 681,
                            "end": 703,
                            "matchedPaperCorpusId": "218889912"
                        },
                        {
                            "start": 703,
                            "end": 723,
                            "matchedPaperCorpusId": "73497737"
                        },
                        {
                            "start": 788,
                            "end": 811,
                            "matchedPaperCorpusId": "218889912"
                        },
                        {
                            "start": 844,
                            "end": 870,
                            "matchedPaperCorpusId": "4704285"
                        },
                        {
                            "start": 937,
                            "end": 953,
                            "matchedPaperCorpusId": "44086178"
                        },
                        {
                            "start": 971,
                            "end": 992,
                            "matchedPaperCorpusId": "914027"
                        },
                        {
                            "start": 1032,
                            "end": 1046,
                            "matchedPaperCorpusId": "22882861"
                        },
                        {
                            "start": 1067,
                            "end": 1088,
                            "matchedPaperCorpusId": "3622700"
                        },
                        {
                            "start": 1088,
                            "end": 1114,
                            "matchedPaperCorpusId": "37308416"
                        },
                        {
                            "start": 1162,
                            "end": 1183,
                            "matchedPaperCorpusId": "53860287"
                        },
                        {
                            "start": 1183,
                            "end": 1209,
                            "matchedPaperCorpusId": "247628244"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.634765625
                }
            ],
            "relevance_judgement": 0.79150390625,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Forgetting When Fine-Tuning Large Language Models\n# Venue: arXiv.org\n# Authors: Damjan Kalajdzievski\n## Abstract\nWe study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting\n## Catastrophic Forgetting\nThe phenomenon of a neural network forgetting a previously learned task when trained on a new one, continues to pose a key challenge in deep learning. The problem was first brought to attention for early neural networks in (McCloskey & Cohen, 1989;Ratcliff, 1990), in a task incremental setup. These works showed that when a neural network trained on a task was further trained to learn a new task, performance on the first task catastrophically degraded. This forgetting was given the moniker \"Catastrophic Forgetting\". Forgetting has continued to pose a serious problem in the domain of task sequential learning or continual learning (Kemker et al., 2018;Goodfellow et al., 2014;De Lange et al., 2022;Parisi et al., 2019). \n\nApproaches to mitigate forgetting can be broadly categorized (De Lange et al., 2022) into (i) regularization methods (Kirkpatrick et al., 2017;Ahn et al., 2019), (ii) ensembling and parameter isolation methods (Xu & Zhu, 2018;Rusu et al., 2016;Aljundi et al., 2017), and (iii) experience replay/rehearsal (ROBINS, 1995;Rebuffi et al., 2017;Isele & Cosgun, 2018;Lopez-Paz & Ranzato, 2017;Gepperth & Karaoguz, 2016;Chaudhry et al., 2019;Rolnick et al., 2019;Purushwalkam et al., 2022). \n\nRecently, it has been demonstrated that LLMs struggle with forgetting when fine-tuning, especially when adapting to a small fine-tuning dataset. In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training.\n...\nFor example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts. \n\nThe works (Lee et al., 2019;Wiese et al., 2017) focus on using techniques that mitigate forgetting by regularizing outputs or parameters to remain close to the pre-trained model, to actually improve fine-tuning performance and stability. This can be reasoned to entail that forgetting while fine-tuning degrades the ability of the model to adapt and be fine-tuned, and forgetting occurs during the process of fine-tuning on a single task. \n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model.\n\n## Conclusion\nIn conclusion, using a LoRA fine-tuning setup, we empirically demonstrated that during fine-tuning on a downstream task forgetting is strongly predicted by a linear function of fine-tuning loss, and a shifted power law in the number of non-embedding parameters fine-tuned and the number of update steps. In addition, we used our setup to identify that similar scaling laws to those for pre-training LLMs identified by (Kaplan et al., 2020;Henighan et al., 2020) held for fine-tuning. In particular, the fine-tuning loss was also fit by a shifted power law function. We then examined the forgetting behaviour in model generation, and showed that both model safety and reasoning benchmark performance suffer from forgetting. \n\nTo quantify forgetting consistently, we used the cross-entropy loss between the fine-tuned model and the base model's predictions. We reasoned in section 3.1 why this metric is the most appropriate, and why usual metrics for loss may be inadequate. \n\nIn light of our laws for forgetting, which show that forgetting is a consequence of fine-tuning performance, we underscore the need for techniques to mitigate forgetting in LLMs. As such, an avenue for future work would be to develop and evaluate techniques for mitigating forgetting in our setup, and compare precisely how much the functional relationships may change to be more favourable to fine-tuning without forgetting.",
            "reference_string": "[266933290 | Kalajdzievski | 2024 | Citations: 11]"
        },
        {
            "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 73,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02486, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2217529598",
                    "name": "Jinpeng Chen"
                },
                {
                    "authorId": "2258230512",
                    "name": "Runmin Cong"
                },
                {
                    "authorId": "2334518513",
                    "name": "Yuzhi Zhao"
                },
                {
                    "authorId": "2359170282",
                    "name": "Hongzheng Yang"
                },
                {
                    "authorId": "2337369321",
                    "name": "Guangneng Hu"
                },
                {
                    "authorId": "2221824976",
                    "name": "H. Ip"
                },
                {
                    "authorId": "2287942988",
                    "name": "Sam Kwong"
                }
            ],
            "abstract": "Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.",
            "corpus_id": 278327022,
            "sentences": [
                {
                    "corpus_id": "278327022",
                    "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
                    "text": "Several existing works also aim to understand the phenomenon of catastrophic forgetting. For instance, (Li et al., 2024c) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen. Among these studies, (Zheng et al., 2025) is most relevant to ours. It introduces the concept of spurious forgetting, where the model loses task alignment without any genuine loss of knowledge. This notion is partially similar with our definition of superficial forgetting. However, spurious forgetting emphasizes the recoverability and assumes that no actual knowledge has been forgotten. In contrast, superficial forgetting does not make assumptions about recoverability or knowledge retention.",
                    "score": 0.7858579349434189,
                    "section_title": "A.5. Understanding Catastrophic Forgetting",
                    "char_start_offset": 32597,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 310
                        },
                        {
                            "start": 311,
                            "end": 568
                        },
                        {
                            "start": 569,
                            "end": 636
                        },
                        {
                            "start": 637,
                            "end": 762
                        },
                        {
                            "start": 763,
                            "end": 842
                        },
                        {
                            "start": 843,
                            "end": 958
                        },
                        {
                            "start": 959,
                            "end": 1065
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 103,
                            "end": 121,
                            "matchedPaperCorpusId": "260334888"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.791015625
                }
            ],
            "relevance_judgement": 0.791015625,
            "relevance_judgment_input_expanded": "# Title: SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning\n# Venue: arXiv.org\n# Authors: Jinpeng Chen, Runmin Cong, Yuzhi Zhao, Hongzheng Yang, Guangneng Hu, H. Ip, Sam Kwong\n## Abstract\nMultimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.\n## A.5. Understanding Catastrophic Forgetting\nSeveral existing works also aim to understand the phenomenon of catastrophic forgetting. For instance, (Li et al., 2024c) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen. Among these studies, (Zheng et al., 2025) is most relevant to ours. It introduces the concept of spurious forgetting, where the model loses task alignment without any genuine loss of knowledge. This notion is partially similar with our definition of superficial forgetting. However, spurious forgetting emphasizes the recoverability and assumes that no actual knowledge has been forgotten. In contrast, superficial forgetting does not make assumptions about recoverability or knowledge retention.",
            "reference_string": "[278327022 | Chen et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Better Fine-Tuning by Reducing Representational Collapse",
            "venue": "International Conference on Learning Representations",
            "year": 2020,
            "reference_count": 36,
            "citation_count": 210,
            "influential_citation_count": 36,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2008.03156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2201435",
                    "name": "Armen Aghajanyan"
                },
                {
                    "authorId": "1519979046",
                    "name": "Akshat Shrivastava"
                },
                {
                    "authorId": "3377939",
                    "name": "Anchit Gupta"
                },
                {
                    "authorId": "39589154",
                    "name": "Naman Goyal"
                },
                {
                    "authorId": "1982950",
                    "name": "Luke Zettlemoyer"
                },
                {
                    "authorId": "2701297",
                    "name": "S. Gupta"
                }
            ],
            "abstract": "Although widely adopted, existing approaches for fine-tuning pre-trained language models have been shown to be unstable across hyper-parameter settings, motivating recent work on trust region methods. In this paper, we present a simplified and efficient method rooted in trust region theory that replaces previously used adversarial objectives with parametric noise (sampling from either a normal or uniform distribution), thereby discouraging representation change during fine-tuning when possible without hurting performance. We also introduce a new analysis to motivate the use of trust region methods more generally, by studying representational collapse; the degradation of generalizable representations from pre-trained models as they are fine-tuned for a specific end task. Extensive experiments show that our fine-tuning method matches or exceeds the performance of previous trust region methods on a range of understanding and generation tasks (including DailyMail/CNN, Gigaword, Reddit TIFU, and the GLUE benchmark), while also being much faster. We also show that it is less prone to representation collapse; the pre-trained models maintain more generalizable representations every time they are fine-tuned.",
            "corpus_id": 221083147,
            "sentences": [
                {
                    "corpus_id": "221083147",
                    "title": "Better Fine-Tuning by Reducing Representational Collapse",
                    "text": "Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989). \n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set. \n\nMeasuring such degradations is non-trivial. Simple metrics such as the distance between pre-trained representations and fine-tuned representations is not sufficient (e.g. adding a constant to the pretrained representations will not change representation power, but will change distances). One approach would be to estimate mutual information of representations across tasks before and after fine-tuning, but estimation of mutual information is notoriously hard, especially in high-dimensions (Tschannen et al., 2019). We instead propose a series of probing experiments meant to provide us with empirical evidence of the existence of representation collapse on the GLUE benchmark (Wang et al., 2018).",
                    "score": 0.5088318824053075,
                    "section_title": "REPRESENTATIONAL COLLAPSE",
                    "char_start_offset": 14719,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 297
                        },
                        {
                            "start": 300,
                            "end": 484
                        },
                        {
                            "start": 485,
                            "end": 641
                        },
                        {
                            "start": 642,
                            "end": 848
                        },
                        {
                            "start": 851,
                            "end": 894
                        },
                        {
                            "start": 895,
                            "end": 1021
                        },
                        {
                            "start": 1022,
                            "end": 1139
                        },
                        {
                            "start": 1140,
                            "end": 1368
                        },
                        {
                            "start": 1369,
                            "end": 1550
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 271,
                            "end": 296,
                            "matchedPaperCorpusId": "61019113"
                        },
                        {
                            "start": 1530,
                            "end": 1549,
                            "matchedPaperCorpusId": "5034059"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75439453125
                }
            ],
            "relevance_judgement": 0.75439453125,
            "relevance_judgment_input_expanded": "# Title: Better Fine-Tuning by Reducing Representational Collapse\n# Venue: International Conference on Learning Representations\n# Authors: Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, S. Gupta\n## Abstract\nAlthough widely adopted, existing approaches for fine-tuning pre-trained language models have been shown to be unstable across hyper-parameter settings, motivating recent work on trust region methods. In this paper, we present a simplified and efficient method rooted in trust region theory that replaces previously used adversarial objectives with parametric noise (sampling from either a normal or uniform distribution), thereby discouraging representation change during fine-tuning when possible without hurting performance. We also introduce a new analysis to motivate the use of trust region methods more generally, by studying representational collapse; the degradation of generalizable representations from pre-trained models as they are fine-tuned for a specific end task. Extensive experiments show that our fine-tuning method matches or exceeds the performance of previous trust region methods on a range of understanding and generation tasks (including DailyMail/CNN, Gigaword, Reddit TIFU, and the GLUE benchmark), while also being much faster. We also show that it is less prone to representation collapse; the pre-trained models maintain more generalizable representations every time they are fine-tuned.\n## REPRESENTATIONAL COLLAPSE\nCatastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989). \n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set. \n\nMeasuring such degradations is non-trivial. Simple metrics such as the distance between pre-trained representations and fine-tuned representations is not sufficient (e.g. adding a constant to the pretrained representations will not change representation power, but will change distances). One approach would be to estimate mutual information of representations across tasks before and after fine-tuning, but estimation of mutual information is notoriously hard, especially in high-dimensions (Tschannen et al., 2019). We instead propose a series of probing experiments meant to provide us with empirical evidence of the existence of representation collapse on the GLUE benchmark (Wang et al., 2018).",
            "reference_string": "[221083147 | Aghajanyan et al. | 2020 | Citations: 210]"
        },
        {
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 50,
            "citation_count": 49,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.13669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284865968",
                    "name": "Zhaorui Yang"
                },
                {
                    "authorId": "1409707585",
                    "name": "Qian Liu"
                },
                {
                    "authorId": "19201674",
                    "name": "Tianyu Pang"
                },
                {
                    "authorId": "2285032001",
                    "name": "Han Wang"
                },
                {
                    "authorId": "46854712",
                    "name": "H. Feng"
                },
                {
                    "authorId": "145314938",
                    "name": "Minfeng Zhu"
                },
                {
                    "authorId": "2256716159",
                    "name": "Wei Chen"
                }
            ],
            "abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.",
            "corpus_id": 267769989,
            "sentences": [
                {
                    "corpus_id": "267769989",
                    "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
                    "text": "In this paper, we perform a systematic evaluation of catastrophic forgetting during the fine-tuning of language models for downstream tasks. Our findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness. To enhance performance on target task while maintaining LMs' broad capabilities, we propose a plug-and-play strategy, SDFT, to reduce distribution shift and miti-  gate catastrophic forgetting. Extensive experiments show that SDFT effectively diminishes forgetting and delivers comparable or superior performance to vanilla fine-tuning on targeted tasks. \n\nOur study is subject to certain limitations. Owing to constraints in computational resources, most of our experiments are based on the Llama-2-7bchat model with LoRA. Further investigations involving larger models and full fine-tuning remain to be explored. Furthermore, our safety evaluations are limited to the Advbench dataset and fixed adversarial suffixes, leaving the robustness against other jailbreaking strategies for future work.",
                    "score": 0.5448322206375934,
                    "section_title": "Conclusions and Limitations",
                    "char_start_offset": 23892,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 140
                        },
                        {
                            "start": 141,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 522
                        },
                        {
                            "start": 523,
                            "end": 683
                        },
                        {
                            "start": 686,
                            "end": 730
                        },
                        {
                            "start": 731,
                            "end": 852
                        },
                        {
                            "start": 853,
                            "end": 943
                        },
                        {
                            "start": 944,
                            "end": 1125
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.75244140625
                },
                {
                    "corpus_id": "267769989",
                    "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
                    "text": "Echoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evi-denced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent. \n\nIn this paper, we propose a novel fine-tuning method, Self-Distillation Fine-Tuning (SDFT), to mitigate catastrophic forgetting during fine-tuning. We hypothesize that catastrophic forgetting stems from the distribution gap between the task dataset and the seed LMs. To address the issue, as shown in Figure 1, SDFT first prompts the seed LM to generate responses that uphold semantic equivalence with the original responses present in the task dataset, resulting in the distilled dataset. A representative example of rewriting is depicted in Figure 2. After rewriting, the self-generated responses serve as surrogate targets during subsequent finetuning. Through the approach, SDFT inherently maintains the original distribution, avoiding distribution shift and thereby preserving capabilities. \n\nWe systematically evaluate SDFT by comparing its performance against that of vanilla fine-tuning and the seed LM across a variety of benchmarks. These benchmarks encompass: (1) diverse downstream tasks, including mathematical reasoning, tool using and code generation; (2) assessments of general helpfulness and safety alignment. Results on all benchmarks demonstrate the superiority of SDFT compared to vanilla fine-tuning. For instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2. The in-depth analysis of our method indicates that increasing the proportion of distilled dataset for fine-tuning leads to a decrease in catastrophic forgetting, thereby confirming that SDFT mitigates catastrophic forgetting by bridging the distribution gap.",
                    "score": 0.6687896791033714,
                    "section_title": "Introduction",
                    "char_start_offset": 1946,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 212
                        },
                        {
                            "start": 213,
                            "end": 309
                        },
                        {
                            "start": 312,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 578
                        },
                        {
                            "start": 579,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 864
                        },
                        {
                            "start": 865,
                            "end": 967
                        },
                        {
                            "start": 968,
                            "end": 1107
                        },
                        {
                            "start": 1110,
                            "end": 1254
                        },
                        {
                            "start": 1255,
                            "end": 1439
                        },
                        {
                            "start": 1440,
                            "end": 1534
                        },
                        {
                            "start": 1535,
                            "end": 1756
                        },
                        {
                            "start": 1757,
                            "end": 1862
                        },
                        {
                            "start": 1863,
                            "end": 2121
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 137,
                            "end": 154,
                            "matchedPaperCorpusId": "263671523"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67333984375
                }
            ],
            "relevance_judgement": 0.75244140625,
            "relevance_judgment_input_expanded": "# Title: Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Zhaorui Yang, Qian Liu, Tianyu Pang, Han Wang, H. Feng, Minfeng Zhu, Wei Chen\n## Abstract\nThe surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.\n## Introduction\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evi-denced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent. \n\nIn this paper, we propose a novel fine-tuning method, Self-Distillation Fine-Tuning (SDFT), to mitigate catastrophic forgetting during fine-tuning. We hypothesize that catastrophic forgetting stems from the distribution gap between the task dataset and the seed LMs. To address the issue, as shown in Figure 1, SDFT first prompts the seed LM to generate responses that uphold semantic equivalence with the original responses present in the task dataset, resulting in the distilled dataset. A representative example of rewriting is depicted in Figure 2. After rewriting, the self-generated responses serve as surrogate targets during subsequent finetuning. Through the approach, SDFT inherently maintains the original distribution, avoiding distribution shift and thereby preserving capabilities. \n\nWe systematically evaluate SDFT by comparing its performance against that of vanilla fine-tuning and the seed LM across a variety of benchmarks. These benchmarks encompass: (1) diverse downstream tasks, including mathematical reasoning, tool using and code generation; (2) assessments of general helpfulness and safety alignment. Results on all benchmarks demonstrate the superiority of SDFT compared to vanilla fine-tuning. For instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2. The in-depth analysis of our method indicates that increasing the proportion of distilled dataset for fine-tuning leads to a decrease in catastrophic forgetting, thereby confirming that SDFT mitigates catastrophic forgetting by bridging the distribution gap.\n\n## Conclusions and Limitations\nIn this paper, we perform a systematic evaluation of catastrophic forgetting during the fine-tuning of language models for downstream tasks. Our findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness. To enhance performance on target task while maintaining LMs' broad capabilities, we propose a plug-and-play strategy, SDFT, to reduce distribution shift and miti-  gate catastrophic forgetting. Extensive experiments show that SDFT effectively diminishes forgetting and delivers comparable or superior performance to vanilla fine-tuning on targeted tasks. \n\nOur study is subject to certain limitations. Owing to constraints in computational resources, most of our experiments are based on the Llama-2-7bchat model with LoRA. Further investigations involving larger models and full fine-tuning remain to be explored. Furthermore, our safety evaluations are limited to the Advbench dataset and fixed adversarial suffixes, leaving the robustness against other jailbreaking strategies for future work.",
            "reference_string": "[267769989 | Yang et al. | 2024 | Citations: 49]"
        },
        {
            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 16,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01241, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2353260836",
                    "name": "Naimul Haque"
                }
            ],
            "abstract": "Large Language Models (LLMs) have significantly advanced Natural Language Processing (NLP), particularly in Natural Language Understanding (NLU) tasks. As we progress toward an agentic world where LLM-based agents autonomously handle specialized tasks, it becomes crucial for these models to adapt to new tasks without forgetting previously learned information - a challenge known as catastrophic forgetting. This study evaluates the continual fine-tuning of various open-source LLMs with different parameter sizes (specifically models under 10 billion parameters) on key NLU tasks from the GLUE benchmark, including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and task-specific adjustments, we assess and compare the models' abilities to retain prior knowledge while learning new tasks. Our results indicate that models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong learning capabilities, making them well-suited for continual learning environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate impressive learning abilities and overall performance after fine-tuning. This work contributes to understanding catastrophic forgetting in LLMs and highlights prompting engineering to optimize model performance for continual learning scenarios.",
            "corpus_id": 277501673,
            "sentences": [
                {
                    "corpus_id": "277501673",
                    "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
                    "text": "Catastrophic forgetting, a phenomenon first identified by (McCloskey and Cohen, 1989), remains a fundamental challenge in sequential learning tasks for neural networks. When models are trained on multiple tasks in sequence, they tend to overwrite previously acquired knowledge, leading to significant performance degradation on earlier tasks. Various methods have been proposed to mitigate this issue, such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), which regularizes weight updates to protect crucial parameters learned from previous tasks. Similarly, memory-based methods like Gradient Episodic Memory (GEM) (Lopez-Paz and Ranzato, 2017) address forgetting by storing and replaying examples from past tasks during training, thereby reducing interference. While recent LLM models show strong zero-shot performance, they often struggle with tasks outside their training and evaluation sets. To address this, (Scialom et al., 2022) propose Continual-T0 (CT0), a fine-tuned LLM capable of learning new tasks while retaining prior knowledge, largely due to the self-supervision pre-training process. (Kemker et al., 2017) showed that Deep neural networks struggle to learn new tasks without forgetting old ones, and various methods have been proposed to mitigate this, but their effectiveness varies depending on the training paradigm and data type. (Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension.",
                    "score": 0.54156437526208,
                    "section_title": "Related Works",
                    "char_start_offset": 2762,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 168
                        },
                        {
                            "start": 169,
                            "end": 342
                        },
                        {
                            "start": 343,
                            "end": 564
                        },
                        {
                            "start": 565,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 913
                        },
                        {
                            "start": 914,
                            "end": 1119
                        },
                        {
                            "start": 1120,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1665
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 58,
                            "end": 85,
                            "matchedPaperCorpusId": "61019113"
                        },
                        {
                            "start": 445,
                            "end": 471,
                            "matchedPaperCorpusId": "4704285"
                        },
                        {
                            "start": 633,
                            "end": 662,
                            "matchedPaperCorpusId": "37308416"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7470703125
                }
            ],
            "relevance_judgement": 0.7470703125,
            "relevance_judgment_input_expanded": "# Title: Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks\n# Venue: arXiv.org\n# Authors: Naimul Haque\n## Abstract\nLarge Language Models (LLMs) have significantly advanced Natural Language Processing (NLP), particularly in Natural Language Understanding (NLU) tasks. As we progress toward an agentic world where LLM-based agents autonomously handle specialized tasks, it becomes crucial for these models to adapt to new tasks without forgetting previously learned information - a challenge known as catastrophic forgetting. This study evaluates the continual fine-tuning of various open-source LLMs with different parameter sizes (specifically models under 10 billion parameters) on key NLU tasks from the GLUE benchmark, including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and task-specific adjustments, we assess and compare the models' abilities to retain prior knowledge while learning new tasks. Our results indicate that models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong learning capabilities, making them well-suited for continual learning environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate impressive learning abilities and overall performance after fine-tuning. This work contributes to understanding catastrophic forgetting in LLMs and highlights prompting engineering to optimize model performance for continual learning scenarios.\n## Related Works\nCatastrophic forgetting, a phenomenon first identified by (McCloskey and Cohen, 1989), remains a fundamental challenge in sequential learning tasks for neural networks. When models are trained on multiple tasks in sequence, they tend to overwrite previously acquired knowledge, leading to significant performance degradation on earlier tasks. Various methods have been proposed to mitigate this issue, such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), which regularizes weight updates to protect crucial parameters learned from previous tasks. Similarly, memory-based methods like Gradient Episodic Memory (GEM) (Lopez-Paz and Ranzato, 2017) address forgetting by storing and replaying examples from past tasks during training, thereby reducing interference. While recent LLM models show strong zero-shot performance, they often struggle with tasks outside their training and evaluation sets. To address this, (Scialom et al., 2022) propose Continual-T0 (CT0), a fine-tuned LLM capable of learning new tasks while retaining prior knowledge, largely due to the self-supervision pre-training process. (Kemker et al., 2017) showed that Deep neural networks struggle to learn new tasks without forgetting old ones, and various methods have been proposed to mitigate this, but their effectiveness varies depending on the training paradigm and data type. (Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension.",
            "reference_string": "[277501673 | Haque | 2025 | Citations: 0]"
        },
        {
            "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 59,
            "citation_count": 19,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.04836, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305524549",
                    "name": "Hongyu Li"
                },
                {
                    "authorId": "46573238",
                    "name": "Liang Ding"
                },
                {
                    "authorId": "2305485528",
                    "name": "Meng Fang"
                },
                {
                    "authorId": "2255502438",
                    "name": "D. Tao"
                }
            ],
            "abstract": "Catastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF.",
            "corpus_id": 270357611,
            "sentences": [
                {
                    "corpus_id": "270357611",
                    "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
                    "text": "Instruction fine-tuning is key to improving the capabilities and controllability of large language models (LLMs) (Brown et al., 2020;Fedus et al., 2022), which have already demonstrated strong performance in various tasks (Zhong et al., 2023;Peng et al., 2023;Lu et al., 2023;Ren et al., 2024).One major obstacle to tuning LLMs is catastrophic forgetting (CF, Kirkpatrick et al., 2017), which means LLMs forget prior knowledge when learning new data.Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities.\n\nUnderstanding the effectiveness of LLMs during fine-tuning is important for downstream tasks, however, the underlying causes of CF remain largely unexplored.There are two types of work addressing CF, from data and model perspectives, respectively.Scialom et al. (2022) propose continual learning with rehearsal in instruction tuning, though its effectiveness varies with task selection.Lin et al. (2023) indicate that both continual learning and weight averaging (Wise-FT) effectively preserve generality.\n\nWhile the above techniques are somewhat successful, they require expensive extra dataconstructing and training costs, and are even sometimes impractical in LLMs, because a) the data cards of many pretrained models are unclear (Shi et al., 2023), making rehearsal unfeasible, and b) anti-forgetting training brings an unstable and expensive training process (Datta et al., 2023).\n\nIn this work, we turn to finding a cheap, stable, and orthogonal solution to alleviate the CF in tuning LLMs.",
                    "score": 0.6739145086359912,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 294
                        },
                        {
                            "start": 294,
                            "end": 450
                        },
                        {
                            "start": 450,
                            "end": 788
                        },
                        {
                            "start": 790,
                            "end": 947
                        },
                        {
                            "start": 947,
                            "end": 1037
                        },
                        {
                            "start": 1037,
                            "end": 1176
                        },
                        {
                            "start": 1176,
                            "end": 1295
                        },
                        {
                            "start": 1297,
                            "end": 1675
                        },
                        {
                            "start": 1677,
                            "end": 1786
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 113,
                            "end": 133,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 1037,
                            "end": 1058,
                            "matchedPaperCorpusId": "252815378"
                        },
                        {
                            "start": 1523,
                            "end": 1541,
                            "matchedPaperCorpusId": "264451585"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7431640625
                }
            ],
            "relevance_judgement": 0.7431640625,
            "relevance_judgment_input_expanded": "# Title: Revisiting Catastrophic Forgetting in Large Language Model Tuning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Hongyu Li, Liang Ding, Meng Fang, D. Tao\n## Abstract\nCatastrophic Forgetting (CF) means models forgetting previously acquired knowledge when learning new data. It compromises the effectiveness of large language models (LLMs) during fine-tuning, yet the underlying causes have not been thoroughly investigated. This paper takes the first step to reveal the direct link between the flatness of the model loss landscape and the extent of CF in the field of LLMs. Based on this, we introduce the sharpness-aware minimization to mitigate CF by flattening the loss landscape. Experiments on three widely-used fine-tuning datasets, spanning different model scales, demonstrate the effectiveness of our method in alleviating CF. Analyses show that we nicely complement the existing anti-forgetting strategies, further enhancing the resistance of LLMs to CF.\n## Introduction\nInstruction fine-tuning is key to improving the capabilities and controllability of large language models (LLMs) (Brown et al., 2020;Fedus et al., 2022), which have already demonstrated strong performance in various tasks (Zhong et al., 2023;Peng et al., 2023;Lu et al., 2023;Ren et al., 2024).One major obstacle to tuning LLMs is catastrophic forgetting (CF, Kirkpatrick et al., 2017), which means LLMs forget prior knowledge when learning new data.Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities.\n\nUnderstanding the effectiveness of LLMs during fine-tuning is important for downstream tasks, however, the underlying causes of CF remain largely unexplored.There are two types of work addressing CF, from data and model perspectives, respectively.Scialom et al. (2022) propose continual learning with rehearsal in instruction tuning, though its effectiveness varies with task selection.Lin et al. (2023) indicate that both continual learning and weight averaging (Wise-FT) effectively preserve generality.\n\nWhile the above techniques are somewhat successful, they require expensive extra dataconstructing and training costs, and are even sometimes impractical in LLMs, because a) the data cards of many pretrained models are unclear (Shi et al., 2023), making rehearsal unfeasible, and b) anti-forgetting training brings an unstable and expensive training process (Datta et al., 2023).\n\nIn this work, we turn to finding a cheap, stable, and orthogonal solution to alleviate the CF in tuning LLMs.",
            "reference_string": "[270357611 | Li et al. | 2024 | Citations: 19]"
        },
        {
            "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 36,
            "citation_count": 22,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12134, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "26339093",
                    "name": "Yige Xu"
                },
                {
                    "authorId": "2325014301",
                    "name": "Xu Guo"
                },
                {
                    "authorId": "49512311",
                    "name": "Zhiwei Zeng"
                },
                {
                    "authorId": "2238949687",
                    "name": "Chunyan Miao"
                }
            ],
            "abstract": "Chain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often require full-model fine-tuning and suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the LLM. Specifically, we employ a lightweight fixed assistant model to speculatively generate instance-specific soft thought tokens as the initial chain of thoughts, which are then mapped into the LLM's representation space via a trainable projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning. Source code is available at https://github.com/xuyige/SoftCoT.",
            "corpus_id": 276421775,
            "sentences": [
                {
                    "corpus_id": "276421775",
                    "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
                    "text": "Yet, these methods require full-model fine-tuning, which incurs substantial computational costs, risks catastrophic forgetting, and limits their transferability across tasks. \n\nWe empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024). Thus, the methodologies of Coconut, which is based on GPT-2 (Radford et al., 2019), and CCoT, which is built upon LLaMA2-7B (Touvron et al., 2023), may not be directly applicable to more recent models such as LLaMA3.1-8B. Therefore, it is crucial to explore alternative methodologies that mitigate catastrophic forgetting while effectively leveraging continuous reasoning techniques in large-scale, instruction-tuned models, which is the main research question of this work. \n\nTo mitigate catastrophic forgetting, a straightforward approach is to freeze the backbone LLM and instead optimize an external model for reasoning. Inspired by prompt tuning (Lester et al., 2021) and speculative decoding (Leviathan et al., 2023), we propose an approach that utilizes an auxiliary small assistant model to generate a sequence of \"thought\" tokens conditioned on a task instruction followed by a specific instance (Li et al., 2023;Shao et al., 2023). These tokens serve as instancespecific prompts to boost LLM's inference. Such an auxiliary prompting mechanism dynamically adapts to different reasoning tasks, thereby improving generalization while preserving the pre-trained knowledge of the LLM. \n\nTo facilitate reasoning in a continuous space, we use soft thought tokens (i.e., the last-layer hidden states from the small assistant model before mapping to the vocabulary space) instead of discrete tokens. This ensures reasoning remains within the continuous latent space. However, a representational gap between the assistant model and the LLM may hinder effective knowledge transfer.",
                    "score": 0.5978028227975836,
                    "section_title": "Introduction",
                    "char_start_offset": 1938,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 174
                        },
                        {
                            "start": 177,
                            "end": 245
                        },
                        {
                            "start": 246,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 579
                        },
                        {
                            "start": 580,
                            "end": 801
                        },
                        {
                            "start": 802,
                            "end": 1054
                        },
                        {
                            "start": 1057,
                            "end": 1204
                        },
                        {
                            "start": 1205,
                            "end": 1521
                        },
                        {
                            "start": 1522,
                            "end": 1594
                        },
                        {
                            "start": 1595,
                            "end": 1769
                        },
                        {
                            "start": 1772,
                            "end": 1980
                        },
                        {
                            "start": 1981,
                            "end": 2047
                        },
                        {
                            "start": 2048,
                            "end": 2160
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 640,
                            "end": 662,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1231,
                            "end": 1252,
                            "matchedPaperCorpusId": "233296808"
                        },
                        {
                            "start": 1278,
                            "end": 1302,
                            "matchedPaperCorpusId": "257079124"
                        },
                        {
                            "start": 1502,
                            "end": 1520,
                            "matchedPaperCorpusId": "256459681"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.74072265625
                }
            ],
            "relevance_judgement": 0.74072265625,
            "relevance_judgment_input_expanded": "# Title: SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs\n# Venue: arXiv.org\n# Authors: Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao\n## Abstract\nChain-of-Thought (CoT) reasoning enables Large Language Models (LLMs) to solve complex reasoning tasks by generating intermediate reasoning steps. However, most existing approaches focus on hard token decoding, which constrains reasoning within the discrete vocabulary space and may not always be optimal. While recent efforts explore continuous-space reasoning, they often require full-model fine-tuning and suffer from catastrophic forgetting, limiting their applicability to state-of-the-art LLMs that already perform well in zero-shot settings with a proper instruction. To address this challenge, we propose a novel approach for continuous-space reasoning that does not require modifying the LLM. Specifically, we employ a lightweight fixed assistant model to speculatively generate instance-specific soft thought tokens as the initial chain of thoughts, which are then mapped into the LLM's representation space via a trainable projection module. Experimental results on five reasoning benchmarks demonstrate that our method enhances LLM reasoning performance through supervised, parameter-efficient fine-tuning. Source code is available at https://github.com/xuyige/SoftCoT.\n## Introduction\nYet, these methods require full-model fine-tuning, which incurs substantial computational costs, risks catastrophic forgetting, and limits their transferability across tasks. \n\nWe empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024). Thus, the methodologies of Coconut, which is based on GPT-2 (Radford et al., 2019), and CCoT, which is built upon LLaMA2-7B (Touvron et al., 2023), may not be directly applicable to more recent models such as LLaMA3.1-8B. Therefore, it is crucial to explore alternative methodologies that mitigate catastrophic forgetting while effectively leveraging continuous reasoning techniques in large-scale, instruction-tuned models, which is the main research question of this work. \n\nTo mitigate catastrophic forgetting, a straightforward approach is to freeze the backbone LLM and instead optimize an external model for reasoning. Inspired by prompt tuning (Lester et al., 2021) and speculative decoding (Leviathan et al., 2023), we propose an approach that utilizes an auxiliary small assistant model to generate a sequence of \"thought\" tokens conditioned on a task instruction followed by a specific instance (Li et al., 2023;Shao et al., 2023). These tokens serve as instancespecific prompts to boost LLM's inference. Such an auxiliary prompting mechanism dynamically adapts to different reasoning tasks, thereby improving generalization while preserving the pre-trained knowledge of the LLM. \n\nTo facilitate reasoning in a continuous space, we use soft thought tokens (i.e., the last-layer hidden states from the small assistant model before mapping to the vocabulary space) instead of discrete tokens. This ensures reasoning remains within the continuous latent space. However, a representational gap between the assistant model and the LLM may hinder effective knowledge transfer.",
            "reference_string": "[276421775 | Xu et al. | 2025 | Citations: 22]"
        },
        {
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.12227, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "66129075",
                    "name": "Gangwei Jiang"
                },
                {
                    "authorId": "2273744210",
                    "name": "Zhaoyi Li"
                },
                {
                    "authorId": "2266241113",
                    "name": "Defu Lian"
                },
                {
                    "authorId": "2261250492",
                    "name": "Ying Wei"
                }
            ],
            "abstract": "Fine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon.",
            "corpus_id": 270562404,
            "sentences": [
                {
                    "corpus_id": "270562404",
                    "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
                    "text": "Fine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon.",
                    "score": 0.4943780965672294,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.7373046875
                },
                {
                    "corpus_id": "270562404",
                    "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
                    "text": "We suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through Knowledge Probability P (y|x) and Instruction Probability P (y c |c, x), respectively (as depicted in Fig. 2).Our empirical analysis within a continual instruction tuning framework reveals distinct forgetting patterns between these two aspects, with shifts in instruction following primarily driving performance declines.\n\nTo investigate the internal changes of the model during forgetting, we introduce the Instruction Vector (IV) framework to extract representations closely associated with the task processing.We hypothesize a straightforward yet robust computational graph for LLMs (see Fig. 1 b), featuring an intermediate variable \u03b8 c crucial for task performance.The presence or absence of \u03b8 c directly impacts the model's capability to handle instruction c.This hypothesis is supported by causal intervention experiments in Sec.3.2.By analyzing IV dynamics pre and post-training, we find minor changes in IV expression with forgetting happens.Furthermore, explicitly incorporating IV into the model's computational graph can recover the mastery of the corresponding instruction.This results indicate that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting.\n\nBuilding on these insights, we develop an IVguided training methodology to mitigate catastrophic forgetting.This method incorporates a progressive IV-intervention training mechanism, in which the IV is initially introduced through intervention and is then gradually phased out during the training process.The deliberate inclusion of IV aids in optimizing the model by ensuring adherence to the IV-related computational graph, thereby minimizing the overshadowing effect of new reasoning pathways.Additionally, we have introduced an IVbased KL-Divergence loss function to reduce the discrepancies between zero-shot and IV-intervened logits, ensuring that the model's behavior remains aligned with the original computational structure.Validated across multiple datasets, this method significantly alleviate forgetting in both general and in-context learning abilities, confirming the link between IV and forgetting.\n\nMain Findings and Contributions.(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.",
                    "score": 0.6298533670958264,
                    "section_title": "Introduction",
                    "char_start_offset": 1804,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 247
                        },
                        {
                            "start": 247,
                            "end": 458
                        },
                        {
                            "start": 460,
                            "end": 650
                        },
                        {
                            "start": 650,
                            "end": 807
                        },
                        {
                            "start": 807,
                            "end": 902
                        },
                        {
                            "start": 902,
                            "end": 973
                        },
                        {
                            "start": 973,
                            "end": 977
                        },
                        {
                            "start": 977,
                            "end": 1088
                        },
                        {
                            "start": 1088,
                            "end": 1223
                        },
                        {
                            "start": 1223,
                            "end": 1372
                        },
                        {
                            "start": 1374,
                            "end": 1482
                        },
                        {
                            "start": 1482,
                            "end": 1679
                        },
                        {
                            "start": 1679,
                            "end": 1870
                        },
                        {
                            "start": 1870,
                            "end": 2107
                        },
                        {
                            "start": 2107,
                            "end": 2287
                        },
                        {
                            "start": 2289,
                            "end": 2321
                        },
                        {
                            "start": 2321,
                            "end": 2604
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66015625
                },
                {
                    "corpus_id": "270562404",
                    "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
                    "text": "Catastrophic forgetting in fine-tuned language models.Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2022;Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;Kotha et al., 2024;Wu et al., 2024b).In context of LLMs, existing approaches towards mit-igating this issue can mostly be categorized into three types: regularizing the update of model parameters (Kirkpatrick et al., 2017;Huang et al., 2021;Cha et al., 2021), replaying previous or selfsynthesized data (Scialom et al., 2022;Huang et al., 2024a) and resisting interference via parameterefficient fine-tuning (Razdaibiedina et al., 2023;Wang et al., 2023a).\n\nMechanistic analysis to fine-tuning.Existing works on analyzing the internal mechanism (R\u00e4uker et al., 2023;Ferrando et al., 2024) of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal transformation on top of the original capability (Jain et al., 2024) (wrappers), subtractable and reusable parameter shift vectors (Huang et al., 2024b;Gao et al., 2024) (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage (Ren et al., 2024).Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point.",
                    "score": 0.6660026237246774,
                    "section_title": "Related work",
                    "char_start_offset": 24807,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 54
                        },
                        {
                            "start": 54,
                            "end": 318
                        },
                        {
                            "start": 318,
                            "end": 566
                        },
                        {
                            "start": 566,
                            "end": 985
                        },
                        {
                            "start": 987,
                            "end": 1023
                        },
                        {
                            "start": 1023,
                            "end": 1571
                        },
                        {
                            "start": 1571,
                            "end": 1738
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 196,
                            "end": 214,
                            "matchedPaperCorpusId": "237416585"
                        },
                        {
                            "start": 529,
                            "end": 548,
                            "matchedPaperCorpusId": "262054014"
                        },
                        {
                            "start": 751,
                            "end": 770,
                            "matchedPaperCorpusId": "233210110"
                        },
                        {
                            "start": 770,
                            "end": 787,
                            "matchedPaperCorpusId": "219636462"
                        },
                        {
                            "start": 832,
                            "end": 854,
                            "matchedPaperCorpusId": "252815378"
                        },
                        {
                            "start": 937,
                            "end": 965,
                            "matchedPaperCorpusId": "256390383"
                        },
                        {
                            "start": 965,
                            "end": 984,
                            "matchedPaperCorpusId": "264426441"
                        },
                        {
                            "start": 1307,
                            "end": 1326,
                            "matchedPaperCorpusId": "265308865"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61083984375
                },
                {
                    "corpus_id": "270562404",
                    "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
                    "text": "(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.(2) We are the first to interpret forgetting with the Instruction Vector framework, identifying inherent changes during fine-tuning.The findings indicate that fine-tuning generally introduces specialized reasoning patterns rather than removing existing skills.(3) We develop an IV-guided training approach that focuses on preserving and realigning the model's computational graph during fine-tuning.This significantly enhances the general and in-context learning capabilities across various datasets in continual learning.",
                    "score": 0.6170135493866886,
                    "section_title": "Introduction",
                    "char_start_offset": 4125,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 283
                        },
                        {
                            "start": 283,
                            "end": 415
                        },
                        {
                            "start": 415,
                            "end": 543
                        },
                        {
                            "start": 543,
                            "end": 682
                        },
                        {
                            "start": 682,
                            "end": 805
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.60888671875
                }
            ],
            "relevance_judgement": 0.7373046875,
            "relevance_judgment_input_expanded": "# Title: Refine Large Language Model Fine-tuning via Instruction Vector\n# Venue: \n# Authors: Gangwei Jiang, Zhaoyi Li, Defu Lian, Ying Wei\n## Abstract\nFine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon.\n## Introduction\nWe suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through Knowledge Probability P (y|x) and Instruction Probability P (y c |c, x), respectively (as depicted in Fig. 2).Our empirical analysis within a continual instruction tuning framework reveals distinct forgetting patterns between these two aspects, with shifts in instruction following primarily driving performance declines.\n\nTo investigate the internal changes of the model during forgetting, we introduce the Instruction Vector (IV) framework to extract representations closely associated with the task processing.We hypothesize a straightforward yet robust computational graph for LLMs (see Fig. 1 b), featuring an intermediate variable \u03b8 c crucial for task performance.The presence or absence of \u03b8 c directly impacts the model's capability to handle instruction c.This hypothesis is supported by causal intervention experiments in Sec.3.2.By analyzing IV dynamics pre and post-training, we find minor changes in IV expression with forgetting happens.Furthermore, explicitly incorporating IV into the model's computational graph can recover the mastery of the corresponding instruction.This results indicate that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting.\n\nBuilding on these insights, we develop an IVguided training methodology to mitigate catastrophic forgetting.This method incorporates a progressive IV-intervention training mechanism, in which the IV is initially introduced through intervention and is then gradually phased out during the training process.The deliberate inclusion of IV aids in optimizing the model by ensuring adherence to the IV-related computational graph, thereby minimizing the overshadowing effect of new reasoning pathways.Additionally, we have introduced an IVbased KL-Divergence loss function to reduce the discrepancies between zero-shot and IV-intervened logits, ensuring that the model's behavior remains aligned with the original computational structure.Validated across multiple datasets, this method significantly alleviate forgetting in both general and in-context learning abilities, confirming the link between IV and forgetting.\n\nMain Findings and Contributions.(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.\n...\n(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.(2) We are the first to interpret forgetting with the Instruction Vector framework, identifying inherent changes during fine-tuning.The findings indicate that fine-tuning generally introduces specialized reasoning patterns rather than removing existing skills.(3) We develop an IV-guided training approach that focuses on preserving and realigning the model's computational graph during fine-tuning.This significantly enhances the general and in-context learning capabilities across various datasets in continual learning.\n\n## Related work\nCatastrophic forgetting in fine-tuned language models.Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2022;Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;Kotha et al., 2024;Wu et al., 2024b).In context of LLMs, existing approaches towards mit-igating this issue can mostly be categorized into three types: regularizing the update of model parameters (Kirkpatrick et al., 2017;Huang et al., 2021;Cha et al., 2021), replaying previous or selfsynthesized data (Scialom et al., 2022;Huang et al., 2024a) and resisting interference via parameterefficient fine-tuning (Razdaibiedina et al., 2023;Wang et al., 2023a).\n\nMechanistic analysis to fine-tuning.Existing works on analyzing the internal mechanism (R\u00e4uker et al., 2023;Ferrando et al., 2024) of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal transformation on top of the original capability (Jain et al., 2024) (wrappers), subtractable and reusable parameter shift vectors (Huang et al., 2024b;Gao et al., 2024) (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage (Ren et al., 2024).Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point.",
            "reference_string": "[270562404 | Jiang et al. | 2024 | Citations: 1]"
        },
        {
            "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
            "venue": "",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.13669, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2152569963",
                    "name": "Shezheng Song"
                },
                {
                    "authorId": "2341857958",
                    "name": "Hao Xu"
                },
                {
                    "authorId": "2152610427",
                    "name": "Jun Ma"
                },
                {
                    "authorId": "1484986130",
                    "name": "Shasha Li"
                },
                {
                    "authorId": "2154357858",
                    "name": "Long Peng"
                },
                {
                    "authorId": "2304446070",
                    "name": "Qian Wan"
                },
                {
                    "authorId": "2110779334",
                    "name": "Xiaodong Liu"
                },
                {
                    "authorId": "2116457427",
                    "name": "Jie Yu"
                }
            ],
            "abstract": "Large Language Models (LLMs) exhibit strong general language capabilities. However, fine-tuning these models on domain-specific tasks often leads to catastrophic forgetting, where the model overwrites or loses essential knowledge acquired during pretraining. This phenomenon significantly limits the broader applicability of LLMs. To address this challenge, we propose a novel approach to compute the element-wise importance of model parameters crucial for preserving general knowledge during fine-tuning. Our method utilizes a dual-objective optimization strategy: (1) regularization loss based on element-wise parameter importance, which constrains the updates to parameters crucial for general knowledge; (2) cross-entropy loss to adapt to domain-specific tasks. Additionally, we introduce layer-wise coefficients to account for the varying contributions of different layers, dynamically balancing the dual-objective optimization. Extensive experiments on scientific, medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our approach mitigates catastrophic forgetting while enhancing model adaptability. Compared to previous methods, our solution is approximately 20 times faster and requires only 10-15% of the storage, highlighting the practical efficiency. The code will be released.",
            "corpus_id": 275820535,
            "sentences": [
                {
                    "corpus_id": "275820535",
                    "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
                    "text": "Large Language Models (LLMs) are pretrained on massive and diverse datasets, equipping them with remarkable general capabilities (Wang & Komatsuzaki, 2021;Touvron et al., 2023b;OpenAI, 2024). This pretraining process allows LLMs to serve as versatile tools for a wide range of natural language processing tasks. However, in domains such as medical and scientific fields, LLMs often struggle  to perform effectively, necessitating fine-tuning domainspecific data. While fine-tuning could enhance the model task-specific performance, it also introduces a critical challenge: catastrophic forgetting (Kirkpatrick et al., 2016;Kemker et al., 2018;Shao & Feng, 2022;Ren et al., 2024). \n\nAs shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering. \n\nAddressing catastrophic forgetting is therefore a crucial requirement for maximizing the utility of LLMs. A successful solution needs to achieve a delicate balance: retaining the essential general knowledge when learning new domainspecific expertise. This balance is critical when fine-tuning LLMs for specialized tasks, as both domain adaptation and generalizability are necessary for practical applications. EW-CLoRA (Xiang et al., 2024) focuses on the issue of catastrophic forgetting in LLM fine-tuning and uses the Fisher matrix to measure the importance of parameters for general capabilities. However, it requires gradients computed with labels from the model distribution, necessitating an additional backpropagation pass for online computation. Thus, its computational cost is very high.",
                    "score": 0.6321651282927647,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 311
                        },
                        {
                            "start": 312,
                            "end": 462
                        },
                        {
                            "start": 463,
                            "end": 679
                        },
                        {
                            "start": 682,
                            "end": 859
                        },
                        {
                            "start": 860,
                            "end": 1001
                        },
                        {
                            "start": 1002,
                            "end": 1129
                        },
                        {
                            "start": 1130,
                            "end": 1276
                        },
                        {
                            "start": 1277,
                            "end": 1425
                        },
                        {
                            "start": 1428,
                            "end": 1533
                        },
                        {
                            "start": 1534,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1837
                        },
                        {
                            "start": 1838,
                            "end": 2027
                        },
                        {
                            "start": 2028,
                            "end": 2181
                        },
                        {
                            "start": 2182,
                            "end": 2224
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 597,
                            "end": 623,
                            "matchedPaperCorpusId": "4704285"
                        },
                        {
                            "start": 623,
                            "end": 643,
                            "matchedPaperCorpusId": "22910766"
                        },
                        {
                            "start": 643,
                            "end": 661,
                            "matchedPaperCorpusId": "247315450"
                        },
                        {
                            "start": 1847,
                            "end": 1867,
                            "matchedPaperCorpusId": "258762577"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.736328125
                },
                {
                    "corpus_id": "275820535",
                    "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
                    "text": "This paper addresses the critical issue of catastrophic forgetting in large language models (LLMs) during domainspecific fine-tuning. We propose a novel fine-tuning framework that preserves general capabilities while enabling efficient adaptation to new domains, minimizing knowledge loss in tasks outside the fine-tuned domain. Additionally, we introduce a layer-wise coefficient to adjust the balance between regularization loss and cross-entropy loss dynamically. This adjustment accounts for the varying contributions of different layers to both generalization and domainspecific learning. Extensive experiments in scientific, physical, and medical tasks show that our framework effectively mitigates catastrophic forgetting while maintaining performance in domain-specific tasks.",
                    "score": 0.6187777433861623,
                    "section_title": "Conclusion",
                    "char_start_offset": 24455,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 133
                        },
                        {
                            "start": 134,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 466
                        },
                        {
                            "start": 467,
                            "end": 593
                        },
                        {
                            "start": 594,
                            "end": 784
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.65380859375
                }
            ],
            "relevance_judgement": 0.736328125,
            "relevance_judgment_input_expanded": "# Title: How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization\n# Venue: \n# Authors: Shezheng Song, Hao Xu, Jun Ma, Shasha Li, Long Peng, Qian Wan, Xiaodong Liu, Jie Yu\n## Abstract\nLarge Language Models (LLMs) exhibit strong general language capabilities. However, fine-tuning these models on domain-specific tasks often leads to catastrophic forgetting, where the model overwrites or loses essential knowledge acquired during pretraining. This phenomenon significantly limits the broader applicability of LLMs. To address this challenge, we propose a novel approach to compute the element-wise importance of model parameters crucial for preserving general knowledge during fine-tuning. Our method utilizes a dual-objective optimization strategy: (1) regularization loss based on element-wise parameter importance, which constrains the updates to parameters crucial for general knowledge; (2) cross-entropy loss to adapt to domain-specific tasks. Additionally, we introduce layer-wise coefficients to account for the varying contributions of different layers, dynamically balancing the dual-objective optimization. Extensive experiments on scientific, medical, and physical tasks using GPT-J and LLaMA-3 demonstrate that our approach mitigates catastrophic forgetting while enhancing model adaptability. Compared to previous methods, our solution is approximately 20 times faster and requires only 10-15% of the storage, highlighting the practical efficiency. The code will be released.\n## Introduction\nLarge Language Models (LLMs) are pretrained on massive and diverse datasets, equipping them with remarkable general capabilities (Wang & Komatsuzaki, 2021;Touvron et al., 2023b;OpenAI, 2024). This pretraining process allows LLMs to serve as versatile tools for a wide range of natural language processing tasks. However, in domains such as medical and scientific fields, LLMs often struggle  to perform effectively, necessitating fine-tuning domainspecific data. While fine-tuning could enhance the model task-specific performance, it also introduces a critical challenge: catastrophic forgetting (Kirkpatrick et al., 2016;Kemker et al., 2018;Shao & Feng, 2022;Ren et al., 2024). \n\nAs shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering. \n\nAddressing catastrophic forgetting is therefore a crucial requirement for maximizing the utility of LLMs. A successful solution needs to achieve a delicate balance: retaining the essential general knowledge when learning new domainspecific expertise. This balance is critical when fine-tuning LLMs for specialized tasks, as both domain adaptation and generalizability are necessary for practical applications. EW-CLoRA (Xiang et al., 2024) focuses on the issue of catastrophic forgetting in LLM fine-tuning and uses the Fisher matrix to measure the importance of parameters for general capabilities. However, it requires gradients computed with labels from the model distribution, necessitating an additional backpropagation pass for online computation. Thus, its computational cost is very high.\n\n## Conclusion\nThis paper addresses the critical issue of catastrophic forgetting in large language models (LLMs) during domainspecific fine-tuning. We propose a novel fine-tuning framework that preserves general capabilities while enabling efficient adaptation to new domains, minimizing knowledge loss in tasks outside the fine-tuned domain. Additionally, we introduce a layer-wise coefficient to adjust the balance between regularization loss and cross-entropy loss dynamically. This adjustment accounts for the varying contributions of different layers to both generalization and domainspecific learning. Extensive experiments in scientific, physical, and medical tasks show that our framework effectively mitigates catastrophic forgetting while maintaining performance in domain-specific tasks.",
            "reference_string": "[275820535 | Song et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
            "venue": "International Conference on Machine Learning",
            "year": 2023,
            "reference_count": 67,
            "citation_count": 18,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2312.12736, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275190738",
                    "name": "Jiachen Zhao"
                },
                {
                    "authorId": "2275284008",
                    "name": "Zhun Deng"
                },
                {
                    "authorId": "40373515",
                    "name": "David Madras"
                },
                {
                    "authorId": "2276186660",
                    "name": "James Zou"
                },
                {
                    "authorId": "2540599",
                    "name": "Mengye Ren"
                }
            ],
            "abstract": "As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the\"ForgetFilter\"algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.",
            "corpus_id": 266374841,
            "sentences": [
                {
                    "corpus_id": "266374841",
                    "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
                    "text": "After Release S 0 S 0 + S 1 S 1 + Figure 1: An LLM will usually evolve through different sessions of training in its life time. Before release, the LLM is first pre-trained (session S 0 ) and then undergoes safety finetuning for alignment (session S 0 +). The released LLM will then be finetuned on some custom downstream data (session S 1 ), which potentially contain unsafe examples. A sequential safety finetuning session (i.e., S 1 +) may be needed again. This work studies the safety concerns of released LLMs by examining the learning process in downstream finetuning and the forgetting patterns during subsequent safety finetuning. Our goal is to design methods that ensure the safety of customized finetuning without compromising learning important downstream knowledge. \n\nthe efficacy of these precautions in resisting potential vulnerabilities during customized finetuning remains uncertain. If aligned LLMs can be jailbroken during customized finetuning, it is crucial to study whether safety finetuning following downstream finetuning is still suitable for recovering the safety in this case. See Figure 1 for a work flow diagram of downstream finetuning and safety finetuning before and after the release of LLMs. Furthermore, catastrophic forgetting (CF) (McCloskey & Cohen, 1989) may happen during safety finetuning, which can cause LLMs to forget previously learned knowledge apart from unsafe knowledge. Therefore, it is imperative to explore strategies in addition to safety finetuning to retain as much downstream knowledge as possible while keeping LLMs safe. \n\nTo this end, in this work we study how LLMs of different scales learn unsafe examples during customized downstream finetuning and more importantly, how they forget those unsafe examples and other data in the sequential safety finetuning stages. We begin by constructing noisy downstream datasets (e.g., question answering) for finetuning, containing a variety of data sources (including unsafe examples). Our investigation confirms the vulnerability of aligned LLMs to downstream finetuning on such noisy datasets containing unsafe examples and shows that larger LMs exhibit a faster acquisition of unsafe knowledge. Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten.",
                    "score": 0.5047634547655313,
                    "section_title": "Finetuning Safety Finetuning",
                    "char_start_offset": 2260,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 127
                        },
                        {
                            "start": 128,
                            "end": 255
                        },
                        {
                            "start": 256,
                            "end": 385
                        },
                        {
                            "start": 386,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 778
                        },
                        {
                            "start": 781,
                            "end": 901
                        },
                        {
                            "start": 902,
                            "end": 1104
                        },
                        {
                            "start": 1105,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1420
                        },
                        {
                            "start": 1421,
                            "end": 1579
                        },
                        {
                            "start": 1582,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 1986
                        },
                        {
                            "start": 1987,
                            "end": 2198
                        },
                        {
                            "start": 2199,
                            "end": 2381
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1269,
                            "end": 1293,
                            "matchedPaperCorpusId": "61019113"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70703125
                }
            ],
            "relevance_judgement": 0.70703125,
            "relevance_judgment_input_expanded": "# Title: Learning and Forgetting Unsafe Examples in Large Language Models\n# Venue: International Conference on Machine Learning\n# Authors: Jiachen Zhao, Zhun Deng, David Madras, James Zou, Mengye Ren\n## Abstract\nAs the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the\"ForgetFilter\"algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score.\n## Finetuning Safety Finetuning\nAfter Release S 0 S 0 + S 1 S 1 + Figure 1: An LLM will usually evolve through different sessions of training in its life time. Before release, the LLM is first pre-trained (session S 0 ) and then undergoes safety finetuning for alignment (session S 0 +). The released LLM will then be finetuned on some custom downstream data (session S 1 ), which potentially contain unsafe examples. A sequential safety finetuning session (i.e., S 1 +) may be needed again. This work studies the safety concerns of released LLMs by examining the learning process in downstream finetuning and the forgetting patterns during subsequent safety finetuning. Our goal is to design methods that ensure the safety of customized finetuning without compromising learning important downstream knowledge. \n\nthe efficacy of these precautions in resisting potential vulnerabilities during customized finetuning remains uncertain. If aligned LLMs can be jailbroken during customized finetuning, it is crucial to study whether safety finetuning following downstream finetuning is still suitable for recovering the safety in this case. See Figure 1 for a work flow diagram of downstream finetuning and safety finetuning before and after the release of LLMs. Furthermore, catastrophic forgetting (CF) (McCloskey & Cohen, 1989) may happen during safety finetuning, which can cause LLMs to forget previously learned knowledge apart from unsafe knowledge. Therefore, it is imperative to explore strategies in addition to safety finetuning to retain as much downstream knowledge as possible while keeping LLMs safe. \n\nTo this end, in this work we study how LLMs of different scales learn unsafe examples during customized downstream finetuning and more importantly, how they forget those unsafe examples and other data in the sequential safety finetuning stages. We begin by constructing noisy downstream datasets (e.g., question answering) for finetuning, containing a variety of data sources (including unsafe examples). Our investigation confirms the vulnerability of aligned LLMs to downstream finetuning on such noisy datasets containing unsafe examples and shows that larger LMs exhibit a faster acquisition of unsafe knowledge. Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten.",
            "reference_string": "[266374841 | Zhao et al. | 2023 | Citations: 18]"
        },
        {
            "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 34,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.14502, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282523455",
                    "name": "Sergey Pletenev"
                },
                {
                    "authorId": "2346325484",
                    "name": "Maria Marina"
                },
                {
                    "authorId": "2268318729",
                    "name": "Daniil Moskovskiy"
                },
                {
                    "authorId": "2329735486",
                    "name": "Vasily Konovalov"
                },
                {
                    "authorId": "2346329178",
                    "name": "Pavel Braslavski"
                },
                {
                    "authorId": "2282523462",
                    "name": "Alexander Panchenko"
                },
                {
                    "authorId": "2253458749",
                    "name": "M. Salnikov"
                }
            ],
            "abstract": "The performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.",
            "corpus_id": 276482659,
            "sentences": [
                {
                    "corpus_id": "276482659",
                    "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
                    "text": "However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2024;Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark. \n\nWe investigate the extent to which additional knowledge can be integrated into LLMs via the LoRA adapter while preserving its general capabilities. We seek to identify the underlying reasons for any performance drops when new information is introduced, and explore strategies to effectively minimize these adverse effects. \n\nOur contributions are as follows: \n\n\u2022 We conducted a series of extensive experiments incorporating into the LoRA model 1, 10, 50, 100, 500 and 3000 facts unknown to the model tracking how the model degrades intrinsically (via positive and negative shifts) and extrinsically (by tracking the degradation of reasoning abilities on the external benchmarks, such as MMLU and TruthfulQA). \n\n\u2022 We introduced two fine-tuning techniques to mitigate negative shifts and degradation of the model's reasoning abilities: (1) adding paraphrased new facts, and (2) adding facts the model already knows -and conducted a careful analysis of the results obtained. \n\n\u2022 Despite the possible degradation of the model, we found positive shifts -the cases where the model learned new knowledge for which it was not trained, and explained the nature of these shifts. \n\nWe release code and data for further usage.1",
                    "score": 0.5681934451572948,
                    "section_title": "Introduction",
                    "char_start_offset": 1801,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 481
                        },
                        {
                            "start": 484,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 806
                        },
                        {
                            "start": 809,
                            "end": 842
                        },
                        {
                            "start": 845,
                            "end": 1192
                        },
                        {
                            "start": 1195,
                            "end": 1455
                        },
                        {
                            "start": 1458,
                            "end": 1652
                        },
                        {
                            "start": 1655,
                            "end": 1699
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 88,
                            "end": 109,
                            "matchedPaperCorpusId": "266348792"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.70068359375
                }
            ],
            "relevance_judgement": 0.70068359375,
            "relevance_judgment_input_expanded": "# Title: How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Sergey Pletenev, Maria Marina, Daniil Moskovskiy, Vasily Konovalov, Pavel Braslavski, Alexander Panchenko, M. Salnikov\n## Abstract\nThe performance of Large Language Models (LLMs) on many tasks is greatly limited by the knowledge learned during pre-training and stored in the model's parameters. Low-rank adaptation (LoRA) is a popular and efficient training technique for updating or domain-specific adaptation of LLMs. In this study, we investigate how new facts can be incorporated into the LLM using LoRA without compromising the previously learned knowledge. We fine-tuned Llama-3.1-8B-instruct using LoRA with varying amounts of new knowledge. Our experiments have shown that the best results are obtained when the training data contains a mixture of known and new facts. However, this approach is still potentially harmful because the model's performance on external question-answering benchmarks declines after such fine-tuning. When the training data is biased towards certain entities, the model tends to regress to few overrepresented answers. In addition, we found that the model becomes more confident and refuses to provide an answer in only few cases. These findings highlight the potential pitfalls of LoRA-based LLM updates and underscore the importance of training data composition and tuning parameters to balance new knowledge integration and general model capabilities.\n## Introduction\nHowever, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2024;Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark. \n\nWe investigate the extent to which additional knowledge can be integrated into LLMs via the LoRA adapter while preserving its general capabilities. We seek to identify the underlying reasons for any performance drops when new information is introduced, and explore strategies to effectively minimize these adverse effects. \n\nOur contributions are as follows: \n\n\u2022 We conducted a series of extensive experiments incorporating into the LoRA model 1, 10, 50, 100, 500 and 3000 facts unknown to the model tracking how the model degrades intrinsically (via positive and negative shifts) and extrinsically (by tracking the degradation of reasoning abilities on the external benchmarks, such as MMLU and TruthfulQA). \n\n\u2022 We introduced two fine-tuning techniques to mitigate negative shifts and degradation of the model's reasoning abilities: (1) adding paraphrased new facts, and (2) adding facts the model already knows -and conducted a careful analysis of the results obtained. \n\n\u2022 Despite the possible degradation of the model, we found positive shifts -the cases where the model learned new knowledge for which it was not trained, and explained the nature of these shifts. \n\nWe release code and data for further usage.1",
            "reference_string": "[276482659 | Pletenev et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
            "venue": "",
            "year": 2024,
            "reference_count": 56,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.08526, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2809051",
                    "name": "Junhao Zheng"
                },
                {
                    "authorId": "2273927657",
                    "name": "Shengjie Qiu"
                },
                {
                    "authorId": "2274027644",
                    "name": "Qianli Ma"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved remarkable success across various tasks, yet their ability to learn incrementally without forgetting remains underexplored. Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning. Existing benchmarks for IL are insufficient due to data leakage issues and the overqualification of LLMs. To address these challenges, we introduce Concept-1K, a novel dataset comprising 1,023 recently emerged concepts across diverse domains. The concepts in Concept-1K are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. Using Concept-1K as a testbed, we aim to answer the question: ``Can LLMs learn new concepts incrementally without forgetting like humans?'' Our investigation reveals that LLMs still suffer from catastrophic forgetting and that LoRA, despite fine-tuning fewer parameters, may lead to more forgetting on training data. Additionally, we explore the roles of in-context learning, model scale, buffer size, and pretraining in IL performance. These findings highlight the strengths and limitations of LLMs in IL scenarios and provide a robust benchmark for future research.",
            "corpus_id": 267636596,
            "sentences": [
                {
                    "corpus_id": "267636596",
                    "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
                    "text": "Main Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data.",
                    "score": 0.7524659139123779,
                    "section_title": "RQ1: Can LLMs learn new concepts incrementally without forgetting?",
                    "char_start_offset": 11215,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 105
                        },
                        {
                            "start": 106,
                            "end": 178
                        },
                        {
                            "start": 179,
                            "end": 307
                        },
                        {
                            "start": 308,
                            "end": 432
                        },
                        {
                            "start": 435,
                            "end": 541
                        },
                        {
                            "start": 542,
                            "end": 696
                        },
                        {
                            "start": 697,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 911
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.69873046875
                }
            ],
            "relevance_judgement": 0.69873046875,
            "relevance_judgment_input_expanded": "# Title: Can LLMs Learn New Concepts Incrementally without Forgetting?\n# Venue: \n# Authors: Junhao Zheng, Shengjie Qiu, Qianli Ma\n## Abstract\nLarge Language Models (LLMs) have achieved remarkable success across various tasks, yet their ability to learn incrementally without forgetting remains underexplored. Incremental learning (IL) is crucial as it enables models to acquire new knowledge while retaining previously learned information, akin to human learning. Existing benchmarks for IL are insufficient due to data leakage issues and the overqualification of LLMs. To address these challenges, we introduce Concept-1K, a novel dataset comprising 1,023 recently emerged concepts across diverse domains. The concepts in Concept-1K are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. Using Concept-1K as a testbed, we aim to answer the question: ``Can LLMs learn new concepts incrementally without forgetting like humans?'' Our investigation reveals that LLMs still suffer from catastrophic forgetting and that LoRA, despite fine-tuning fewer parameters, may lead to more forgetting on training data. Additionally, we explore the roles of in-context learning, model scale, buffer size, and pretraining in IL performance. These findings highlight the strengths and limitations of LLMs in IL scenarios and provide a robust benchmark for future research.\n## RQ1: Can LLMs learn new concepts incrementally without forgetting?\nMain Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data.",
            "reference_string": "[267636596 | Zheng et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
            "venue": "NAACL-HLT",
            "year": 2024,
            "reference_count": 33,
            "citation_count": 7,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.02756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2151036536",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2248150493",
                    "name": "Fei Mi"
                },
                {
                    "authorId": "2165302640",
                    "name": "Yi Chen"
                },
                {
                    "authorId": "2248039956",
                    "name": "Boyang Xue"
                },
                {
                    "authorId": "22642319",
                    "name": "Hongru Wang"
                },
                {
                    "authorId": "2269768949",
                    "name": "Qi Zhu"
                },
                {
                    "authorId": "2274098358",
                    "name": "Kam-Fai Wong"
                },
                {
                    "authorId": "8233941",
                    "name": "Rui-Lan Xu"
                }
            ],
            "abstract": "The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains. Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.",
            "corpus_id": 268247412,
            "sentences": [
                {
                    "corpus_id": "268247412",
                    "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
                    "text": "Catastrophic Forgetting It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency. To mitigate catastrophic forgetting, particularly in the context of continual learning, researchers have explored three kinds of strategies. Exemplar replay involves preserving and revisiting key training examples to maintain model performance (He et al., 2019;Lopez-Paz and Ranzato, 2017). Regularization methods introduce regulation functions in addition to the loss function to constrain the learning process (Lin et al., 2023;Li and Hoiem, 2018). Architectural methods adjust the model's structure by adding parameters specific to new tasks or domains (Zhu et al., 2022). Our task setting is to train an LLM that can competently handle multiple domains concurrently, with minimal impairment to its generalist capabilities, differentiating from continual learning where the model is exposed to tasks sequentially, striving to prevent significant forgetting of earlier tasks (Zhu et al., 2022). \n\nInter-domain Confusion Furthermore, training a single LLM for multiple domains risks triggering inter-domain confusion where the LLM may not perform as well in each domain due to the blending of domain-specific knowledge (Wang et al., 2023b;Sheng et al., 2021). Therefore, some studies have been directed toward identifying commonalities across domains to maintain model performance while preserving the unique characteristics of each domain (Wang et al., 2023b;Sheng et al., 2021). In this paper, we propose to utilize Role Prompting to alleviate inter-domain confusion. \n\nRole Prompting Previous works found that role prompting can significantly improve the performance of LLMs. For example, Character.AI1 proposes a dialogue agent mimicking diversified figures, which can bring enriched user experience. Moreover, Wu et al. (2023b) found LLMs can effectively evaluate summarization results with diversified role prompts from varied perspectives. Kong et al. (2023) found role prompting can also boost the complex reasoning abilities of LLMs.",
                    "score": 0.554083193697988,
                    "section_title": "Related Work",
                    "char_start_offset": 3976,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 234
                        },
                        {
                            "start": 235,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 469
                        },
                        {
                            "start": 470,
                            "end": 619
                        },
                        {
                            "start": 620,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 904
                        },
                        {
                            "start": 905,
                            "end": 1225
                        },
                        {
                            "start": 1228,
                            "end": 1489
                        },
                        {
                            "start": 1490,
                            "end": 1710
                        },
                        {
                            "start": 1711,
                            "end": 1799
                        },
                        {
                            "start": 1802,
                            "end": 1908
                        },
                        {
                            "start": 1909,
                            "end": 2034
                        },
                        {
                            "start": 2035,
                            "end": 2176
                        },
                        {
                            "start": 2177,
                            "end": 2272
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 590,
                            "end": 618,
                            "matchedPaperCorpusId": "37308416"
                        },
                        {
                            "start": 759,
                            "end": 778,
                            "matchedPaperCorpusId": "4853851"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.68212890625
                }
            ],
            "relevance_judgement": 0.68212890625,
            "relevance_judgment_input_expanded": "# Title: Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models\n# Venue: NAACL-HLT\n# Authors: Rui Wang, Fei Mi, Yi Chen, Boyang Xue, Hongru Wang, Qi Zhu, Kam-Fai Wong, Rui-Lan Xu\n## Abstract\nThe growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains. Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.\n## Related Work\nCatastrophic Forgetting It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency. To mitigate catastrophic forgetting, particularly in the context of continual learning, researchers have explored three kinds of strategies. Exemplar replay involves preserving and revisiting key training examples to maintain model performance (He et al., 2019;Lopez-Paz and Ranzato, 2017). Regularization methods introduce regulation functions in addition to the loss function to constrain the learning process (Lin et al., 2023;Li and Hoiem, 2018). Architectural methods adjust the model's structure by adding parameters specific to new tasks or domains (Zhu et al., 2022). Our task setting is to train an LLM that can competently handle multiple domains concurrently, with minimal impairment to its generalist capabilities, differentiating from continual learning where the model is exposed to tasks sequentially, striving to prevent significant forgetting of earlier tasks (Zhu et al., 2022). \n\nInter-domain Confusion Furthermore, training a single LLM for multiple domains risks triggering inter-domain confusion where the LLM may not perform as well in each domain due to the blending of domain-specific knowledge (Wang et al., 2023b;Sheng et al., 2021). Therefore, some studies have been directed toward identifying commonalities across domains to maintain model performance while preserving the unique characteristics of each domain (Wang et al., 2023b;Sheng et al., 2021). In this paper, we propose to utilize Role Prompting to alleviate inter-domain confusion. \n\nRole Prompting Previous works found that role prompting can significantly improve the performance of LLMs. For example, Character.AI1 proposes a dialogue agent mimicking diversified figures, which can bring enriched user experience. Moreover, Wu et al. (2023b) found LLMs can effectively evaluate summarization results with diversified role prompts from varied perspectives. Kong et al. (2023) found role prompting can also boost the complex reasoning abilities of LLMs.",
            "reference_string": "[268247412 | Wang et al. | 2024 | Citations: 7]"
        },
        {
            "title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2249778653",
                    "name": "Minda Hu"
                },
                {
                    "authorId": "2121435800",
                    "name": "Licheng Zong"
                },
                {
                    "authorId": "22642319",
                    "name": "Hongru Wang"
                },
                {
                    "authorId": "30887444",
                    "name": "Jingyan Zhou"
                },
                {
                    "authorId": "2274072714",
                    "name": "Jingjing Li"
                },
                {
                    "authorId": "2307592921",
                    "name": "Yichen Gao"
                },
                {
                    "authorId": "2237563835",
                    "name": "Kam-Fai Wong"
                },
                {
                    "authorId": "2307192846",
                    "name": "Yu Li"
                },
                {
                    "authorId": "2258549144",
                    "name": "Irwin King"
                }
            ],
            "abstract": "Large Language Models (LLMs) have shown great potential in the biomedical domain with the advancement of retrieval-augmented generation (RAG). However, existing retrieval-augmented approaches face challenges in addressing diverse queries and documents, particularly for medical knowledge queries, resulting in sub-optimal performance. To address these limitations, we propose a novel plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search (SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm. By combining the reasoning capabilities of LLMs with the effectiveness of tree search, SeRTS boosts the zero-shot performance of retrieving high-quality and informative results for RAG. We further enhance retrieval performance by fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the trajectories collected by SeRTS as feedback. Controlled experiments using the BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method significantly improves the performance of the BM25 retriever and surpasses the strong baseline of self-reflection in both efficiency and scalability. Moreover, SeRTS generates higher-quality feedback for PPO training than self-reflection. Our proposed method effectively adapts LLMs to document retrieval tasks, enhancing their ability to retrieve highly relevant documents for RAG in the context of medical knowledge queries. This work presents a significant step forward in leveraging LLMs for accurate and comprehensive biomedical question answering.",
            "corpus_id": 270559716,
            "sentences": [
                {
                    "corpus_id": "270559716",
                    "title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation",
                    "text": "PPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement.",
                    "score": 0.5594959531919809,
                    "section_title": "Limitations",
                    "char_start_offset": 23201,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 135
                        },
                        {
                            "start": 136,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 449
                        },
                        {
                            "start": 450,
                            "end": 545
                        },
                        {
                            "start": 546,
                            "end": 717
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.67529296875
                }
            ],
            "relevance_judgement": 0.67529296875,
            "relevance_judgment_input_expanded": "# Title: SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, Irwin King\n## Abstract\nLarge Language Models (LLMs) have shown great potential in the biomedical domain with the advancement of retrieval-augmented generation (RAG). However, existing retrieval-augmented approaches face challenges in addressing diverse queries and documents, particularly for medical knowledge queries, resulting in sub-optimal performance. To address these limitations, we propose a novel plug-and-play LLM-based retrieval method called Self-Rewarding Tree Search (SeRTS) based on Monte Carlo Tree Search (MCTS) and a self-rewarding paradigm. By combining the reasoning capabilities of LLMs with the effectiveness of tree search, SeRTS boosts the zero-shot performance of retrieving high-quality and informative results for RAG. We further enhance retrieval performance by fine-tuning LLMs with Proximal Policy Optimization (PPO) objectives using the trajectories collected by SeRTS as feedback. Controlled experiments using the BioASQ-QA dataset with GPT-3.5-Turbo and LLama2-7b demonstrate that our method significantly improves the performance of the BM25 retriever and surpasses the strong baseline of self-reflection in both efficiency and scalability. Moreover, SeRTS generates higher-quality feedback for PPO training than self-reflection. Our proposed method effectively adapts LLMs to document retrieval tasks, enhancing their ability to retrieve highly relevant documents for RAG in the context of medical knowledge queries. This work presents a significant step forward in leveraging LLMs for accurate and comprehensive biomedical question answering.\n## Limitations\nPPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement.",
            "reference_string": "[270559716 | Hu et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 85,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.10306, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2290987204",
                    "name": "Hengyuan Zhang"
                },
                {
                    "authorId": "2297192655",
                    "name": "Yanru Wu"
                },
                {
                    "authorId": "2346989732",
                    "name": "Dawei Li"
                },
                {
                    "authorId": "2346998771",
                    "name": "Sak Yang"
                },
                {
                    "authorId": "2287784354",
                    "name": "Rui Zhao"
                },
                {
                    "authorId": "2290854859",
                    "name": "Yong Jiang"
                },
                {
                    "authorId": "2186741338",
                    "name": "Fei Tan"
                }
            ],
            "abstract": "Aligned Large Language Models (LLMs) showcase remarkable versatility, capable of handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected to exhibit speciality, excelling in specific applications. However, fine-tuning with extra data, a common practice to gain speciality, often leads to catastrophic forgetting (CF) of previously acquired versatility, hindering the model's performance across diverse tasks. In response to this challenge, we propose CoFiTune, a coarse to fine framework in an attempt to strike the balance between speciality and versatility. At the coarse-grained level, an empirical tree-search algorithm is utilized to pinpoint and update specific modules that are crucial for speciality, while keeping other parameters frozen; at the fine-grained level, a soft-masking mechanism regulates the update to the LLMs, mitigating the CF issue without harming speciality. In an overall evaluation of both speciality and versatility, CoFiTune consistently outperforms baseline methods across diverse tasks and model scales. Compared to the full-parameter SFT, CoFiTune leads to about 14% versatility improvement and marginal speciality loss on a 13B model. Lastly, based on further analysis, we provide a speculative insight into the information forwarding process in LLMs, which helps explain the effectiveness of the proposed method. The code is available at https://github.com/rattlesnakey/CoFiTune.",
            "corpus_id": 276574830,
            "sentences": [
                {
                    "corpus_id": "276574830",
                    "title": "Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models",
                    "text": "Aligned LLMs mainly undergo a two-step procedure: initial pre-training on web-scale text corpora, followed by fine-tuning on diverse instructions to align with human intentions. They exhibit remarkable versatility, showcasing their ability to handle various real-world tasks, such as reasoning, common sense question-answering, and instruction following (Zhao et al., 2023;Achiam et al., 2023;Lu et al., 2023b). \n\nDespite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023c), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c). \n\nAs a relatively new problem, the CF in LLMs remains under-explored. We categorize the existing studies into regularization-based, weight-based, and architecture-based methods1 . Regularizationbased methods (Lin et al., 2023b;Smith et al., 2023) add extra terms into loss function to penalize parameter changes. Weight-based methods (Wortsman et al., 2022;Ke et al., 2022) introduce weight coefficients for parameters to regulate their updates. Architecture-based methods (Wang et al., 2023d;Razdaibiedina et al., 2023) design and exclusively fine-tune extra modules outside the original model.",
                    "score": 0.6054206540964103,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 177
                        },
                        {
                            "start": 178,
                            "end": 411
                        },
                        {
                            "start": 414,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 908
                        },
                        {
                            "start": 909,
                            "end": 1179
                        },
                        {
                            "start": 1182,
                            "end": 1249
                        },
                        {
                            "start": 1250,
                            "end": 1359
                        },
                        {
                            "start": 1360,
                            "end": 1492
                        },
                        {
                            "start": 1493,
                            "end": 1625
                        },
                        {
                            "start": 1626,
                            "end": 1775
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 393,
                            "end": 410,
                            "matchedPaperCorpusId": "252668803"
                        },
                        {
                            "start": 546,
                            "end": 564,
                            "matchedPaperCorpusId": "265294420"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.66357421875
                }
            ],
            "relevance_judgement": 0.66357421875,
            "relevance_judgment_input_expanded": "# Title: Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models\n# Venue: \n# Authors: Hengyuan Zhang, Yanru Wu, Dawei Li, Sak Yang, Rui Zhao, Yong Jiang, Fei Tan\n## Abstract\nAligned Large Language Models (LLMs) showcase remarkable versatility, capable of handling diverse real-world tasks. Meanwhile, aligned LLMs are also expected to exhibit speciality, excelling in specific applications. However, fine-tuning with extra data, a common practice to gain speciality, often leads to catastrophic forgetting (CF) of previously acquired versatility, hindering the model's performance across diverse tasks. In response to this challenge, we propose CoFiTune, a coarse to fine framework in an attempt to strike the balance between speciality and versatility. At the coarse-grained level, an empirical tree-search algorithm is utilized to pinpoint and update specific modules that are crucial for speciality, while keeping other parameters frozen; at the fine-grained level, a soft-masking mechanism regulates the update to the LLMs, mitigating the CF issue without harming speciality. In an overall evaluation of both speciality and versatility, CoFiTune consistently outperforms baseline methods across diverse tasks and model scales. Compared to the full-parameter SFT, CoFiTune leads to about 14% versatility improvement and marginal speciality loss on a 13B model. Lastly, based on further analysis, we provide a speculative insight into the information forwarding process in LLMs, which helps explain the effectiveness of the proposed method. The code is available at https://github.com/rattlesnakey/CoFiTune.\n## Introduction\nAligned LLMs mainly undergo a two-step procedure: initial pre-training on web-scale text corpora, followed by fine-tuning on diverse instructions to align with human intentions. They exhibit remarkable versatility, showcasing their ability to handle various real-world tasks, such as reasoning, common sense question-answering, and instruction following (Zhao et al., 2023;Achiam et al., 2023;Lu et al., 2023b). \n\nDespite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023c), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c). \n\nAs a relatively new problem, the CF in LLMs remains under-explored. We categorize the existing studies into regularization-based, weight-based, and architecture-based methods1 . Regularizationbased methods (Lin et al., 2023b;Smith et al., 2023) add extra terms into loss function to penalize parameter changes. Weight-based methods (Wortsman et al., 2022;Ke et al., 2022) introduce weight coefficients for parameters to regulate their updates. Architecture-based methods (Wang et al., 2023d;Razdaibiedina et al., 2023) design and exclusively fine-tune extra modules outside the original model.",
            "reference_string": "[276574830 | Zhang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 77,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09982, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "51153332",
                    "name": "Vithursan Thangarasa"
                },
                {
                    "authorId": "2325876819",
                    "name": "Ganesh Venkatesh"
                },
                {
                    "authorId": "2325902410",
                    "name": "Nish Sinnadurai"
                },
                {
                    "authorId": "2212029838",
                    "name": "Sean Lie"
                }
            ],
            "abstract": "Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning. To recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it can lead to catastrophic forgetting by shifting the model's learned data distribution. Therefore, addressing the degradation from both pruning and SFT is essential to preserve the original model's quality. In this work, we utilize self-data distilled fine-tuning to address these challenges. Our approach leverages the original, unpruned model to generate a distilled dataset that preserves semantic richness and mitigates catastrophic forgetting by maintaining alignment with the base model's knowledge. Empirically, we demonstrate that self-data distillation consistently outperforms standard SFT, improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct (i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our method retains 91.2% of the original model's accuracy compared to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore, combining self-data distilled models through model merging yields enhanced quality retention. Additionally, leveraging these pruned models in speculative decoding increases token acceptance rates, thereby improving inference efficiency in applied settings.",
            "corpus_id": 273345395,
            "sentences": [
                {
                    "corpus_id": "273345395",
                    "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
                    "text": "Yang et al. (2024) investigated self-distillation as a way to alleviate distribution shifts, improving model quality during SFT while improving generalization across tasks. Our self-data distillation method builds on these techniques by leveraging the original unpruned model to generate a distilled dataset for finetuning the pruned model. This enhances semantic alignment and mitigates the quality degradation seen after pruning. Furthermore, while our approach can be combined with KD methods to enhance generalization and recover quality while lowering computational costs, we leave the exploration of such combinations for future work. \n\nCatastrophic Forgetting One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;Korbak et al., 2022). Regularization techniques such as Elastic Weight Consolidation (Kirkpatrick et al., 2017) aim to alleviate this by controlling parameter updates, but are taskdependent and require careful tuning (Huang et al., 2021). Architecture-based methods, which allocate separate parameters for each task (Razdaibiedina et al., 2023), preserve task-specific knowledge but add complexity and overhead, reducing the overall efficiency of model compression. \n\nReplay-based techniques (Ostapenko et al., 2022;Rolnick et al., 2019;Sun et al., 2019) store data subsets from previous tasks for rehearsal, either through direct storage or synthesis via generative models. However, these methods demand substantial memory to store large datasets and are often impractical due to privacy concerns or lack of access to past data. Our self-data distilled fine-tuning approach avoids these challenges by aligning the fine-tuning dataset with the original model's learned distribution, preserving knowledge across tasks without requiring new parameters or architectural changes. This method offers a robust solution for mitigating catastrophic forgetting while maintaining model quality after pruning.",
                    "score": 0.5523957603656091,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 32757,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 340
                        },
                        {
                            "start": 341,
                            "end": 431
                        },
                        {
                            "start": 432,
                            "end": 640
                        },
                        {
                            "start": 643,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1330
                        },
                        {
                            "start": 1333,
                            "end": 1539
                        },
                        {
                            "start": 1540,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1940
                        },
                        {
                            "start": 1941,
                            "end": 2063
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 865,
                            "end": 885,
                            "matchedPaperCorpusId": "249921347"
                        },
                        {
                            "start": 950,
                            "end": 976,
                            "matchedPaperCorpusId": "4704285"
                        },
                        {
                            "start": 1082,
                            "end": 1102,
                            "matchedPaperCorpusId": "233210110"
                        },
                        {
                            "start": 1357,
                            "end": 1381,
                            "matchedPaperCorpusId": "250265129"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.640625
                }
            ],
            "relevance_judgement": 0.640625,
            "relevance_judgment_input_expanded": "# Title: Self-Data Distillation for Recovering Quality in Pruned Large Language Models\n# Venue: arXiv.org\n# Authors: Vithursan Thangarasa, Ganesh Venkatesh, Nish Sinnadurai, Sean Lie\n## Abstract\nLarge language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning. To recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it can lead to catastrophic forgetting by shifting the model's learned data distribution. Therefore, addressing the degradation from both pruning and SFT is essential to preserve the original model's quality. In this work, we utilize self-data distilled fine-tuning to address these challenges. Our approach leverages the original, unpruned model to generate a distilled dataset that preserves semantic richness and mitigates catastrophic forgetting by maintaining alignment with the base model's knowledge. Empirically, we demonstrate that self-data distillation consistently outperforms standard SFT, improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct (i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our method retains 91.2% of the original model's accuracy compared to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore, combining self-data distilled models through model merging yields enhanced quality retention. Additionally, leveraging these pruned models in speculative decoding increases token acceptance rates, thereby improving inference efficiency in applied settings.\n## RELATED WORK\nYang et al. (2024) investigated self-distillation as a way to alleviate distribution shifts, improving model quality during SFT while improving generalization across tasks. Our self-data distillation method builds on these techniques by leveraging the original unpruned model to generate a distilled dataset for finetuning the pruned model. This enhances semantic alignment and mitigates the quality degradation seen after pruning. Furthermore, while our approach can be combined with KD methods to enhance generalization and recover quality while lowering computational costs, we leave the exploration of such combinations for future work. \n\nCatastrophic Forgetting One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;Korbak et al., 2022). Regularization techniques such as Elastic Weight Consolidation (Kirkpatrick et al., 2017) aim to alleviate this by controlling parameter updates, but are taskdependent and require careful tuning (Huang et al., 2021). Architecture-based methods, which allocate separate parameters for each task (Razdaibiedina et al., 2023), preserve task-specific knowledge but add complexity and overhead, reducing the overall efficiency of model compression. \n\nReplay-based techniques (Ostapenko et al., 2022;Rolnick et al., 2019;Sun et al., 2019) store data subsets from previous tasks for rehearsal, either through direct storage or synthesis via generative models. However, these methods demand substantial memory to store large datasets and are often impractical due to privacy concerns or lack of access to past data. Our self-data distilled fine-tuning approach avoids these challenges by aligning the fine-tuning dataset with the original model's learned distribution, preserving knowledge across tasks without requiring new parameters or architectural changes. This method offers a robust solution for mitigating catastrophic forgetting while maintaining model quality after pruning.",
            "reference_string": "[273345395 | Thangarasa et al. | 2024 | Citations: 2]"
        },
        {
            "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models",
            "venue": "NAACL-HLT",
            "year": 2022,
            "reference_count": 66,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2210.04802, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3021793",
                    "name": "Hossein Hajipour"
                },
                {
                    "authorId": "2263370859",
                    "name": "Ning Yu"
                },
                {
                    "authorId": "40141524",
                    "name": "Cristian-Alexandru Staicu"
                },
                {
                    "authorId": "2260336471",
                    "name": "Mario Fritz"
                }
            ],
            "abstract": "Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on four state-of-the-art pretrained models and applied to two code generation tasks, exposes multiple failure modes attributed to OOD generalization issues. Additionally, our analysis uncovers that LoRA fine-tuning consistently exhibits significantly better OOD generalization performance than full fine-tuning across various scenarios.",
            "corpus_id": 264720747,
            "sentences": [
                {
                    "corpus_id": "264720747",
                    "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models",
                    "text": "Despite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;Chen et al., 2020). Furthermore, Kumar et al. (2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks. In this work, for the first time, we systematically investigate the behavior of the fine-tuned source code models by carefully designing various OOD scenarios.",
                    "score": 0.5158101808535552,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 7721,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 204
                        },
                        {
                            "start": 205,
                            "end": 404
                        },
                        {
                            "start": 405,
                            "end": 564
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 185,
                            "end": 203,
                            "matchedPaperCorpusId": "216553067"
                        },
                        {
                            "start": 218,
                            "end": 237,
                            "matchedPaperCorpusId": "247011290"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62890625
                }
            ],
            "relevance_judgement": 0.62890625,
            "relevance_judgment_input_expanded": "# Title: SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models\n# Venue: NAACL-HLT\n# Authors: Hossein Hajipour, Ning Yu, Cristian-Alexandru Staicu, Mario Fritz\n## Abstract\nLarge code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on four state-of-the-art pretrained models and applied to two code generation tasks, exposes multiple failure modes attributed to OOD generalization issues. Additionally, our analysis uncovers that LoRA fine-tuning consistently exhibits significantly better OOD generalization performance than full fine-tuning across various scenarios.\n## INTRODUCTION\nDespite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;Chen et al., 2020). Furthermore, Kumar et al. (2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks. In this work, for the first time, we systematically investigate the behavior of the fine-tuned source code models by carefully designing various OOD scenarios.",
            "reference_string": "[264720747 | Hajipour et al. | 2022 | Citations: 5]"
        },
        {
            "title": "Personalized Large Language Models",
            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
            "year": 2024,
            "reference_count": 58,
            "citation_count": 20,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.09269, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284064954",
                    "name": "Stanislaw Wo'zniak"
                },
                {
                    "authorId": "2208962106",
                    "name": "Bartlomiej Koptyra"
                },
                {
                    "authorId": "32559047",
                    "name": "Arkadiusz Janz"
                },
                {
                    "authorId": "1724788",
                    "name": "Przemyslaw Kazienko"
                },
                {
                    "authorId": "2284064003",
                    "name": "Jan Koco'n"
                }
            ],
            "abstract": "Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.",
            "corpus_id": 267657819,
            "sentences": [
                {
                    "corpus_id": "267657819",
                    "title": "Personalized Large Language Models",
                    "text": "On the other hand, LLMs are sophisticated zero-shot reasoners [2]. One can use their abilities to solve downstream tasks with in-context-learning [33] and extensive prompt-based inference [34]. Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting [35], where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- [38]. Techniques such as multitask learning or balancing pre-training and task-specific data might be beneficial for retaining the performance of LLMs in multiple downstream tasks. \n\nTo the best of our knowledge, LLM fine-tuning for subjective tasks via user ID inputs, such as personalized emotion recognition or personalized hate speech detection, has not been extensively evaluated, and further research is needed in this area.",
                    "score": 0.5350856037628021,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 5428,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 193
                        },
                        {
                            "start": 194,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 454
                        },
                        {
                            "start": 455,
                            "end": 624
                        },
                        {
                            "start": 625,
                            "end": 799
                        },
                        {
                            "start": 802,
                            "end": 1049
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 62,
                            "end": 65,
                            "matchedPaperCorpusId": "249017743"
                        },
                        {
                            "start": 146,
                            "end": 150,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 188,
                            "end": 192,
                            "matchedPaperCorpusId": "258762525"
                        },
                        {
                            "start": 506,
                            "end": 510,
                            "matchedPaperCorpusId": "2691726"
                        },
                        {
                            "start": 619,
                            "end": 623,
                            "matchedPaperCorpusId": "262055661"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62548828125
                }
            ],
            "relevance_judgement": 0.62548828125,
            "relevance_judgment_input_expanded": "# Title: Personalized Large Language Models\n# Venue: 2024 IEEE International Conference on Data Mining Workshops (ICDMW)\n# Authors: Stanislaw Wo'zniak, Bartlomiej Koptyra, Arkadiusz Janz, Przemyslaw Kazienko, Jan Koco'n\n## Abstract\nLarge language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.\n## II. RELATED WORK\nOn the other hand, LLMs are sophisticated zero-shot reasoners [2]. One can use their abilities to solve downstream tasks with in-context-learning [33] and extensive prompt-based inference [34]. Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting [35], where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- [38]. Techniques such as multitask learning or balancing pre-training and task-specific data might be beneficial for retaining the performance of LLMs in multiple downstream tasks. \n\nTo the best of our knowledge, LLM fine-tuning for subjective tasks via user ID inputs, such as personalized emotion recognition or personalized hate speech detection, has not been extensively evaluated, and further research is needed in this area.",
            "reference_string": "[267657819 | Wo'zniak et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Vision-Language Instruction Tuning: A Review and Analysis",
            "venue": "Trans. Mach. Learn. Res.",
            "year": 2023,
            "reference_count": 123,
            "citation_count": 11,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2311.08172, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256784925",
                    "name": "Chen Li"
                },
                {
                    "authorId": "152988335",
                    "name": "Yixiao Ge"
                },
                {
                    "authorId": "2243307404",
                    "name": "Dian Li"
                },
                {
                    "authorId": "2265579883",
                    "name": "Ying Shan"
                }
            ],
            "abstract": "Instruction tuning is a crucial supervised training phase in Large Language Models (LLMs), aiming to enhance the LLM's ability to generalize instruction execution and adapt to user preferences. With the increasing integration of multi-modal data into LLMs, there is growing interest in Vision-Language Instruction Tuning (VLIT), which presents more complex characteristics compared to pure text instruction tuning. In this paper, we systematically review the latest VLIT settings and corresponding datasets in multi-modal LLMs and provide insights into the intrinsic motivations behind their design. For the first time, we offer a detailed multi-perspective categorization for existing VLIT datasets and identify the characteristics that high-quality VLIT data should possess. By incorporating these characteristics as guiding principles into the existing VLIT data construction process, we conduct extensive experiments and verify their positive impact on the performance of tuned multi-modal LLMs. Furthermore, we discuss the current challenges and future research directions of VLIT, providing insights for the continuous development of this field. The code and dataset related to this paper have been open-sourced at https://github.com/palchenli/VL-Instruction-Tuning.",
            "corpus_id": 265158118,
            "sentences": [
                {
                    "corpus_id": "265158118",
                    "title": "Vision-Language Instruction Tuning: A Review and Analysis",
                    "text": "Existing MLLMs have demonstrated strong instruction execution and generalization abilities in some simple tasks. However, they generally have shortcomings when dealing with some special difficult samples, such as fine-grained content understanding, complex reasoning, and multimodal content editing. The chain-of-thought (Wei et al., 2022) and in-context learning (Dong et al., 2022) can alleviate the occurrence of the above problems to some extent, but the cost of obtaining high-quality annotated data is too high, and the model computation cost it brings is also unacceptable. Therefore, how to efficiently solve such difficult samples is a problem worthy of attention. \n\nSelective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes. \n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter. \n\nLimited Emergence. As a significant purpose of instruction tuning, instruction generalization is an advantage that LLM (and MLLM) should possess, stemming from the emergence phenomenon. In the multimodal scenario, emergence shows more characters, such as supporting visual objects that have not appeared. However, recently, a large number of MLLMs have not shown good performance in either of the above two scenarios. Therefore, deeper consideration is needed on how to design MLLMs and corresponding instruction tuning data.",
                    "score": 0.5049103603053067,
                    "section_title": "Difficult Sample.",
                    "char_start_offset": 50998,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 112
                        },
                        {
                            "start": 113,
                            "end": 299
                        },
                        {
                            "start": 300,
                            "end": 580
                        },
                        {
                            "start": 581,
                            "end": 673
                        },
                        {
                            "start": 676,
                            "end": 697
                        },
                        {
                            "start": 698,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1180
                        },
                        {
                            "start": 1181,
                            "end": 1330
                        },
                        {
                            "start": 1333,
                            "end": 1418
                        },
                        {
                            "start": 1419,
                            "end": 1591
                        },
                        {
                            "start": 1592,
                            "end": 1657
                        },
                        {
                            "start": 1660,
                            "end": 1678
                        },
                        {
                            "start": 1679,
                            "end": 1845
                        },
                        {
                            "start": 1846,
                            "end": 1964
                        },
                        {
                            "start": 1965,
                            "end": 2077
                        },
                        {
                            "start": 2078,
                            "end": 2185
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 321,
                            "end": 339,
                            "matchedPaperCorpusId": "246411621"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62451171875
                }
            ],
            "relevance_judgement": 0.62451171875,
            "relevance_judgment_input_expanded": "# Title: Vision-Language Instruction Tuning: A Review and Analysis\n# Venue: Trans. Mach. Learn. Res.\n# Authors: Chen Li, Yixiao Ge, Dian Li, Ying Shan\n## Abstract\nInstruction tuning is a crucial supervised training phase in Large Language Models (LLMs), aiming to enhance the LLM's ability to generalize instruction execution and adapt to user preferences. With the increasing integration of multi-modal data into LLMs, there is growing interest in Vision-Language Instruction Tuning (VLIT), which presents more complex characteristics compared to pure text instruction tuning. In this paper, we systematically review the latest VLIT settings and corresponding datasets in multi-modal LLMs and provide insights into the intrinsic motivations behind their design. For the first time, we offer a detailed multi-perspective categorization for existing VLIT datasets and identify the characteristics that high-quality VLIT data should possess. By incorporating these characteristics as guiding principles into the existing VLIT data construction process, we conduct extensive experiments and verify their positive impact on the performance of tuned multi-modal LLMs. Furthermore, we discuss the current challenges and future research directions of VLIT, providing insights for the continuous development of this field. The code and dataset related to this paper have been open-sourced at https://github.com/palchenli/VL-Instruction-Tuning.\n## Difficult Sample.\nExisting MLLMs have demonstrated strong instruction execution and generalization abilities in some simple tasks. However, they generally have shortcomings when dealing with some special difficult samples, such as fine-grained content understanding, complex reasoning, and multimodal content editing. The chain-of-thought (Wei et al., 2022) and in-context learning (Dong et al., 2022) can alleviate the occurrence of the above problems to some extent, but the cost of obtaining high-quality annotated data is too high, and the model computation cost it brings is also unacceptable. Therefore, how to efficiently solve such difficult samples is a problem worthy of attention. \n\nSelective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes. \n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter. \n\nLimited Emergence. As a significant purpose of instruction tuning, instruction generalization is an advantage that LLM (and MLLM) should possess, stemming from the emergence phenomenon. In the multimodal scenario, emergence shows more characters, such as supporting visual objects that have not appeared. However, recently, a large number of MLLMs have not shown good performance in either of the above two scenarios. Therefore, deeper consideration is needed on how to design MLLMs and corresponding instruction tuning data.",
            "reference_string": "[265158118 | Li et al. | 2023 | Citations: 11]"
        },
        {
            "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 39,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.10056, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261355905",
                    "name": "Yongquan He"
                },
                {
                    "authorId": "2292011040",
                    "name": "Xuancheng Huang"
                },
                {
                    "authorId": "2261366844",
                    "name": "Minghao Tang"
                },
                {
                    "authorId": "2292412508",
                    "name": "Lingxun Meng"
                },
                {
                    "authorId": "2292290608",
                    "name": "Xiang Li"
                },
                {
                    "authorId": "2291964148",
                    "name": "Wei Lin"
                },
                {
                    "authorId": "2108347848",
                    "name": "Wenyuan Zhang"
                },
                {
                    "authorId": "2118545089",
                    "name": "Yifu Gao"
                }
            ],
            "abstract": "Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.",
            "corpus_id": 268510177,
            "sentences": [
                {
                    "corpus_id": "268510177",
                    "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
                    "text": "Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.",
                    "score": 0.4940442658411062,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62451171875
                }
            ],
            "relevance_judgement": 0.62451171875,
            "relevance_judgment_input_expanded": "# Title: Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning\n# Venue: arXiv.org\n# Authors: Yongquan He, Xuancheng Huang, Minghao Tang, Lingxun Meng, Xiang Li, Wei Lin, Wenyuan Zhang, Yifu Gao\n## Abstract\nInstruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.\n",
            "reference_string": "[268510177 | He et al. | 2024 | Citations: 5]"
        },
        {
            "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "year": 2023,
            "reference_count": 287,
            "citation_count": 51,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.09218, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2920297",
                    "name": "Zhenyi Wang"
                },
                {
                    "authorId": "151497321",
                    "name": "Enneng Yang"
                },
                {
                    "authorId": "2172820082",
                    "name": "Li Shen"
                },
                {
                    "authorId": "2151748239",
                    "name": "Heng Huang"
                }
            ],
            "abstract": "Forgetting refers to the loss or deterioration of previously acquired knowledge. While existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new task, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications.",
            "corpus_id": 259951356,
            "sentences": [
                {
                    "corpus_id": "259951356",
                    "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning",
                    "text": "Fine-tuning a foundation model can result in the forgetting of pre-trained knowledge, which may lead to sub-optimal performance on downstream tasks. Forgetting occurs when the target model deviates significantly from the pre-trained model during the fine-tuning process [133]. This deviation increases the likelihood of overfitting to a small fine-tuning set [134], which can contribute to forgetting. There are several simple and effective strategies to mitigate forgetting during the fine-tuning process. These include techniques such as learning rate decreasing [134], weight decay [135], [136], and Mixout regularization [133]. Furthermore, Fatemi et al. [137] find that in the study of mitigating the gender bias of the pre-trained language model, the pre-trained knowledge will be forgotten when the small neutral data is fine-tuned, which will hurt the downstream task performance. Dong et al. [138] observe that adversarial fine-tuning of pre-trained language models is prone to severe catastrophic forgetting, causing the loss of previously captured general and robust linguistic features. To address these issues, they propose a Robust Informative Fine-Tuning method from an information-theoretical perspective. In addition, an approach called Recall and Learn, proposed in Chen et al. [139], addresses the forgetting issue by utilizing Pretraining Simulation and Objective Shifting. This approach enables multi-task fine-tuning without relying on the data from the pretraining tasks. Zhang et al. [140] conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text. Conjugate Prompting [141] seeks to reduce the forgetting of prior task knowledge during the fine-tuning of new tasks by counteracting changes in implicit task inference. Furthermore, several recent studies have shown that merging pretrained and fine-tuned models directly at the parameter level can also help mitigate forgetting [142], [143]. Moreover, recent research [144] highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter [144] introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs.",
                    "score": 0.5572826856150825,
                    "section_title": "Forgetting in Fine-Tuning Foundation Models",
                    "char_start_offset": 41659,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 276
                        },
                        {
                            "start": 277,
                            "end": 401
                        },
                        {
                            "start": 402,
                            "end": 506
                        },
                        {
                            "start": 507,
                            "end": 631
                        },
                        {
                            "start": 632,
                            "end": 888
                        },
                        {
                            "start": 889,
                            "end": 1098
                        },
                        {
                            "start": 1099,
                            "end": 1221
                        },
                        {
                            "start": 1222,
                            "end": 1393
                        },
                        {
                            "start": 1394,
                            "end": 1494
                        },
                        {
                            "start": 1495,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1810
                        },
                        {
                            "start": 1811,
                            "end": 1983
                        },
                        {
                            "start": 1984,
                            "end": 2114
                        },
                        {
                            "start": 2115,
                            "end": 2324
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 364,
                            "matchedPaperCorpusId": "40100965"
                        },
                        {
                            "start": 565,
                            "end": 570,
                            "matchedPaperCorpusId": "40100965"
                        },
                        {
                            "start": 585,
                            "end": 590,
                            "matchedPaperCorpusId": "2925420"
                        },
                        {
                            "start": 901,
                            "end": 906,
                            "matchedPaperCorpusId": "245116938"
                        },
                        {
                            "start": 1296,
                            "end": 1301,
                            "matchedPaperCorpusId": "216553067"
                        },
                        {
                            "start": 1508,
                            "end": 1513,
                            "matchedPaperCorpusId": "271745852"
                        },
                        {
                            "start": 1970,
                            "end": 1975,
                            "matchedPaperCorpusId": "237420687"
                        },
                        {
                            "start": 2010,
                            "end": 2015,
                            "matchedPaperCorpusId": "266374841"
                        },
                        {
                            "start": 2151,
                            "end": 2156,
                            "matchedPaperCorpusId": "266374841"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62060546875
                }
            ],
            "relevance_judgement": 0.62060546875,
            "relevance_judgment_input_expanded": "# Title: A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning\n# Venue: IEEE Transactions on Pattern Analysis and Machine Intelligence\n# Authors: Zhenyi Wang, Enneng Yang, Li Shen, Heng Huang\n## Abstract\nForgetting refers to the loss or deterioration of previously acquired knowledge. While existing surveys on forgetting have primarily focused on continual learning, forgetting is a prevalent phenomenon observed in various other research domains within deep learning. Forgetting manifests in research fields such as generative models due to generator shifts, and federated learning due to heterogeneous data distributions across clients. Addressing forgetting encompasses several challenges, including balancing the retention of old task knowledge with fast learning of new task, managing task interference with conflicting goals, and preventing privacy leakage, etc. Moreover, most existing surveys on continual learning implicitly assume that forgetting is always harmful. In contrast, our survey argues that forgetting is a double-edged sword and can be beneficial and desirable in certain cases, such as privacy-preserving scenarios. By exploring forgetting in a broader context, we present a more nuanced understanding of this phenomenon and highlight its potential advantages. Through this comprehensive survey, we aspire to uncover potential solutions by drawing upon ideas and approaches from various fields that have dealt with forgetting. By examining forgetting beyond its conventional boundaries, we hope to encourage the development of novel strategies for mitigating, harnessing, or even embracing forgetting in real applications.\n## Forgetting in Fine-Tuning Foundation Models\nFine-tuning a foundation model can result in the forgetting of pre-trained knowledge, which may lead to sub-optimal performance on downstream tasks. Forgetting occurs when the target model deviates significantly from the pre-trained model during the fine-tuning process [133]. This deviation increases the likelihood of overfitting to a small fine-tuning set [134], which can contribute to forgetting. There are several simple and effective strategies to mitigate forgetting during the fine-tuning process. These include techniques such as learning rate decreasing [134], weight decay [135], [136], and Mixout regularization [133]. Furthermore, Fatemi et al. [137] find that in the study of mitigating the gender bias of the pre-trained language model, the pre-trained knowledge will be forgotten when the small neutral data is fine-tuned, which will hurt the downstream task performance. Dong et al. [138] observe that adversarial fine-tuning of pre-trained language models is prone to severe catastrophic forgetting, causing the loss of previously captured general and robust linguistic features. To address these issues, they propose a Robust Informative Fine-Tuning method from an information-theoretical perspective. In addition, an approach called Recall and Learn, proposed in Chen et al. [139], addresses the forgetting issue by utilizing Pretraining Simulation and Objective Shifting. This approach enables multi-task fine-tuning without relying on the data from the pretraining tasks. Zhang et al. [140] conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text. Conjugate Prompting [141] seeks to reduce the forgetting of prior task knowledge during the fine-tuning of new tasks by counteracting changes in implicit task inference. Furthermore, several recent studies have shown that merging pretrained and fine-tuned models directly at the parameter level can also help mitigate forgetting [142], [143]. Moreover, recent research [144] highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter [144] introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs.",
            "reference_string": "[259951356 | Wang et al. | 2023 | Citations: 51]"
        },
        {
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256382094",
                    "name": "Kushagra Bhushan"
                },
                {
                    "authorId": "1392630568",
                    "name": "Yatin Nandwani"
                },
                {
                    "authorId": "2345003462",
                    "name": "Dinesh Khandelwal"
                },
                {
                    "authorId": "2320314900",
                    "name": "Sonam Gupta"
                },
                {
                    "authorId": "2345005348",
                    "name": "Gaurav Pandey"
                },
                {
                    "authorId": "1916865",
                    "name": "Dinesh Raghu"
                },
                {
                    "authorId": "2243011716",
                    "name": "Sachindra Joshi"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.",
            "corpus_id": 276287820,
            "sentences": [
                {
                    "corpus_id": "276287820",
                    "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                    "text": "RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts. However, it suffers from conditional memorization bias and canonical answer overfitting. On the other hand, PA-RAG uses context augmentation and answer paraphrasing to address these issues. Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting. The Self-Synthesized Rehearsal (SSR) (Gupta et al., 2024;Huang et al., 2024) framework uses the LLM to generate synthetic rehearsal data, reducing reliance on stored instances.",
                    "score": 0.5526449852979607,
                    "section_title": "Related Work",
                    "char_start_offset": 7826,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 125
                        },
                        {
                            "start": 126,
                            "end": 214
                        },
                        {
                            "start": 215,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 832
                        },
                        {
                            "start": 833,
                            "end": 1009
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61474609375
                }
            ],
            "relevance_judgement": 0.61474609375,
            "relevance_judgment_input_expanded": "# Title: Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.\n## Related Work\nRAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts. However, it suffers from conditional memorization bias and canonical answer overfitting. On the other hand, PA-RAG uses context augmentation and answer paraphrasing to address these issues. Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting. The Self-Synthesized Rehearsal (SSR) (Gupta et al., 2024;Huang et al., 2024) framework uses the LLM to generate synthetic rehearsal data, reducing reliance on stored instances.",
            "reference_string": "[276287820 | Bhushan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
            "venue": "",
            "year": 2025,
            "reference_count": 32,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.14387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "William F. Shen"
                },
                {
                    "authorId": "1702997626",
                    "name": "Xinchi Qiu"
                },
                {
                    "authorId": "2313189467",
                    "name": "Nicola Cancedda"
                },
                {
                    "authorId": "2298756346",
                    "name": "N. Lane"
                }
            ],
            "abstract": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.",
            "corpus_id": 279410046,
            "sentences": [],
            "relevance_judgement": 0.61474609375,
            "relevance_judgment_input_expanded": "# Title: Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning\n# Venue: \n# Authors: William F. Shen, Xinchi Qiu, Nicola Cancedda, N. Lane\n## Abstract\nExisting work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.\n",
            "reference_string": "[279410046 | Shen et al. | 2025 | Citations: 0]"
        },
        {
            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
            "venue": "IR-RAG@SIGIR",
            "year": 2024,
            "reference_count": 54,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17587, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303471987",
                    "name": "Emile Contal"
                },
                {
                    "authorId": "2283934772",
                    "name": "Garrin McGoldrick"
                }
            ],
            "abstract": "Large Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.",
            "corpus_id": 270068369,
            "sentences": [
                {
                    "corpus_id": "270068369",
                    "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
                    "text": "Large Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
                    "score": 0.6172990573969164,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 202
                        },
                        {
                            "start": 203,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 574
                        },
                        {
                            "start": 577,
                            "end": 722
                        },
                        {
                            "start": 723,
                            "end": 910
                        },
                        {
                            "start": 911,
                            "end": 1080
                        },
                        {
                            "start": 1083,
                            "end": 1245
                        },
                        {
                            "start": 1246,
                            "end": 1295
                        },
                        {
                            "start": 1296,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1613
                        },
                        {
                            "start": 1614,
                            "end": 1769
                        },
                        {
                            "start": 1770,
                            "end": 2052
                        },
                        {
                            "start": 2055,
                            "end": 2194
                        },
                        {
                            "start": 2195,
                            "end": 2307
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 198,
                            "end": 201,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 906,
                            "end": 909,
                            "matchedPaperCorpusId": "235458009"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61376953125
                }
            ],
            "relevance_judgement": 0.61376953125,
            "relevance_judgment_input_expanded": "# Title: RAGSys: Item-Cold-Start Recommender as RAG System\n# Venue: IR-RAG@SIGIR\n# Authors: Emile Contal, Garrin McGoldrick\n## Abstract\nLarge Language Models (LLM) hold immense promise for real-world applications, but their generic knowledge often falls short of domain-specific needs. Fine-tuning, a common approach, can suffer from catastrophic forgetting and hinder generalizability. In-Context Learning (ICL) offers an alternative, which can leverage Retrieval-Augmented Generation (RAG) to provide LLMs with relevant demonstrations for few-shot learning tasks. This paper explores the desired qualities of a demonstration retrieval system for ICL. We argue that ICL retrieval in this context resembles item-cold-start recommender systems, prioritizing discovery and maximizing information gain over strict relevance. We propose a novel evaluation method that measures the LLM's subsequent performance on NLP tasks, eliminating the need for subjective diversity scores. Our findings demonstrate the critical role of diversity and quality bias in retrieved demonstrations for effective ICL, and highlight the potential of recommender system techniques in this domain.\n## Introduction\nLarge Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
            "reference_string": "[270068369 | Contal et al. | 2024 | Citations: 5]"
        },
        {
            "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT",
            "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
            "year": 2020,
            "reference_count": 81,
            "citation_count": 60,
            "influential_citation_count": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.blackboxnlp-1.17.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2010.09313, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1999172945",
                    "name": "Jonas Wallat"
                },
                {
                    "authorId": "2109471460",
                    "name": "Jaspreet Singh"
                },
                {
                    "authorId": "39775488",
                    "name": "Avishek Anand"
                }
            ],
            "abstract": "Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT\u2019s final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.",
            "corpus_id": 224704480,
            "sentences": [
                {
                    "corpus_id": "224704480",
                    "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT",
                    "text": "However, one thing that has been studied is the effect that models lose the ability to do their pre-training task when being fine-tuned. This sequential learning problem has also been referred to as catastrophic inference (McCloskey and Cohen 1989) or catastrophic forgetting (Zacarias and Alexandre 2018;Mosbach, Andriushchenko, and Klakow 2020). This is an ongoing problem as general AI models will need to be able to learn and perform multiple tasks without forgetting how to perform the former. Zacarias and Alexandre (2018) suggest slowing down learning of important weights for the initial task, whereas Mosbach, Andriushchenko, and Klakow (2020) investigated the role of catastrophic forgetting on fine-tuning stability, finding that the last few layers are usually replaced with task-specific knowledge. If and how factual knowledge is affected by catastrophic forgetting is largely unexplored. In this work, we will take a first step into understanding how fine-tuning tasks and data effect both the learning and forgetting of factual knowledge. \n\nThere has been recent work on forgetting in attention modules when dealing with long contexts (Child et al. 2019;Schlag, Irie, and Schmidhuber 2021). When the capacity is exhausted, previous information is dropped in favor of the more recent context. By learning which parts of the context are important and systematically expiring ones faster that are not, Sukhbaatar et al. (2021) reduced the necessary capacity to attend to extremely long sequences. The work in this article is different in that we use probes without contexts as we want to retrieve knowledge from the model's parametric memory. Therefore, we do not investigate the attention modules but the learned embeddings.",
                    "score": 0.6051754797554411,
                    "section_title": "Learning and forgetting",
                    "char_start_offset": 12678,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 347
                        },
                        {
                            "start": 348,
                            "end": 498
                        },
                        {
                            "start": 499,
                            "end": 811
                        },
                        {
                            "start": 812,
                            "end": 902
                        },
                        {
                            "start": 903,
                            "end": 1054
                        },
                        {
                            "start": 1057,
                            "end": 1206
                        },
                        {
                            "start": 1207,
                            "end": 1307
                        },
                        {
                            "start": 1308,
                            "end": 1509
                        },
                        {
                            "start": 1510,
                            "end": 1655
                        },
                        {
                            "start": 1656,
                            "end": 1738
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 276,
                            "end": 305,
                            "matchedPaperCorpusId": "3528793"
                        },
                        {
                            "start": 499,
                            "end": 528,
                            "matchedPaperCorpusId": "3528793"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61328125
                }
            ],
            "relevance_judgement": 0.61328125,
            "relevance_judgment_input_expanded": "# Title: BERTnesia: Investigating the capture and forgetting of knowledge in BERT\n# Venue: BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP\n# Authors: Jonas Wallat, Jaspreet Singh, Avishek Anand\n## Abstract\nProbing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT\u2019s final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.\n## Learning and forgetting\nHowever, one thing that has been studied is the effect that models lose the ability to do their pre-training task when being fine-tuned. This sequential learning problem has also been referred to as catastrophic inference (McCloskey and Cohen 1989) or catastrophic forgetting (Zacarias and Alexandre 2018;Mosbach, Andriushchenko, and Klakow 2020). This is an ongoing problem as general AI models will need to be able to learn and perform multiple tasks without forgetting how to perform the former. Zacarias and Alexandre (2018) suggest slowing down learning of important weights for the initial task, whereas Mosbach, Andriushchenko, and Klakow (2020) investigated the role of catastrophic forgetting on fine-tuning stability, finding that the last few layers are usually replaced with task-specific knowledge. If and how factual knowledge is affected by catastrophic forgetting is largely unexplored. In this work, we will take a first step into understanding how fine-tuning tasks and data effect both the learning and forgetting of factual knowledge. \n\nThere has been recent work on forgetting in attention modules when dealing with long contexts (Child et al. 2019;Schlag, Irie, and Schmidhuber 2021). When the capacity is exhausted, previous information is dropped in favor of the more recent context. By learning which parts of the context are important and systematically expiring ones faster that are not, Sukhbaatar et al. (2021) reduced the necessary capacity to attend to extremely long sequences. The work in this article is different in that we use probes without contexts as we want to retrieve knowledge from the model's parametric memory. Therefore, we do not investigate the attention modules but the learned embeddings.",
            "reference_string": "[224704480 | Wallat et al. | 2020 | Citations: 60]"
        },
        {
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 49,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.12036, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2311696660",
                    "name": "Giorgio Roffo"
                }
            ],
            "abstract": "This tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategies, including instruction fine-tuning, parameter-efficient methods like LoRA, and Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced Self-Training (ReST). Additionally, it provides a comprehensive survey of transformer architectures and training techniques for LLMs. The source code can be accessed by contacting the author via email for a request.",
            "corpus_id": 271244634,
            "sentences": [
                {
                    "corpus_id": "271244634",
                    "title": "Exploring Advanced Large Language Models with LLMsuite",
                    "text": "Upon preparing the instruction dataset, it is partitioned into training, validation, and test splits.During fine-tuning, prompts from the training set are used to generate completions, which are compared against the expected responses.The model outputs a probability distribution across tokens, and the cross-entropy loss function calculates the discrepancy between the generated and expected distributions.This loss is then used to update the model's weights through backpropagation [14].The fine-tuning process involves multiple batches and epochs, continuously updating the model's weights to enhance performance.Evaluation using holdout validation and test datasets yields metrics such as validation accuracy and test accuracy, providing a measure of the model's performance.The result of fine-tuning is an improved version of the base model, referred to as an instruct model, optimized for specific tasks.Instruction fine-tuning is the predominant method for fine-tuning LLMs today, typically implied when discussing fine-tuning in this context.Large Language Models (LLMs) are renowned for their versatility in performing various language tasks within a single model.However, specific applications often necessitate excelling in a single task.Fine-tuning a pre-trained model on a particular task, such as summarization, can significantly enhance performance, achievable with relatively small datasets, often just 500-1,000 examples, compared to the billions of texts used during pre-training [31,3].Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27].To mitigate catastrophic forgetting, one must assess its impact on their specific use case.If the primary requirement is reliable performance on the fine-tuned task, the loss of multitask capabilities may be acceptable.However, if maintaining generalized capabilities is essential, multitask fine-tuning offers an alternative.",
                    "score": 0.5847418859468139,
                    "section_title": "Improving Performance of Large Language Models through Fine-Tuning",
                    "char_start_offset": 31834,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 101
                        },
                        {
                            "start": 101,
                            "end": 235
                        },
                        {
                            "start": 235,
                            "end": 407
                        },
                        {
                            "start": 407,
                            "end": 489
                        },
                        {
                            "start": 489,
                            "end": 616
                        },
                        {
                            "start": 616,
                            "end": 779
                        },
                        {
                            "start": 779,
                            "end": 910
                        },
                        {
                            "start": 910,
                            "end": 1050
                        },
                        {
                            "start": 1050,
                            "end": 1173
                        },
                        {
                            "start": 1173,
                            "end": 1249
                        },
                        {
                            "start": 1249,
                            "end": 1505
                        },
                        {
                            "start": 1505,
                            "end": 1678
                        },
                        {
                            "start": 1678,
                            "end": 1810
                        },
                        {
                            "start": 1810,
                            "end": 1967
                        },
                        {
                            "start": 1967,
                            "end": 2058
                        },
                        {
                            "start": 2058,
                            "end": 2186
                        },
                        {
                            "start": 2186,
                            "end": 2293
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1498,
                            "end": 1502,
                            "matchedPaperCorpusId": "160025533"
                        },
                        {
                            "start": 1502,
                            "end": 1504,
                            "matchedPaperCorpusId": "218971783"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61279296875
                }
            ],
            "relevance_judgement": 0.61279296875,
            "relevance_judgment_input_expanded": "# Title: Exploring Advanced Large Language Models with LLMsuite\n# Venue: arXiv.org\n# Authors: Giorgio Roffo\n## Abstract\nThis tutorial explores the advancements and challenges in the development of Large Language Models (LLMs) such as ChatGPT and Gemini. It addresses inherent limitations like temporal knowledge cutoffs, mathematical inaccuracies, and the generation of incorrect information, proposing solutions like Retrieval Augmented Generation (RAG), Program-Aided Language Models (PAL), and frameworks such as ReAct and LangChain. The integration of these techniques enhances LLM performance and reliability, especially in multi-step reasoning and complex task execution. The paper also covers fine-tuning strategies, including instruction fine-tuning, parameter-efficient methods like LoRA, and Reinforcement Learning from Human Feedback (RLHF) as well as Reinforced Self-Training (ReST). Additionally, it provides a comprehensive survey of transformer architectures and training techniques for LLMs. The source code can be accessed by contacting the author via email for a request.\n## Improving Performance of Large Language Models through Fine-Tuning\nUpon preparing the instruction dataset, it is partitioned into training, validation, and test splits.During fine-tuning, prompts from the training set are used to generate completions, which are compared against the expected responses.The model outputs a probability distribution across tokens, and the cross-entropy loss function calculates the discrepancy between the generated and expected distributions.This loss is then used to update the model's weights through backpropagation [14].The fine-tuning process involves multiple batches and epochs, continuously updating the model's weights to enhance performance.Evaluation using holdout validation and test datasets yields metrics such as validation accuracy and test accuracy, providing a measure of the model's performance.The result of fine-tuning is an improved version of the base model, referred to as an instruct model, optimized for specific tasks.Instruction fine-tuning is the predominant method for fine-tuning LLMs today, typically implied when discussing fine-tuning in this context.Large Language Models (LLMs) are renowned for their versatility in performing various language tasks within a single model.However, specific applications often necessitate excelling in a single task.Fine-tuning a pre-trained model on a particular task, such as summarization, can significantly enhance performance, achievable with relatively small datasets, often just 500-1,000 examples, compared to the billions of texts used during pre-training [31,3].Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27].To mitigate catastrophic forgetting, one must assess its impact on their specific use case.If the primary requirement is reliable performance on the fine-tuned task, the loss of multitask capabilities may be acceptable.However, if maintaining generalized capabilities is essential, multitask fine-tuning offers an alternative.",
            "reference_string": "[271244634 | Roffo | 2024 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "278327022",
            "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
            "text": "Several existing works also aim to understand the phenomenon of catastrophic forgetting. For instance, (Li et al., 2024c) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen. Among these studies, (Zheng et al., 2025) is most relevant to ours. It introduces the concept of spurious forgetting, where the model loses task alignment without any genuine loss of knowledge. This notion is partially similar with our definition of superficial forgetting. However, spurious forgetting emphasizes the recoverability and assumes that no actual knowledge has been forgotten. In contrast, superficial forgetting does not make assumptions about recoverability or knowledge retention.",
            "score": 0.7858579349434189,
            "section_title": "A.5. Understanding Catastrophic Forgetting",
            "char_start_offset": 32597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 103,
                    "end": 121,
                    "matchedPaperCorpusId": "260334888"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.791015625
        },
        {
            "corpus_id": "267636596",
            "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
            "text": "Main Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data.",
            "score": 0.7524659139123779,
            "section_title": "RQ1: Can LLMs learn new concepts incrementally without forgetting?",
            "char_start_offset": 11215,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 432
                },
                {
                    "start": 435,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 911
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69873046875
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving a satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs' knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.",
            "score": 0.7292896464464648,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8603515625
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "In this study, we conducted an empirical investigation into the catastrophic forgetting (CF) phenomenon experienced by large language models (LLMs) during continual instruction tuning. Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills. Furthermore, our comparative analysis showed that the decoder-only model, BLOOMZ, demonstrates a superior ability to retain knowledge and skills during continual fine-tuning when compared to the encoder-decoder model, mT0. Additionally, we discovered that employing general instruction tuning techniques may help alleviate the CF problem in LLMs. Our empirical study suggests that exploring more effective methods to mitigate CF in LLMs during continual fine-tuning is a promising research direction. When applying LLMs, practitioners should remain vigilant and pay close attention to the issue of knowledge forgetting that may occur after instruction tuning. Addressing this challenge is crucial to ensure the reliable and consistent performance of LLMs in real-world applications.",
            "score": 0.7275733955580117,
            "section_title": "Conclusion",
            "char_start_offset": 22052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1242
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93212890625
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "In contrast, our work provides a comprehensive examination of improving LLMs' factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO.Concurrent to our work, Kang et al. (2024) find that LLMs tend to hallucinate when facing unfamiliar queries.They consider improving LLMs' factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with FLAME (see case studies in Section 5.5).It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation).In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions.",
            "score": 0.712510724863794,
            "section_title": "Related Work",
            "char_start_offset": 6250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 751
                },
                {
                    "start": 751,
                    "end": 858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0999755859375
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts. \n\nThe works (Lee et al., 2019;Wiese et al., 2017) focus on using techniques that mitigate forgetting by regularizing outputs or parameters to remain close to the pre-trained model, to actually improve fine-tuning performance and stability. This can be reasoned to entail that forgetting while fine-tuning degrades the ability of the model to adapt and be fine-tuned, and forgetting occurs during the process of fine-tuning on a single task. \n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model.",
            "score": 0.7095603065851567,
            "section_title": "Catastrophic Forgetting",
            "char_start_offset": 5604,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 419
                },
                {
                    "start": 422,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 860
                },
                {
                    "start": 863,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1169
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "Instruction fine-tuning (Peng et al., 2023;Chung et al., 2024) has emerged as an indispensable ingredient in the development of Large Language Models (LLMs) (Brown et al., 2020;Radford et al., 2019;Touvron et al., 2023b),enabling them to meet the demands of specific domains (Roziere et al., 2023;Thirunavukarasu et al., 2023) and human preferences (Ouyang et al., 2022).However, a notable concern with this fine-tuning is \"catastrophic forgetting\" (McCloskey and Cohen, 1989;Kirkpatrick et al., 2017), where models may lose essential skills (Dou et al., 2023;Chen et al., 2023)  such as mathematical reasoning while adjusting to user instructions.This raises questions about which abilities are most susceptible to forgetting and the underlying causes of these losses in LLMs.\n\nResearch on LLM forgetting (Luo et al., 2024;Wang et al., 2023b;Wu et al., 2024a) generally examines changes in abilities like reading comprehension, factual retention, mathematical skills, and code generation, underscoring the existence of catastrophic forgetting.Despite these findings, there is a notable gap in understanding the internal mechanisms responsible for these losses.To date, only a few studies, such as Kotha et al. (2024) proposing the task inference hypothesis, have begun to explore how conflicts between task processors might lead to forgetting.Nevertheless, the literature still lacks comprehensive insights into the exact changes that result in forgetting, leaving open questions about whether these changes involve overwriting of old modules or if they are simply overshadowed by new, specialized patterns.\n\nIn this paper, we first present a novel perspective to investigate catastrophic forgetting in LLMs, focusing on the capabilities developed during pretraining and alignment phases.We suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through Knowledge Probability P (y|x) and Instruction Probability P (y c |c, x), respectively (as depicted in Fig. 2).",
            "score": 0.6948803808840244,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 371
                },
                {
                    "start": 371,
                    "end": 648
                },
                {
                    "start": 648,
                    "end": 777
                },
                {
                    "start": 779,
                    "end": 1044
                },
                {
                    "start": 1044,
                    "end": 1161
                },
                {
                    "start": 1161,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1608
                },
                {
                    "start": 1610,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 62,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 157,
                    "end": 177,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 177,
                    "end": 198,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 297,
                    "end": 326,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 349,
                    "end": 370,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 449,
                    "end": 476,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1198,
                    "end": 1217,
                    "matchedPaperCorpusId": "262054014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58740234375
        },
        {
            "corpus_id": "258686278",
            "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
            "text": "LLMs injected with non-linguistic skills forgo their linguistic skills: Consider the task of strict arithmetic reasoning as shown in Figure 1, a subset of possible quantitative reasoning tasks. If a base BERT model (Devlin et al., 2019) is further trained on this non-linguistic task, it suffers significant degradation on 8/9 GLUE tasks (Wang et al., 2018) that evaluate the natural language understanding (NLU) capabilties of the model, as showcased in Table 1. This observation has long been known in the deep learning literature as catastrophic forgetting (Kirkpatrick et al., 2017), wherein when a model pre-trained on task A is further trained on task B, the parameters in the model vital for task A adapt their values to meet the requirements of task B.\n\nLLMs exhibit unconventional forgetting: What is interesting, based on our findings, is that in the case of LLMs, the forgetting of linguistic skills is not evenly spread -the forgetting is rather taskspecific. Akin to other neural network applications, the forgetting of linguistic skills may likely be grouped as performance loss over a single task A; however, as seen in Table 1, the GLUE tasks suffer various ranges of degradation -the task of finding the referent of a pronoun (WNLI, Levesque et al. (2012)) does not seem to suffer at all, while the grammatical correctness assessment task (CoLA, Warstadt et al. (2019)) suffers severe degradation.\n\nAs proponents for skill-empowered LLMs, we thus make a case for disclosing the performance on general NLU tasks when models are trained for superior performance on niche skill-sets such as non-linguistics, an area left wanting in the Math-NLP front. Because of this task-specific forgetting, quantitative reasoning models trained in a Q&A fashion may not showcase degradation in similarly modeled downstream tasks such as SQuAD (Rajpurkar et al., 2016) and DROP (Dua et al., 2019)thus disclosing performance across a range of NLU tasks is crucial.\n\nSubstantiating forgetting on the basis of parameter sharing: To establish that observed performance degradation can indeed be accredited to catastrophic forgetting, we take an information theoretic lens to pry into parameter-sharing tendencies across tasks with the aid of Fisher information (Ri",
            "score": 0.6884855090482072,
            "section_title": "Necessitating the Re-thinking",
            "char_start_offset": 3278,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 338,
                    "end": 357,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1363,
                    "end": 1385,
                    "matchedPaperCorpusId": "44072099"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37548828125
        },
        {
            "corpus_id": "276409016",
            "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
            "text": "Continual instruction tuning (Peng et al., 2023;Chung et al., 2024) has emerged as an indispensable ingredient in the development of Large Language Models (LLMs) (Brown et al., 2020;Radford et al., 2019;Touvron et al., 2023b), enabling them to meet the demands of specific domains (Roziere et al., 2023;Thirunavukarasu et al., 2023;Xue et al., 2024) and human preferences (Ouyang et al., 2022). However, a notable concern with such continual tuning is \"catastrophic forgetting\" (Mc-Closkey & Cohen, 1989;Kirkpatrick et al., 2017), where models may lose essential skills (Dou et al., 2023;Chen et al., 2023) such as mathematical reasoning while adjusting to user instructions. While instruction tuning effectively evolves LLMs, it's critical to characterize and mitigate forgetting within these models. \n\nResearch on LLM forgetting (Luo et al., 2024;Wang et al., 2023c;Wu et al., 2024a) generally examines alterations in specific abilities like reading comprehension, factual knowledge, mathematical reasoning skills, and so on, underscoring the universal existence of catastrophic forgetting. As they have primarily studied from a single training sequence, they fail to establish the connection between model forgetting and the characteristics of training data. Concurrently, there is a notable gap in understanding the internal mechanisms that underlie model forgetting. To date, only a limited body of research has ventured into this area; notably, the work of Kotha et al. (2024), proposing the task inference hypothesis, explores how conflicts between task processors lead to forgetting. Nevertheless, the existing literature still struggles to track the internal mechanisms behind forgetting, which is crucial for understanding why and when forgetting occurs in language models after learning new tasks and how to avoid it. \n\nIn this study, we conduct thorough experiments on various continual instruction tuning benchmarks covering multiple language models, task sequences, and evaluation metrics. Our investigation focuses on the research question: When does forgetting happen?",
            "score": 0.6846087059205974,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 67,
                    "matchedPaperCorpusId": "253018554"
                },
                {
                    "start": 162,
                    "end": 182,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 182,
                    "end": 203,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 303,
                    "end": 332,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 372,
                    "end": 393,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 504,
                    "end": 529,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1463,
                    "end": 1482,
                    "matchedPaperCorpusId": "262054014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52685546875
        },
        {
            "corpus_id": "277349374",
            "title": "Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach",
            "text": "Finally, text not included in the latter stages of training is less likely to be memorized, even if it appears frequently in the overall training corpus (Menta et al. (2025)). \n\nIndeed, recent research has uncovered systematic patterns in how fact memorization scales with model size and training duration. Lu et al. (2024) found that the LLM fact knowledge capacity has a linear relationship with model size and a negative exponential relationship with training epochs. These findings suggest that memorizing the entirety of a knowledge base like Wikidata would require training an LLM with approximately 1000 billion non-embedding parameters for 100 epochs, indicating that complete factual memorization may be impractical within current training paradigms. \n\nThis work also details how LLMs struggle with memorizing redundant facts in a unified way, showing inefficiency in handling correlated information. Thus, LLMs tend to prioritize memorizing frequent and difficult facts, with subsequent facts potentially overwriting prior memorization, which can hinder retention of low-frequency information. Chang et al. (2024) provide insights into how LLMs acquire factual knowledge during the pre-training phase: acquisition occurs through a process of progressively increasing the probability of fact-based information presented in the training data at each step, though this increase can be diluted by subsequent forgetting. Counterintuitively, they found that training on more data doesn't necessarily improve a model's capability to acquire and main-tain such knowledge. Models undergo both forgetting of memorization and generalization of factual data, with models trained on duplicated data exhibiting faster forgetting. \n\nAdditionally, training with larger batch sizes can enhance models' robustness to forgetting. These findings help explain observed behaviors of LLMs, such as poor performance on long-tail knowledge and the benefits of deduplicating training corpora. \n\nIn order to address the challenge of hallucination in LLMs, Lin et al. (2024) developed FLAME (FactuaLity-aware AlignMEnt), a procedure for enhancing the factual accuracy of LLMs during alignment. Their research identified key factors that lead to hallucination during both supervised fine-tuning (SFT) and reinforcement learning (RL) phases of alignment.",
            "score": 0.6841535548149427,
            "section_title": "Introduction",
            "char_start_offset": 6290,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1976
                },
                {
                    "start": 1979,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2334
                }
            ],
            "ref_mentions": [
                {
                    "start": 307,
                    "end": 323,
                    "matchedPaperCorpusId": "270703692"
                },
                {
                    "start": 1104,
                    "end": 1123,
                    "matchedPaperCorpusId": "270559235"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1995849609375
        },
        {
            "corpus_id": "274982256",
            "title": "Chained Tuning Leads to Biased Forgetting",
            "text": "Catastrophic Forgetting in LLMs. Catastrophic forgetting and continual learning has been long studied in machine learning (Goodfellow et al., 2014;Ramasesh et al., 2020). Proposed mitigations for catastrophic forgetting include weight regularization on subsequent tasks (Kirkpatrick et al., 2017;Zenke et al., 2017), and replay-based methods by injecting samples from previous tasks (Chaudhry et al., 2018;Lopez-Paz and Ranzato, 2017;Chaudhry et al., 2019). There is also work studying the relationship between loss pass and curvature and forgetting (Mirzadeh et al., 2020a). \n\nRecent work has shown that large language models are susceptible to catastrophic forgetting and that forgetting can increase as model size increases (Luo et al., 2023). Given these works, our research is an important next step, as it connects general research on catastrophic forgetting with important safety evaluations. Previous studies (Bianchi et al., 2023)  incorporation of a mixture of safety data during the capability fine-tuning stage (Qi et al., 2023;Jan et al., 2024). However, our study is the first to investigate the effects of data rehearsal techniques in a chained fine-tuning context. \n\nBiases and Safety of LLMs. Language models pre-trained on large corpora can contain cultural biases (Blodgett et al., 2020;Sun et al., 2019;Smith et al., 2022) and produce harmful output and contents (Gehman et al., 2020;Weidinger et al., 2021). \n\nAlthough LLMs are increasingly subsequently finetuned on safety and/or alignment datasets, such guardrails can be undermined through adversarial attack (Perez et al., 2022), in a continual learning setting (Qi et al., 2023), or by altering prompt and data training mix during safety tuning (Lyu et al., 2024). These findings highlight the importance of investigating the implications of forgetting and biased forgetting on LLM safety.",
            "score": 0.6800236001550577,
            "section_title": "Related Works",
            "char_start_offset": 20669,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 32
                },
                {
                    "start": 33,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 575
                },
                {
                    "start": 578,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1428
                },
                {
                    "start": 1431,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1865
                }
            ],
            "ref_mentions": [
                {
                    "start": 122,
                    "end": 147,
                    "matchedPaperCorpusId": "12730344"
                },
                {
                    "start": 296,
                    "end": 315,
                    "matchedPaperCorpusId": "10409742"
                },
                {
                    "start": 406,
                    "end": 434,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 1383,
                    "end": 1404,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 1583,
                    "end": 1603,
                    "matchedPaperCorpusId": "246634238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.671875
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Scialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning. However, their analysis primarily focuses on the forgetting phenomenon observed in downstream tasks, such as summarization and style transfer. The evolution of general knowledge stored in the original pre-trained LLMs during instruction tuning remains unexplored. Luo et al. (2023b) conduct an analysis using probing methods on pre-trained language models to examine the problem of generality destruction and general knowledge forgetting during continual fine-tuning. Nevertheless, their study is restricted to encoderonly models and classification tasks. In this work, we draw attention to the following fundamental questions regarding forgetting in generative LLMs: \n\n1. Are the general knowledge stored in LLMs forgotten during continual instruction tuning? 2. What types of general knowledge are saliently forgotten? 3. What are the effects of model scales, model architectures, and general instruction tuning in the forgetting problem? \n\nTo address these questions, we conduct an empirical study on various LLLMs, such BLOOMZ, mT0 (Muennighoff et al., 2022), LLAMA Touvron et al. (2023), and ALPACA Taori et al. (2023) to analyze the catastrophic forgetting (CF) problem during continual instruction tuning. We continually train the original LLMs with five instruction tasks and evaluate the retention of the general knowledge within the model from three perspectives: domain knowledge, reasoning, and reading comprehension. Furthermore, we investigate the evolution of bias in LLMs throughout the tuning process. To gain insights into the effect of model architecture, we compare the performance of BLOOMZ with that of mT0 (Muennighoff et al., 2022) (an encoder-decoder model), which is fine-tuned using similar datasets. We also investigate the impact of general instruction tuning on the CF problem by comparing the performance of the initial model with the instruction-tuned version such as (BLOOM, BLOOMZ) and (LLAMA (Touvron et al., 2023), ALPACA (Taori et al., 2023)). \n\nOur findings reveal that the forgetting problem is generally present in LLMs.",
            "score": 0.6788799324231951,
            "section_title": "Introduction",
            "char_start_offset": 2002,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 2078
                },
                {
                    "start": 2081,
                    "end": 2158
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86865234375
        },
        {
            "corpus_id": "261705846",
            "title": "Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics",
            "text": "Given these LLMs are further fine-tuned with multi-modal data, it might be intuitively expected that their standard NLP capabilities could degrade. Such a phenomenon is commonly referred to as catastrophic forgetting [15] or in the AI alignment community -the alignment tax [3,13]. \n\nInterestingly, contrary to these assumptions, our results presented in table 2 show that MM-lora (marked in the gray background) results in only an average 0.17% performance decrease across five NLP capability benchmarks and four models, after applying visual instruction tuning. More notably, in certain instances, MM-lora even modestly improves performance on these benchmarks. \n\nIn conjunction with the insights from Section 3.1, these observations altogether highlight the ability of visual-instruction-tuned LLMs in both maintaining the strong capability on standard NLP benchmarks and aligning better with human values, not to mention the additional capability of recognizing visual inputs. Such findings pave new avenues for both academic exploration and practical implementations within multi-modal domains. We believe these insights should catalyze further investigations into the tuning of LLMs with multi-modal interactions. Table 4: Performances of the MLLM family on MSCOCO [20] with corrupted visual inputs.",
            "score": 0.6749690140863035,
            "section_title": "Standard NLP Abilities",
            "char_start_offset": 10051,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 281
                },
                {
                    "start": 284,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1305
                }
            ],
            "ref_mentions": [
                {
                    "start": 1271,
                    "end": 1275,
                    "matchedPaperCorpusId": "14113767"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.402099609375
        },
        {
            "corpus_id": "271601132",
            "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI",
            "text": "Task Dependence. LLMs learn general representations of both syntactic and semantic knowledge during their pretraining on the large corpus. This enables them to serve as a more general-purpose tool to solve different tasks. The patterns in the representations are intertwined and can be vulnerable when combined with downstream tasks. Catastrophic forgetting is a typical example of such vulnerability, which often occurs during transfer learning. The model can lose its generalization ability and overfit to the target domain in a catastrophic forgetting (Luo et al., 2023;Zhai et al., 2023;Wang et al., 2023a). Thus, when testing the efficacy of an unlearning method for the LLMs, it is important to conduct comprehensive fidelity experiments on datasets from various domains. Nevertheless, most of the existing work tests the retaining and forgetting performance of the unlearning methods on specific datasets (Chen & Yang, 2023a;Yao et al., 2023). The impact of the unlearning process on the model's generalization ability to other tasks is rarely verified. Although the model preserves its performance on the current task, it remains uncertain whether the nuanced modifications in parameters during unlearning force the model to compromise its capability in other tasks. \n\nWe argue that an effective unlearning method should minimize the performance degradation of the target model on diverse tasks. \n\nForget or Lie? Nowadays, we hold a higher expectation of generative models than before. This gap of expectation is more prominent in terms of LLMs. In the previous era, we mainly focused on improving the fluency and stability of the generation. Therefore, as the unlearning results from (Eldan & Russinovich, 2023) shown in Figure 2, a worse performance or a fabricated description for the sensitive instances could be viewed as a successful unlearning result. However, simply generating incorrect outputs can no longer be enough when we are expecting factual and reliable generation. LLMs are being transformed to function as knowledge bases (AlKhamissi et al., 2022) consisting of structural representations of facts and relations. Besides, substantial efforts have been devoted to reducing hallucinations (Rawte et al., 2023).",
            "score": 0.6742996889791713,
            "section_title": "FOR LLMS",
            "char_start_offset": 13189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 17,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1060
                },
                {
                    "start": 1061,
                    "end": 1274
                },
                {
                    "start": 1277,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1493
                },
                {
                    "start": 1494,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2235
                }
            ],
            "ref_mentions": [
                {
                    "start": 912,
                    "end": 932,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.331298828125
        },
        {
            "corpus_id": "270357611",
            "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
            "text": "Instruction fine-tuning is key to improving the capabilities and controllability of large language models (LLMs) (Brown et al., 2020;Fedus et al., 2022), which have already demonstrated strong performance in various tasks (Zhong et al., 2023;Peng et al., 2023;Lu et al., 2023;Ren et al., 2024).One major obstacle to tuning LLMs is catastrophic forgetting (CF, Kirkpatrick et al., 2017), which means LLMs forget prior knowledge when learning new data.Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities.\n\nUnderstanding the effectiveness of LLMs during fine-tuning is important for downstream tasks, however, the underlying causes of CF remain largely unexplored.There are two types of work addressing CF, from data and model perspectives, respectively.Scialom et al. (2022) propose continual learning with rehearsal in instruction tuning, though its effectiveness varies with task selection.Lin et al. (2023) indicate that both continual learning and weight averaging (Wise-FT) effectively preserve generality.\n\nWhile the above techniques are somewhat successful, they require expensive extra dataconstructing and training costs, and are even sometimes impractical in LLMs, because a) the data cards of many pretrained models are unclear (Shi et al., 2023), making rehearsal unfeasible, and b) anti-forgetting training brings an unstable and expensive training process (Datta et al., 2023).\n\nIn this work, we turn to finding a cheap, stable, and orthogonal solution to alleviate the CF in tuning LLMs.",
            "score": 0.6739145086359912,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 450
                },
                {
                    "start": 450,
                    "end": 788
                },
                {
                    "start": 790,
                    "end": 947
                },
                {
                    "start": 947,
                    "end": 1037
                },
                {
                    "start": 1037,
                    "end": 1176
                },
                {
                    "start": 1176,
                    "end": 1295
                },
                {
                    "start": 1297,
                    "end": 1675
                },
                {
                    "start": 1677,
                    "end": 1786
                }
            ],
            "ref_mentions": [
                {
                    "start": 113,
                    "end": 133,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1037,
                    "end": 1058,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 1523,
                    "end": 1541,
                    "matchedPaperCorpusId": "264451585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "273507804",
            "title": "Exploring Forgetting in Large Language Model Pre-Training",
            "text": "Catastrophic forgetting (McCloskey and Cohen, 1989;Ratcliff, 1990) poses a significant challenge to the development of models Traditionally, the challenge of catastrophic forgetting in neural networks is especially pronounced when models are tasked with retaining knowledge across diverse datasets (Sun et al., 2020;Jin et al., 2021;de Masson D'Autume et al., 2019;Wang et al., 2020;Qin et al., 2022). This issue arises due to the shift in input distribution across different tasks, which leads to the model's inability to remember past knowledge and capability effectively. \n\nAlthough pioneer efforts have explored the forgetting issue in LLM fine-tuning, which primarily addresses task-specific forgetting, there is a lack of research on finer-grained forgetting in pretraining. Luo et al. (2023), Wang et al. (2023b), and Wu et al. (2024) focused on forgetting in finetuning by measuring the performance of new tasks with continual tuning. Other efforts (Tirumala et al., 2022;Biderman et al., 2023a) studied samplelevel memorization, where some experiments imply the existence of forgetting in LLM pre-training. Nonetheless, these studies have devoted limited attention to systematically exploring and quantifying the forgetting in pre-training. \n\nForgetting in pre-training is a critical issue that must be addressed. It is prevalent among current LLMs and significantly affects their performance. Usually, models are believed to acquire various factual knowledge during the pre-training phase, and during the fine-tuning phase, they enhance their task-related capabilities (Chang et al., 2024). Intuitively, LLMs may give unsatisfactory replies to fact-relevant queries, even when the necessary information was present in the pre-training data. This indicates forgetting. Despite being easily noticed, measuring this forgetting in pre-training is very challenging. In contrast to works studying fine-tuning that measure with specific task-related metrics (e.g., QA accuracy), the pre-training data is too diverse, inherently consisting of dozens of tasks, making it almost impossible to use a specific ability-related metric to reflect forgetting. Moreover, there's almost no metrics designed for forgetting.",
            "score": 0.670057163345758,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1777
                },
                {
                    "start": 1778,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2153
                },
                {
                    "start": 2154,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 24,
                    "end": 51,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 51,
                    "end": 66,
                    "matchedPaperCorpusId": "18556305"
                },
                {
                    "start": 298,
                    "end": 316,
                    "matchedPaperCorpusId": "198968327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3857421875
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "Echoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evi-denced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent. \n\nIn this paper, we propose a novel fine-tuning method, Self-Distillation Fine-Tuning (SDFT), to mitigate catastrophic forgetting during fine-tuning. We hypothesize that catastrophic forgetting stems from the distribution gap between the task dataset and the seed LMs. To address the issue, as shown in Figure 1, SDFT first prompts the seed LM to generate responses that uphold semantic equivalence with the original responses present in the task dataset, resulting in the distilled dataset. A representative example of rewriting is depicted in Figure 2. After rewriting, the self-generated responses serve as surrogate targets during subsequent finetuning. Through the approach, SDFT inherently maintains the original distribution, avoiding distribution shift and thereby preserving capabilities. \n\nWe systematically evaluate SDFT by comparing its performance against that of vanilla fine-tuning and the seed LM across a variety of benchmarks. These benchmarks encompass: (1) diverse downstream tasks, including mathematical reasoning, tool using and code generation; (2) assessments of general helpfulness and safety alignment. Results on all benchmarks demonstrate the superiority of SDFT compared to vanilla fine-tuning. For instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2. The in-depth analysis of our method indicates that increasing the proportion of distilled dataset for fine-tuning leads to a decrease in catastrophic forgetting, thereby confirming that SDFT mitigates catastrophic forgetting by bridging the distribution gap.",
            "score": 0.6687896791033714,
            "section_title": "Introduction",
            "char_start_offset": 1946,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 309
                },
                {
                    "start": 312,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 137,
                    "end": 154,
                    "matchedPaperCorpusId": "263671523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67333984375
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "Catastrophic forgetting in fine-tuned language models.Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2022;Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;Kotha et al., 2024;Wu et al., 2024b).In context of LLMs, existing approaches towards mit-igating this issue can mostly be categorized into three types: regularizing the update of model parameters (Kirkpatrick et al., 2017;Huang et al., 2021;Cha et al., 2021), replaying previous or selfsynthesized data (Scialom et al., 2022;Huang et al., 2024a) and resisting interference via parameterefficient fine-tuning (Razdaibiedina et al., 2023;Wang et al., 2023a).\n\nMechanistic analysis to fine-tuning.Existing works on analyzing the internal mechanism (R\u00e4uker et al., 2023;Ferrando et al., 2024) of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal transformation on top of the original capability (Jain et al., 2024) (wrappers), subtractable and reusable parameter shift vectors (Huang et al., 2024b;Gao et al., 2024) (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage (Ren et al., 2024).Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point.",
            "score": 0.6660026237246774,
            "section_title": "Related work",
            "char_start_offset": 24807,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 54,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 985
                },
                {
                    "start": 987,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1571
                },
                {
                    "start": 1571,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 214,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 529,
                    "end": 548,
                    "matchedPaperCorpusId": "262054014"
                },
                {
                    "start": 751,
                    "end": 770,
                    "matchedPaperCorpusId": "233210110"
                },
                {
                    "start": 770,
                    "end": 787,
                    "matchedPaperCorpusId": "219636462"
                },
                {
                    "start": 832,
                    "end": 854,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 937,
                    "end": 965,
                    "matchedPaperCorpusId": "256390383"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "264426441"
                },
                {
                    "start": 1307,
                    "end": 1326,
                    "matchedPaperCorpusId": "265308865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61083984375
        },
        {
            "corpus_id": "274982256",
            "title": "Chained Tuning Leads to Biased Forgetting",
            "text": "Catastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2014;Kirkpatrick et al., 2016;Kemker et al., 2018). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;Razdaibiedina et al., 2023;Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023). \n\nDespite empirical observations of catastrophic forgetting, it remains unclear which post-training recipes can lead to forgetting. In this paper, we study how different finetuning regimes, including task ordering and learning rate, influence the severity of catastrophic forgetting. Specifically, we investigate a new sub-phenomenon of catastrophic forgetting that we call biased forgetting, whereby model performance degrades disproportionately for safety-related tasks, or for specific demographic groups, and propose a simple metric to quantify it. Empirically, we found that the order of finetuning on different tasks matters: safety/bias task are more prone to forgetting when subsequently finetuned with a capability task as compared to the other order. We hypothesized that certain tasks may be more prone to forgetting due to the width of the minima obtained the end of training, and empirically observed that safety tasks converged to sharper minima. Based on these observations, we explored two approaches for mitigating biased forgetting: determining task ordering using first task loss curvature, and retraining using a small portion of the forgotten task.",
            "score": 0.6659202965854761,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 1034
                },
                {
                    "start": 1037,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 178,
                    "matchedPaperCorpusId": "18556305"
                },
                {
                    "start": 239,
                    "end": 264,
                    "matchedPaperCorpusId": "12730344"
                },
                {
                    "start": 289,
                    "end": 309,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 460,
                    "end": 487,
                    "matchedPaperCorpusId": "256390383"
                },
                {
                    "start": 889,
                    "end": 909,
                    "matchedPaperCorpusId": "246634238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85400390625
        },
        {
            "corpus_id": "270620654",
            "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
            "text": "Upstream examples and learned tasks. LLMs are commonly pre-trained with language modeling objectives over a massive collection of corpora, and optionally post-trained (instruction-tuned) to better follow human instructions. We use upstream data to refer to language modeling or instruction tuning training data used at the pre-training or post-training phase of LLMs. For upstream data of language modeling, we define each upstream example x j \u2208 x 1..N as a chunk of document (e.g., a Wikipedia article) of a model-specific maximum number of tokens. For instruction tuning, each x j \u2208 x 1..N corresponds to a pair of instructions and ground truth responses. \n\nMeasuring forgetting. We fine-tune an LLM (or an instruction-tuned LLM) on one unseen instruction-tuning task T i from a collection of tasks T 1..M . This results in M separately fine-tuned models f 1..M . We then evaluate performance degradation on each upstream example x j \u2208 x 1..N . We use log perplexity as the main performance metric as it is applicable to both language modeling and instruction tuning, and is known to correlate well with other dataset-specific metrics (Hoffmann et al., 2022). For instruction tuning tasks with a restricted output space (e.g., multi-choice questions), we also measure binary exact matches (EM). We measure forgetting z ij that occurs on an upstream example x j \u2208 x 1..N as increase in log perplexity or drop of exact match after fine-tuning the LM on a new task T i \u2208 T 1..M . We record forgetting z ij in an association matrix Z \u2208 R M \u00d7N . Our analysis requires open access to upstream data of LLMs. We perform main experiments with OLMo-1B, OLMo-7B, and OLMo-7B-Instruct (Groeneveld et al., 2024).",
            "score": 0.6607287037384882,
            "section_title": "Collecting Statistics of Forgetting",
            "char_start_offset": 5343,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 657
                },
                {
                    "start": 660,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1542
                },
                {
                    "start": 1543,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1701
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1734619140625
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "In conclusion, using a LoRA fine-tuning setup, we empirically demonstrated that during fine-tuning on a downstream task forgetting is strongly predicted by a linear function of fine-tuning loss, and a shifted power law in the number of non-embedding parameters fine-tuned and the number of update steps. In addition, we used our setup to identify that similar scaling laws to those for pre-training LLMs identified by (Kaplan et al., 2020;Henighan et al., 2020) held for fine-tuning. In particular, the fine-tuning loss was also fit by a shifted power law function. We then examined the forgetting behaviour in model generation, and showed that both model safety and reasoning benchmark performance suffer from forgetting. \n\nTo quantify forgetting consistently, we used the cross-entropy loss between the fine-tuned model and the base model's predictions. We reasoned in section 3.1 why this metric is the most appropriate, and why usual metrics for loss may be inadequate. \n\nIn light of our laws for forgetting, which show that forgetting is a consequence of fine-tuning performance, we underscore the need for techniques to mitigate forgetting in LLMs. As such, an avenue for future work would be to develop and evaluate techniques for mitigating forgetting in our setup, and compare precisely how much the functional relationships may change to be more favourable to fine-tuning without forgetting.",
            "score": 0.6585829851785138,
            "section_title": "Conclusion",
            "char_start_offset": 26998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6552734375
        },
        {
            "corpus_id": "277501673",
            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
            "text": "(Hui et al., 2024) demonstrate impressive learning abilities and overall performance after fine-tuning. \n\nIn summary, this work contributes by: \n\n\u2022 Evaluating catastrophic forgetting and task learning across diverse LLMs using sequential arXiv:2504.01241v1 [cs.CL] 1 Apr 2025 fine-tuning on specific NLU tasks. \n\n\u2022 Highlighting the importance of prompt engineering and fine-tuning strategies for optimizing model performance. \n\n\u2022 Providing insights into the performance of models with different parameter sizes, identifying those best suited for continual learning. \n\n\u2022 Proposing continual fine-tuning as a key strategy for future LLM agents to handle multiple tasks without sacrificing accuracy.",
            "score": 0.6491513402288472,
            "section_title": "Introduction",
            "char_start_offset": 2048,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 143
                },
                {
                    "start": 146,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 425
                },
                {
                    "start": 428,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 696
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.454345703125
        },
        {
            "corpus_id": "277501673",
            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
            "text": "This study explored catastrophic forgetting in large language models during sequential fine-tuning on multiple NLU tasks. We found that smaller models like Phi-3.5-mini effectively minimize forgetting while maintaining learning capabilities. Prompt engineering and fine-tuning strategies significantly impact model performance in continual learning settings. Models such as Orca-2-7b and Qwen2.5-7B showed strong learning abilities but varied in forgetting. Careful model selection and tuning can enhance handling multiple tasks without sacrificing accuracy, which is crucial for developing autonomous LLM-based agents. Future work should explore more advanced continual learning techniques to mitigate catastrophic forgetting.",
            "score": 0.6482726986108143,
            "section_title": "Conclusion",
            "char_start_offset": 10171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 727
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5869140625
        },
        {
            "corpus_id": "269362836",
            "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
            "text": "Recent advances in large language models (LLMs) have demonstrated considerable potential for achieving artificial general intelligence (AGI) [1,6,22,40,173,186,230,231]. Researchers have observed that complex abilities such as multi-step reasoning, few-shot in-context learning, and instruction following improve as the scale of parameter size increases [159,250,252,253,277]. The development of LLMs is impactful and revolutionary, prompting machine learning practitioners to reconsider traditional computational paradigms for once-challenging human-level tasks. However, LLMs are typically trained on static, pre-collected datasets encompassing general domains, leading to gradual performance degradation over time [5,52,95,96,100,137] and across different content domains [35,44,69,71,100,104,183,184,221]. \n\nAdditionally, a single pre-trained large model cannot meet every user need and requires further fine-tuning [10,16,37,106,182,254,255,255,281,299,299]. While one potential solution is re-collecting pre-training data and re-training models with additional specific needs, this approach is prohibitively expensive and impractical in real-world scenarios. \n\nTo efficiently adapt LLMs to downstream tasks while minimizing performance degradation on previous knowledge domains, researchers employ the methodology of Continual Learning (CL), also known as lifelong learning or incremental learning [38,178,232,237]. Inspired by the incremental learning pattern observed in human brains [101,153,154,175], CL trains machine learning models sequentially on a series of tasks with the expectation of maintaining performance across all tasks [57,58,113,124]. Throughout training, models have limited or no access to previous data, posing a challenge in retaining past knowledge as optimization constraints from unseen previous data are absent during current-task learning [124,135,213]. This challenge, known as catastrophic forgetting [155], has been a central focus in continual learning research since its inception. Over the years, researchers have explored various techniques to mitigate forgetting.",
            "score": 0.6459303865851423,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 809
                },
                {
                    "start": 812,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1164
                },
                {
                    "start": 1167,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2021
                },
                {
                    "start": 2022,
                    "end": 2106
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 149,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 156,
                    "end": 160,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 367,
                    "end": 371,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 371,
                    "end": 375,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 717,
                    "end": 720,
                    "matchedPaperCorpusId": "235417543"
                },
                {
                    "start": 720,
                    "end": 723,
                    "matchedPaperCorpusId": "235669861"
                },
                {
                    "start": 729,
                    "end": 733,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 733,
                    "end": 737,
                    "matchedPaperCorpusId": "246652070"
                },
                {
                    "start": 775,
                    "end": 779,
                    "matchedPaperCorpusId": "258833488"
                },
                {
                    "start": 785,
                    "end": 788,
                    "matchedPaperCorpusId": "236976189"
                },
                {
                    "start": 788,
                    "end": 792,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "252815848"
                },
                {
                    "start": 800,
                    "end": 804,
                    "matchedPaperCorpusId": "247447415"
                },
                {
                    "start": 804,
                    "end": 808,
                    "matchedPaperCorpusId": "198968327"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 927,
                    "end": 930,
                    "matchedPaperCorpusId": "263829616"
                },
                {
                    "start": 934,
                    "end": 938,
                    "matchedPaperCorpusId": "238856821"
                },
                {
                    "start": 938,
                    "end": 942,
                    "matchedPaperCorpusId": "258556987"
                },
                {
                    "start": 942,
                    "end": 946,
                    "matchedPaperCorpusId": "258887506"
                },
                {
                    "start": 946,
                    "end": 950,
                    "matchedPaperCorpusId": "258887506"
                },
                {
                    "start": 950,
                    "end": 954,
                    "matchedPaperCorpusId": "248986947"
                },
                {
                    "start": 1412,
                    "end": 1416,
                    "matchedPaperCorpusId": "254335115"
                },
                {
                    "start": 1416,
                    "end": 1420,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "2832081"
                },
                {
                    "start": 1505,
                    "end": 1509,
                    "matchedPaperCorpusId": "1753586"
                },
                {
                    "start": 1648,
                    "end": 1651,
                    "matchedPaperCorpusId": "214612169"
                },
                {
                    "start": 1651,
                    "end": 1655,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1655,
                    "end": 1659,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1874,
                    "end": 1879,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 1883,
                    "end": 1887,
                    "matchedPaperCorpusId": "264305893"
                },
                {
                    "start": 1938,
                    "end": 1943,
                    "matchedPaperCorpusId": "61019113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.326171875
        },
        {
            "corpus_id": "270620654",
            "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
            "text": "Our research tries to better demystify and mitigate forgetting in LLM fine-tuning. We expect that the better understanding alongside the reduced forgetting can, in turn, encourage model developers to promptly update their LLMs to address limitations of the models and improve their models in continuing efforts. We expect the broader application of the continual learning practice will reduce training costs compared to re-training models, and ultimately result in more powerful models under a controlled training budget. \n\nAlthough we do not see direct negative impact of predicting example forgetting, we highlight that in real-world continual learning setups, blindly mitigating forgetting may result in outdated knowledge and data privacy breaches in LLMs. Dissecting beneficial and intended forgetting from unintended or catastrophic forgetting requires attention in real-world setups. We plot forgetting (log-perplexity increase) that occurs on an upstream example (in x-axis) after learning a new task (in y-axis). Log-perplexity increase can be zero or negative, which indicates no forgetting.",
            "score": 0.6456242533322423,
            "section_title": "B Broader Impact",
            "char_start_offset": 33597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1646728515625
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning. Additionally, we observe that the bias in LLMs is mitigated throughout the continual instruction tuning process. When comparing BLOOMZ with mT0 at comparable model scale, we find that BLOOMZ experiences a relatively milder forgetting problem, suggesting that the decoder-only architecture may be better at retaining information during continual instruction tuning. Lastly, empirical results on LLAMA and its instruction-tuned version (i.e., ALPACA) indicate that diverse instruction tuning can help alleviate the CF phenomenon for LLMs in further continual fine-tuning. \n\nThe contribution of our paper can be summarized as follows: \n\n1. We take an initial step to analyze the catastrophic forgetting (CF) problem during continual instruction tuning by an empirically study, where a specific evaluation setting is designed from the perspective of general knowledge such as domain knowledge, reasoning, reading comprehension and the bias problem. 2. We provide an initial research evidence that the CF problem generally exists in the continual instruction tuning process for different models such as BLOOMZ, mT0, LLAMA and ALPACA. We also show that the model architecture, model scale and general instruction tuning have different effects on the CF problem.",
            "score": 0.6429140039115573,
            "section_title": "Introduction",
            "char_start_offset": 4083,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 978
                },
                {
                    "start": 981,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1353
                },
                {
                    "start": 1354,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "We conducted a causal mediate analysis (Sec.3.2) on the fine-tuned model and observed a significant shift in the set S of casual attention heads.The results are reported in Fig. 5 (c).This suggests that the original capability of the model to process tasks was suppressed by new, specialized patterns, leading to a decrease in general capability.\n\nFurthermore, we conducted an intervention experiment on the CommonsenseQA task with the model fine-tuned on the TRACE benchmark (refer to Fig. 7).The results show that the model exhibited significant forgetting in both 0-shot and 10-shot performance, dropping to 0.03 and 0.15, respectively.However, integrating IV into the model (as shown in Fig. 4(g)), i.e., h l = h l + \u03b8 c , result in a substantial recovery in model performance, achieving 0.47 with the current model's IV and 0.49 with the initial model's IV.This demonstrates that by explicitly adding IV back to the computation graph, the model can still adhere to current task instructions, indicating that the observed forgetting is not due to a loss of the model's ability to handle instructions.\n\nIn conclusion, our analysis suggests that forgetting in large language models (LLMs) results from a dynamic conflict between the dominance and suppression of existing computation graphs and new, specialized reasoning patterns learned from finetuning.This extends previous findings Kotha et al. (2024) by utilizing IV framework to explore the underlying processes of forgetting in these models and confirming its theoretical underpinnings.",
            "score": 0.6388152933887652,
            "section_title": "Fine-tuning Dynamics",
            "char_start_offset": 19228,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 346
                },
                {
                    "start": 348,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 639
                },
                {
                    "start": 639,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 1104
                },
                {
                    "start": 1106,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1544
                }
            ],
            "ref_mentions": [
                {
                    "start": 1387,
                    "end": 1406,
                    "matchedPaperCorpusId": "262054014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45263671875
        },
        {
            "corpus_id": "275820535",
            "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
            "text": "Large Language Models (LLMs) are pretrained on massive and diverse datasets, equipping them with remarkable general capabilities (Wang & Komatsuzaki, 2021;Touvron et al., 2023b;OpenAI, 2024). This pretraining process allows LLMs to serve as versatile tools for a wide range of natural language processing tasks. However, in domains such as medical and scientific fields, LLMs often struggle  to perform effectively, necessitating fine-tuning domainspecific data. While fine-tuning could enhance the model task-specific performance, it also introduces a critical challenge: catastrophic forgetting (Kirkpatrick et al., 2016;Kemker et al., 2018;Shao & Feng, 2022;Ren et al., 2024). \n\nAs shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering. \n\nAddressing catastrophic forgetting is therefore a crucial requirement for maximizing the utility of LLMs. A successful solution needs to achieve a delicate balance: retaining the essential general knowledge when learning new domainspecific expertise. This balance is critical when fine-tuning LLMs for specialized tasks, as both domain adaptation and generalizability are necessary for practical applications. EW-CLoRA (Xiang et al., 2024) focuses on the issue of catastrophic forgetting in LLM fine-tuning and uses the Fisher matrix to measure the importance of parameters for general capabilities. However, it requires gradients computed with labels from the model distribution, necessitating an additional backpropagation pass for online computation. Thus, its computational cost is very high.",
            "score": 0.6321651282927647,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 679
                },
                {
                    "start": 682,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1425
                },
                {
                    "start": 1428,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 597,
                    "end": 623,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 623,
                    "end": 643,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 643,
                    "end": 661,
                    "matchedPaperCorpusId": "247315450"
                },
                {
                    "start": 1847,
                    "end": 1867,
                    "matchedPaperCorpusId": "258762577"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "The phenomenon of a neural network forgetting a previously learned task when trained on a new one, continues to pose a key challenge in deep learning. The problem was first brought to attention for early neural networks in (McCloskey & Cohen, 1989;Ratcliff, 1990), in a task incremental setup. These works showed that when a neural network trained on a task was further trained to learn a new task, performance on the first task catastrophically degraded. This forgetting was given the moniker \"Catastrophic Forgetting\". Forgetting has continued to pose a serious problem in the domain of task sequential learning or continual learning (Kemker et al., 2018;Goodfellow et al., 2014;De Lange et al., 2022;Parisi et al., 2019). \n\nApproaches to mitigate forgetting can be broadly categorized (De Lange et al., 2022) into (i) regularization methods (Kirkpatrick et al., 2017;Ahn et al., 2019), (ii) ensembling and parameter isolation methods (Xu & Zhu, 2018;Rusu et al., 2016;Aljundi et al., 2017), and (iii) experience replay/rehearsal (ROBINS, 1995;Rebuffi et al., 2017;Isele & Cosgun, 2018;Lopez-Paz & Ranzato, 2017;Gepperth & Karaoguz, 2016;Chaudhry et al., 2019;Rolnick et al., 2019;Purushwalkam et al., 2022). \n\nRecently, it has been demonstrated that LLMs struggle with forgetting when fine-tuning, especially when adapting to a small fine-tuning dataset. In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training.",
            "score": 0.6315978914291953,
            "section_title": "Catastrophic Forgetting",
            "char_start_offset": 3928,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 1210
                },
                {
                    "start": 1213,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1872
                }
            ],
            "ref_mentions": [
                {
                    "start": 223,
                    "end": 248,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 248,
                    "end": 263,
                    "matchedPaperCorpusId": "18556305"
                },
                {
                    "start": 636,
                    "end": 657,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 657,
                    "end": 681,
                    "matchedPaperCorpusId": "12730344"
                },
                {
                    "start": 681,
                    "end": 703,
                    "matchedPaperCorpusId": "218889912"
                },
                {
                    "start": 703,
                    "end": 723,
                    "matchedPaperCorpusId": "73497737"
                },
                {
                    "start": 788,
                    "end": 811,
                    "matchedPaperCorpusId": "218889912"
                },
                {
                    "start": 844,
                    "end": 870,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 937,
                    "end": 953,
                    "matchedPaperCorpusId": "44086178"
                },
                {
                    "start": 971,
                    "end": 992,
                    "matchedPaperCorpusId": "914027"
                },
                {
                    "start": 1032,
                    "end": 1046,
                    "matchedPaperCorpusId": "22882861"
                },
                {
                    "start": 1067,
                    "end": 1088,
                    "matchedPaperCorpusId": "3622700"
                },
                {
                    "start": 1088,
                    "end": 1114,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 1162,
                    "end": 1183,
                    "matchedPaperCorpusId": "53860287"
                },
                {
                    "start": 1183,
                    "end": 1209,
                    "matchedPaperCorpusId": "247628244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.634765625
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "We suggest that the task proficiency in LLMs involves understanding task-specific knowledge and following instructions, assessed through Knowledge Probability P (y|x) and Instruction Probability P (y c |c, x), respectively (as depicted in Fig. 2).Our empirical analysis within a continual instruction tuning framework reveals distinct forgetting patterns between these two aspects, with shifts in instruction following primarily driving performance declines.\n\nTo investigate the internal changes of the model during forgetting, we introduce the Instruction Vector (IV) framework to extract representations closely associated with the task processing.We hypothesize a straightforward yet robust computational graph for LLMs (see Fig. 1 b), featuring an intermediate variable \u03b8 c crucial for task performance.The presence or absence of \u03b8 c directly impacts the model's capability to handle instruction c.This hypothesis is supported by causal intervention experiments in Sec.3.2.By analyzing IV dynamics pre and post-training, we find minor changes in IV expression with forgetting happens.Furthermore, explicitly incorporating IV into the model's computational graph can recover the mastery of the corresponding instruction.This results indicate that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting.\n\nBuilding on these insights, we develop an IVguided training methodology to mitigate catastrophic forgetting.This method incorporates a progressive IV-intervention training mechanism, in which the IV is initially introduced through intervention and is then gradually phased out during the training process.The deliberate inclusion of IV aids in optimizing the model by ensuring adherence to the IV-related computational graph, thereby minimizing the overshadowing effect of new reasoning pathways.Additionally, we have introduced an IVbased KL-Divergence loss function to reduce the discrepancies between zero-shot and IV-intervened logits, ensuring that the model's behavior remains aligned with the original computational structure.Validated across multiple datasets, this method significantly alleviate forgetting in both general and in-context learning abilities, confirming the link between IV and forgetting.\n\nMain Findings and Contributions.(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.",
            "score": 0.6298533670958264,
            "section_title": "Introduction",
            "char_start_offset": 1804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 458
                },
                {
                    "start": 460,
                    "end": 650
                },
                {
                    "start": 650,
                    "end": 807
                },
                {
                    "start": 807,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 973
                },
                {
                    "start": 973,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1223
                },
                {
                    "start": 1223,
                    "end": 1372
                },
                {
                    "start": 1374,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1679
                },
                {
                    "start": 1679,
                    "end": 1870
                },
                {
                    "start": 1870,
                    "end": 2107
                },
                {
                    "start": 2107,
                    "end": 2287
                },
                {
                    "start": 2289,
                    "end": 2321
                },
                {
                    "start": 2321,
                    "end": 2604
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "Notably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024). Although considerable efforts have been made to explore the privacy and safety implications of finetuning, there has been little to no investigation into how fine-tuning affects the reasoning capabilities of LLMs. If fine-tuning LLMs diminishes their reasoning abilities, LLMs may lose their core appeal to users (Brown et al., 2020;Wei et al., 2022b) A key method for eliciting reasoning in LLMs is the Chain-of-Thought reasoning approach Wei et al. (2022b). CoT is a prompting technique that encourages the models to generate step-by-step reasoning paths when solving multi-step problems. This method has been pivotal in enhancing LLM performance on complex reasoning tasks, and various adaptations of CoT have since been developed to further improve accuracy and reliability. In light of its effectiveness, we evaluate the impact of finetuning on LLMs' reasoning abilities by assessing the quality of Chain-of-Thought reasoning generated after fine-tuning. Our work. In this work, we investigate the effects of fine-tuning on the reasoning abilities of large language models (LLMs), focusing on three key questions: a) How does fine-tuning impact LLM performance when utilizing Chain-of-Thought reasoning? b) Does fine-tuning affect the faithfulness of CoT reasoning? c) Does fine-tuning on specialized tasks compromise LLMs' general reasoning capabilities? Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate.",
            "score": 0.6296302693449948,
            "section_title": "Introduction",
            "char_start_offset": 1590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 424,
                    "end": 444,
                    "matchedPaperCorpusId": "267149066"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.890625
        },
        {
            "corpus_id": "277501673",
            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
            "text": "Large Language Models (LLMs) have significantly advanced Natural Language Processing (NLP), particularly in Natural Language Understanding (NLU) tasks. As we progress toward an agentic world where LLM-based agents autonomously handle specialized tasks, it becomes crucial for these models to adapt to new tasks without forgetting previously learned information - a challenge known as catastrophic forgetting. This study evaluates the continual fine-tuning of various open-source LLMs with different parameter sizes (specifically models under 10 billion parameters) on key NLU tasks from the GLUE benchmark, including SST-2, MRPC, CoLA, and MNLI. By employing prompt engineering and task-specific adjustments, we assess and compare the models' abilities to retain prior knowledge while learning new tasks. Our results indicate that models such as Phi-3.5-mini exhibit minimal forgetting while maintaining strong learning capabilities, making them well-suited for continual learning environments. Additionally, models like Orca-2-7b and Qwen2.5-7B demonstrate impressive learning abilities and overall performance after fine-tuning. This work contributes to understanding catastrophic forgetting in LLMs and highlights prompting engineering to optimize model performance for continual learning scenarios.",
            "score": 0.6278442899602917,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55517578125
        },
        {
            "corpus_id": "273507771",
            "title": "Learning Mathematical Rules with Large Language Models",
            "text": "Catastrophic forgetting is a well-known issue when fine-tuning large language models, where the model's performance on the pre-training task decreases after fine-tuning on a new task. We evaluate our models on 3 established benchmarks before and after fine-tuning. The results are presented in table 15. While there is a small performance drop after fine-tuning, the model still performs well on the general knowledge tasks.",
            "score": 0.6250509436437293,
            "section_title": "D.4 Benchmark Performance",
            "char_start_offset": 49821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 264
                },
                {
                    "start": 265,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4970703125
        },
        {
            "corpus_id": "262055661",
            "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models",
            "text": "Fine-Tuning and Catastrophic Forgetting. Fine-tuning large pre-trained models has significantly transformed the field of natural language processing [1,2,[30][31][32]. Despite its ubiquity and remarkable achievements, fine-tuning LLM still suffers from core machine learning problems such as catas-trophic forgetting [33]. Catastrophic forgetting widely appears in LLM fine-tuning [19,21,[34][35][36] or in-context learning [37,38], as the LLMs tend to overfit to the small fine-tuning dataset resulting in losing performance on other tasks [34]. Various approaches have been proposed to mitigate the catastrophic forgetting problem in LLM fine-tuning, including pre-trained weight decay [36], learning rate decay [34], regularizations [35], and adversarial fine-tuning [19]. However, in MLLM, such a catastrophic forgetting phenomenon has not been thoroughly studied yet. Our work is most related to several evaluation metrics for MLLMs [17,25], which proposed a comprehensive framework for evaluating the perception and recognition [17] or hallucinations [25], while the proposed EMT specifically aims at evaluating the catastrophic forgetting in MLLMs. \n\nMultimodal Large Language Models. Multimodal Large Language Models (MLLMs) have emerged as a significant advancement in vision-language models, which significantly improves the model's reasoning capability. These models are designed to process and interpret information from multiple modalities, such as text and images, to perform complex tasks that require a comprehensive understanding of the context. Recent works [6-10, 26, 39-42] have contributed to the development and enhancement of MLLMs by leveraging the strong reasoning capability of LLMs such as LLaMA [14,15]. LLaVA [7], as presented in the paper under discussion, represents a novel approach to instruction tuning on machine-generated multimodal language-image instruction-following data, achieving impressive multimodal chat abilities and state-of-the-art accuracy on Science QA [29]. Following the instruction tuning approach, various works came out focusing on other modalities such as video [43] and point cloud [44].",
            "score": 0.6247102235802812,
            "section_title": "Related Works",
            "char_start_offset": 4962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 41,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1155
                },
                {
                    "start": 1158,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2144
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 154,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 158,
                    "end": 162,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 317,
                    "end": 321,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 381,
                    "end": 385,
                    "matchedPaperCorpusId": "245116938"
                },
                {
                    "start": 385,
                    "end": 388,
                    "matchedPaperCorpusId": "249921347"
                },
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "202750126"
                },
                {
                    "start": 396,
                    "end": 400,
                    "matchedPaperCorpusId": "219558527"
                },
                {
                    "start": 424,
                    "end": 428,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 688,
                    "end": 692,
                    "matchedPaperCorpusId": "219558527"
                },
                {
                    "start": 736,
                    "end": 740,
                    "matchedPaperCorpusId": "202750126"
                },
                {
                    "start": 770,
                    "end": 774,
                    "matchedPaperCorpusId": "245116938"
                },
                {
                    "start": 2003,
                    "end": 2007,
                    "matchedPaperCorpusId": "252383606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "In this section, we present a new perspective to investigate catastrophic forgetting in LLMs, concentrating on the capabilities embedded within pretraining and instruction tuning stages, as opposed to focusing on pure performance shifts as noted in earlier studies (Wang et al., 2023b;Zhai et al., 2023).We start with a discussion on the capabilities encoded in LLMs, proceed to develop continual instruction tuning setup to investigate forgetting, and conclude with the empirical observations.\n\nLet M denote the model pre-trained on large scale data corpus D P T = {X i } with the language modeling task (Brown et al., 2020;Radford et al., 2019).We assume that M has built an impressive ability to capture world knowledge across various domains, i.e., M assigns the maximum likelihood to P (y|x, M ) for certain datasets denoted by D K = {(x i , y i )} \u2208 D P T .Here, the pair [x i , y i ] may represent a segment extracted from raw text X j .For example, consider x being \"The capital city of Japan is\" and y being \"Tokyo\"; such a pairing frequently appears in blogs.In this paper, we refer to P (y|x, M ) as the Knowledge Probability, which serves as a metric for evaluating the model's proficiency in comprehending world knowledge.\n\nWhile processing instructional data, the model M is presented with the dataset  where each tuple consists of an instruction c, an input prompt x i , and an expected output y c i .For instance, c might be \"Choose the best answer from A, B, C, and D (with options given).\",x could be \"The capital city of Japan is\", and y c would be \"D\", which aligns with the answer \"Tokyo\".The model is supposed to generate y c that accurately responds to the instruction c with the context of x, i.e., maximize P (y c |c, x, M ), which is termed as the Instruction Probability.\n\nIn this paper, when discussing catastrophic forgetting of a task, we consider alterations in both Knowledge and Instruction Probabilities.",
            "score": 0.6244187926807031,
            "section_title": "Catastrophic Forgetting in LLMs",
            "char_start_offset": 4965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 494
                },
                {
                    "start": 496,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 863
                },
                {
                    "start": 863,
                    "end": 944
                },
                {
                    "start": 944,
                    "end": 1069
                },
                {
                    "start": 1069,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1416
                },
                {
                    "start": 1416,
                    "end": 1508
                },
                {
                    "start": 1508,
                    "end": 1610
                },
                {
                    "start": 1610,
                    "end": 1798
                },
                {
                    "start": 1800,
                    "end": 1938
                }
            ],
            "ref_mentions": [
                {
                    "start": 605,
                    "end": 625,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 625,
                    "end": 646,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "270620654",
            "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
            "text": "There has been a growing need for continued fine-tuning of LLMs to mitigate harmful behaviors, update outdated knowledge, and adapt to unseen tasks and domains. Although fine-tuning allows efficient and incremental adaptation of models, models may suffer from catastrophic forgetting (Mc-Closkey & Cohen, 1989;Goodfellow et al., 2014) of upstream knowledge learned in the pre-training or instruction-tuning phase, causing unintended prediction changes over known information. This is problematic for the performance and reliability of online deployed LLMs, limiting the feasibility of continual fine-tuning in practice (Raffel, 2023;Shi et al., 2024). \n\nExisting works demonstrate that replaying or mixing in past examples are effective and scalable approaches to mitigate LLM forgetting (Scialom et al., 2022;Roth et al., 2024;Li et al., 2024b;Ibrahim et al., 2024;Ye et al., 2024). These approaches, however, often rely on random sampling of past examples; knowing what models forget after fine-tuning allows more efficient and targeted mitigation of forgetting -e.g., by prioritizing the replay of more forgotten examples (Toneva et al., 2019;Aljundi et al., 2019a). In this paper, we explore how forgetting caused by unseen tasks can be efficiently predicted, and more specifically, from the forgetting that occurred while learning other tasks. The complexity of associations between learned tasks and forgotten examples plays an important role in predictability; Figure 1 (a) illustrates a hypothetical scenario where certain upstream examples suffer more forgetting regardless of the learned tasks, making forgetting easily predictable; in contrast, (b) exemplifies upstream example forgetting that is highly dependent on the learned tasks. Existing theoretical and empirical study on the associations between learned and forgotten tasks focus on shallower models (Lee et al., 2021;Goldfarb et al., 2024;Ramasesh et al., 2021); the problem is under-explored for LLM forgetting or in an example level.",
            "score": 0.6209784992221707,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1169
                },
                {
                    "start": 1170,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 310,
                    "end": 334,
                    "matchedPaperCorpusId": "12730344"
                },
                {
                    "start": 619,
                    "end": 633,
                    "matchedPaperCorpusId": "253114949"
                },
                {
                    "start": 633,
                    "end": 650,
                    "matchedPaperCorpusId": "269362836"
                },
                {
                    "start": 788,
                    "end": 810,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 845,
                    "end": 866,
                    "matchedPaperCorpusId": "268379604"
                },
                {
                    "start": 1125,
                    "end": 1146,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1146,
                    "end": 1168,
                    "matchedPaperCorpusId": "199552250"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5888671875
        },
        {
            "corpus_id": "273662392",
            "title": "UFT: Unifying Fine-Tuning of SFT and RLHF/DPO/UNA through a Generalized Implicit Reward Function",
            "text": "By pretraining on trillions of tokens, an LLM gains the capability of text generation. However, to enhance its utility and reduce potential harm, SFT and alignment are applied sequentially to the pretrained model. Due to the differing nature and objective functions of SFT and alignment, catastrophic forgetting has become a significant issue. To address this, we introduce Unified Fine-Tuning (UFT), which integrates SFT and alignment into a single training stage using the same objective and loss functions through an implicit reward function. Our experimental results demonstrate that UFT outperforms SFT on instruction-tuning data alone. Moreover, when combining instruction-tuning data with alignment data, UFT effectively prevents catastrophic forgetting across these two stages and shows a clear advantage over sequentially applying SFT and alignment. This is evident in the significant improvements observed in the \\textbf{ifeval} task for instruction-following and the \\textbf{truthful-qa} task for factuality. The proposed general fine-tuning framework UFT establishes an effective and efficient pretraining-UFT paradigm for LLM training.",
            "score": 0.6201420131929802,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5283203125
        },
        {
            "corpus_id": "275820535",
            "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
            "text": "This paper addresses the critical issue of catastrophic forgetting in large language models (LLMs) during domainspecific fine-tuning. We propose a novel fine-tuning framework that preserves general capabilities while enabling efficient adaptation to new domains, minimizing knowledge loss in tasks outside the fine-tuned domain. Additionally, we introduce a layer-wise coefficient to adjust the balance between regularization loss and cross-entropy loss dynamically. This adjustment accounts for the varying contributions of different layers to both generalization and domainspecific learning. Extensive experiments in scientific, physical, and medical tasks show that our framework effectively mitigates catastrophic forgetting while maintaining performance in domain-specific tasks.",
            "score": 0.6187777433861623,
            "section_title": "Conclusion",
            "char_start_offset": 24455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 784
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65380859375
        },
        {
            "corpus_id": "270068369",
            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
            "text": "Large Language Models (LLMs) have emerged as a powerful tool for natural language processing, demonstrating remarkable abilities in areas like text completion, summarization, and question answering [1]. One of their most intriguing capabilities is their potential to learn \"common sense\" -general knowledge about the world that allows them to reason and make inferences beyond the literal meaning of text. This has fueled excitement about the possibility of achieving zero-shot learning, where LLMs can solve unseen problems without any prior training on specific tasks [2]. \n\nHowever, a crucial distinction exists between generic public knowledge and the specific private knowledge required for most real-world use cases. While LLMs excel at generic text completion or chat-like interactions, practical applications often demand solving specific and repeatable downstream tasks within a particular domain [3]. This typically necessitates knowledge specific to a business or organization, such as understanding internal processes, up-to-date product details, or customer behavior. \n\nFine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]. As a final consideration, keeping LLMs constantly up-to-date using fine-tuning can be infeasible, especially for domains with frequently changing information like e-commerce product inventory, whereas it is easy to update a database in real-time from which information is retrieved. \n\nAs an alternative to fine-tuning, In-Context Learning (ICL) offers a promising approach for leveraging LLMs in scenarios with limited data. This approach exploits the demonstrated ability of LLMs for \"meta-learning\" -essentially, learning how to learn.",
            "score": 0.6172990573969164,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1080
                },
                {
                    "start": 1083,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 2052
                },
                {
                    "start": 2055,
                    "end": 2194
                },
                {
                    "start": 2195,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 198,
                    "end": 201,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 906,
                    "end": 909,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61376953125
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "(1) We introduce a new perspective on catastrophic forgetting by using Knowledge and Instruction Probability to evaluate how well LLMs retain taskspecific knowledge and follow instructions after tuning, showing that changes in instruction adherence mainly drive performance declines.(2) We are the first to interpret forgetting with the Instruction Vector framework, identifying inherent changes during fine-tuning.The findings indicate that fine-tuning generally introduces specialized reasoning patterns rather than removing existing skills.(3) We develop an IV-guided training approach that focuses on preserving and realigning the model's computational graph during fine-tuning.This significantly enhances the general and in-context learning capabilities across various datasets in continual learning.",
            "score": 0.6170135493866886,
            "section_title": "Introduction",
            "char_start_offset": 4125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 415
                },
                {
                    "start": 415,
                    "end": 543
                },
                {
                    "start": 543,
                    "end": 682
                },
                {
                    "start": 682,
                    "end": 805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60888671875
        },
        {
            "corpus_id": "274982256",
            "title": "Chained Tuning Leads to Biased Forgetting",
            "text": "Large language models (LLMs) are often fine-tuned for use on downstream tasks, though this can degrade capabilities learned during previous training. This phenomenon, often referred to as catastrophic forgetting, has important potential implications for the safety of deployed models. In this work, we first show that models trained on downstream tasks forget their safety tuning to a greater extent than models trained in the opposite order. Second, we show that forgetting disproportionately impacts safety information about certain groups. To quantify this phenomenon, we define a new metric we term biased forgetting. We conduct a systematic evaluation of the effects of task ordering on forgetting and apply mitigations that can help the model recover from the forgetting observed. We hope our findings can better inform methods for chaining the finetuning of LLMs in continual learning settings to enable training of safer and less toxic models.",
            "score": 0.6078527361454482,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78759765625
        },
        {
            "corpus_id": "276574830",
            "title": "Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models",
            "text": "Aligned LLMs mainly undergo a two-step procedure: initial pre-training on web-scale text corpora, followed by fine-tuning on diverse instructions to align with human intentions. They exhibit remarkable versatility, showcasing their ability to handle various real-world tasks, such as reasoning, common sense question-answering, and instruction following (Zhao et al., 2023;Achiam et al., 2023;Lu et al., 2023b). \n\nDespite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023c), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c). \n\nAs a relatively new problem, the CF in LLMs remains under-explored. We categorize the existing studies into regularization-based, weight-based, and architecture-based methods1 . Regularizationbased methods (Lin et al., 2023b;Smith et al., 2023) add extra terms into loss function to penalize parameter changes. Weight-based methods (Wortsman et al., 2022;Ke et al., 2022) introduce weight coefficients for parameters to regulate their updates. Architecture-based methods (Wang et al., 2023d;Razdaibiedina et al., 2023) design and exclusively fine-tune extra modules outside the original model.",
            "score": 0.6054206540964103,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 411
                },
                {
                    "start": 414,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1249
                },
                {
                    "start": 1250,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1775
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 410,
                    "matchedPaperCorpusId": "252668803"
                },
                {
                    "start": 546,
                    "end": 564,
                    "matchedPaperCorpusId": "265294420"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66357421875
        },
        {
            "corpus_id": "224704480",
            "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT",
            "text": "However, one thing that has been studied is the effect that models lose the ability to do their pre-training task when being fine-tuned. This sequential learning problem has also been referred to as catastrophic inference (McCloskey and Cohen 1989) or catastrophic forgetting (Zacarias and Alexandre 2018;Mosbach, Andriushchenko, and Klakow 2020). This is an ongoing problem as general AI models will need to be able to learn and perform multiple tasks without forgetting how to perform the former. Zacarias and Alexandre (2018) suggest slowing down learning of important weights for the initial task, whereas Mosbach, Andriushchenko, and Klakow (2020) investigated the role of catastrophic forgetting on fine-tuning stability, finding that the last few layers are usually replaced with task-specific knowledge. If and how factual knowledge is affected by catastrophic forgetting is largely unexplored. In this work, we will take a first step into understanding how fine-tuning tasks and data effect both the learning and forgetting of factual knowledge. \n\nThere has been recent work on forgetting in attention modules when dealing with long contexts (Child et al. 2019;Schlag, Irie, and Schmidhuber 2021). When the capacity is exhausted, previous information is dropped in favor of the more recent context. By learning which parts of the context are important and systematically expiring ones faster that are not, Sukhbaatar et al. (2021) reduced the necessary capacity to attend to extremely long sequences. The work in this article is different in that we use probes without contexts as we want to retrieve knowledge from the model's parametric memory. Therefore, we do not investigate the attention modules but the learned embeddings.",
            "score": 0.6051754797554411,
            "section_title": "Learning and forgetting",
            "char_start_offset": 12678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1206
                },
                {
                    "start": 1207,
                    "end": 1307
                },
                {
                    "start": 1308,
                    "end": 1509
                },
                {
                    "start": 1510,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 305,
                    "matchedPaperCorpusId": "3528793"
                },
                {
                    "start": 499,
                    "end": 528,
                    "matchedPaperCorpusId": "3528793"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "274982256",
            "title": "Chained Tuning Leads to Biased Forgetting",
            "text": "Based on these observations, we explored two approaches for mitigating biased forgetting: determining task ordering using first task loss curvature, and retraining using a small portion of the forgotten task. \n\nWe hope these findings provide empirical guidelines for optimizing the finetuning process of LLMs to mitigate biased forgetting, and encourage more careful investigation into the causes of biased forgetting in LLM training. gorize finetuning as being either capability tuning or safety tuning depending on its primary intended purpose. In capability tuning, the model is tuned to improve its performance on some pre-specified task, such as scientific reasoning or factual question answering. In safety tuning, the LLM is finetuned to generate responses that are deemed safer, less biased, less toxic, or generally less harmful. 1  To measure the disparity of forgetting, we define a new metric called biased forgetting which measures the difference between the average and group forgetting across demographics. We investigate the effect of biased forgetting in controlled settings, keeping all other things equal in order to isolate the contributions of each LLM training decision. A controlled setting will enable us to better understand which design decisions impact the amount of biased forgetting, and determine how best to target mitigations. In particular, we focus here on the order of finetuning, the methods for finetuning, and the effect of different learning rate and batch size on forgetting and biased forgetting. \n\nWith respect to tuning order, we adhere to the following approach to training LLMs, which involves two stages: (1) the pretraining stage where the model is trained to encode general-purpose representations via self-supervised learning on a large unlabeled text corpus, (2) the finetuning stage(s) where the model is trained on one or more smaller scale datasets in sequence, often with supervision to make the model more aligned to downstream tasks (via supervised finetuning) or human preferences (via reinforcement learning approaches). While safety tuning may be performed in-house 1 While there can be reasonable debate over what is and is not considered safer, biased, toxic or harmful, our goal is to demonstrate that the order of finetuning tasks when training LLMs negatively affects the forgetting of bias and safety tasks more than for capability tasks.",
            "score": 0.6015336808542089,
            "section_title": "Introduction",
            "char_start_offset": 2011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 211,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2403
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4541015625
        },
        {
            "corpus_id": "276647867",
            "title": "Continual Memorization of Factoids in Language Models",
            "text": "Ouyang et al. (2022) find that this alignment tax can be partly mitigated by mixing pre-training data into the alignment data, and Luo et al. (2023a) find that LMs forget less when the instruction-tuning data is more diverse. Kotha et al. (2024) find that fine-tuning LMs leads to bigger performance degradation on tasks that are more similar to the fine-tuning task (as measured by likelihood under the learned fine-tuning distribution). See Shi et al. (2024) and Wu et al. (2024) for more extensive surveys of continual learning in the context of LMs. \n\nFine-tuning on unfamiliar facts. Our work builds on several recent observations about the effect of fine-tuning an LM on unfamiliar facts. Kang et al. (2024)  Model editing and unlearning. Our work is also related to a line of research aimed at explicitly modifying facts that are encoded in an LLM-for example, to update information about entities to reflect changes in the world (e.g. Zhu et al., 2020;Mitchell et al., 2022;Meng et al., 2022;2023). Studies have shown that these methods can update individual facts, but do not lead to consistent changes about all of the implications of these updates (Zhong et al., 2023;Cohen et al., 2024). A related line of work has investigated whether specific information can be deliberately removed, or \"unlearned,\" from neural networks (e.g. Graves et al., 2021;Zhang et al., 2023). Our focus in this paper is on introducing new knowledge while retaining existing knowledge, rather than on modifying or erasing existing knowledge.",
            "score": 0.600304352562469,
            "section_title": "Related Work",
            "char_start_offset": 23093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1529
                }
            ],
            "ref_mentions": [
                {
                    "start": 226,
                    "end": 245,
                    "matchedPaperCorpusId": "262054014"
                },
                {
                    "start": 982,
                    "end": 1000,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1159,
                    "end": 1179,
                    "matchedPaperCorpusId": "258865984"
                },
                {
                    "start": 1179,
                    "end": 1198,
                    "matchedPaperCorpusId": "260356612"
                },
                {
                    "start": 1341,
                    "end": 1361,
                    "matchedPaperCorpusId": "224817947"
                },
                {
                    "start": 1361,
                    "end": 1380,
                    "matchedPaperCorpusId": "245502053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "267406164",
            "title": "Continual Learning for Large Language Models: A Survey",
            "text": "Task-incremental Continual Instruction Tuning (Taskincremental CIT) aims to continuously finetune LLMs on a sequence of task-specific instructions and acquire the ability to solve novel tasks. A straightforward solution is to continuously generate instruction-tuning data for new tasks and directly fine-tune LLMs on it [Wang et al., 2023b]. However, studies have shown that continuously finetuning LLMs on task-specific data would cause a catastrophic forgetting of the learned knowledge and problem-solving skills in previous tasks [Kotha et al., 2023]. TAPT [Gururangan et al., 2020] presents a simple data selection strategy that retrieves unlabeled text from the in-domain corpus, aligning it with the task distribution. This retrieved text is then utilized to finetune LLMs, preventing catastrophic forgetting and enhancing argument performance. To mitigate catastrophic forgetting, Contunual-T0 [Scialom et al., 2022] employs rehearsal with a memory buffer [Shin et al., 2017] to store previous tasks data and replay them during training. ConTinTin [Yin et al., 2022] presents InstructionSpeak, which includes two strategies that make full use of task instructions to improve forward-transfer and backward-transfer. The first strategy involves learning from negative outputs, while the second strategy focuses on revisiting instructions from previous tasks. RationaleCL [Xiong et al., 2023] conducts contrastive rationale replay to alleviate catastrophic forgetting. DynaInst [Mok et al., 2023] proposes a hybrid approach incorporating a Dynamic Instruction Replay and a local minima-inducing regularizer. These two components enhance the general-izability of LLMs and decrease memory and computation usage in the replay module. Unlike previous replay-based or regularization-based methods, SLM [Anonymous, 2024b] incorporates vector space retrieval into the language model, which aids in achieving scalable knowledge expansion and management. This enables LLMs' quick adaptation to novel tasks without compromising performance caused by catastrophic forgetting. \n\nLLMs with billions of parameters introduce a huge computational burden for conducting continual learning.",
            "score": 0.5988295592277236,
            "section_title": "Task-incremental CIT",
            "char_start_offset": 10746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1612
                },
                {
                    "start": 1613,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2069
                },
                {
                    "start": 2072,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "265294410"
                },
                {
                    "start": 902,
                    "end": 924,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 964,
                    "end": 983,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 1377,
                    "end": 1397,
                    "matchedPaperCorpusId": "263835243"
                },
                {
                    "start": 1483,
                    "end": 1501,
                    "matchedPaperCorpusId": "263671523"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41748046875
        },
        {
            "corpus_id": "276421775",
            "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
            "text": "Yet, these methods require full-model fine-tuning, which incurs substantial computational costs, risks catastrophic forgetting, and limits their transferability across tasks. \n\nWe empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024). Thus, the methodologies of Coconut, which is based on GPT-2 (Radford et al., 2019), and CCoT, which is built upon LLaMA2-7B (Touvron et al., 2023), may not be directly applicable to more recent models such as LLaMA3.1-8B. Therefore, it is crucial to explore alternative methodologies that mitigate catastrophic forgetting while effectively leveraging continuous reasoning techniques in large-scale, instruction-tuned models, which is the main research question of this work. \n\nTo mitigate catastrophic forgetting, a straightforward approach is to freeze the backbone LLM and instead optimize an external model for reasoning. Inspired by prompt tuning (Lester et al., 2021) and speculative decoding (Leviathan et al., 2023), we propose an approach that utilizes an auxiliary small assistant model to generate a sequence of \"thought\" tokens conditioned on a task instruction followed by a specific instance (Li et al., 2023;Shao et al., 2023). These tokens serve as instancespecific prompts to boost LLM's inference. Such an auxiliary prompting mechanism dynamically adapts to different reasoning tasks, thereby improving generalization while preserving the pre-trained knowledge of the LLM. \n\nTo facilitate reasoning in a continuous space, we use soft thought tokens (i.e., the last-layer hidden states from the small assistant model before mapping to the vocabulary space) instead of discrete tokens. This ensures reasoning remains within the continuous latent space. However, a representational gap between the assistant model and the LLM may hinder effective knowledge transfer.",
            "score": 0.5978028227975836,
            "section_title": "Introduction",
            "char_start_offset": 1938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 177,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 579
                },
                {
                    "start": 580,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1769
                },
                {
                    "start": 1772,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2047
                },
                {
                    "start": 2048,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 640,
                    "end": 662,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1231,
                    "end": 1252,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 1278,
                    "end": 1302,
                    "matchedPaperCorpusId": "257079124"
                },
                {
                    "start": 1502,
                    "end": 1520,
                    "matchedPaperCorpusId": "256459681"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74072265625
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "We also list the results from Llama-2-Chat 70B (row 0) and observe that despite of its strong instruction following capability, it tends to output many more incorrect facts.This results demonstrate that standard alignment, even on proprietary commercial data, may encourage LLMs to hallucinate.In contrast, our factuality-aware alignment guides LLMs to output more factual responses without degradation in their general instruction following capabilities.It is worth noting that SFT fact and DPO fact are similar to SFT and DPO fine-tuning proposed by Tian et al. (2024), which improve LLMs' factuality but degrade instruction following capability.",
            "score": 0.5975120058617782,
            "section_title": "Comparisons of SFT",
            "char_start_offset": 21846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 294
                },
                {
                    "start": 294,
                    "end": 455
                },
                {
                    "start": 455,
                    "end": 648
                }
            ],
            "ref_mentions": [
                {
                    "start": 552,
                    "end": 570,
                    "matchedPaperCorpusId": "265158181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1680908203125
        },
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "Large language models have emerged as powerful tools for general intelligence, showcasing advanced natural language processing capabilities that find applications across diverse domains. Despite their impressive performance, recent studies have highlighted the potential for significant enhancements in LLMs' task-specific performance through fine-tuning strategies like Reinforcement Learning with Human Feedback (RLHF), supervised fine-tuning (SFT), and Quantized Low-Rank Adapters (Q-LoRA) method. However, previous works have shown that while fine-tuning offers significant performance gains, it also leads to challenges such as catastrophic forgetting and privacy and safety risks. To this end, there has been little to no work in \\textit{understanding the impact of fine-tuning on the reasoning capabilities of LLMs}. Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes.",
            "score": 0.5967443267920667,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Large language models (LLMs) have demonstrated impressive general capabilities in tackling a wide range of tasks (Bubeck et al., 2023). However, when it comes to real-world applications, users often find that certain specific abilities need enhancement. In such cases, relevant task-specific data are adopted to fine-tune the model in instructions to improve its performance in particular tasks (Touvron et al., 2023;Scialom et al., 2022). However, the widely used LLMs such as LLAMA-Chat (Touvron et al., 2023), ChatGPT1 , and Claude-22 have been fine-tuned, and the continual instruction tuning in specific tasks may affect the general knowledge stored in LLMs since the original training data are inaccessible. The study of continual instruction tuning is crucial for the practical application of LLMs. For example, in Table 1, an LLM is first fine-tuned for the task of title generation and then learns to generate an explanation. This scenario is related to the continual learning paradigm in machine learning, where a model is trained on a sequence of tasks (Lopez-Paz & Ranzato, 2017;Wu et al., 2022;Ke & Liu, 2022). A significant challenge in this paradigm is catastrophic forgetting (CF), in which a model forgets previously learned knowledge due to parameter updates during the learning of new tasks, leading to decreased performance on earlier tasks (Li & Hoiem, 2017;Lopez-Paz & Ranzato, 2017). \n\nInstruction: Make a title for this article with 'computer': japan's NEC corp. and UNK computer corp. of the United States said Wednesday they had agreed to join forces ... Output: NEC UNK in computer sales tie-up. Instruction: Explain why the two following sentences are unrelated: Sentence 1: 'This church choir sings to the masses as they sing joyous songs from the book at a church'; Sentence 2: 'The church has cracks in the ceiling.' Output: There is no indication that there are cracks in the ceiling of the church. \n\nTable 1: Fine-tuning examples in an instruction form. Scialom et al. (2022) investigate the CF issues of T0 models during continual instruction tuning.",
            "score": 0.5961608561003537,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1406
                },
                {
                    "start": 1409,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 1930
                },
                {
                    "start": 1933,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2084
                }
            ],
            "ref_mentions": [
                {
                    "start": 417,
                    "end": 438,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 1091,
                    "end": 1107,
                    "matchedPaperCorpusId": "247717213"
                },
                {
                    "start": 1361,
                    "end": 1379,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6748046875
        },
        {
            "corpus_id": "266844262",
            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
            "text": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.",
            "score": 0.5959711563541887,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "278368492",
            "title": "OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models",
            "text": "First, Shi et al. (2024) shows that unlearned data can often be recovered via membership inference attacks (MIAs), suggesting that memorized information is not fully erased. Second, balancing effective unlearning with performance preservation on retained data remains challenging. Techniques like gradient descent or KL-divergence on retained data often fail to maintain model utility in real-world scenarios, particularly due to the impracticality of defining clear retain set boundaries without access to proprietary training corpora. Finally, existing evaluations are often insufficient, lacking comprehensiveness and reliability in verifying whether the forget set has been fully removed and whether model performance remains intact (Liu et al., 2024b). \n\nTo address the challenges, we propose OBLIVI-ATE, a robust and practical LLM unlearning framework that effectively removes target data while preserving model performance (e.g., on various downstream tasks) and fluency-defined as the ability to generate coherent and precise responses-on the re-Figure 1: Overview of OBLIVIATE, a robust and practical unlearning framework for LLMs tain set. Figure 1 outlines OBLIVIATE with three critical loss functions: masked loss for the forget set, and distillation and world fact losses for the retain set. Additionally, we utilize low-rank adapters (LoRA) (Hu et al., 2022) for fine-tuning efficiency. \n\nTo optimize forget quality for strict regulatory compliance (Ginart et al., 2019), we introduce a masked loss that enforces zero-generation probability for targeted content, facilitating \"aggressive\" forgetting, inspired by multimodal unlearning (Li et al., 2024a). However, this aggressive approach can degrade model performance and fluency on the retain set, often producing incoherent outputs (Thaker et al., 2024). To mitigate potential catastrophic forgetting, we incorporate two additional losses: distillation and world fact. The distillation loss aligns the model with teacher models trained on related documents, preserving performance and fluency on the retain set. The world fact loss uses encyclopedic data (e.g., WikiText (Merity et al., 2017)) to maintain general factual knowledge.",
            "score": 0.5958756156130632,
            "section_title": "Introduction",
            "char_start_offset": 1765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2199
                }
            ],
            "ref_mentions": [
                {
                    "start": 7,
                    "end": 24,
                    "matchedPaperCorpusId": "10488675"
                },
                {
                    "start": 1463,
                    "end": 1484,
                    "matchedPaperCorpusId": "195886255"
                },
                {
                    "start": 1649,
                    "end": 1667,
                    "matchedPaperCorpusId": "269930051"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.176513671875
        },
        {
            "corpus_id": "262054014",
            "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
            "text": "To demonstrate a more natural instance of catastrophic forgetting, we consider what happens to a language model after we fine-tune on code. If we refer to L REASON as the capability that solves a natural language reasoning task while L CODE does the same for coding, we can idealize the model's completion as \n\nConjugate prompting for MNLI. To test forgetting, we use the XNLI benchmark (Conneau et al., 2018), a multi-lingual version of MNLI (Williams et al., 2018) from GLUE (Wang et al., 2019) that tests sentence entailment (evaluation details in Appendix E.1, examples in Appendix E.2). When we compare LLaMa-2 (Touvron et al., 2023b) against its English code fine-tuned variant Code LLaMa (Rozi\u00e8re et al., 2023), the model gets lower performance on English prompts, performing 8.36% worse (Table 2). However, for French, Spanish, and German inputs, the accuracy changes by less than 2% in magnitude. In fact, the accuracy of Code LLaMa on Spanish and French XNLI slightly increases after fine-tuning, possibly from increased reasoning capabilities associated with code training (Fu & Khot, 2022;Ma et al., 2023) combined with better task inference. For this reasoning task, it is preferable to prompt the fine-tuned model in Spanish instead of English.",
            "score": 0.5937412784661846,
            "section_title": "EFFECTS OF CODE FINE-TUNING ON NATURAL LANGUAGE REASONING",
            "char_start_offset": 24377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 308
                },
                {
                    "start": 311,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1258
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 466,
                    "matchedPaperCorpusId": "3432876"
                },
                {
                    "start": 477,
                    "end": 496,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2393798828125
        },
        {
            "corpus_id": "270067818",
            "title": "Online Merging Optimizers for Boosting Rewards and Mitigating Tax in Alignment",
            "text": "The Alignment Tax can be viewed as a form of catastrophic forgetting in CL.Catastrophic forgetting refers to a dramatic degradation of performance on old tasks when learning new tasks (Goodfellow et al., 2013;Kirkpatrick et al., 2017;Wang et al., 2024).For example, constraining the shift in output space between reference and policy models (Ouyang et al., 2022;Rafailov et al., 2023)) is a common strategy employed in continual learning (Li and Hoiem, 2017;Rebuffi et al., 2017;Wang et al., 2024).However, traditional methods mitigating forgetting issues in CL usually require data for previous training tasks for experience replay, which may be impractical for current open-or closed-source LLMs, whose alignment data is in-house and already multi-task.Although we mainly experiment with our online merging optimizers on LLM alignment, they can be applied to a wider range of CL scenarios in the future.\n\nRobust Fine-tuning.Overcoming alignment tax can be seen from another perspective, similar to overcoming generalization degradation, involving the research area of robust fine-tuning.Nonetheless, robust fine-tuning methods often necessitate supplementary forward and backward computations, rendering them inefficient for large-scale models (Liang et al., 2021;Aghajanyan et al., 2021  ($)   Merged Delta (\")\n\nFigure 1: An illustrastion of RLHF with online merging optimizers described in \u00a74.In each RLHF iteration, we first obtain the update weight \u2206\u03b8 (t) , and then sparcify it and make a consensus with the delta parameters of the reference model.We use this merged delta as the update of the policy model in this iteration.We also compare online merging with offline merging, shown in the lower part of the figure and further introduced in \u00a73.et al., 2020;Yuan et al., 2023a).Some works aim to enhance robustness by eliminating invaluable parameter updates during training to achieve robust generalization (Lee et al., 2020;Jiang et al., 2022;Xu et al., 2021).",
            "score": 0.5894212425479213,
            "section_title": "Continual Learning (CL).",
            "char_start_offset": 5068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 75,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 498
                },
                {
                    "start": 498,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 905
                },
                {
                    "start": 907,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1089
                },
                {
                    "start": 1089,
                    "end": 1313
                },
                {
                    "start": 1315,
                    "end": 1397
                },
                {
                    "start": 1397,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1752
                },
                {
                    "start": 1752,
                    "end": 1785
                },
                {
                    "start": 1785,
                    "end": 1969
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 209,
                    "matchedPaperCorpusId": "12730344"
                },
                {
                    "start": 234,
                    "end": 252,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 341,
                    "end": 362,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 362,
                    "end": 384,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 438,
                    "end": 458,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "206596260"
                },
                {
                    "start": 479,
                    "end": 497,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 1266,
                    "end": 1289,
                    "matchedPaperCorpusId": "221083147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3525390625
        },
        {
            "corpus_id": "276409016",
            "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
            "text": "Catastrophic forgetting in fine-tuned language models. Fine-tuning foundational LLMs (Touvron et al., 2023a;b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2022;Zhang et al., 2024a;b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024). However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Luo et al., 2024;Kotha et al., 2024;Wu et al., 2024b). In context of LLMs, existing approaches towards mitigating this issue can mostly be categorized into three types: regularizing the update of model parameters (Huang et al., 2021;Cha et al., 2021), replaying previous or self-synthesized data (Scialom et al., 2022;Huang et al., 2024a) and resisting interference via parameter-efficient fine-tuning (Razdaibiedina et al., 2023;Wang et al., 2023b). In this work, we aims to characterize CF in LLMs through the function vector, concluding that such forgetting primarily stems from biases in function activation rather than the overwriting of task processing functions. \n\nTo this end, We propose function vector guided training, a regularization-based method to protect task activation from being improperly destroyed during fine-tuning to cure the forgetting issue. \n\nMechanistic analysis to fine-tuning.",
            "score": 0.5887120215644982,
            "section_title": "RELATED WORK",
            "char_start_offset": 31008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1362
                },
                {
                    "start": 1365,
                    "end": 1401
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 215,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 470,
                    "end": 496,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 513,
                    "end": 532,
                    "matchedPaperCorpusId": "262054014"
                },
                {
                    "start": 709,
                    "end": 729,
                    "matchedPaperCorpusId": "233210110"
                },
                {
                    "start": 729,
                    "end": 746,
                    "matchedPaperCorpusId": "219636462"
                },
                {
                    "start": 792,
                    "end": 814,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 898,
                    "end": 926,
                    "matchedPaperCorpusId": "256390383"
                },
                {
                    "start": 926,
                    "end": 945,
                    "matchedPaperCorpusId": "264426441"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5615234375
        },
        {
            "corpus_id": "273507804",
            "title": "Exploring Forgetting in Large Language Model Pre-Training",
            "text": "Catastrophic forgetting remains a formidable obstacle to building an omniscient model in large language models (LLMs). Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on forgetting during pre-training. We systematically explored the existence and measurement of forgetting in pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention. Based on our revised assessment of forgetting metrics, we explored low-cost, straightforward methods to mitigate forgetting during the pre-training phase. Further, we carefully analyzed the learning curves, offering insights into the dynamics of forgetting. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs.",
            "score": 0.5879835753126598,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.456787109375
        },
        {
            "corpus_id": "277501673",
            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
            "text": "We employed a Continual Instruction Fine-tuning (Huang et al., 2024) approach, shown in the Fig- ure 1, sequentially adapting the base model M 0 on tasks {T 1 , T 2 , . . . , T n } from the GLUE benchmark (Wang et al., 2018). The goal was to evaluate how well the model M i , fine-tuned on task T i , retained knowledge from previous tasks {T 1 , . . . , T i\u22121 }. \n\nThe methodology comprised two main stages. First, we prepared the dataset for each task T i using prompt engineering (PE) (Chen et al., 2024). Let X be the original dataset for T i , transformed into a structured prompt dataset X \u2032 as follows: \n\nwhere PE(\u2022) represents the prompt engineering function. Prompts were designed to guide the model to perform task-specific instructions and the exact prompts and their expected outputs are shown in Table 1 4 Evaluation and Measurement of Catastrophic Forgetting and Learning \n\nAfter each fine-tuning episode, model M i was evaluated on all previous tasks {T 1 , T 2 , . . . , T i } to assess catastrophic forgetting and learning. Accuracy was used as the evaluation metric, comparing postfine-tuning performance to the base performance to detect any degradation or improvement. Forgetting was quantified as the difference between the maximum accuracy on each task during fine-tuning and the final accuracy: \n\nwhere a k,t is the accuracy after fine-tuning on task k and a T,t is the final accuracy.",
            "score": 0.5867447305186063,
            "section_title": "Methodology",
            "char_start_offset": 4443,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 363
                },
                {
                    "start": 366,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 609
                },
                {
                    "start": 612,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 885
                },
                {
                    "start": 888,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1408
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "Upon preparing the instruction dataset, it is partitioned into training, validation, and test splits.During fine-tuning, prompts from the training set are used to generate completions, which are compared against the expected responses.The model outputs a probability distribution across tokens, and the cross-entropy loss function calculates the discrepancy between the generated and expected distributions.This loss is then used to update the model's weights through backpropagation [14].The fine-tuning process involves multiple batches and epochs, continuously updating the model's weights to enhance performance.Evaluation using holdout validation and test datasets yields metrics such as validation accuracy and test accuracy, providing a measure of the model's performance.The result of fine-tuning is an improved version of the base model, referred to as an instruct model, optimized for specific tasks.Instruction fine-tuning is the predominant method for fine-tuning LLMs today, typically implied when discussing fine-tuning in this context.Large Language Models (LLMs) are renowned for their versatility in performing various language tasks within a single model.However, specific applications often necessitate excelling in a single task.Fine-tuning a pre-trained model on a particular task, such as summarization, can significantly enhance performance, achievable with relatively small datasets, often just 500-1,000 examples, compared to the billions of texts used during pre-training [31,3].Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27].To mitigate catastrophic forgetting, one must assess its impact on their specific use case.If the primary requirement is reliable performance on the fine-tuned task, the loss of multitask capabilities may be acceptable.However, if maintaining generalized capabilities is essential, multitask fine-tuning offers an alternative.",
            "score": 0.5847418859468139,
            "section_title": "Improving Performance of Large Language Models through Fine-Tuning",
            "char_start_offset": 31834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 235
                },
                {
                    "start": 235,
                    "end": 407
                },
                {
                    "start": 407,
                    "end": 489
                },
                {
                    "start": 489,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1173
                },
                {
                    "start": 1173,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1505
                },
                {
                    "start": 1505,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1810
                },
                {
                    "start": 1810,
                    "end": 1967
                },
                {
                    "start": 1967,
                    "end": 2058
                },
                {
                    "start": 2058,
                    "end": 2186
                },
                {
                    "start": 2186,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 1498,
                    "end": 1502,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 1502,
                    "end": 1504,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "266191340",
            "title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models",
            "text": "Lower Bound? \n\nSequential fine-tuning (SEQ) has long been regarded as the lower bound of IL. In this subsection, we revisit SEQ from the probing perspective, and we find that SEQ is severely underestimated when using PLMs for IL. \n\nThe backbone was small and randomly initialized in early studies exploring IL (Kirkpatrick et al., 2017;French, 1999;McCloskey and Cohen, 1989). They find that SEQ usually results in models forgetting all previous knowledge when learning new tasks. Recent IL studies in NLP (Razdaibiedina et al., 2023;Zheng et al., 2022;Huang et al., 2021;Sun et al., 2019) also observe that SEQ leads to worse performance. However, in the era of PLMs, fine-tuning has proven an effective technique for adapting PLMs to different domains or downstream tasks (Aghajanyan et al., 2020;Devlin et al., 2019;Radford et al., 2018). If fine-tuning really causes PLMs to forget nearly all previous knowledge in IL, it should also cause PLMs to forget all pre-trained knowledge when adapting to new tasks. Obviously, this assumption is not true since fine-tuning is still an effective method for PLMs (OpenAI, 2023). \n\nThe observed and probing performance on classincremental intent classification with generative models are summarized in Fig. 3. The results on other IL settings, downstream tasks and back- 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14  bones are in Appendix D.2. Fig. 3a shows that the observed performance drops dramatically from approximately 98% to 10% as more new tasks are learned, in line with what we understand of catastrophic forgetting. However, Fig. 3c describes an entirely different phenomenon. The PLMs achieve high probing performance after learning the first task. And the linear probing performance has barely decreased since the second task. In other words, PLMs preserve the knowledge to classify all 15 tasks even when adapting to only new tasks sequentially. This phenomenon is contradictory to what we know about catastrophic forgetting and SEQ.",
            "score": 0.5833691506595566,
            "section_title": "Is Sequential Fine-tuning Really the",
            "char_start_offset": 7633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 15,
                    "end": 92
                },
                {
                    "start": 93,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1012
                },
                {
                    "start": 1013,
                    "end": 1123
                },
                {
                    "start": 1126,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1622
                },
                {
                    "start": 1623,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 1982
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 375,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 506,
                    "end": 534,
                    "matchedPaperCorpusId": "256390383"
                },
                {
                    "start": 553,
                    "end": 572,
                    "matchedPaperCorpusId": "233210110"
                },
                {
                    "start": 572,
                    "end": 589,
                    "matchedPaperCorpusId": "209475822"
                },
                {
                    "start": 774,
                    "end": 799,
                    "matchedPaperCorpusId": "221083147"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303955078125
        },
        {
            "corpus_id": "276775615",
            "title": "Superficial Self-Improved Reasoners Benefit from Model Merging",
            "text": "Catastrophic forgetting is a related but distinct phenomenon compared to superficial self-improved reasoners. Specifically, catastrophic forgetting refers to the loss of previously acquired knowledge when deep learning models are trained on new data. This issue occurs because model parameters are optimized based on the most recent training data, causing earlier learned representations to be dramatically overwritten. While both catastrophic forgetting and superficial self-improved reasoners result in degraded performance due to further fine-tuning, their effects differ. After fine-tuning on new data, catastrophic forgetting results in a performance loss on previously learned tasks, whereas superficial selfimproved reasoners result in diminished generalization capabilities on out-of-domain (OOD) tasks. This discrepancy arises because in catastrophic forgetting, fine-tuning on data for new tasks causes the model to lose knowledge from previous tasks. In contrast, superficial self-improved reasoners do not lead to forgetting too much past information but instead shift towards overfitting due to potentially biased knowledge, which may self-enhance along with the iteration of synthesizing new data and fine-tuning on it.",
            "score": 0.5821317080216747,
            "section_title": "E The Connection to Catastrophic Forgetting",
            "char_start_offset": 26195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426513671875
        },
        {
            "corpus_id": "266374841",
            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
            "text": "Despite the effectiveness of safety finetuning in recovering safety, it remains unclear whether important downstream data unrelated to safety will also be forgotten in LLMs during safety finetuning, potentially harming the downstream task performance. This section studies how previously learned data from different sources during downstream finetuning will be forgotten during sequentially finetuning language models at various scales on safe data. \n\nAs is shown in Figure 3, during safety finetuning, all types of previously learned examples in the noisy downstream dataset will experience forgetting more or less including important downstream task data (i.e., highlighted in blue in Figure 3). This may lead to the forgetting of factual knowledge instilled into the pre-trained LMs through customized finetuning (see an example in Figure 11 of Appendix). In light of this, there is a need for an alternative method that can recover the model's safety without compromising learning new downstream data. Discrepancies in forgetting. Our results unveil the discrepancies in forgetting samples from different sources. From Figure 3, the previously acquired unsafe examples in D noisy are observed to experience a considerably more rapid and pronounced rate of forgetting compared to other segments of D noisy . This effect is particularly noticeable when contrasting with the data that is safety-irrelevant, i.e., D task . This same conspicuous discrepancy in forgetting behavior persists in all three aspects of safety we study, underscoring the consistency of our findings. However, when the safe examples in safety finetuning session are sampled from a different category of safety from the unsafe examples in noisy data, discrepancies can no longer be observed and unsafe examples and downstream task examples will experience forgetting at a similar pace (see more detailed discussion in Appendix E). \n\nDiscrepancies in forgetting emerge when LMs are large enough. We then investigate whether discrepancies in forgetting consistently exist in LMs of different sizes, or only in large-scale models.",
            "score": 0.5806310817950185,
            "section_title": "Forgetting during Safety Finetuning",
            "char_start_offset": 15698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 449
                },
                {
                    "start": 452,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1310
                },
                {
                    "start": 1311,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1575
                },
                {
                    "start": 1576,
                    "end": 1904
                },
                {
                    "start": 1907,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287841796875
        },
        {
            "corpus_id": "268513020",
            "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
            "text": "Recently, LLMs have demonstrated impressive performance across various NLP tasks.After being unsupervised pre-trained on large-scale raw text, LLMs are further trained via instruction tuning to generate appropriate outputs based on the given input instructions (Sanh et al., 2021;Mishra et al., 2021;Chung et al., 2022).Prior works supervised fine-tuned (SFT) LLMs with datasets consisting of {instruction, input, output} pairs, as shown in Table 1, and evaluated on another set of held-out tasks (Wei et al., 2021;Longpre et al., 2023).They demonstrate that the performance of unseen tasks can be improved with more tasks and templates.To improve the diversity and complexity of instruction, a broad range of open-source instruction tuning datasets are proposed.Some are gathered through crowd-sourcing (Conover et al., 2023;Zhou et al., 2023) while others are distilled from strong proprietary models (Wang et al., 2022a;Peng et al., 2023;Taori et al., 2023).\n\nWith the help of various low-cost methods of constructing high-quality templates, instruction datasets can expand easily over time as new tasks appear.When the data scale grows dynamically, we can easily obtain sufficient task-specific data.Considering this, rather than evaluating zero-shot ability on held-out tasks, we are more concerned about adapting an instruction-tuned model to a new task without suffering from catastrophic forgetting.In this work, we fine-tune LLMs in a continuous manner and analyze their performance on previous tasks, aiming to explore the forgetting issue in a changeable environment.",
            "score": 0.5806023826320796,
            "section_title": "Instruction Tuning",
            "char_start_offset": 3995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 320
                },
                {
                    "start": 320,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 637
                },
                {
                    "start": 637,
                    "end": 763
                },
                {
                    "start": 763,
                    "end": 961
                },
                {
                    "start": 963,
                    "end": 1114
                },
                {
                    "start": 1114,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1578
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15771484375
        },
        {
            "corpus_id": "271769753",
            "title": "Knowledge Acquisition through Continued Pretraining is Difficult: A Case Study on r/AskHistorians",
            "text": "As described above, there are various techniques for modifying LLMs and instilling knowledge, with each technique having its own advantages and disadvantages. Yu et al. (2020) distinguish between internal and external knowledge sources for LLMs: \n\nRegarding internal knowledge, Kandpal et al. (2023) argue that unsupervised pretraining and SFT are good at making LLMs learn broad world knowledge and specific tasks, respectively, but fail at injecting specific facts and niche knowledge they consider long-tail knowledge. Other research indicates that fine-tuning on specific data can lead to catastrophic forgetting on previously learned tasks (Kirkpatrick et al., 2016;Kemker et al., 2018), while the concept of continual learning advocates approaches that aim to prevent this (Zhou et al., 2024;Scialom et al., 2022). In contrast, Liu et al. (2023a) present a model that is specifically fine-tuned on a dataset related to chip-design tasks -the authors show that a model specifically pretrained on a highly domainspecific dataset yields improved performance on related tasks. As an alternative, Jiang et al. (2024) propose pre-instruction tuning to inject knowledge before fine-tuning on documents, which seems to improve on this task, but is more difficult to implement correctly. Alignment techniques such as DPO (while more efficient than PPO; Schulman et al. 2017) are costly approaches that focus on nuanced alignment of LLMs using a very small learning rate. Furthermore, very recent work indicates that using LoRA adapters for training reduces the learning and forgetting effects (Biderman et al., 2024). In our experiments, we focus on internal LLM knowledge and investigate how strongly these techniques can affect an LLM's knowledge when trained and evaluated in the historical domain. We disregard more complex specialized techniques such as knowledge editing, which aims to modify a model's parameters (Wang et al., 2023) or its outputs through a smaller language model (Liu et al., 2024) or a steering vector (Rimsky et al., 2023), due to their complexity and lack of support in common libraries such as PyTorch.",
            "score": 0.5797825496971662,
            "section_title": "Knowledge Injection",
            "char_start_offset": 6989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1614
                },
                {
                    "start": 1615,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 2128
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 175,
                    "matchedPaperCorpusId": "222272210"
                },
                {
                    "start": 278,
                    "end": 299,
                    "matchedPaperCorpusId": "253522998"
                },
                {
                    "start": 671,
                    "end": 691,
                    "matchedPaperCorpusId": "22910766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375732421875
        },
        {
            "corpus_id": "273163321",
            "title": "How much can we forget about Data Contamination?",
            "text": "Data Contamination. The GPT-3 paper (Brown et al., 2020) uses an n-gram-based approach to differentiate between \"clean\" and \"dirty\" benchmark questions. This approach has since been used in many LLM reports (Chowdhery et al., Touvron et al., 2023, 2023), including Llama 3 (Dubey et al., 2024), where it is estimated that there might be a performance gain of up to 8 and 14 percentage points on PiQA and HellaSwag, respectively. The GPT-4 technical report (Achiam et al., 2023) remarkably concluded that \"contamination overall has very little effect on the reported results\". This has since given rise to a literature that aims to detect (Oren et al., 2024), mitigate (Li et al., 2024), and estimate the effect of (Yang et al., Bordt et al., 2023Bordt et al., , 2024) ) data contamination under various assumptions, but crucially without access to the training data. This literature often challenges the conclusion that contamination overall has little effect in GPT-4 (Xu et al., 2024). \n\nForgetting. In machine learning, the term forgetting is frequently associated with \"catastrophic\" forgetting, where learning new tasks hurt the performance at previously solved tasks (Lopez-Paz and Ranzato, 2017). In the context of LLMs, catastrophic forgetting can occur during fine-tuning (Luo et al., 2023) or continual learning (Huang et al., 2024). In contrast, this paper studies forgetting as a potential \"natural\" phenomenon of learning (Toneva et al., 2019). Tirumala et al. [66] study forgetting in language modeling and find, similar to Toneva et al. [67], that forgetting can be exponentially slow. In contrast, Jagielski et al. [39] find that models empirically do forget examples over time. Pagliardini et al. [58] propose to add a second momentum term to the AdamW optimizer, and show that this slows down the forgetting of past gradients. \n\nData Attribution.",
            "score": 0.5794315136491269,
            "section_title": "Related Work",
            "char_start_offset": 5731,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 428
                },
                {
                    "start": 429,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1600
                },
                {
                    "start": 1601,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1844
                },
                {
                    "start": 1847,
                    "end": 1864
                }
            ],
            "ref_mentions": [
                {
                    "start": 36,
                    "end": 56,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 746,
                    "end": 769,
                    "matchedPaperCorpusId": "269009574"
                },
                {
                    "start": 1631,
                    "end": 1635,
                    "matchedPaperCorpusId": "250243645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1505126953125
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "We study and quantify the problem of forgetting when fine-tuning pre-trained large language models (LLMs) on a downstream task. We find that parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adapters (LoRA), still suffer from catastrophic forgetting. In particular, we identify a strong inverse linear relationship between the fine-tuning performance and the amount of forgetting when fine-tuning LLMs with LoRA. We further obtain precise scaling laws that show forgetting increases as a shifted power law in the number of parameters fine-tuned and the number of update steps. We also examine the impact of forgetting on knowledge, reasoning, and the safety guardrails trained into Llama 2 7B chat. Our study suggests that forgetting cannot be avoided through early stopping or by varying the number of parameters fine-tuned. We believe this opens up an important safety-critical direction for future research to evaluate and develop fine-tuning schemes which mitigate forgetting",
            "score": 0.5788587423326661,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79150390625
        },
        {
            "corpus_id": "273384438",
            "title": "CurieLM: Enhancing Large Language Models for Nuclear Domain Applications",
            "text": "In our experiments, we fine-tuned both Mistral-7B-Instruct and Dolphin-Mistral-7B-Instruct on the nuclear dataset presented in Section 4. The dataset was split into training and testing sets, with a sequence length of 2048 tokens (one token generally corresponds to \u00be a word). \n\nBoth experiments were conducted with the same hyperparameters for LoRA and training. Interestingly, we observed that both experiments followed the same pattern of training. Initially, the evaluation loss decreased rapidly, indicating effective learning. However, after a period of stability, the evaluation loss started to increase. \n\nTo prevent overfitting and catastrophic forgetting [23] (a common phenomenon in LLM fine-tuning where the model forgets previously learned information), we decided to stop training before the evaluation loss started to increase. This strategy ensured that the models did not lose their already developed capacities, such as reasoning. \n\nThese experiments underscore the potential of fine-tuning LLMs for domain-specific tasks, despite the challenges. The next section will delve into the evaluation of these models and the results obtained.",
            "score": 0.5783334087577121,
            "section_title": "Experiments",
            "char_start_offset": 16685,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 279,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 948
                },
                {
                    "start": 951,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.422607421875
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "The almost similar behavior observed in the multi-task fine-tuning of base and instruct models (using the same amount of data without incorporating generic instruction data) on seen tasks indicates that both models benefit nearly equally from fine-tuning on tasks present in the training corpus. However, our results on three unseen financial tasks, as shown in Table 3 and Figures 5 and 6, indicate that multi-task fine-tuned base models exhibit significantly greater performance declines compared to their instruct-tuned counterparts when generic instruction data is absent during fine-tuning. This highlights the risk of performance degradation for base models on tasks they haven't encountered, a trend consistent with prior research on catastrophic forgetting [46,65]. In contrast, instruct-tuned models retain stronger generalization capabilities, making them more robust in handling unseen tasks. \n\nFor example, on the Argument Unit Classification task, the Llama3-8B model's performance drops to 13.4%, while on the Causal Classification task, the Mistral-7B model experiences a decline to 31.8%. Notably, the Phi-3-mini model, despite its smaller size of 3.8 billion parameters, shows only a 2% performance drop, suggesting that it retains its general capabilities better than larger models after fine-tuning. These findings emphasize the effectiveness of instruct-tuned models in maintaining performance across unfamiliar tasks. \n\nOverall, while multi-task fine-tuning significantly improves performance on tasks within the training data, it can also increase the risk of overfitting and catastrophic forgetting, particularly when generic instruction data is not included in the training process. These results underscore the importance of diverse instruction data in enhancing a model's ability to generalize effectively across unseen tasks.",
            "score": 0.5768242832613089,
            "section_title": "Generalization Multi-Task Fine-Tuned model to Unseen Tasks",
            "char_start_offset": 34858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1438
                },
                {
                    "start": 1441,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.515625
        },
        {
            "corpus_id": "270560253",
            "title": "Self-training Large Language Models through Knowledge Detection",
            "text": "One natural concern regarding fine-tuning is the impact on out-of-distribution benchmarks. More specifically, we want to see if continual training on instances where the model is sufficiently knowledgeable, can induce catastrophic forgetting effects. We use G SF T as the baseline and compare the performance after DPO with and without filtering on the preference dataset. To do so, we conducted evaluations on two benchmarks, Open LLM leaderboard, and a dataset consisting of instructions filtered out from D * . The first benchmark assesses LLMs on commonsense reasoning, general knowledge, and sentence completion. The second set refers to the samples that are labeled as known and were thus left out in D * after filtering. Ideally, this experiment seeks to study if G SF T , after doing SFT on its own outputs, will encounter any deterioration in its knowledge after doing preference tuning on instances where it was deemed to be knowledgable.. Due to the high cost of evaluating the full dataset, we randomly sample 200 instances, similar to the primary test set. The known dataset is filtered using a value of 0.5 for \u03c4 K . RQ2 Effects of filtering on knowledge retention: Based on Table 1, we observe that performing knowledge filtering retains the performance of the model on out-of-distribution tasks. Performing preference tuning on the full dataset conversely suffers a performance degradation despite being exposed to a more diverse dataset. This is particularly true for TruthfulQA, which may be less surprising given the results in Figure 2. Likewise in Table 3, performing knowledge filtering is shown to suffer a lower losing rate as compared to without. This is surprising since the evaluation is conducted on samples where preference tuning was conducted in the case of w/o filtering. This finding supports our initial belief that over-training on known instances can have adverse effects on the model.",
            "score": 0.5754658404768935,
            "section_title": "Catastrophic Forgetting",
            "char_start_offset": 19889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 617
                },
                {
                    "start": 618,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1921
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49462890625
        },
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "We begin by discussing the prior works that study fine-tuning methods in LLMs. \n\nFine-Tuning LLMs and limitations: Various fine-tuning techniques have been proposed to more effectively adapt LLMs to new tasks. Among these techniques, the most widely used are LoRA and QLoRA, which improve efficiency by decomposing gradient matrices into low-rank matrices during fine-tuning. In this work, we use the QLoRA finetuning technique in conjunction with the supervised fine-tuning method (SFT) to efficiently fine-tune LLMs. Although fine-tuning is crucial for improving LLM performance in specific tasks, it can also lead to unintended side effects. For instance, Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks. Similarly, Singh et al. (2024) and Zeng et al. (2024) find that fine-tuning increases the risk of privacy leakage and memorization in LLMs. Moreover, fine-tuning can sometimes reverse previously established safety mechanisms, such as undoing learned toxicity filters (Kumar et al., 2024). Other studies, such as Navigli et al. (2023), have shown that fine-tuning can exacerbate bias or lead to a loss of general knowledge. Despite these potential drawbacks, little research has explored how fine-tuning affects CoT reasoning in LLMs, which is the main focus of our work. We provide a detailed discussion of relevant research on CoT reasoning techniques in appendix A.",
            "score": 0.5751567168248836,
            "section_title": "Related Work",
            "char_start_offset": 3808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 81,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 824
                },
                {
                    "start": 825,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1113
                },
                {
                    "start": 1114,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1395
                },
                {
                    "start": 1396,
                    "end": 1492
                }
            ],
            "ref_mentions": [
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "267149066"
                },
                {
                    "start": 1137,
                    "end": 1158,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81884765625
        },
        {
            "corpus_id": "266191340",
            "title": "Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models",
            "text": "Learning knowledge incrementally without much forgetting is an essential ability of human beings but still an unsolved challenge for neural networks in achieving human-level intelligence (French, 1999). Incrementally learning a sequence of tasks can be formulated into the paradigm of * *Corresponding author 1 https://github.com/zzz47zzz/pretrained-lm-forincremental-learning We report the average accuracy after learning the final task. The detailed results are provided in Table 1. \n\nIncremental Learning (IL) and has been impeded by catastrophic forgetting (Kirkpatrick et al., 2017). Catastrophic forgetting refers to neural networks forgetting previous knowledge after learning new tasks (McCloskey and Cohen, 1989). \n\nRecent years have witnessed significant breakthroughs in Pre-trained Language Models (PLMs) in vision and NLP tasks. Most recent studies of IL use PLMs as the backbone and design various methods for alleviating catastrophic forgetting in NLP tasks. However, is forgetting really catastrophically in PLMs? More specifically, how can we quantify the forgetting and how much knowledge is forgotten in various IL scenarios when using various backbones and methods on various tasks? \n\nTo answer the above question, we carry out extensive experiments for exploring forgetting in more than 20 methods on four classification tasks (Text Classification, Intent Classification, Relation Extraction, and Named Entity Recognition) under the two most popular IL settings (Class-Incremental and Task-Incremental) with various model architecture (encoder only and decoder only) and scales (from 19M to 1.21B number of parameters). Through extensive experiments, we have several core findings: \n\n\u2022 The popular assumption that PLMs suffer from catastrophic forgetting does not hold. \n\nEven under sequential fine-tuning (SEQ), the PLMs maintain the knowledge without much forgetting (Sec. 3.2). From the probing perspective, most existing IL methods do not learn incremental knowledge for PLMs (Sec. 4.2). \n\n\u2022 By combining SEQ with simple strategies (Sec. 4.1), we propose SEQ* and find that SEQ* has competitive or even superior performance than SOTA IL methods (Fig. 1, Sec. 4.2).",
            "score": 0.5728291782445324,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 722
                },
                {
                    "start": 725,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1790
                },
                {
                    "start": 1793,
                    "end": 1895
                },
                {
                    "start": 1896,
                    "end": 1901
                },
                {
                    "start": 1902,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2012
                },
                {
                    "start": 2015,
                    "end": 2062
                },
                {
                    "start": 2063,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 694,
                    "end": 721,
                    "matchedPaperCorpusId": "61019113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2156982421875
        },
        {
            "corpus_id": "270620654",
            "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
            "text": "Large Language models (LLMs) suffer from forgetting of upstream data when fine-tuned. Despite efforts on mitigating forgetting, few have investigated whether, and how forgotten upstream examples are dependent on newly learned tasks. Insights on such dependencies enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices. We show that the matrices are often well-approximated with low-rank matrices, indicating the dominance of simple associations between the learned tasks and forgotten upstream examples. Leveraging the analysis, we predict forgetting of upstream examples when fine-tuning on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. The approach, despite simplicity, outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs for predicting forgetting. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay at fine-tuning. Project page: https://inklab.usc.edu/lm-forgetting-prediction/",
            "score": 0.5716531117380894,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.37158203125
        },
        {
            "corpus_id": "267636596",
            "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
            "text": "We categorize existing studies on understanding the incremental learning ability of LLMs into three parts: (1) Understanding Forgetting, (2) Understanding Memorization, and (3) Applications in NLP. Due to space limitations, the detailed discussion is provided in the Appendix A. Understanding Forgetting. Earlier studies, such as French (1999), assess catastrophic forgetting by measuring performance degradation on old tasks. Recently, studies (Tao et al., 2023;Zheng et al., 2023b) use probing techniques to measure forgetting in incremental learning. Zheng et al. (2023b) uses probing techniques to show that LLMs have superior performance on evaluated datasets even before IL. Our work is inspired by Zheng et al. (2023b) and proposes a novel dataset to minimize the influence of data leakage issues.",
            "score": 0.5698921204238865,
            "section_title": "Related Work",
            "char_start_offset": 19235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 804
                }
            ],
            "ref_mentions": [
                {
                    "start": 445,
                    "end": 463,
                    "matchedPaperCorpusId": "257279790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40087890625
        },
        {
            "corpus_id": "269214372",
            "title": "Exploring the landscape of large language models: Foundations, techniques, and challenges",
            "text": "This may cause a decline in performance across other tasks or domains.This phenomenon hampers the model's ability to maintain a balanced and versatile understanding across a wide range of contexts, highlighting the trade-off between specialization for a particular task and the risk of forgetting valuable general knowledge acquired during its initial pre-training phase.\n\nOne strategy to address the catastrophic forgetting is to fine-tune the LLM on multiple tasks at the same time (H.W. Chung et al., 2022;Karimi Mahabadi et al., 2021).The Fine-tuned Language Net (FLAN) offers a set of templates and techniques for instruction fine-tuning of LLMs on various tasks, with the goal of retaining and consolidating model's generalization abilities (Wei et al., 2022).FLAN was already utilized to fine-tune powerful models such as FLAN-T5 and FLAN-PaLM (H.W. Chung et al., 2022).It is worth noting that a downside of multi-task fine-tuning is the need for a large number of training samples across multiple tasks.\n\nAnother strategy to cope with the catastrophic forgetting is to avoid modifying majority of the model's parameters, which is discussed in the next subsection.Fig. 2 illustrates the two domainadaptation paradigms for a movie review title generation example task.",
            "score": 0.5683703587223641,
            "section_title": "Fine-tuning",
            "char_start_offset": 14178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 70,
                    "end": 371
                },
                {
                    "start": 373,
                    "end": 487
                },
                {
                    "start": 487,
                    "end": 539
                },
                {
                    "start": 539,
                    "end": 766
                },
                {
                    "start": 766,
                    "end": 854
                },
                {
                    "start": 854,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1011
                },
                {
                    "start": 1013,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1274
                }
            ],
            "ref_mentions": [
                {
                    "start": 747,
                    "end": 765,
                    "matchedPaperCorpusId": "237416585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32373046875
        },
        {
            "corpus_id": "276482659",
            "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
            "text": "However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2024;Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark. \n\nWe investigate the extent to which additional knowledge can be integrated into LLMs via the LoRA adapter while preserving its general capabilities. We seek to identify the underlying reasons for any performance drops when new information is introduced, and explore strategies to effectively minimize these adverse effects. \n\nOur contributions are as follows: \n\n\u2022 We conducted a series of extensive experiments incorporating into the LoRA model 1, 10, 50, 100, 500 and 3000 facts unknown to the model tracking how the model degrades intrinsically (via positive and negative shifts) and extrinsically (by tracking the degradation of reasoning abilities on the external benchmarks, such as MMLU and TruthfulQA). \n\n\u2022 We introduced two fine-tuning techniques to mitigate negative shifts and degradation of the model's reasoning abilities: (1) adding paraphrased new facts, and (2) adding facts the model already knows -and conducted a careful analysis of the results obtained. \n\n\u2022 Despite the possible degradation of the model, we found positive shifts -the cases where the model learned new knowledge for which it was not trained, and explained the nature of these shifts. \n\nWe release code and data for further usage.1",
            "score": 0.5681934451572948,
            "section_title": "Introduction",
            "char_start_offset": 1801,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 481
                },
                {
                    "start": 484,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 842
                },
                {
                    "start": 845,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1455
                },
                {
                    "start": 1458,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1699
                }
            ],
            "ref_mentions": [
                {
                    "start": 88,
                    "end": 109,
                    "matchedPaperCorpusId": "266348792"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "Specially, continual pre-training updates LLMs with large and unlabeled domain-specific corpora, which mainly focuses on memorizing and injecting new knowledge into the parameters of LLMs.However, these approaches might result in catastrophic forgetting and performance degradation in general language tasks (Kar et al., 2022;Mehta et al., 2023).Another line of work has explored conducting instruction fine-tuning by synthesizing domain-related instructions (Cheng et al., 2024;Jiang et al., 2024).Nevertheless, these studies require additional models to synthesize amounts of instructions highly related to specific domains, resulting in high computational costs.It is noted that our method differs from these works in several ways.Firstly, we disentangle domain adaptation into knowledge memorization and capability elicitation, focusing on learning domain-specific knowledge and solving domain tasks with learned knowledge, respectively.Secondly, we employ token swap self-distillation in the knowledge mixture pre-training to retain general knowledge and avoid catastrophic forgetting.",
            "score": 0.5680904687753995,
            "section_title": "DETAILED ANALYSIS",
            "char_start_offset": 31877,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 188
                },
                {
                    "start": 188,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 941
                },
                {
                    "start": 941,
                    "end": 1090
                }
            ],
            "ref_mentions": [
                {
                    "start": 308,
                    "end": 326,
                    "matchedPaperCorpusId": "250343263"
                },
                {
                    "start": 326,
                    "end": 345,
                    "matchedPaperCorpusId": "245329773"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2298583984375
        },
        {
            "corpus_id": "273233211",
            "title": "SLIM: Let LLM Learn More and Forget Less with Soft LoRA and Identity Mixture",
            "text": "In recent years, large language models (LLM) have demonstrated extraordinary general capabilities, and are widely used for question-answering, code generation, mathematics, and other fields. However, the increasing utilization of LLM in specialized domains needs to support tasks with a mixture of world and specific knowledge. Meanwhile, since the scale of the LLM is large, fine-tuning the entire model could be expensive, and thus may not be affordable for many users. Furthermore, after finetuning the downstream tasks, the LLM may lose the Figure 1: The radar chart of the performance of the parameter-efficient fine-tuning methods. The blue benchmarks are evaluated after fine-tuning on other downstream datasets. Compared with the state-of-theart parameter-efficient methods, the proposed SLIM could achieve comparable downstream performance while mitigating catastrophic forgetting. \n\ngeneral capability, due to catastrophic forgetting. How to reduce the training cost while mitigating catastrophic forgetting without loss of learning capacity, is an essential problem in the utilization of LLM, especially when utilized in a scenario where both common and professional capabilities are needed. \n\nAn intuitive solution is to train only a small bunch of parameters. For example, low-rank adaptation (LoRA) (Hu et al., 2022), a widely used PEFT approach, is validated to forget fewer pieces of knowledge compared with fine-tuning the full weight of the base model, with limitations of learning capacity (Biderman et al., 2024). Chaudhry et al. (2020) propose to optimize the model in the orthogonal subspace with low-rank adaptation to alleviate forgetting. Singh et al. (2020) propose to insert calibrating modules to the base model for learning the downstream tasks. Liang et al. (2023) utilize learnable mask prompts to fine-tune the CLIP without loss of text-vision alignment. \n\nThese approaches attempt to introduce minimal changes to the base model to keep its raw capacity. Some approaches utilize data replay to reduce forgetting. For example, Glorioso et al. (2024) utilize a 60% data replay for continual training. Zhu et al. (2024) utilize a 30% replay rate in the continual pre-training for the coding model.",
            "score": 0.5675825711324816,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1202
                },
                {
                    "start": 1205,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1533
                },
                {
                    "start": 1534,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1774
                },
                {
                    "start": 1775,
                    "end": 1886
                },
                {
                    "start": 1889,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2130
                },
                {
                    "start": 2131,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 1313,
                    "end": 1330,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1509,
                    "end": 1532,
                    "matchedPaperCorpusId": "269791237"
                },
                {
                    "start": 1534,
                    "end": 1556,
                    "matchedPaperCorpusId": "225040673"
                },
                {
                    "start": 1664,
                    "end": 1683,
                    "matchedPaperCorpusId": "227178110"
                },
                {
                    "start": 1775,
                    "end": 1794,
                    "matchedPaperCorpusId": "252780581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2265625
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Instruction Tuning. Instruction tuning has proven to be effective in aligning responses from pre-trained language models with human intents or preferences (Ouyang et al., 2022;Stiennon et al., 2020;Min et al., 2021). This technique refines a model's ability to predict a specific response to a given prompt, which may optionally include an instruction that outlines a task for the model. Examples of such models include T0 (Sanh et al., 2021), mT0 (Muennighoff et al., 2022), and BLOOMZ (Muennighoff et al., 2022). It has been demonstrated that instruction tuning can enhance the ability of language models to generalize to unseen tasks without prior exposure (Wei et al., 2021;Sanh et al., 2021). In this work, we focus on fine-tuning LLMs in a continual manner and analyze the catastrophic forgetting (CF) phenomenon during training. Specifically, instructions for a particular type of task (such as generating headlines) are used to tune the LLMs in each training phase, and the model does not have access to previously learned tasks. \n\nEvaluation of CF in Continual Learning. Various training strategies have been proposed to address the problem of catastrophic forgetting (CF) in continual learning (Riemer et al., 2019;Buzzega et al., 2020;Ke et al., 2022;Chen et al., 2022;Luo et al., 2023a). Previous studies have primarily measured CF by evaluating the performance decrease in previously learned tasks during continual learning or the average performance of learned tasks at the end of training. However, Davari et al. (2022) discovered that even when the model performance on previously learned tasks is preserved, the representations still suffer from significant drift due to parameter updates. As a result, they propose using an optimal linear classifier of learned tasks to measure performance, with changes considered as a surrogate to quantify CF. Similarly, Wu et al. (2022) employs layer-wise and task-wise probing to analyze CF in each layer for previously learned tasks. Luo et al. (2023b) propose using a series of probing tasks to evaluate the knowledge stored in LLMs and analyze the generality of the models.",
            "score": 0.5650253918205671,
            "section_title": "Related Work",
            "char_start_offset": 5764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 19
                },
                {
                    "start": 20,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1706
                },
                {
                    "start": 1707,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2132
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 176,
                    "end": 198,
                    "matchedPaperCorpusId": "221665105"
                },
                {
                    "start": 1204,
                    "end": 1225,
                    "matchedPaperCorpusId": "53100211"
                },
                {
                    "start": 1514,
                    "end": 1534,
                    "matchedPaperCorpusId": "247748982"
                },
                {
                    "start": 1875,
                    "end": 1891,
                    "matchedPaperCorpusId": "247717213"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7119140625
        },
        {
            "corpus_id": "269148645",
            "title": "Towards Practical Tool Usage for Continually Learning LLMs",
            "text": "In this work, we explore the potential use of tools in continual learning for LLMs.We apply this setup within a simple arithmetic reasoning setup, where a language model is taught multiple math functions.Our results demonstrate that LLMs that learning to generate answers based on tools both adapt faster to new tasks while also maintaining greater performance on prior tasks.We further validate these conclusions through a continual learning benchmark based on the GLUE natural language understanding benchmark.However, continual learning remains unsolved, as cases still exist where all models fail to demonstrate the ability to autonomously solve the benchmark.This emphasizes the need for models which can adapt to the world in the same manner as conscious humans and by highlighting current limitations and the potential for tool LLMs in this setting, these results hopefully delineate paths for future research which can allow for more practical LLMs deployed in the real world.Some limitations of this work can be noted in the simplicity of the tools that are explored as well as the degree of relatedness that exists between each tool and how they are used.\n\nFirst, we note that there exists some relatedness between a number of different functions which we learn due to the granularity at which they are used, which may or may not have resulted in some potential benefits in terms of avoiding catastrophic forgetting.We maintain, however, that we provide enough experimental analysis and results such that this should not pose an issue with the results, hence we believe this to have had minimal effect on potentially producing optimistic results.\n\nSecond, forgetting (Chaudhry et al., 2018) is a limited metric, as the concept of 'more' forgetting is not well-defined.For example, suppose we take accuracy as our performance metric and are comparing two scenarios.Scenario A has performance degrade by 25% from a peak performance of 80% (80% \u2192 60%).Scenario B observes a 10% performance degradation from a peak performance of 30% (30% \u2192 27%).In this case, despite scenario B observing less forgetting, we may consider it more problematic as the performance was initially significantly worse than A. As such, developing better metrics for capturing these types of phenomena is important for better analysis within continual learning settings.",
            "score": 0.5642648621005357,
            "section_title": "Conclusion",
            "char_start_offset": 27652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 204
                },
                {
                    "start": 204,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 664
                },
                {
                    "start": 664,
                    "end": 984
                },
                {
                    "start": 984,
                    "end": 1165
                },
                {
                    "start": 1167,
                    "end": 1426
                },
                {
                    "start": 1426,
                    "end": 1656
                },
                {
                    "start": 1658,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 1874
                },
                {
                    "start": 1874,
                    "end": 1959
                },
                {
                    "start": 1959,
                    "end": 2052
                },
                {
                    "start": 2052,
                    "end": 2351
                }
            ],
            "ref_mentions": [
                {
                    "start": 1677,
                    "end": 1699,
                    "matchedPaperCorpusId": "4047127"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06488037109375
        },
        {
            "corpus_id": "270702941",
            "title": "Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs",
            "text": "Many practitioners fine-tune already pre-trained LLMs with less data and compute instead of training them from scratch [84,85,5].While this reduces the cost of LLM training significantly, fine-tuning each and every parameter of these large models for each (or a few) task is still very costly.This has led to a number of parameter-efficient fine-tuning (PEFT) methods reducing the number of trainable parameters during fine-tuning [25,26,27,28,86,87,34,88].Among different PEFT methods, low-rank adaptation (LoRA) [6] and its variants [89,90,91,92] have shown similar performance to full fine-tuning in many tasks while reducing the number of trainable parameters through low-rank approximation to model updates during fine-tuning.Our PEFT method, LoTA, while reducing the number of trainable parameters significantly via sparsity, has various other benefits in different applications, such as avoiding catastrophic forgetting (of especially safety alignment), enabling fine-tuning on new tasks more successfully, model merging using sparse task vectors, unlearning, and communication-efficient federated learning (FL).We demonstrate that full fine-tuning and the existing PEFT methods fall short in these applications and significantly underperform LoTA.\n\nCatastrophic Forgetting.When LLMs go through sequential (or continual) multitask learning, i.e., fine-tuned on different tasks sequentially, they often suffer from performance loss on earlier tasks-known as catastrophic forgetting [9,10,11,12,13].To mitigate this, a number of data-centric and architectural solutions have been proposed for language and other domains.Replay-based methods [93,94] add a portion of the previously learned data during fine-tuning on a new task, which raises privacy concerns as it requires constant access to previously learned data.Regularization-based approaches [95,96] tend to have poor adaptability to specific tasks.An architecture-based approach, \"progressive prompts\" [97], sequentially concatenates soft prompts as they are being learned for each task-showing some resistance against forgetting.However, they require access to task identifiers at inference for each task, which is not always feasible.",
            "score": 0.5624373199827639,
            "section_title": "Related Work",
            "char_start_offset": 34543,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 129,
                    "end": 293
                },
                {
                    "start": 293,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 731
                },
                {
                    "start": 731,
                    "end": 1119
                },
                {
                    "start": 1119,
                    "end": 1255
                },
                {
                    "start": 1257,
                    "end": 1281
                },
                {
                    "start": 1281,
                    "end": 1504
                },
                {
                    "start": 1504,
                    "end": 1625
                },
                {
                    "start": 1625,
                    "end": 1821
                },
                {
                    "start": 1821,
                    "end": 1910
                },
                {
                    "start": 1910,
                    "end": 2092
                },
                {
                    "start": 2092,
                    "end": 2198
                }
            ],
            "ref_mentions": [
                {
                    "start": 119,
                    "end": 123,
                    "matchedPaperCorpusId": "248693283"
                },
                {
                    "start": 123,
                    "end": 126,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 126,
                    "end": 128,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 431,
                    "end": 435,
                    "matchedPaperCorpusId": "257631760"
                },
                {
                    "start": 435,
                    "end": 438,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 438,
                    "end": 441,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 441,
                    "end": 444,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 447,
                    "end": 450,
                    "matchedPaperCorpusId": "257921386"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 535,
                    "end": 539,
                    "matchedPaperCorpusId": "258841328"
                },
                {
                    "start": 539,
                    "end": 542,
                    "matchedPaperCorpusId": "265294948"
                },
                {
                    "start": 542,
                    "end": 545,
                    "matchedPaperCorpusId": "264128197"
                },
                {
                    "start": 545,
                    "end": 548,
                    "matchedPaperCorpusId": "264172315"
                },
                {
                    "start": 1488,
                    "end": 1491,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1494,
                    "end": 1497,
                    "matchedPaperCorpusId": "251648120"
                },
                {
                    "start": 1500,
                    "end": 1503,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 1646,
                    "end": 1650,
                    "matchedPaperCorpusId": "206596260"
                },
                {
                    "start": 1853,
                    "end": 1857,
                    "matchedPaperCorpusId": "233210110"
                },
                {
                    "start": 1857,
                    "end": 1860,
                    "matchedPaperCorpusId": "4254748"
                },
                {
                    "start": 1964,
                    "end": 1968,
                    "matchedPaperCorpusId": "256390383"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "268680747",
            "title": "Span-Oriented Information Extraction - A Unifying Perspective on Information Extraction",
            "text": "Sometimes, the pre-trained features from an LLM do not align well with the task that is being asked of the system.This misalignment will degrade the system's performance.In these cases it is common for LLMs to undergo a fine-tuning process, which adapts the pre-trained model parameters, including span features, to the specific task.Previous studies have shown that fine-tuning the model outperforms feature engineering with similar settings [21,46], but can be prone to catastrophic forgetting [53] and other maladies; see the survey by Li et al. [49] for details.One major problem is that model tuning requires that the model be loaded into memory and trained, which, for even medium-sized LLMs, is a non-trivial task.",
            "score": 0.562270981320291,
            "section_title": "Model Tuning",
            "char_start_offset": 49134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 334
                },
                {
                    "start": 334,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 721
                }
            ],
            "ref_mentions": [
                {
                    "start": 443,
                    "end": 447,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 447,
                    "end": 450,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 496,
                    "end": 500,
                    "matchedPaperCorpusId": "236493269"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443359375
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "LLMs may inadvertently learn copyrighted material, raising significant legal and ethical concerns [1], and broader questions about responsible AI use [40]. In this context, the California Consumer Privacy Act [139] and the General Data Protection Regulations of the European Union [63] stipulate the right to data forgetting. The foundational model's knowledge must be adapted to comply with these regulations. However, the cost of excluding copyrighted data for re-training from scratch is prohibitive. For instance, training a Llama-2-70B from scratch requires 1,720,320 GPU hours [189]. Traditional methods often use gradient ascent (GA) to achieve forgetting by fine-tuning the model using the GA algorithm on the specific data to be forgotten [187,222]. Unfortunately, this approach typically catastrophically destroys other parts of the model's knowledge. That is, forgetting specific knowledge also erases other knowledge that should be retained. Recently, many studies based on model merging techniques have demonstrated the potential to forget LLM-specific knowledge without harming other knowledge [40,66,71]. \n\nUnlike the GA-based approach, the model merging approach does not require additional data for other tasks to maintain old knowledge. To achieve forgetting, model merging typically incorporates a negatively fine-tuned model into the target model (i.e., the task-specific fine-tuned knowledge is subtracted from the target model). For example, Task Arithmetic [71] shows that negating task vectors degrade performance on specific tasks without substantial changes to the control tasks. Experiments demonstrate that model merging can forget the knowledge of the target task in a fine-tuned model without harming performance on control tasks. Similarly, Stable Sequential Unlearning (SSU) [40] extends this forgetting to the setting of sequential unlearning on LLMs, where different copyrighted content must be unlearned at different time steps. Knowledge forgetting can also forget samples that represent bad behavior during pretraining. For instance, FuseToForget [230] employs model merging as a debiasing tool to reduce privacy issues in language models. FLearning [135] first subtracts the parameters related to the data to be forgotten and then fine-tunes the parameters with new data to achieve accurate knowledge updates.",
            "score": 0.561389048696815,
            "section_title": "Knowledge Unlearning of LLMs",
            "char_start_offset": 37197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1119
                },
                {
                    "start": 1122,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2347
                }
            ],
            "ref_mentions": [
                {
                    "start": 281,
                    "end": 285,
                    "matchedPaperCorpusId": "86416362"
                },
                {
                    "start": 748,
                    "end": 753,
                    "matchedPaperCorpusId": "238198525"
                },
                {
                    "start": 753,
                    "end": 757,
                    "matchedPaperCorpusId": "264172840"
                },
                {
                    "start": 1112,
                    "end": 1115,
                    "matchedPaperCorpusId": "260925619"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03277587890625
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "In our study, we introduce Instruction Vector (IV), which enables detailed analysis of LLMs task processing capabilities.By analyzing IV dynamics before and after training, we show that forgetting is caused by the overlay of new reasoning patterns over pre-existing skills, while the performance can be recovered by adding the IV to the computation graph.Additionally, our proposal of IV-guided training as a fine-tuning method successfully reduces forgetting by maintaining harmony between the model's computation graph and the IV-associated one.These findings offer valuable insights into the internal mechanisms causing for-getting in LLMs and are expected to contribute to advancing the development and application of LLMs alignment.",
            "score": 0.5612430159240073,
            "section_title": "Conclusion",
            "char_start_offset": 26559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 121,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 737
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1697998046875
        },
        {
            "corpus_id": "276287820",
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "text": "Building on this, we propose to systematically augment the fine-tuning data by synthetically generating multiple answers for each question. This encourages LLMs to memorize and extract the domain knowledge effectively while minimizing the overfitting on stylistic features. \n\nFurthermore, to address the challenge of catastrophic forgetting (Zhang and Wu, 2024;Ke et al., 2023;Jang et al., 2022)-where an LLM's general language understanding deteriorates as domain-specific fine-tuning overwrites prior knowledge-we introduce a self-selective rehearsal replay buffer (Gupta et al., 2024;Huang et al., 2024). This buffer contains samples from an instruction tuning dataset. But unlike traditional replay buffers, the self-selective approach uses LLM's own predictions rather than the gold response to retain the LLM's versatility across tasks when injecting knowledge. \n\nIn addition, we also introduce the novel use of Domain Identifiers-phrases pre-pended to questions-to help the LLM distinguish new knowledge from its existing skills. This improves domainspecific accuracy and also mitigates catastrophic forgetting, preserving LLM's general competence during fine-tuning. \n\nTo validate PA-RAG's ability to inject new domain-specific knowledge, we require a corpus that the LLM hasn't seen during pre-training or instruction tuning. In the absence of such a corpus, we create two datasets using domain-specific books that were published in 20241 . In our experiments, PA-RAG achieves a significant increase in performance against other methods on this domainspecific benchmark while maintaining general reasoning capabilities that we measure using various benchmarks such as MMLU (Hendrycks et al., 2021), TruthfulQA (Lin et al., 2022), Hellaswag (Zellers et al., 2019), and GSM8k (Cobbe et al., 2021).",
            "score": 0.5606329475100718,
            "section_title": "Introduction",
            "char_start_offset": 4150,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 273
                },
                {
                    "start": 276,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1174
                },
                {
                    "start": 1177,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1804
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50146484375
        },
        {
            "corpus_id": "275906908",
            "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning",
            "text": "Instruction-tuning Instruction following [5,25] is widely used to align responses of LLMs with target task values. Practically, it requires instruction tasks dataset to fine-tune LLMs, where each dataset consists of instructions and desired responses. Hence, the performance of instruction-tuning heavily relies on the quality of instruction data such as context richness [35,39,43]. In addition, Ghosh et al. [9] investigated the limitation of instruction-tuning of LLMs about the catastrophic forgetting from pattern-copying behaviors and hallucinations with Lora/full fine-tuning. These works give us a good start on how performance degrades in terms of models' response behaviors and benchmarks when fine-tuning with instruction following dataset. \n\nUsing LLM-generated data for instruction fine-tuning As Chung et al. [5], Wang et al. [35] showed, LLMs break down easily after training with different tasks. Several remedies to the performance improvement focus on training data augmentation [36,40]. For instance, [28] uses much larger LLMs (e.g., GPT-4 and Claude) to generate responses of questions as training labels, which improves the performance on both the target task and other non-target tasks. However, such a distillation method neglects the correctness of generated labels, so that more incorrect responses could be trained as the amount of generated data increases, and thus, using equal or smaller-sized models for distillation could be challenging. [41] prompts LLMs to simply rephrase the response of existing ground truth to generate labels to match similar styles of the LLMs for fine-tuning. However, rephrasing the ground truth answer limits the output distribution and results in lower performance in our study. Furthermore, [11] exploits a base LLM as a judge to pick out answerable and unanswerable questions to compose a new training dataset to improve target task performance only on the QA/conversation dataset. Although using Mistral 7B Instruct to generate acceptable responses for answerable questions improves target and non-target tasks, it is rarely discussed that if the proposed method is applicable to different model sizes and series.",
            "score": 0.560194811583667,
            "section_title": "Related work",
            "char_start_offset": 25188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 751
                },
                {
                    "start": 754,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1005
                },
                {
                    "start": 1006,
                    "end": 1209
                },
                {
                    "start": 1210,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2176
                }
            ],
            "ref_mentions": [
                {
                    "start": 379,
                    "end": 382,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 410,
                    "end": 413,
                    "matchedPaperCorpusId": "267548105"
                },
                {
                    "start": 997,
                    "end": 1001,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 1020,
                    "end": 1024,
                    "matchedPaperCorpusId": "270211000"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "267769989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52490234375
        },
        {
            "corpus_id": "270559716",
            "title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation",
            "text": "PPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement.",
            "score": 0.5594959531919809,
            "section_title": "Limitations",
            "char_start_offset": 23201,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 545
                },
                {
                    "start": 546,
                    "end": 717
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67529296875
        },
        {
            "corpus_id": "276409016",
            "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
            "text": "We further evaluate LLMs with more sequences, obtaining similar observations as shown in Appendix F. \n\nTraining stage perspective: forgetting may naturally mitigate during training. Figure 1 illustrates the extent of forgetting at each training phase across various evaluation datasets. Contrary to the nearly consistent performance drop seen in previous studies (Luo et al., 2024;Wu et al., 2022), we frequently observe a phenomenon where performance initially decreases but later rebounds. For instance, the fourth column of (a)-(II) shows the \"Ob.\" test score dropping to 67% on the M2 model; however, after two stages, the performance leaps to 122%. This contradiction further raises further questions about how sequential fine-tuning on new tasks impacts the internal capabilities of LLMs, allowing them to recover previously forgotten capabilities. \n\nModel perspective: forgetting is model-dependent. We compare forgetting between LLMs; specifically, for the \"Ob.\" task performance shown in (a)-(II) and (c)-(II), continual instruction tuning of Llama2-7b-chat demonstrates a performance increase of up to 146% relative to using the Llama2-7b-chat itself, while that of Llama3-7b-chat shows a decrease to 73%. This suggest\u0161 that forgetting is not only task-related but also heavily influenced by model-dependent factors such as model size, architecture, and the diversity of pre-training data. These factors shape each model's unique capacity to tasks, revealing that the mere feature similarity between tasks (e.g., hidden states in the last layer) is insufficient to predict model-dependent forgetting patterns (see Appendix F). \n\nThe previous section prompts a more effective measure for characterizing catastrophic forgetting, surpassing those traditionally used in continual learning research with small models, such as feature similarity (Ramasesh et al., 2020;Lee et al., 2021) and readout similarity (Lee et al., 2021) between tasks. We have proven them loosely correlated with forgetting under LLMs. Other model-dependent measures, such as the \u21132-distance of model parameters after tuning on new tasks (Lin et al., 2023;Evron et al., 2024), necessitate expensive training for their computation.",
            "score": 0.5591736443042888,
            "section_title": "CATASTROPHIC FORGETTING OF LLMS",
            "char_start_offset": 14687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 103,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 854
                },
                {
                    "start": 857,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1636
                },
                {
                    "start": 1639,
                    "end": 1947
                },
                {
                    "start": 1948,
                    "end": 2014
                },
                {
                    "start": 2015,
                    "end": 2209
                }
            ],
            "ref_mentions": [
                {
                    "start": 381,
                    "end": 397,
                    "matchedPaperCorpusId": "247717213"
                },
                {
                    "start": 1873,
                    "end": 1890,
                    "matchedPaperCorpusId": "235790418"
                },
                {
                    "start": 1914,
                    "end": 1932,
                    "matchedPaperCorpusId": "235790418"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2626953125
        },
        {
            "corpus_id": "270562504",
            "title": "Soft Prompting for Unlearning in Large Language Models",
            "text": "We include our main results with LLaMA-2-7B in Table 2. We report performance metrics for the original pre-trained LLM denoted as Vanilla and the fine-tuned base model denoted as QLoRA. We notice that the Vanilla results are considerably poorer for SST-2 compared to Yelp which validates our setup of fine-tuning the original LLM on the datasets for memorization. We attribute the difference in utility to the fact that the text sequences in Yelp are significantly longer and provide more contextual information. Nonetheless, after fine-tuning with QLoRA, the LLM's performance increases to similar margins for both datasets. QLoRA fine-tuning similarly improves LLM's predictions for the WMDP+SciQ dataset. \n\nFrom Table 2, we observe that SPUL significantly reduces accuracy and F1 on D tr f compared to QLoRA demonstrating forgetting efficiency. At the same time, the difference in utility between SPUL and QLoRA for D tr r is minimal showing that our method can promote unlearning while also preserving inference utility. Moreover, the metrics for D te f and D te r reflect those reported for D tr f and D tr r showing that the soft prompts effectively impose unlearning constraints on samples unseen during training. We observe similar performance trends for Yelp and WMDP+SciQ. Although the  performance drop for D tr f and D te r in Yelp are not equally as large as SST-2, the forget utility with the learned tokens is significantly lesser in comparison to the base model. We conjecture that the additional context provided by descriptive Yelp reviews restricts the forgetting capacity of the LLM. Also point out that utility loss in retain sets is smaller than forget sets. We also notice that SPUL performs exceedingly well on the WMDP+SciQ with the highest differences between the retain and forget metrics. \n\nFurthermore, SPUL outperforms baseline methods by a significant margin; compared to GA and RL, which optimize model parameters based only on the D tr f , SPUL consistently preserves inference utility on the retain sets with comparable or even lower metrics on the forget set. For WMDP+SciQ, both baselines underperform in the forgetting task.",
            "score": 0.5588448682523508,
            "section_title": "Main Results",
            "char_start_offset": 19142,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 56,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 625
                },
                {
                    "start": 626,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1816
                },
                {
                    "start": 1819,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0743408203125
        },
        {
            "corpus_id": "271218270",
            "title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models",
            "text": "Large language models (LLMs) and multimodal models (MMs) have demonstrated remarkable capabilities across numerous domains, as highlighted by OpenAI (2023) and Bubeck et al. (2023). However, whereas both LLMs and MMs pre-trained on extensive data excel in general language understanding and multimodal tasks, they may not be optimized for every specific task of interest prompted by instructions. Therefore, there is a need for continual instruction learning to adapt LLMs and MMs to evolving tasks and domains. Indeed, continual instruction learning is essential for LLMs such as GPT (Radford et al., 2019) and MMs such as LLaVA (Liu et al., 2024b,a) to maintain their effectiveness and relevance in handling a wide range of tasks and domains. \n\nSuch models are trained on vast amounts of data and fine-tuned for specific applications, often by learning tasks sequentially (Luo et al., 2023), i.e. learning on datasets pertaining to one task all at once, before moving on to the next task. The challenge lies in their ability to continually learn and adapt as they encounter new tasks and information. However, in continual instruction learning scenarios, where models are sequentially trained on different tasks or datasets, catastrophic forgetting occurs when the model's parameters are updated to accommodate new information, leading to degradation or complete loss of performance on previously learned tasks. \n\nA typical way to balance new learning with the retention of previously acquired capabilities in LLMs is through replaying old data. However, with the rapid iterations of LLMs for diverse and complex use cases, retaining old data becomes exceptionally challenging. Moreover, continually tuning a model with a large number of parameters is highly costly in terms of both computation and memory usage. Parameter-efficient fine-tuning (PEFT) such as low-rank adaptation (LoRA) (Hu et al., 2022) provides an option of lightweight with portable parameters, which could be paired with a model to perform specific tasks. Therefore, in this work, we focus on alleviating catastrophic forgetting during continual instruction tuning, particularly with minimal data retention and its interplay with PEFT.",
            "score": 0.5579328687448581,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2208
                }
            ],
            "ref_mentions": [
                {
                    "start": 1889,
                    "end": 1906,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.376220703125
        },
        {
            "corpus_id": "259951356",
            "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning",
            "text": "Fine-tuning a foundation model can result in the forgetting of pre-trained knowledge, which may lead to sub-optimal performance on downstream tasks. Forgetting occurs when the target model deviates significantly from the pre-trained model during the fine-tuning process [133]. This deviation increases the likelihood of overfitting to a small fine-tuning set [134], which can contribute to forgetting. There are several simple and effective strategies to mitigate forgetting during the fine-tuning process. These include techniques such as learning rate decreasing [134], weight decay [135], [136], and Mixout regularization [133]. Furthermore, Fatemi et al. [137] find that in the study of mitigating the gender bias of the pre-trained language model, the pre-trained knowledge will be forgotten when the small neutral data is fine-tuned, which will hurt the downstream task performance. Dong et al. [138] observe that adversarial fine-tuning of pre-trained language models is prone to severe catastrophic forgetting, causing the loss of previously captured general and robust linguistic features. To address these issues, they propose a Robust Informative Fine-Tuning method from an information-theoretical perspective. In addition, an approach called Recall and Learn, proposed in Chen et al. [139], addresses the forgetting issue by utilizing Pretraining Simulation and Objective Shifting. This approach enables multi-task fine-tuning without relying on the data from the pretraining tasks. Zhang et al. [140] conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text. Conjugate Prompting [141] seeks to reduce the forgetting of prior task knowledge during the fine-tuning of new tasks by counteracting changes in implicit task inference. Furthermore, several recent studies have shown that merging pretrained and fine-tuned models directly at the parameter level can also help mitigate forgetting [142], [143]. Moreover, recent research [144] highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter [144] introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs.",
            "score": 0.5572826856150825,
            "section_title": "Forgetting in Fine-Tuning Foundation Models",
            "char_start_offset": 41659,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1393
                },
                {
                    "start": 1394,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1810
                },
                {
                    "start": 1811,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 364,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 565,
                    "end": 570,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 585,
                    "end": 590,
                    "matchedPaperCorpusId": "2925420"
                },
                {
                    "start": 901,
                    "end": 906,
                    "matchedPaperCorpusId": "245116938"
                },
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "216553067"
                },
                {
                    "start": 1508,
                    "end": 1513,
                    "matchedPaperCorpusId": "271745852"
                },
                {
                    "start": 1970,
                    "end": 1975,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 2010,
                    "end": 2015,
                    "matchedPaperCorpusId": "266374841"
                },
                {
                    "start": 2151,
                    "end": 2156,
                    "matchedPaperCorpusId": "266374841"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62060546875
        },
        {
            "corpus_id": "263828951",
            "title": "Do Large Language Models Know about Facts?",
            "text": "In Table 2, we present the average results of 10 accessible LLMs operating under varying settings on Pinocchio, run three times each. From Table 2, we draw the following conclusions: \n\n\u2022 Regarding overall performance, we observe that, on average, LLMs without instruction tuning underperform those with instruction tuning by 16.0%. GPT family LLMs undergoing RLHF exhibit superior results, indicating that instruction tuning and RLHF optimize alignment with human knowledge, thereby improving factual question response accuracy. \u2022 Results obtained using the Few-shot setting significantly outperform those obtained when simply asking factual questions to LLMs in the Zero-shot setting, especially for models without RLHF, exhibiting an average improvement of 7.3%. This highlights the capability of some sample prompts to better extract the inherent factual knowledge of LLMs. \u2022 Using the CoT method, we observed a relative boost in performance in LLMs subjected to instruction tuning and RLHF, improving by an average of 2.1%. Notably, the factual accuracy of LLMs like OPT, BLOOM, and LLaMA was mostly stable or even decreased. A review of outputs from these untuned LLMs revealed that, post-CoT application, LLMs tend to produce related content considerations, and extensive considerations often overshadow factual discernment tasks, causing incorrect factual label outputs. In contrast, for instruction-tuned LLMs, the CoT method facilitates enhanced exploration of factual entity relations in questions, resulting in accurate factual labels. See Appendix A.5 for detailed case analyses. \u2022 The OPT model, without being tuned to instructions, struggles significantly to output correct factual labels under the settings of Zero-shot and Zero-shot CoT, often resulting in either a repetition of the original question or a refusal to output any content at all. This issue is somewhat alleviated under the settings of Few-shot and Few-shot CoT. \u2022 Additionally, we studied the hyperparameters of LLMs. Due to limited computing resources, we only explored Vicuna-7B and Vicuna-13B. We found that as model parameters increase, performance on factual questions improves correspondingly, with an average increase of 5.4%. This indicates that LLMs with more parameters can store more world knowledge and have stronger factual knowledge recognition capabilities.",
            "score": 0.5571517858012689,
            "section_title": "MAIN RESULTS",
            "char_start_offset": 17308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1860
                },
                {
                    "start": 1861,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2078
                },
                {
                    "start": 2079,
                    "end": 2215
                },
                {
                    "start": 2216,
                    "end": 2354
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.061431884765625
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "In this paper, when discussing catastrophic forgetting of a task, we consider alterations in both Knowledge and Instruction Probabilities.Typically, a test instance x i is typically presented as a tuple (x i , y i , c, y c i ) (examples are listed in Fig. 2), with shifts in P (y c i |c, x i , M ) signaling variations in the model's proficiency in instruction processing and knowledge understanding and shifts in P (y i |x i , M ) solely reflect changes in the world knowledge comprehension.Our work go beyond simple performance metrics evaluation, offering a detailed examination of distinct capabilities amidst CF.This method reveals if performance degradation stems from an actual loss of world knowledge or a reduction in the ability to follow instructions.\n\nContinual instruction tuning setup.To explore CF in LLMs, we conduct an empirical study within the continual instruction tuning framework.In this setup, a model is sequentially trained on a series of streaming tasks, denoted as {D c 1 , D c 2 , ..., D c T }.\n\nHere, D ct = {(c t , x i , y c i )} symbolizes the t-th task associated with a specific instruction c t .While learning each task D ct , the model can only access to the corresponding data, with the goal of minimizing loss on all learned tasks.Specifically, the model is optimized with min M\n\nwhere N is the size of training set and \u2113 is usually the cross-entropy loss on the entire vocabulary.In addition to avoiding forgetting on previous learned tasks {D c 1 , ..., D c t\u22121 }, the model is also evaluated on held-out evaluation sets (e.g., Common-senseQA (Talmor et al., 2018), MMLU (Hendrycks et al., 2020)) to measure its general ability.\n\nWe select two different continual instruction tuning benchmarks.The first is from TRACE (Wang et al., 2023b) benchmark, which consists of 6 different complex generation tasks including multichoice QA, code generation, mathematical reasoning and summary.The second is called FUNC, adapted from the datasets in Todd et al. (2023), in which tasks have clear and simple instruc-tions.",
            "score": 0.5566304464075504,
            "section_title": "Catastrophic Forgetting in LLMs",
            "char_start_offset": 6765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 492
                },
                {
                    "start": 492,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 762
                },
                {
                    "start": 764,
                    "end": 799
                },
                {
                    "start": 799,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 1022
                },
                {
                    "start": 1024,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1268
                },
                {
                    "start": 1268,
                    "end": 1315
                },
                {
                    "start": 1317,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1667
                },
                {
                    "start": 1669,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 1922
                },
                {
                    "start": 1922,
                    "end": 2049
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244140625
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.",
            "score": 0.5556245847873633,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.233154296875
        },
        {
            "corpus_id": "266335873",
            "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
            "text": "For most of the downstream tasks (e.g., NLI and summarization), with the expansion of training data, performance on these tasks remains stable after improvement. Whereas, for the world knowledge benchmark, a significant decline can be witnessed after a large amount of instruction data. \n\nperts, which can improve the LLM's performance on multiple downstream tasks. \n\nTo demonstrate the efficacy of our proposed method, we conduct extensive experiments across a range of downstream tasks. Experiment results show that LoRAMoE can significantly improve LLM's ability to address the various downstream tasks by fine-tuning the model on a large amount of instruction data, while maintaining the world knowledge stored in the model. In addition, we further evaluate our method by visualizing the expert weight for tasks. The result indicates that LoRAMoE adequately alleviates world knowledge forgetting and achieves an improvement of models by fostering collaboration among experts. The main contributions of our paper are as follows: \n\n1. We find that significantly increasing the amount of instruct data during the SFT phase can damage the world knowledge inside the LLMs. The need for improvement in downstream tasks by scaling up instruction data conflicts with maintaining the world knowledge inside the model. \n\n2. We introduce LoRAMoE, a novelty framework for SFT, which introduces LoRAs as experts and integrates them by the router. Lo-RAMoE can enhance the model's ability to address downstream tasks, while alleviating the world knowledge forgetting. \n\n3. Extensive Experiments demonstrate the efficacy of our proposed approach in multi-tasks and mitigating the forgetting of world knowledge inside the model. The visualizing experiment shows that LoRAMoE can achieve an improvement by fostering collaboration among experts.",
            "score": 0.5549374421769507,
            "section_title": "Introduction",
            "char_start_offset": 2017,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 286
                },
                {
                    "start": 289,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1031
                },
                {
                    "start": 1034,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1437
                },
                {
                    "start": 1438,
                    "end": 1557
                },
                {
                    "start": 1560,
                    "end": 1716
                },
                {
                    "start": 1717,
                    "end": 1831
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "276768395",
            "title": "LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models",
            "text": "Large language models (LLMs) have shown startof-the-art performance in a wide range of challenging tasks (Radford et al., 2021;Ziegler et al., 2020;Kirillov et al., 2023;OpenAI et al., 2023;Touvron et al., 2023;Dubey et al., 2024). Fine-tuning LLMs is indispensable for post-training techniques such as instruction tuning (Zhang et al., 2024) and reinforcement learning from human feedback (Ouyang et al., 2022), and adapting LLMs to specific downstream tasks (Hu et al., 2022). \n\nLLMs now have billions of parameters (Ope-nAI et al., 2023;Dubey et al., 2024;Touvron et al., 2023), making full fine-tuning computationally prohibitive. To address this, parameterefficient fine-tuning (PEFT) methods like adapterbased approaches (Houlsby et al., 2019), prompt tuning (Lester et al., 2021), and low-rank adaptation (LoRA) (Hu et al., 2022) have emerged. LoRA, a leading PEFT method, assumes weight changes during fine-tuning have a low-rank structure and uses two low-rank matrices to represent them. By freezing pre-trained weights and fine-tuning only these matrices, LoRA reduces trainable parameters and computational costs significantly. \n\nFull fine-tuning of LLMs often causes catastrophic forgetting, where the model loses pretrained knowledge. Recent studies (Biderman et al., 2024) show that LoRA suffers less from this issue compared to full fine-tuning, but it still exhibits significant forgetting (Biderman et al., 2024;Dou et al., 2024a;Wu et al., 2024a). Some recent studies (Yang et al., 2024b;Wang et al., 2024b;Dou et al., 2024a) have proposed several methods to mitigate the issue of catastrophic forgetting in LoRA.",
            "score": 0.5546294072699114,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1139
                },
                {
                    "start": 1142,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1632
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 127,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 390,
                    "end": 411,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 460,
                    "end": 477,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 727,
                    "end": 749,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 765,
                    "end": 786,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 1264,
                    "end": 1287,
                    "matchedPaperCorpusId": "269791237"
                },
                {
                    "start": 1407,
                    "end": 1430,
                    "matchedPaperCorpusId": "269791237"
                },
                {
                    "start": 1430,
                    "end": 1448,
                    "matchedPaperCorpusId": "271915471"
                },
                {
                    "start": 1448,
                    "end": 1465,
                    "matchedPaperCorpusId": "266755997"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.412109375
        },
        {
            "corpus_id": "268247412",
            "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
            "text": "Catastrophic Forgetting It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency. To mitigate catastrophic forgetting, particularly in the context of continual learning, researchers have explored three kinds of strategies. Exemplar replay involves preserving and revisiting key training examples to maintain model performance (He et al., 2019;Lopez-Paz and Ranzato, 2017). Regularization methods introduce regulation functions in addition to the loss function to constrain the learning process (Lin et al., 2023;Li and Hoiem, 2018). Architectural methods adjust the model's structure by adding parameters specific to new tasks or domains (Zhu et al., 2022). Our task setting is to train an LLM that can competently handle multiple domains concurrently, with minimal impairment to its generalist capabilities, differentiating from continual learning where the model is exposed to tasks sequentially, striving to prevent significant forgetting of earlier tasks (Zhu et al., 2022). \n\nInter-domain Confusion Furthermore, training a single LLM for multiple domains risks triggering inter-domain confusion where the LLM may not perform as well in each domain due to the blending of domain-specific knowledge (Wang et al., 2023b;Sheng et al., 2021). Therefore, some studies have been directed toward identifying commonalities across domains to maintain model performance while preserving the unique characteristics of each domain (Wang et al., 2023b;Sheng et al., 2021). In this paper, we propose to utilize Role Prompting to alleviate inter-domain confusion. \n\nRole Prompting Previous works found that role prompting can significantly improve the performance of LLMs. For example, Character.AI1 proposes a dialogue agent mimicking diversified figures, which can bring enriched user experience. Moreover, Wu et al. (2023b) found LLMs can effectively evaluate summarization results with diversified role prompts from varied perspectives. Kong et al. (2023) found role prompting can also boost the complex reasoning abilities of LLMs.",
            "score": 0.554083193697988,
            "section_title": "Related Work",
            "char_start_offset": 3976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1225
                },
                {
                    "start": 1228,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1710
                },
                {
                    "start": 1711,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2272
                }
            ],
            "ref_mentions": [
                {
                    "start": 590,
                    "end": 618,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 759,
                    "end": 778,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68212890625
        },
        {
            "corpus_id": "276408337",
            "title": "Mitigating Visual Knowledge Forgetting in MLLM Instruction-tuning via Modality-decoupled Gradient Descent",
            "text": "Catastrophic forgetting, a persistent challenge in continual learning, occurs when a model forgets previous knowledge while learning new tasks, reducing its performance on earlier tasks [55]. This issue has gained attention in LLMs due to the growing need for continual pre-training and instruction tuning [38,62]. MLLMs, which integrate multiple modalities through feature encoders projecting inputs into the LLM's token space, are also prone to catastrophic forgetting [68]. While several methods have attempted to address this issue by adapting continual learning techniques, ranging from fine-tuning and task-orthogonal gradient descent to knowledge distillation and replay-based strategies [47,62,71,73], these approaches often fall short of preserving rich visual representations. For instance, fine-tuning MLLMs on new tasks tends to overfit textual instructions, inadvertently suppressing visual features, and even parameter-efficient adaptations like LoRA have been shown to suffer from forgetting [7,32]. Model Tailor [73] addresses forgetting by adapting the LLM backbone across reasoning tasks but neglects the critical visual knowledge forgetting problem, which may lead to visual hallucination or deficiency problems while generalizing to various tasks [68]. In contrast, our method offers a more principled and synchronized approach to instruction tuning that jointly optimizes the alignment between the visual encoder and the LLM, effectively preserving pre-trained visual knowledge while mitigating the degradation of visual representation learning in previous works.",
            "score": 0.5540007334960637,
            "section_title": "Related Work 2.1 Visual Knowledge Forgetting in MLLMs",
            "char_start_offset": 5042,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1584
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.455078125
        },
        {
            "corpus_id": "235166235",
            "title": "Continual World: A Robotic Benchmark For Continual Reinforcement Learning",
            "text": "In Table 6, we recall for convenience the summary of our results for the experiments on the CW20 sequence (additional backward transfer results are described in Section E.1). We also recall that the methods were tuned with the objective of maximizing the final average performance. Our main findings are that most methods are efficient with mitigating forgetting but have an unsatisfying forward transfer. The best method, PackNet, has forward transfer close to Fine-tuning, which is a strong baseline for this metric. However, both fall below the reference value obtained from the analysis of the transfer matrix RT = 0.46. We recall that CW10 and CW20 sequences are defined in Section A.2. In the next sections, we discuss the results in more detail. We also provide the following visualizations: \n\n\u2022 Figure 14 -performance curves averaged over tasks. \n\n\u2022 Figure 15 -performance curves for the active task. \n\n\u2022 Figure 16 -performance curves for all tasks. Useful for qualitative studies of transfer and forgetting. \u2022 Figure 17 -visualization of forward transfer for each task. \n\n\u2022 Figure 18 -forgetting curves averaged over tasks. 6 by presenting more detailed forgetting results, see also Figure 18 and Figure 16. The latter is convenient to observe the evaluation dynamics of each task. Fine-tuning and AGEM exhibit rapid catastrophic forgetting after task switch (see Figure 16). It is expected for the former, but quite surprising for the latter. We conjecture possible reasons in Section C.5. Some mild forgetting can also be observed for Perfect Memory (it is not directly observed in the metric due to the poor training but can be visible on the graphs). The rest of the methods are quite efficient in mitigating forgetting. This also includes the very basic L2 method (which is at the cost of poor transfer, however).",
            "score": 0.5537694138090199,
            "section_title": "E CW20 results",
            "char_start_offset": 58139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 798
                },
                {
                    "start": 801,
                    "end": 853
                },
                {
                    "start": 856,
                    "end": 908
                },
                {
                    "start": 911,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1078
                },
                {
                    "start": 1081,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1216
                },
                {
                    "start": 1217,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1827
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1514892578125
        },
        {
            "corpus_id": "271543955",
            "title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning",
            "text": "We verify the effectiveness of MoFO on instruction finetuning and continual fine-tuning. We use Llama-2-7B (Touvron et al., 2023), Gemma-2B-IT (Team et al., 2024), and TinyLlama-1.1B (Zhang et al., 2024b) as our base models. The instruction fine-tuning datasets cover questionanswer pairs from different domains like mathematical reasoning and medical knowledge. Specifically, the datasets include: MetaMathQA (Yu et al., 2024a) and PMC-LLaMA-Instructions (Wu et al., 2024). We randomly sample 39.5K and 51K instances from these datasets, respectively, for training the LLMs. Additionally, We investigate the performance of MoFO in the continual fine-tuning scenario by implementing our approach on the TRACE benchmark dataset (Wang et al., 2023b). \n\nEvaluation metrics for instruction fine-tuning. We employ widely used benchmarks to assess the performance and potential forgetting effects on the general capabilities of LLMs after instruction fine-tuning. These benchmarks include MMLU (Hendrycks et al., 2021) (0-shot) for factual knowledge; ARC-Challenge, ARC-Easy (Clark et al., 2018), and HellaSwag (Zellers et al., 2019) (0-shot) for commonsense reasoning (CR); GSM8K (Cobbe et al., 2021) (5-shot) for mathematical reasoning; HumanEval (HEval) (Chen et al., 2021) (pass@10) for code generation; PubMedQA (Jin et al., 2019), MedMCQA (Pal et al., 2022), and MedQA (Jin et al., 2021) (0-shot) for medical question answering (MedQ)4 ; IFEval (0-shot) for instruction following. \n\nEvaluation metrics for continual fine-tuning.",
            "score": 0.5531201366425335,
            "section_title": "Experimental Settings",
            "char_start_offset": 18129,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 748
                },
                {
                    "start": 751,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1528
                }
            ],
            "ref_mentions": [
                {
                    "start": 456,
                    "end": 473,
                    "matchedPaperCorpusId": "269136910"
                },
                {
                    "start": 988,
                    "end": 1011,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1105,
                    "end": 1126,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 1311,
                    "end": 1329,
                    "matchedPaperCorpusId": "202572622"
                },
                {
                    "start": 1369,
                    "end": 1386,
                    "matchedPaperCorpusId": "221970190"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1370849609375
        },
        {
            "corpus_id": "273502371",
            "title": "Opportunities and Challenges of Generative-AI in Finance",
            "text": "In scenarios, where utilizing LLMs out of the box is not working for the task at hand, then users could leverage finetuning to train the LLMs to perform better at the specific tasks the user has in mind. At a high level, fine-tuning techniques can be divided into the following categories. \n\n1) Instruction Fine-tuning: In instruction fine-tuning [50], the pre-trained LLMs are further trained on a labeled set of prompts and answer pairs. This newly trained LLM is tuned to answer specifically to specific kinds of instruction/prompt. As the name suggests, the training data need to be in the form of instructions. So users need to either collect the training data in the form of instructions or could leverage prompt template libraries like prompt-engine-py or dynamic prompts to take normal datasets and convert them into instruction datasets for fine-tuning. \n\n2) Task specific Fine-tuning: This technique [51] involves the users fine-tuning a pre-trained Gen-AI model to perform a specific kind of task in mind. For example, the users might fine-tune the pre-trained LLM to perform sentiment detection given an input prompt. It involves very few examples for the LLM to train on, but still requires a decent amount of compute as the entire model needs to be loaded into memory for the training part. \n\nTask specific fine-tuning [51] is prone to exhibit a phenomenon called catastrophic forgetting. In catastrophic forgetting, the underlying LLM has forgotten the knowledge of the world it had obtained as part of its pre-training and its performance on the other tasks after task specific fine-tuning is much worse than its performance on the same tasks before fine-tuning. In order to circumvent the issue of catastrophic forgetting, users can employ multi-task instruction fine-tuning or employ parameter efficient fine-tuning described in section IV-B3. \n\n3) Parameter Efficient Fine-tuning: Instruction Fine-Tuning IV-B1 or Task specific fine-tuning IV-B2 are resource intensive and are plagued with catastrophic forgetting. In order to circumvent both these issues users can leverage Parameter Efficient Fine-tuning (PEFT) [52].",
            "score": 0.5529943225280588,
            "section_title": "B. Fine-tuning",
            "char_start_offset": 15297,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 862
                },
                {
                    "start": 865,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2138
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309326171875
        },
        {
            "corpus_id": "276287820",
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "text": "RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts. However, it suffers from conditional memorization bias and canonical answer overfitting. On the other hand, PA-RAG uses context augmentation and answer paraphrasing to address these issues. Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting. The Self-Synthesized Rehearsal (SSR) (Gupta et al., 2024;Huang et al., 2024) framework uses the LLM to generate synthetic rehearsal data, reducing reliance on stored instances.",
            "score": 0.5526449852979607,
            "section_title": "Related Work",
            "char_start_offset": 7826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1009
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61474609375
        },
        {
            "corpus_id": "268513020",
            "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
            "text": "Forgetting Rate.For a further catastrophic forgetting analysis, several methods (Kemker et al., 2018;Luo et al., 2023) quantify the forgetting issue by evaluating performance decrease as training incrementally.Consequently, we propose a forgetting rate defined as: where R * i is the initial Rouge-L of task i after training on the corresponding task, and R \u22121 i is the final Rouge-L of task i in the last training stage.\n\nWe evaluate the forgetting rate with Curriculum training order and report the results of No Replay and InsCL in Figure 4.It can be inferred that there is no inevitable relationship between task order and forgetting rate.For tasks that require complex reasoning, Program Execution and Code severely suffer from forgetting with the No Replay setting.Additionally, a large training data scale does not necessarily lead to a small forgetting rate.For example, Classification and Generation are the top-2 tasks with large training data and exhibit smaller forgetting rates, while Program Execution with the third largest dataset suffers from the largest forgetting rate.With our InsCL, the forgetting rates of almost all tasks are below 20%, which means that most of the previous knowledge is preserved.Forgetting Category.When all the tasks have been trained under the No Replay setting, we collect previous tasks' instances with a decreased Rouge-L, called forgetting instances.We randomly sampled 200 forgetting instances from each previous task, manually analyzing the forgetting category for a detailed conclusion.We divide forgetting instances into two categories based on the instruction's following ability: (1) Instruction-Related: The output is relevant to the instruction, according to the space defined by the instruction.This category indicates LLMs do not forget the corresponding instruction following ability.(2) Instruction-Unrelated: The output is unrelated to the instruction.We demonstrate representative cases and respective explanations in Appendix A.3.",
            "score": 0.5526426933572941,
            "section_title": "Forgetting Analysis",
            "char_start_offset": 20365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 16
                },
                {
                    "start": 16,
                    "end": 210
                },
                {
                    "start": 210,
                    "end": 421
                },
                {
                    "start": 423,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 643
                },
                {
                    "start": 643,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 1088
                },
                {
                    "start": 1088,
                    "end": 1221
                },
                {
                    "start": 1221,
                    "end": 1241
                },
                {
                    "start": 1241,
                    "end": 1398
                },
                {
                    "start": 1398,
                    "end": 1537
                },
                {
                    "start": 1537,
                    "end": 1752
                },
                {
                    "start": 1752,
                    "end": 1843
                },
                {
                    "start": 1843,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 1993
                }
            ],
            "ref_mentions": [
                {
                    "start": 80,
                    "end": 101,
                    "matchedPaperCorpusId": "22910766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2081298828125
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "Yang et al. (2024) investigated self-distillation as a way to alleviate distribution shifts, improving model quality during SFT while improving generalization across tasks. Our self-data distillation method builds on these techniques by leveraging the original unpruned model to generate a distilled dataset for finetuning the pruned model. This enhances semantic alignment and mitigates the quality degradation seen after pruning. Furthermore, while our approach can be combined with KD methods to enhance generalization and recover quality while lowering computational costs, we leave the exploration of such combinations for future work. \n\nCatastrophic Forgetting One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;Korbak et al., 2022). Regularization techniques such as Elastic Weight Consolidation (Kirkpatrick et al., 2017) aim to alleviate this by controlling parameter updates, but are taskdependent and require careful tuning (Huang et al., 2021). Architecture-based methods, which allocate separate parameters for each task (Razdaibiedina et al., 2023), preserve task-specific knowledge but add complexity and overhead, reducing the overall efficiency of model compression. \n\nReplay-based techniques (Ostapenko et al., 2022;Rolnick et al., 2019;Sun et al., 2019) store data subsets from previous tasks for rehearsal, either through direct storage or synthesis via generative models. However, these methods demand substantial memory to store large datasets and are often impractical due to privacy concerns or lack of access to past data. Our self-data distilled fine-tuning approach avoids these challenges by aligning the fine-tuning dataset with the original model's learned distribution, preserving knowledge across tasks without requiring new parameters or architectural changes. This method offers a robust solution for mitigating catastrophic forgetting while maintaining model quality after pruning.",
            "score": 0.5523957603656091,
            "section_title": "RELATED WORK",
            "char_start_offset": 32757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 865,
                    "end": 885,
                    "matchedPaperCorpusId": "249921347"
                },
                {
                    "start": 950,
                    "end": 976,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1082,
                    "end": 1102,
                    "matchedPaperCorpusId": "233210110"
                },
                {
                    "start": 1357,
                    "end": 1381,
                    "matchedPaperCorpusId": "250265129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.640625
        },
        {
            "corpus_id": "266374841",
            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
            "text": "Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten. \n\nBut surprisingly, we discover that LLMs are much more likely to forget unsafe examples than other downstream examples after safety finetuning. Such results may be different from the conventional wisdom that all previously learned examples are expected to be forgotten similarly during sequential finetuning, due to task switching (Kemker et al., 2018). Furthermore, the discrepancies in forgetting are significantly more prominent in larger language models (e.g. LLaMA 7B) compared to smaller ones (e.g. GPT-2 M). We find this property holds consistent across three notions of safety: unbiasedness, non-toxicity, and harmlessness. \n\nInspired by this selective forgetting behavior, we propose the ForgetFilter algorithm, where we attempt to filter out unsafe examples during finetuning based on the rate at which they are forgotten after reviewing safe examples. ForgetFilter can flexibly screen implicit unsafe examples based on data, while many existing filters (Korbak et al., 2023;Askell et al., 2021;Gehman et al., 2020) are constrained to only toxic content. We compare ForgetFilter with other defense strategies such as example replay (Chaudhry et al., 2019) and moral self-correction (Ganguli et al., 2023). Experiments show our ForgetFilter algorithm outperforms these baseline methods in terms of both safety metrics and downstream task performances. Finally, we evaluate the long-term safety of LLMs by considering a challenging \"interleaved training\" setup where a model is alternately finetuned on safe and unsafe examples. We find that ForgetFilter again provides the strongest long-term protection against learning unsafe examples. \n\nIn summary, our contributions are: 1. We focus on the safety issue of LLMs that are released to the public for customized fintuning. We study the impact of unsafe examples in finetuning with noisy downstream data and then investigate the forgetting patterns of LMs at different scales during subsequent safety finetuning. We confirm that safety finetuning will lead to forgetting of important downstream task data despite the recovery of model safety.",
            "score": 0.5523342001755531,
            "section_title": "Finetuning Safety Finetuning",
            "char_start_offset": 4459,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 537
                },
                {
                    "start": 538,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1830
                },
                {
                    "start": 1833,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2154
                },
                {
                    "start": 2155,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 515,
                    "end": 536,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 1148,
                    "end": 1169,
                    "matchedPaperCorpusId": "257020046"
                },
                {
                    "start": 1326,
                    "end": 1349,
                    "matchedPaperCorpusId": "67855851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5771484375
        },
        {
            "corpus_id": "271534130",
            "title": "The Emerged Security and Privacy of LLM Agent: A Survey with Case Studies",
            "text": "During the training process of LLMs, limitations in the data and learning algorithms can introduce technical vulnerabilities [107], impeding the generation of accurate and reliable information. \n\n\u2022 Hallucination. \n\nThe contemporary conception of hallucination in LLM agents, as delineated in the research by [39], is identified as instances where the output produced by these models is either incongruous or unreliable to the input or source content provided. The phenomenon of hallucinations in LLM agents is a complex issue stemming from multiple stages of the model's development process, including the nature of training data, the architectural design of the model, and the strategies employed during decoding. Misinformation and biases in the training data can lead to the generation of inaccurate or biased outputs, which in turn result in different types of hallucinations [48]. Furthermore, flaws in the model's architecture, such as limited directional representation and issues with attention mechanisms, along with exposure bias, further contribute to the occurrence of hallucinations [55]. Additionally, the randomness inherent in the decoding algorithms of these models can also lead to hallucinations, especially as this randomness increases [8]. \n\nCatastrophic forgetting is a significant challenge encountered during the LLM agents fine-tuning and in-context learning processes. This phenomenon occurs when a large language model is fine-tuned on a small, specific dataset, causing it to overfit to this new data and, as a result, lose its previously acquired performance on other tasks [34,109,120]. [61] discovers that catastrophic forgetting is significantly influenced by factors such as model size, architectural design, and the methods employed in continual fine-tuning and instruction tuning. As the scale of LLM increases, catastrophic forgetting tends to become more severe. Moreover, the architectural design of the model, particularly those focusing on decoder-only structures, can influence the extent of catastrophic forgetting [127]. Additionally, during the process of continual instruction adjustment, the lack of effective regularization strategies or failure to balance new and old information can accelerate forgetting [24,62]. Introducing more instructional tasks in continual training typically leads to more pronounced forgetting [72]. \u2022 Misunderstanding.",
            "score": 0.5518387570426125,
            "section_title": "Technical Vulnerabilities.",
            "char_start_offset": 20635,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 196,
                    "end": 212
                },
                {
                    "start": 215,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1101
                },
                {
                    "start": 1102,
                    "end": 1260
                },
                {
                    "start": 1263,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2262
                },
                {
                    "start": 2263,
                    "end": 2373
                },
                {
                    "start": 2374,
                    "end": 2393
                }
            ],
            "ref_mentions": [
                {
                    "start": 125,
                    "end": 130,
                    "matchedPaperCorpusId": "263887013"
                },
                {
                    "start": 880,
                    "end": 884,
                    "matchedPaperCorpusId": "235829052"
                },
                {
                    "start": 1603,
                    "end": 1607,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 2254,
                    "end": 2258,
                    "matchedPaperCorpusId": "222133066"
                },
                {
                    "start": 2258,
                    "end": 2261,
                    "matchedPaperCorpusId": "249713696"
                },
                {
                    "start": 2368,
                    "end": 2372,
                    "matchedPaperCorpusId": "258426472"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42529296875
        },
        {
            "corpus_id": "273022799",
            "title": "FlipGuard: Defending Preference Alignment against Update Regression with Constrained Optimization",
            "text": "A closely related research topic to our work is catastrophic forgetting in sequential learning (Robins, 1995;Atkinson et al., 2018) and continual learning (Kirkpatrick et al., 2017;Nguyen et al., 2019), a phenomenon in machine learning where a model, when exposed to new data, tends to forget previously acquired knowledge. Another related topic is alignment tax (Bai et al., 2022), which refers to the performance degradation of LLM on standard knowledge and reasoning benchmarks. Model update regression differs in that we focus on how the model performs on the same task after updates.",
            "score": 0.5501918809252864,
            "section_title": "Related Work",
            "char_start_offset": 5162,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 588
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 109,
                    "matchedPaperCorpusId": "22882861"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4248046875
        },
        {
            "corpus_id": "276259004",
            "title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation",
            "text": "Yet, the root causes of this performance decline and potential mitiga-tion strategies remain under-explored. \n\nIn this study, we aim to demystify the factors contributing to the performance degradation of shorttext tasks after context window extension. Our analysis identifies two critical factors: distribution drift and catastrophic forgetting. The key findings of our work include: (1) Continual pretraining seeks to recover the original model's internal distribution, but the restoration is inherently imperfect. (2) Distribution shift in hidden states potentially leads to performance degradation. (3) During continual pre-training, the performance on short-text tasks initially improves but subsequently declines as training progresses, highlighting the presence of catastrophic forgetting. (4) Replaying short text data is effective in mitigating forgetting and improving performance stability. \n\nBased on our observations, we propose a novel approach called Long Context Pre-training with Restoration Distillation (LongReD) to mitigate the degradation in short-text capabilities of longcontext LLMs. The central idea is that the shorttext capacities of the extended model can be better preserved if it accurately simulates the original distributions before extension. To achieve this, in addition to typical long-text training, we introduce a short-text distillation objective, which employs the original model as a teacher to distill its hidden states on short texts into the extended model. This training objective minimizes distribution drift and alleviates catastrophic forgetting. Moreover, we propose a short-to-long distillation training objective to bridge the gap between short-text distillation and long-text training. In this setup, the original and extended models are fed with normal positional indices and skipped positional indices, respectively. By applying the distillation on the output distributions of the last layer, the short-text capacities can be effectively transferred and integrated into long-text processing. \n\nTo the best of our knowledge, this work represents the first systematic analysis of the reasons behind the performance degradation of long-context LLMs on short-text tasks. Furthermore, we propose a general method to mitigate this short-text degradation. To assess the effectiveness of our method, we extend the context window of Llama-3-8B and Mistral-7B-v0.3 and assess their performance on both short-text and long-text tasks.",
            "score": 0.550029318500569,
            "section_title": "Introduction",
            "char_start_offset": 1736,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 252
                },
                {
                    "start": 253,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 796
                },
                {
                    "start": 797,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2219
                },
                {
                    "start": 2220,
                    "end": 2301
                },
                {
                    "start": 2302,
                    "end": 2476
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42236328125
        },
        {
            "corpus_id": "272753147",
            "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data",
            "text": "We validate whether LLMs can retain their open domain knowledge and problem-solving abilities, thereby avoiding catastrophic forgetting, after undergoing task-specific adaptation. We do so by evaluating models that have been tuned with the maximum amount of synthetic data, specifically SFT-S+10k on CLUTRR and SFT-S+5k on StepGame, on MMLU (Hendrycks et al., 2021) Results presented in Table 1 show minor variations in performance and, in some cases, improvements-as observed for SFT-S+10k on CLUTTR during MMLU-across most benchmarks between the original model without SFT and those tuned on the two reasoning tasks. These observations suggest that task-specific adaptation of LLMs for reasoning tasks is feasible without sacrificing factual knowledge and generalization abilities, provided that the synthetic data for SFT is carefully curated.",
            "score": 0.5499337605749663,
            "section_title": "LLM Benchmarks Evaluation",
            "char_start_offset": 12501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 846
                }
            ],
            "ref_mentions": [
                {
                    "start": 341,
                    "end": 365,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.312744140625
        },
        {
            "corpus_id": "270562315",
            "title": "Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models",
            "text": "While our work provides an in-depth analysis of how to enhance model's factuality on OOD queries, it is still subject to the following limitations: (1) Our research primarily concentrates on how to enhance the factuality of LLMs through fine-tuning.However, it naturally raises concerns about whether the same process could adversely affect other capabilities that LLMs have already developed, such as mathematical reasoning, code completion and safety preservation.Future research could systematically investigate the changes in various capabilities of LLMs during the fine-tuning process.\n\n(2) Our work does not delve into the internals of LLMs.It would be interesting to develop a more fine-grained understanding of behaviour changes within the LLMs induced by fine-tuning.For instance, is there a tiny region within LLMs focusing on the concept of factuality, and do the primary changes in parameters occur in this area?We leave the exploration for future work.",
            "score": 0.5468515988255586,
            "section_title": "Limitations",
            "char_start_offset": 26061,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 590
                },
                {
                    "start": 592,
                    "end": 647
                },
                {
                    "start": 647,
                    "end": 776
                },
                {
                    "start": 776,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.172119140625
        },
        {
            "corpus_id": "271218270",
            "title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models",
            "text": "Therefore, in this work, we focus on alleviating catastrophic forgetting during continual instruction tuning, particularly with minimal data retention and its interplay with PEFT. \n\nWe propose a novel continual instruction tuning method, SwitchCIT, that alleviates forgetting of previously seen tasks by introducing a switch network to identify a task for a user query, leveraging the clustering phenomenon of task-specific instruction vectors (Wu and Varshney, 2024). For each new task, we fine-tune the task performance by including extra parameters created by PEFT methods such as LoRA (a self-expansion process) in addition to a pre-trained large base model making the method more practical. \n\nOur method represents a distinct paradigm compared to the current mainstream machine learning (ML) system design, where a single large model simultaneously or continually learns to perform as many tasks as possible. This difference may encourage a rethinking or reformulation of existing ML system designs. \n\nCatastrophic forgetting in neural networks is related to the palimpsest phenomenon that new memories rapidly overwrite old ones (Zenke and Laborieux, 2024). SwitchCIT may be considered as a way to avoid the need to overwrite the old memories of previously learned tasks by introducing extra parameters for a new task. Moreover, there exists a line of methods inspired by synaptic consolidation in brains that reduces the learning rate on specific weights based on their importance to previously encountered tasks. Our method has the advantage of enabling the full tuning of all weights to adapt to a specific task without restricting any particular set of weights. \n\nA recent study (Anonymous, 2024b) attributes forgetting from fine-tuning a large model to the learned bias of its alignment toward a specific task but without fundamentally altering the model's transferable knowledge across tasks. Our proposed paradigm aligns well with this perspective: we fine-tune separate models for distinct tasks to prevent potential forgetting of prior task alignments and avoid conflicts among alignments for different tasks. While a limitation of this paradigm is its ineffectiveness in leveraging transferable knowledge across tasks, this concern is alleviated in the era of large models; we expect most essential transferable knowledge to be acquired during the pre-training stage, reducing the need for further obtaining it during continual fine-tuning. \n\nWe summarize our contributions as follows:",
            "score": 0.5466341753649628,
            "section_title": "Introduction",
            "char_start_offset": 2044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 182,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 695
                },
                {
                    "start": 698,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1004
                },
                {
                    "start": 1007,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2456
                },
                {
                    "start": 2459,
                    "end": 2501
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.233154296875
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "To understand the impact of distribution shift on catastrophic forgetting, we conducted experiments using the baseline model (i.e., Llama3.1-8B Instruct) and its pruned variants fine-tuned with both supervised fine-tuning (SFT) and selfdata distilled fine-tuning (Self-Data FT). Specifically, we pruned 6 decoder layers, reducing the model from 32 to 26 layers, and evaluated the models on the GSM8k dataset. \n\nFor these experiments, we generated model responses using the baseline and pruned variants on the GSM8k dataset to capture how the distribution shift affects reasoning tasks post-pruning. Following Yang et al. (2024), to quantify the distribution shift, we employed Sentence-BERT (Reimers & Gurevych, 2019) to derive sentence embeddings from the model-generated responses. Then, similar to the method proposed by Zhang et al. (2023), we calculated the cosine similarity between the sentence embeddings of the pruned models and those generated by the original Llama3.1-8B Instruct model. \n\nA lower cosine similarity score indicates a greater distribution shift, suggesting a higher risk of catastrophic forgetting. Conversely, higher similarity scores indicate better preservation of the original model's knowledge and a lower risk of forgetting. These metrics allowed us to assess the extent to which SFT and Self-Data FT preserved the learned distribution of the base model, with the latter showing superior performance in mitigating forgetting, as detailed in our ablations in Section 3.",
            "score": 0.54647804390819,
            "section_title": "C EXPERIMENTAL SETUP FOR UNDERSTANDING CATASTROPHIC FORGETTING",
            "char_start_offset": 43679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1500
                }
            ],
            "ref_mentions": [
                {
                    "start": 609,
                    "end": 627,
                    "matchedPaperCorpusId": "267769989"
                },
                {
                    "start": 691,
                    "end": 717,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2666015625
        },
        {
            "corpus_id": "274234789",
            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
            "text": "Large Language Models (LLMs) have emerged as powerful tools with significant potential in various domains, such as finance (Lee et al., 2024), medical diagnostics (Karabacak and Margetis, 2023), personalized education (Ling et al., 2023), content creation (Leiker et al., 2023), and storytelling (Xie et al., 2023). Built on transformer-based architectures with billions of parameters, these models undergo extensive training on large-scale datasets, which equip them with capabilities for handling complex natural language processing tasks. However, these pre-trained models often face challenges in domains that demand specialized knowledge, such as medical fields (Jin et al., 2021;Pal et al., 2022) and legal services (Yue et al., 2023). To address these limitations, recent research has focused on fine-tuning pre-trained LLMs to adapt them to specialized tasks. This process involves training a pre-trained model on a smaller task-specific dataset while retaining most of its learned parameters. Common fine-tuning methods such as Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2024), supervised fine-tuning (Mecklenburg et al., 2024), and DPO (Rafailov et al., 2023) have been widely adopted for efficiently adapting LLMs to specialized tasks. Studies have demonstrated that finetuning LLMs in domain-specific tasks, especially those not observed during the pre-training phase, significantly improves their performance in those areas (Zhang et al., 2024;Jeong, 2024). \n\nDespite these advantages, fine-tuning LLMs also introduces several challenges. Notably, prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024), ii) deactivation of safety filters initially embedded in LLMs (Kumar et al., 2024), making the models vulnerable to generating harmful content, and iii) increased risk of privacy breaches due to the higher extraction rate of finetuning data (Singh et al., 2024;Zeng et al., 2024).",
            "score": 0.546263736501502,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 2038
                }
            ],
            "ref_mentions": [
                {
                    "start": 218,
                    "end": 237,
                    "matchedPaperCorpusId": "259089245"
                },
                {
                    "start": 296,
                    "end": 314,
                    "matchedPaperCorpusId": "260125086"
                },
                {
                    "start": 667,
                    "end": 685,
                    "matchedPaperCorpusId": "221970190"
                },
                {
                    "start": 685,
                    "end": 702,
                    "matchedPaperCorpusId": "247763070"
                },
                {
                    "start": 1169,
                    "end": 1192,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1460,
                    "end": 1480,
                    "matchedPaperCorpusId": "268032247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57421875
        },
        {
            "corpus_id": "276725054",
            "title": "LLM Post-Training: A Deep Dive into Reasoning Large Language Models",
            "text": "In parallel, process [327,328] vs. outcome reward optimization [329] (Figure 7f) remains an open question: while process-based rewards help guide incremental improvements, outcome-focused metrics are simpler but may not capture crucial intermediate decision-making steps. Beyond reward structure, fine-tuning LLMs on new tasks still encounter issues like catastrophic forgetting [330] and potential data leakage [331,332], underscoring the need for parameter-efficient methods [60] and privacy-preserving strategies such as differential privacy [333] and federated learning [334]. Human feedback, while central to alignment, is inherently costly and limited in scope; methods like Constitutional AI [53] and RLAIF [95] seek to automate parts of this oversight, though they introduce fresh concerns about bias calibration [335] and model self-consistency [184]. Finally, test-time scaling [111] and dynamic reasoning [336] frameworks pose further challenges: models must learn when to allocate more computation for complex queries, how to adapt verification modules [337] efficiently, and how to maintain robust performance even when facing adversarial inputs. These converging research directions-spanning reward modeling, decoding strategies, interpretability, personalization, and safe fine-tuning-highlight the multifaceted role of RL in LLMs and collectively shape the future trajectory of large-scale language model development. Below, we delve into some of these directions in greater detail. Fine-tuning challenges. Fine-tuning remains one of the most direct post-training methods to adapt LLMs to specific tasks or domains, yet it faces several open challenges. One fundamental issue is catastrophic forgetting -when updating an LLM on new data causes it to lose or degrade previously learned capabilities. Even advanced PEFT methods like LoRA [60], which greatly reduce the number of trainable weights, do not fully solve this problem [330]. Future work can explore better continual learning strategies and regularization techniques so that models can acquire new skills without erasing old ones. For example, new fine-tuning algorithms (e.g. CURLoRA [330]) explicitly aim to stabilize training and preserve prior knowledge while adding new tasks.",
            "score": 0.546106252106626,
            "section_title": "Future Directions",
            "char_start_offset": 93837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1522
                },
                {
                    "start": 1523,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2151
                },
                {
                    "start": 2152,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 26,
                    "end": 30,
                    "matchedPaperCorpusId": "264306288"
                },
                {
                    "start": 63,
                    "end": 68,
                    "matchedPaperCorpusId": "267750213"
                },
                {
                    "start": 412,
                    "end": 417,
                    "matchedPaperCorpusId": "259243928"
                },
                {
                    "start": 417,
                    "end": 421,
                    "matchedPaperCorpusId": "259108266"
                },
                {
                    "start": 854,
                    "end": 859,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1065,
                    "end": 1070,
                    "matchedPaperCorpusId": "271963324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57080078125
        },
        {
            "corpus_id": "244119123",
            "title": "Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning",
            "text": "Inspired by the great success of the pretraining/finetuning paradigm on a broad range of tasks (Peters et al., 2018;Devlin et al., 2019), a line of research inserts an intermediate task between pretraining and fine-tuning on a target task (Phang et al., 2018;Wang et al., 2019a;Pruksachatkun et al., 2020). However, Wang et al. (2019a) found that MT used as an intermediate task led to performance degeneration in various target tasks, such as natural language inference and sentiment classification. 2 They argue that the considerable difference between MLM pretraining and MT causes catastrophic forgetting (CF). Pruksachatkun et al. (2020) suggest injecting the MLM objective during intermediate training as a possible way to mitigate CF, which we empirically test in this paper.",
            "score": 0.5454245527692643,
            "section_title": "MT as an Intermediate Task",
            "char_start_offset": 7547,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 782
                }
            ],
            "ref_mentions": [
                {
                    "start": 95,
                    "end": 116,
                    "matchedPaperCorpusId": "3626819"
                },
                {
                    "start": 259,
                    "end": 278,
                    "matchedPaperCorpusId": "182952400"
                },
                {
                    "start": 278,
                    "end": 305,
                    "matchedPaperCorpusId": "220045835"
                },
                {
                    "start": 316,
                    "end": 335,
                    "matchedPaperCorpusId": "182952400"
                },
                {
                    "start": 615,
                    "end": 642,
                    "matchedPaperCorpusId": "220045835"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282470703125
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "In this paper, we perform a systematic evaluation of catastrophic forgetting during the fine-tuning of language models for downstream tasks. Our findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness. To enhance performance on target task while maintaining LMs' broad capabilities, we propose a plug-and-play strategy, SDFT, to reduce distribution shift and miti-  gate catastrophic forgetting. Extensive experiments show that SDFT effectively diminishes forgetting and delivers comparable or superior performance to vanilla fine-tuning on targeted tasks. \n\nOur study is subject to certain limitations. Owing to constraints in computational resources, most of our experiments are based on the Llama-2-7bchat model with LoRA. Further investigations involving larger models and full fine-tuning remain to be explored. Furthermore, our safety evaluations are limited to the Advbench dataset and fixed adversarial suffixes, leaving the robustness against other jailbreaking strategies for future work.",
            "score": 0.5448322206375934,
            "section_title": "Conclusions and Limitations",
            "char_start_offset": 23892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 683
                },
                {
                    "start": 686,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 943
                },
                {
                    "start": 944,
                    "end": 1125
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75244140625
        },
        {
            "corpus_id": "269282911",
            "title": "When Life gives you LLMs, make LLM-ADE: Large Language Models with Adaptive Data Engineering",
            "text": "Direct improvements to the core structure of LLMs can yield benefits across various applications, reducing the need for extensive downstream task-specific tuning.However, continuous domain specific training, risks diminishing the model's broad applicability and is vulnerable to double descent and catastrophic forgetting, where model performance degrades or essential knowledge is lost (Belkin et al., 2019;Lopez-Paz and Ranzato, 2017).Notably, data duplication during training exacerbates performance issues (Hernandez et al., 2022), and",
            "score": 0.5444268172606254,
            "section_title": "Introduction",
            "char_start_offset": 1872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 437
                },
                {
                    "start": 437,
                    "end": 539
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 408,
                    "matchedPaperCorpusId": "198496504"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "270559235",
            "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?",
            "text": "Training steps and the forgetting of acquired factual knowledge have a power-law relationship The exponential trend of forgetting has been reported in various aspects of LLM training, including memorization in pretraining [46] and task performances in continual learning [33,39]. Motivated by this, we investigate whether the exponential trend of forgetting persists in the context of factual knowledge acquisition in LLM pretraining. Figure 4 illustrates the trend of retainability against the training steps past the local acquisition maxima. We find that the trend of R(p, t) against log(t) fits a linear function very well (R 2 > 0.80 for memorization and semantic generalization, and R 2 > 0.65 for compositional generalization). This trend is persistent across all acquisition depths, and all training conditions ( \u00a7E.4 and \u00a7E.5). Guided by empirical observations, we model the trend of forgetting using a power-law model in further investigations. \n\nHow quickly is the acquired factual knowledge lost? The absolute value of the slope of the fitted lines in Figure 4 can be interpreted as the decay constant (a) of retainability, formally, \n\nThus, the measured decay constant represents how fast (in terms of fraction) the model loses the improvement of log probability. Table 2 shows the decay constants of retainability measured for three OLMo-7B intermediate checkpoints, for duplication and paraphrase injection scenarios. \n\nThere are several observations in Table 2. First, the forgetting in compositional generalization is slower (the decay constant a is smaller) than in memorization and semantic generalization. Combined with the observations in previous sections, the acquisition of compositional generalization accumulates most slowly but is more robust to forgetting. Second, the forgetting tends to be slower in the paraphrase injection scenario compared to the duplication injection scenario. This finding will be further discussed in \u00a74.4, regarding the importance of deduplicating training data. Finally, the decay constants are similar for the two earlier checkpoints but smaller for the late checkpoint in the duplication injection scenario. We demonstrate that this is due to the reduced learning rate from  learning rate scheduling (Appendix Table 5), as the decay constants show no decrease for the later checkpoint when each checkpoint is trained with the same constant learning rate (Appendix Table 9).",
            "score": 0.5422801592304101,
            "section_title": "Forgetting in factual knowledge acquisition",
            "char_start_offset": 17531,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 954
                },
                {
                    "start": 957,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1432
                },
                {
                    "start": 1435,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1911
                },
                {
                    "start": 1912,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2164
                },
                {
                    "start": 2165,
                    "end": 2430
                }
            ],
            "ref_mentions": [
                {
                    "start": 222,
                    "end": 226,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 275,
                    "end": 278,
                    "matchedPaperCorpusId": "220525308"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267578125
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "Although recent research (Luo et al., 2023b;Scialom et al., 2022) highlights the significance of continual learning for LMs, there are scant feasible solutions for LLMs. In this paper, we conduct a comprehensive evaluation of the catastrophic forgetting issue during the fine-tuning of LLMs and propose a simple yet effective strategy specifically designed for LLMs. \n\nAlignment As the capabilities of Large Language Models (LLMs) expand, so does the potential for generating toxic content, engendering significant safety concerns (Perez et al., 2022;Ganguli et al., 2022). In response, various strategies have been proposed to align LLMs with human ethical standards and prevent the generation of toxic content. Prevalent methods including instruction tuning (Ouyang et al., 2022a;Touvron et al., 2023), reinforcement learning from human feedback (Ouyang et al., 2022a;Bai et al., 2022), and self-alignment techniques (Sun et al., 2023). Employing these alignment techniques, LLMs strike a dedicate tradeoff between utility and safety (Bianchi et al., 2023;Qi et al., 2024). While these methods have demonstrated efficacy in safety alignment, they do not cover further risks that arise from fine-tuning. Recent research reveals that even fine-  tuning with benign data can lead to compromised safety (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). Our proposed strategy can effectively mitigate such safety degradation. \n\nPrompting Based Learning Recently, the use of prompting in LLMs to generate responses for model training has garnered significant interest. Approaches like self-instruct (Wang et al., 2022) and WizardLM (Xu et al., 2024) utilize the generated responses for supervised fine-tuning, with the latter employing GPT-4 as the generator. Other methods, such as Self-Refine (Madaan et al., 2024) and Self-Reward (Yuan et al., 2024), use the responses as feedback to iteratively refine the model's outputs.",
            "score": 0.5421910371774692,
            "section_title": "Related Work",
            "char_start_offset": 5852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 366
                },
                {
                    "start": 369,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1948
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 65,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 760,
                    "end": 782,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 848,
                    "end": 870,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 919,
                    "end": 937,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 1058,
                    "end": 1074,
                    "matchedPaperCorpusId": "263671523"
                },
                {
                    "start": 1301,
                    "end": 1318,
                    "matchedPaperCorpusId": "263671523"
                },
                {
                    "start": 1654,
                    "end": 1671,
                    "matchedPaperCorpusId": "258298159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2235107421875
        },
        {
            "corpus_id": "277501673",
            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
            "text": "Catastrophic forgetting, a phenomenon first identified by (McCloskey and Cohen, 1989), remains a fundamental challenge in sequential learning tasks for neural networks. When models are trained on multiple tasks in sequence, they tend to overwrite previously acquired knowledge, leading to significant performance degradation on earlier tasks. Various methods have been proposed to mitigate this issue, such as Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), which regularizes weight updates to protect crucial parameters learned from previous tasks. Similarly, memory-based methods like Gradient Episodic Memory (GEM) (Lopez-Paz and Ranzato, 2017) address forgetting by storing and replaying examples from past tasks during training, thereby reducing interference. While recent LLM models show strong zero-shot performance, they often struggle with tasks outside their training and evaluation sets. To address this, (Scialom et al., 2022) propose Continual-T0 (CT0), a fine-tuned LLM capable of learning new tasks while retaining prior knowledge, largely due to the self-supervision pre-training process. (Kemker et al., 2017) showed that Deep neural networks struggle to learn new tasks without forgetting old ones, and various methods have been proposed to mitigate this, but their effectiveness varies depending on the training paradigm and data type. (Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension.",
            "score": 0.54156437526208,
            "section_title": "Related Works",
            "char_start_offset": 2762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1665
                }
            ],
            "ref_mentions": [
                {
                    "start": 58,
                    "end": 85,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 445,
                    "end": 471,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 633,
                    "end": 662,
                    "matchedPaperCorpusId": "37308416"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7470703125
        },
        {
            "corpus_id": "277065774",
            "title": "HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model",
            "text": "Recent years have witnessed remarkable advancements in Multimodal Large Language Models (MLLMs) (Yin et al., 2023) , which extend the capabilities of Large Language Models (Touvron et al., 2023) through sophisticated vision-text feature alignment mechanisms (Liu et al., 2024b) and autoregressive generation frameworks. The integration of large-scale training corpora and extensive model parameters (Yang et al., 2024;Kaplan et al., 2020) has enabled MLLMs to achieve state-of-theart performance across diverse downstream applications (Zhang et al., 2024;Lu et al., 2024), demonstrating the potential for complex world understand-* Equal Contribution. \n\ning and representing a significant milestone toward the realization of artificial general intelligence. \n\nAs a pivotal component for MLLMs, instruction tuning (Zhang et al., 2023b) enhances instructionfollowing capabilities of pre-trained models, effectively bridging general-purpose pretraining and domain-specific applications by aligning model behavior with user intent. However, in practical applications, users often perform continuous finetuning on diverse datasets at different times to meet specific needs. This requires the model to effectively incorporate new knowledge while overcoming catastrophic forgetting (Li and Hoiem, 2017;Kirkpatrick et al., 2017) on previous tasks. \n\nRecent work (Chen et al., 2024) construct a benchmark to evaluate the capability of MLLMs in continual instruction tuning (CIT) and reveal that there is a serious catastrophic forgetting phenomenon in MLLMs. In terms of methodology, they propose MoELoRA to mitigate the model's forgetting of old instructions. Based on this, Modal-Prompt (Zeng et al., 2024) dynamically selects optimal prompts during inference by jointly leveraging textual and visual features, thereby enhancing model performance. However, we observe that the downstream datasets used by these methods partially overlaps with the tasks encountered during the supervised fine-tuning (SFT) phase of MLLM. Such information leakage (Kim et al., 2023) compromises the reliability of evaluating whether a method mitigates forgetting or if the model inherently retains this capability, thereby diminishing the challenge of continual instruction tuning.",
            "score": 0.5407210778430646,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 651
                },
                {
                    "start": 654,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1339
                },
                {
                    "start": 1342,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2012
                },
                {
                    "start": 2013,
                    "end": 2255
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 277,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 555,
                    "end": 571,
                    "matchedPaperCorpusId": "264491155"
                },
                {
                    "start": 1275,
                    "end": 1295,
                    "matchedPaperCorpusId": "4853851"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3203125
        },
        {
            "corpus_id": "268667576",
            "title": "Can 3D Vision-Language Models Truly Understand Natural Language?",
            "text": "However, the fine-tuned \"3D-LLM\" experienced similar performance declines across most splits.Both pre-trained and fine-tuned models showed degradation in the tone split which simulates stylistic diversity in daily communication.The fine-tuned \"3D-LLM\" exhibited more pronounced losses in the tone split than its solely pre-trained counterpart, indicating that while task-specific fine-tuning enhances downstream task performance, it may lead to catastrophic forgetting, reducing overall model robustness.",
            "score": 0.5403649316350245,
            "section_title": "D More Metrics of Systematic Evaluation",
            "char_start_offset": 45094,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 93,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 504
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56201171875
        },
        {
            "corpus_id": "276409016",
            "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
            "text": "Mechanistic analysis to fine-tuning. Existing works on analyzing the internal mechanism (R\u00e4uker et al., 2023;Ferrando et al., 2024) of fine-tuning mainly focus on the question that how LLMs acquire new capacity in the learning process, arguing that models learn a minimal transformation on top of the original capability (Jain et al., 2024) (wrappers), subtractable and reusable parameter shift vectors (Huang et al., 2024b;Gao et al., 2024) (task vectors) and to align input queries with their internal knowledge that are already acquired in the pre-training stage (Ren et al., 2024). Nevertheless the inherent reason for the forgetting issue brought by fine-tuning currently remains unclear, and hence our work instead targets on this important point. We have successfully identified the compact task representation, known as the function vector, can tracks task forgetting in LLMs. Our empirical data indicate a strong correlation between shifts in the function vector and the phenomenon of task forgetting. \n\nIn this study, we tackle the issue of catastrophic forgetting in Large Language Models (LLMs) via a detailed investigation using the Function Vector (FV) approach, highlighting its pivotal role in characterizing and mitigating forgetting phenomena. Our analysis across a vast array of benchmarks reveals that model forgetting is intricately linked to shifts in latent concept variables (characterized by function vector), facilitated by our novel function vector-guided training strategy. This method, integrating a regularization term with a function vector-guided Kullback-Leibler divergence loss, significantly curtails forgetting, thereby enhancing both general and in-context learning capabilities of LLMs in continual learning settings.",
            "score": 0.5388447722367763,
            "section_title": "RELATED WORK",
            "char_start_offset": 32373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 37,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 340,
                    "matchedPaperCorpusId": "265308865"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5087890625
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Initial Simp Emdg InqQG Exp HGen Initial Simp Emdg InqQG Exp HGen Initial Simp Emdg InqQG Exp HGen significant forgetting in the first instruction tuning, which suggests that models without general instruction tuning may have less ability to retain knowledge during continual fine-tuning. The better retention of knowledge implies that general instruction tuning can mitigate catastrophic forgetting in LLMs during further continual fine-tuning. This finding highlights the importance of general instruction tuning in preserving the acquired knowledge and skills of LLMs when they undergo subsequent task-specific fine-tuning. \n\nTo further demonstrate the effect of general instruction tuning, we mix 10,000 general instruction data samples from ALPACA (Taori et al., 2023) with the continual instruction tasks to train the LLAMA-7b model. For the sake of brevity, we present the performance of one data split from each evaluation set (MMLU-human, Hellaswag, and Race-middle) to illustrate the effect in Figure 6. The results clearly show that the forgetting during continual instruction tuning can be mitigated to a certain extent by incorporating general instruction data. For instance, the performance of MMLU-human in the initial LLAMA-7b model is 34.72%, but it decreases to 26.8% when trained solely on the instruction data. However, when trained on the mixed data, the performance becomes 30%. These findings further further show that general instruction tuning can help alleviate the CF problem encountered during continual instruction tuning.",
            "score": 0.5380658725166665,
            "section_title": "ALPACA-7B",
            "char_start_offset": 20486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 626
                },
                {
                    "start": 629,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1330
                },
                {
                    "start": 1331,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1551
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.693359375
        },
        {
            "corpus_id": "272770124",
            "title": "Diabetica: Adapting Large Language Model to Enhance Multiple Medical Tasks in Diabetes Care and Management",
            "text": "Given that data quality is a key determinant of model performance, we further conducted data refinement to construct a high-quality dataset. Note that although RLHF (Ouyang et al., 2022) is frequently used to improve the LLM alignment with human preference after conducting vanilla finetuning, it always requires expensive preference-labeling process for reward modeling in open-ended scenarios (Xu et al., 2024). To address this, motivated by previous research (Yang et al., 2024c) that designs a self-distillation method to enhance model performance during the continual fine-tuning, we apply an improved self-distillation pipeline. This approach is effective in our case for reducing the data distribution shift relative to the knowledge contained in the LLM. \n\nPreliminary. Though LLMs showcase outstanding performance in various language tasks, they often face limitations with downstream tasks that require continual fine-tuning. Specifically, we refer to an LLM in need of fine-tuning as a seed LLM, denoted as f and parameterized by \u03b8. The seed LLM typically undergoes vanilla fine-tuning to map any natural language instruction x to its corresponding output y by updating the model parameters. This update aims at minimizing the disparity between the data distribution and the LLM distribution: \n\nwhich seeks to minimize the negative log likelihood P of the target output y given the input x with the model parameters \u03b8. L converges when the fine-tuned LLM's generated response matches y, i.e., the distribution of fine-tuned LLM aligns with the task data distribution. This process can inject the knowledge contained in the data into the LLM. \n\nMethod. Note that vanilla fine-tuning an LLM on a collected dataset, whose distribution is far from the LLM's, can be harmful to the LLM's original alignment with human preference and lead to catastrophic forgetting in general instruction-following capabilities, which consequently results to the decrease of LLM's response quality (Ren et al., 2024a). To address these issues in vanilla fine-tuning, we propose a modified self-distillation (SD) pipeline to make the LLM better align the distribution of the collected dialogue dataset.",
            "score": 0.5376098212352776,
            "section_title": "DATA REFINEMENT",
            "char_start_offset": 4056,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 762
                },
                {
                    "start": 765,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1303
                },
                {
                    "start": 1306,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 165,
                    "end": 186,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.34619140625
        },
        {
            "corpus_id": "273901289",
            "title": "SEEKR: Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models",
            "text": "Consequently, a relatively large amount of replay data is required by these methods to achieve good results. \n\nRecently, many studies have investigated the attention weights of different heads to analyze the interpretability of the internal mechanisms in LLMs (Vig and Belinkov, 2019;Wang et al., 2023a). Inspired by this, we explore whether attention weights play a critical role in knowledge retention during continual learning in LLMs. As shown in Figure 1, grafting the attention weights from the LLM of the old tasks to the final LLM after continual learning can maintain better performance on old tasks, which suggests that the attention weights could be crucial to alleviate the catastrophic forgetting problem and achieve more comprehensive knowledge retention 1 . However, naively preserving the attention weights of all heads in the LLM by distillation introduces significant computational costs. Previous studies have observed a functional specialization phenomenon among attention heads in LLMs (Vig and Belinkov, 2019;Jo and Myaeng, 2020;Li et al., 2023), which indicates the susceptibility of attention heads to forgetting and their importance to previous tasks vary. This property allows us to selectively focus on the valuable attention heads for efficient knowledge retention. \n\nTo this end, we propose a finer-grained model distillation method called SElective attEntionguided Knowledge Retention (SEEKR) for continual learning of large language models, which employs attention distillation on the most valuable heads in LLMs to achieve efficient knowledge retention. Specifically, we develop knowledgeretention-oriented head importance measures, which consider both forgettability and task sensitivity, to identify the most valuable heads for distillation. The forgettability, measured by the cumulative changes in attention weights during continual learning, indicates the generality of knowledge and the necessity of distillation. An attention head with higher forgettability indicates a greater need for knowledge retention. The task sensitivity, calculated as the first-order derivative of the task loss, evaluates the importance of maintaining the attention weights of an attention head for a given task. An attention head with greater sensitivity should 1 Attention grafting can only be used during inference with both the source and target models, which is an infeasible solution for continual learning. We employ this technique solely for exploratory experiments.",
            "score": 0.5374825809680854,
            "section_title": "Introduction",
            "char_start_offset": 1927,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 111,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1293
                },
                {
                    "start": 1296,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2228
                },
                {
                    "start": 2229,
                    "end": 2429
                },
                {
                    "start": 2430,
                    "end": 2490
                }
            ],
            "ref_mentions": [
                {
                    "start": 1031,
                    "end": 1051,
                    "matchedPaperCorpusId": "220047270"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1685791015625
        },
        {
            "corpus_id": "261530292",
            "title": "Explainability for Large Language Models: A Survey",
            "text": "The rapid development of LLMs has raised concerns about their trustworthiness, as they have the potential to exhibit undesirable behaviors such as generating hallucination, a phenomenon in which models generate output that is irrelevant and nonsensical in a natural manner (Zhang et al., 2023;Huang et al., 2023). There emerges increasing interest from the community in understanding how hallucination is produced and how to reduce hallucination generation. \n\nRecent analysis research indicates that the hallucination phenomenon stems from various problems within datasets (Dziri et al., 2022), which can be categorized into two main classes: 1) a lack of relevant data, 2) repeated data. For example, long-tail knowledge is prevalent in training data and LLMs easily fall short in learning such knowledge (Kandpal et al., 2023). On the other hand, deduplicating data is challenging to be done perfectly. Duplicate data within the training dataset can noticeably impair the model's performance. Hernandez et al. (2022) find that the performance of an 800M parameter model can degrade to that of a 400M parameter model by only repeating 10% of the training data. When examining the model's performance in terms of scaling laws, a certain range of repetition frequency in the middle could have a detrimental impact. This range is hypothesized to lead the model to memorize the data and consequently consume a large portion of its capacity. \n\nMoreover, recent studies find that hallucination also arises from certain limitations inherent to models. McKenna et al. (2023) demonstrate that LLMs still rely on memorization at the sentence level and statistical patterns at the corpora level instead of robust reasoning. This is evidenced by their analysis of various LLM families' performance on natural language inference tasks. Further, Wu et al. (2023b) reveal that LLMs are imperfect in both memorization and reasoning regarding ontological knowledge. Berglund et al. (2023) points out that LLMs usually suffer from logical deduction due to reversal curse. LLMs tend to be overconfident in their output and struggle to identify the factual knowledge boundary precisely (Ren et al., 2023).",
            "score": 0.5362022218347006,
            "section_title": "Explaining Hallucination",
            "char_start_offset": 60440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 457
                },
                {
                    "start": 460,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1713
                },
                {
                    "start": 1714,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2186
                }
            ],
            "ref_mentions": [
                {
                    "start": 293,
                    "end": 312,
                    "matchedPaperCorpusId": "215416110"
                },
                {
                    "start": 573,
                    "end": 593,
                    "matchedPaperCorpusId": "248227301"
                },
                {
                    "start": 1833,
                    "end": 1850,
                    "matchedPaperCorpusId": "259370557"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.047332763671875
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "The second is called FUNC, adapted from the datasets in Todd et al. (2023), in which tasks have clear and simple instruc-tions.For example, task Verb-Spanish and Last-Spanish are both translation task but differ in the selection from list.For the general evaluation datasets, we utilize Hellaswag (Zellers et al., 2019), ARC-challenge (Clark et al., 2018), Com-monsenseQA (Talmor et al., 2018), and MMLUsocial (Hendrycks et al., 2020).The detailed dataset information and evaluation metrics are present in Appendix A.\n\nWe adopt LLAMA2-7B-Chat (Touvron et al., 2023b) as the base model, with its effectiveness in both understanding world knowledge and following instructions.Without specific notification, the model is fine-tuned with LORA approach (Hu et al., 2021), using the Adam optimizer with a learning rate set to 1e-4.Additional details regarding the implementation are provided in the Appendix C.\n\nForgetting properties in knowledge and instruction probabilities.In our empirical study, we aim to investigate the factors responsible for the model performance drop.To show this, we present the accuracy curve for task in knowledge and instruction forms (cases in Fig. 2) during continual tuning in Fig. 3. Knowledge accuracy is determined by evaluating P (y|x), whereas instruction accuracy is derived from P (y c |c, x).The reported accuracy follows the evaluation method in Brown et al. (2020); Bordes et al. (2016) which involves choosing the label with the highest log-likelihood.The results reveal a consistent presence of the forgetting effect in LLMs across both general and newly acquired tasks throughout continual instruction tuning.More observations are as follow:\n\n1) Instruction Following Accuracy Decline.At the end of training sequence, the average instruction accuracy for the general evaluation set decreases by 10.24 as compared to the pre-trained model.On the other hand, knowledge accuracy sees an average increase of 1.93.This suggests loss in instruction following ability is the reason for task performance drop.",
            "score": 0.5353731469455221,
            "section_title": "Catastrophic Forgetting in LLMs",
            "char_start_offset": 8687,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 239
                },
                {
                    "start": 239,
                    "end": 435
                },
                {
                    "start": 435,
                    "end": 517
                },
                {
                    "start": 519,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 904
                },
                {
                    "start": 906,
                    "end": 971
                },
                {
                    "start": 971,
                    "end": 1072
                },
                {
                    "start": 1072,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1491
                },
                {
                    "start": 1491,
                    "end": 1650
                },
                {
                    "start": 1650,
                    "end": 1682
                },
                {
                    "start": 1684,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1879
                },
                {
                    "start": 1879,
                    "end": 1950
                },
                {
                    "start": 1950,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 1383,
                    "end": 1402,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1468505859375
        },
        {
            "corpus_id": "267751282",
            "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
            "text": "This observed decline, which is defined as catastrophic forgetting in MLLMs, highlights a critical challenge that has been seldom explored. This naturally raises a pivotal question: Current approaches to mitigate catastrophic forgetting in machine learning (Goodfellow et al., 2013;Masana et al., 2022;Yang et al., 2023) are primarily devised for small models and rely heavily on full-model fine-tuning. In MLLMs, full-model fine-tuning on new tasks leads to heightened computational demands, escalating data storage and training costs, thus becoming impractical with more downstream tasks and highlighting its inefficiency for large-scale use. Furthermore, while parameter-efficient methods like Low-Rank Adaptation (LoRA) (Hu et al., 2021) are designed to reduce computational and memory burdens, they fall short in addressing catastrophic forgetting in MLLMs. During fine-tuning with LoRA, a substantial number of redundant parameter modifications may still be retained, which ineffectively serve the target task and simultaneously erode the pre-trained knowledge (empirically verified in Figure 4). After fine-tuning on two distinct tasks (in orange), InstructBLIP and LLaVa1.5 exhibit a significant performance decline on their original tasks (in blue). Our method offers a remedy to this issue, mitigating the adverse effects of catastrophic forgetting. \n\nIn response to these challenges, we introduce Model Tailor-a parameter-efficient post-training method that meticulously integrates fine-tuned parameters into the pre-trained model fabric. Just as a tailor selects patches to enhance a garment, Model Tailor discerns and modifies a minimal set of parameters, known as the \"model patch\", from the finetuned model to fortify the model's capabilities on new tasks without forfeiting its pre-trained proficiencies. Guided by the principles of the Lottery Ticket Hypothesis (Frankle & Carbin, 2019;Li et al., 2023a), the model patch is identified via a fusion strategy that analyzes both parameter changes and loss variations.",
            "score": 0.5350864979630118,
            "section_title": "Introduction",
            "char_start_offset": 1556,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1102
                },
                {
                    "start": 1103,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1359
                },
                {
                    "start": 1362,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 2031
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 302,
                    "matchedPaperCorpusId": "234353728"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54931640625
        },
        {
            "corpus_id": "267657819",
            "title": "Personalized Large Language Models",
            "text": "On the other hand, LLMs are sophisticated zero-shot reasoners [2]. One can use their abilities to solve downstream tasks with in-context-learning [33] and extensive prompt-based inference [34]. Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting [35], where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- [38]. Techniques such as multitask learning or balancing pre-training and task-specific data might be beneficial for retaining the performance of LLMs in multiple downstream tasks. \n\nTo the best of our knowledge, LLM fine-tuning for subjective tasks via user ID inputs, such as personalized emotion recognition or personalized hate speech detection, has not been extensively evaluated, and further research is needed in this area.",
            "score": 0.5350856037628021,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 5428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 799
                },
                {
                    "start": 802,
                    "end": 1049
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 65,
                    "matchedPaperCorpusId": "249017743"
                },
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 188,
                    "end": 192,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 506,
                    "end": 510,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 619,
                    "end": 623,
                    "matchedPaperCorpusId": "262055661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62548828125
        },
        {
            "corpus_id": "266335873",
            "title": "LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin",
            "text": "In this section, we dissect the reason behind the decline on these world knowledge benchmarks during the expansion of fine-tuning data. We find this results from the occurrence of irreversible knowledge forgetting inside the LLM. \n\nThe performance on world knowledge benchmarks highly relies on the knowledge and skills learned during pre-training phase. To investigate the relationship between the performance on world knowledge benchmarks and the knowledge embedded in pre-trained models (Petroni et al., 2019;Roberts et al., 2020;AlKhamissi et al., 2022), we conduct fine-tuning solely on the CBQA dataset with 250k samples and run evaluation on the test sets without train-test overlap. Results in Figure 3 show initial training boosts performance significantly, especially the first 1% (approximately 1k samples), with limited gains thereafter. This is because early fine-tuning aligns existing knowledge with new instructions, improving CBQA results. However, due to minimal training-testing data over- Given this, it is naturally assumed that the diminished performance on knowledge benchmark stems from the damage of knowledge stored in the LLM due to large-scale instruction tuning. \n\nTo verify the hypothesis, we sequentially fine-tuned a model using two datasets, first excluding CBQA data, then with CBQA data. Results presented in Table 1 show a great decline in knowledge capabilities versus the original LLM. This indicates that the world knowledge within the model was compromised during the first stage of large-scale fine-tuning, resulting in the model's inability to forge the alignment between human instructions and the already destroyed knowledge in the subsequent stage of fine-tuning solely with CBQA. \n\nTo sum up, the pursuit of enhancing performance on downstream tasks through the expansion of training data conflicts the preservation of world knowledge within the model in vanilla SFT.",
            "score": 0.5350109086469245,
            "section_title": "The Irreversible Knowledge Forgetting",
            "char_start_offset": 5237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 229
                },
                {
                    "start": 232,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1191
                },
                {
                    "start": 1194,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1913
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.384033203125
        },
        {
            "corpus_id": "276580882",
            "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention and Low-Rank Adaptation in Large Language Models",
            "text": "With the rapid development of large language models (LLMs), fully fine-tuning (FT) these models is becoming increasingly infeasible due to high computational demands. Moreover, FT also increases the risk of catastrophic forgetting. As an alternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only a small subset of parameters, LoRA achieves performance similar to FT while significantly reducing resource requirements. However, since LoRA inherits FT's design, the issue of catastrophic forgetting still remains. To address these limitations, we propose SECURA: Sigmoid-Enhanced CUR Decomposition LoRA, a novel PEFT variant designed to mitigate catastrophic forgetting while improving fine-tuning performance. Our method introduces a novel normalization technique, Sigmoid-based Magnitude Norm (S-MagNorm), which enhances parameter retention and fine-tuning efficiency. SECURA has been evaluated on a diverse range of tasks, including mathematical problem-solving (GSM8K), complex question-answering (CNNDM), translation (NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results demonstrate that it achieves an average fine-tuning improvement of 3.59% across four MCQ tasks and 2.51% across five QA tasks on Gemma2 2B, Qwen2 1.5B, Qwen2 7B, Llama3 8B, and Llama3.1 8B, outperforming DoRA. Additionally, SECURA demonstrates superior knowledge retention capabilities, achieving state-of-the-art performance in 16 continual learning tests and maintaining more than 70% accuracy on LLMs' basic knowledge compared to Experience Replay (ER), sequential learning (SEQ), EWC, I-LoRA, and CUR-LoRA.",
            "score": 0.5345150390584935,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.321044921875
        },
        {
            "corpus_id": "266374841",
            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
            "text": "Safe customized finetuning of LLMs. Given the rising popularity of third-party personalization of released LLMs, it is essential to ensure outputs of LLMs are aligned with human preferences after customization. Finetuning, either via reinforcement learning from human feedback (RLHF) (Ziegler et al., 2019) or standard supervised learning, is currently a common approach attempting to achieve this alignment. Some works show that supervised finetuning on curated data through maximum likelihood estimation has been shown to be similarly effective (Sun et al., 2023;Zhou et al., 2023;Rafailov et al., 2023;Dong et al., 2023) to the more involved RLHF. While the majority of recent works focus on safety alignment before the release of LLMs, few have investigated the safety issues in finetuning released models. Our work evaluates different methods of making downstream finetuning safe and explores long-term safety of LLMs as well. \n\nNeural networks forgetting. Catastrophic forgetting (Kirkpatrick et al., 2017;Ritter et al., 2018), usually observed in multi-task learning, describes the phenomenon of neural networks forgetting past learned information when trained on new tasks. Toneva et al. (2019) have observed that these forgetting events happen even when the training data are sampled from the same task distribution, finding that some examples are frequently forgotten, while others are never forgotten. They also find examples with wrong labels are forgotten at a higher rate compared to the ones with correct labels. Several prior works find that larger models suffer less from forgetting (Tirumala et al., 2022;Ramasesh et al., 2021;Mirzadeh et al., 2022). Notably, two recent works pointed out ChatGPT experiences decreasing performance on diverse tasks over time, which could be caused by the forgetting during consecutive finetuning (Tu et al., 2023;Chen et al., 2023). Current LLMs usually experience different finetuning sessions continuously, while their forgetting behaviors during the process remain unclear and require more investigation.",
            "score": 0.5344106947603268,
            "section_title": "Related Work",
            "char_start_offset": 29195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 36,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1412
                },
                {
                    "start": 1413,
                    "end": 1527
                },
                {
                    "start": 1528,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1884
                },
                {
                    "start": 1885,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 1012,
                    "end": 1032,
                    "matchedPaperCorpusId": "29169199"
                },
                {
                    "start": 1182,
                    "end": 1202,
                    "matchedPaperCorpusId": "55481903"
                },
                {
                    "start": 1600,
                    "end": 1623,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1623,
                    "end": 1645,
                    "matchedPaperCorpusId": "251648120"
                },
                {
                    "start": 1645,
                    "end": 1667,
                    "matchedPaperCorpusId": "239616391"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439697265625
        },
        {
            "corpus_id": "267769989",
            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
            "text": "Fine-Tuning Fine-tuning is a prevalent strategy for improving the performance of models on downstream tasks, as demonstrated in domains including coding (Roziere et al., 2023;Luo et al., 2024), arithmetic (Luo et al., 2023a), healthcare (Jin et al., 2023) and finance (Wu et al., 2023). Vanilla finetuning directly maximizes the log-likelihood of target responses. Similar to our work, Self-Play Finetuning (Chen et al., 2024) employs the identical LLM as both generator and discriminator, steering the model to prefer annotated response over generated outputs. As the LLM's distribution ultimately converges with that of the training data, the method does not alleviate forgetting during fine-tuning. \n\nContinual Learning Fine-tuning enables models to adapt to new data distributions, improving their efficacy on downstream tasks. However, this process can lead to the loss of previously acquired knowledge, an issue known as catastrophic forgetting (French, 1999). A related domain is continual learning (Kirkpatrick et al., 2017;Lopez-Paz and Ranzato, 2017), which seeks to enable models to acquire new knowledge while mitigating such forgetting. Traditional methods often depend on the preservation of historical data for replay (Scialom et al., 2022;Luo et al., 2023b), the computation of parameter importance (Kirkpatrick et al., 2017;Aljundi et al., 2018), or the assignment of distinct neurons to different tasks (Mallya and Lazebnik, 2018). However, fine-tuning LLMs is particularly challenging due to their extensive parameter and task space, compounded by the frequent unavailability of original training datasets, which diminishes the feasibility of these established techniques (Kirkpatrick et al., 2017;Lopez-Paz and Ranzato, 2017;Scialom et al., 2022). Although recent research (Luo et al., 2023b;Scialom et al., 2022) highlights the significance of continual learning for LMs, there are scant feasible solutions for LLMs.",
            "score": 0.5344074481180138,
            "section_title": "Related Work",
            "char_start_offset": 4084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 701
                },
                {
                    "start": 704,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1937
                }
            ],
            "ref_mentions": [
                {
                    "start": 175,
                    "end": 192,
                    "matchedPaperCorpusId": "259164815"
                },
                {
                    "start": 951,
                    "end": 965,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 1032,
                    "end": 1060,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 1233,
                    "end": 1255,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 1421,
                    "end": 1448,
                    "matchedPaperCorpusId": "35249701"
                },
                {
                    "start": 1717,
                    "end": 1745,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 1745,
                    "end": 1766,
                    "matchedPaperCorpusId": "252815378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "270559235",
            "title": "How Do Large Language Models Acquire Factual Knowledge During Pretraining?",
            "text": "In this work, we study how LLMs acquire factual knowledge during pretraining. Our findings and contributions can be summarized as follows: \n\n\u2022 We propose methods, datasets, and metrics for performing a fine-grained analysis of factual knowledge acquisition dynamics during LLM pretraining. \n\n\u2022 We demonstrate that factual knowledge acquisition in LLM pretraining is achieved through accumulating micro-acquisitions, each of which occurs whenever the model is updated after seeing the factual knowledge. When the model is not presented with factual knowledge, forgetting occurs and the acquisition of the knowledge is gradually diluted. \n\n\u2022 However, while the amount of immediate improvement in log probability upon observation of the knowledge increases for larger models, the amount does not significantly increase throughout the progress of pretraining. This finding suggests that the benefits of scaling the model size and pretraining tokens are qualitatively different. \n\n\u2022 There is a power-law relationship between training steps and forgetting of acquired factual knowledge, in terms of both memorization and generalization. Also, pretraining LLMs with deduplicated data and larger batch sizes enhances the acquisition of factual knowledge, making them more robust against forgetting the learned factual knowledge. \n\n\u2022 We provide potential explanations for recently observed, yet underexplored behaviors of LLMs. First, we propose that the improved performance of LLMs through data scaling results from consistent improvements rather than an emergent ability to acquire factual knowledge more quickly during pretraining. Second, we hypothesize that LLMs struggle to acquire unpopular knowledge because they need sufficient exposure to factual knowledge with intervals shorter than the learnability threshold to increase the probability. Third, our findings suggest that deduplicating the pretraining corpus improves LLM performance by preventing the model from assigning a higher probability to duplicated sequences and helping it retain acquired generalization longer. \n\nOverall, we demonstrate the importance of understanding the factual knowledge acquisition dynamics of LLMs to understand the behavior of LLMs, opening up a promising avenue for future research.",
            "score": 0.5333327587292535,
            "section_title": "Discussion and Conclusions",
            "char_start_offset": 25481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 138
                },
                {
                    "start": 141,
                    "end": 289
                },
                {
                    "start": 292,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1320
                },
                {
                    "start": 1323,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2075
                },
                {
                    "start": 2078,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1134033203125
        },
        {
            "corpus_id": "270123761",
            "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
            "text": "In this section, we discuss the following research questions (RQ) of the TAIA strategy: \n\nRQ1: Does TAIA suit full fine-tuning where the catastrophic forgetting is even more severe? [68] finds that supervised fine-tuning hurts LLMs few-shot performance on unseen tasks. Can TAIA restore similar few-shot ability as the base LLM? RQ5: Fine-tuning converges to the downstream distribution, leading to the diminishing rank compared to the base LLM. How does the rank change when TAIA is adopted? \n\nResponse to RQ1: TAIA is also applicable to the full fine-tuning technique. Our analysis and empirical study focused on PEFT scenarios, mitigating catastrophic forgetting. To test TAIA in a full fine-tuning context, we maintained the same experiment settings as with LoRA tuning but lowered the learning rate to 5e \u2212 5 for stability and used the CoT-Collection as the fine-tuning corpus. Testing on Qwen1.8b and 7b sizes, the results (Table 4) indicate TAIA maintains superior performance in reasoning tasks (SVAMP, MATH, CommonsenseQA). However, due to extensive parameter modifications during full fine-tuning, TAIA experiences significant catastrophic forgetting in knowledge-intensive tasks (MMLU, MMedBench). Despite this, it still outperforms the vanilla inference method, validating its applicability and generalization in full fine-tuning scenarios. \n\nResponse to RQ2: TAIA significantly reduces harmfulness and improves helpfulness. The analysis and experiments above have demonstrated that TAIA enables LLMs to generalize on OOD data, reducing dependency on data quality. To explore if TAIA can handle training data with harmful information while enhancing LLM usefulness without substantially increasing harmfulness, we followed Qi et al. [51] to red-team LLaMA2-7B-chat using three attack levels and evaluated on Advbench [10]. We used 100 of the most harmful samples from the Anthropic red team dataset [16], 10 identity-shifting samples from Qi et al. [51], and benign data from Alpaca-GPT4 [49]. For models tuned on benign data, we also tested helpfulness on AlpacaEval [29].",
            "score": 0.5317489497745372,
            "section_title": "Analysis",
            "char_start_offset": 21072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 90,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 492
                },
                {
                    "start": 495,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1834
                },
                {
                    "start": 1835,
                    "end": 2005
                },
                {
                    "start": 2006,
                    "end": 2085
                }
            ],
            "ref_mentions": [
                {
                    "start": 182,
                    "end": 186,
                    "matchedPaperCorpusId": "253244132"
                },
                {
                    "start": 1829,
                    "end": 1833,
                    "matchedPaperCorpusId": "252992988"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353515625
        },
        {
            "corpus_id": "229363334",
            "title": "Seeing past words: Testing the cross-modal capabilities of pretrained V&L models on counting tasks",
            "text": "In these experiments, we probe pretrained V&L models without any additional fine-tuning. correct and foiled examples well, as shown by 96-98 p c and \u223c 96 p f . When using ViLBERT 12-in-1, results on GCC are considerably worse compared to ViLBERT. This is surprising, since ViLBERT 12-in-1 was trained using more tasks and considerably more data than ViLBERT. Finally, LXMERT performs worst overall among all three models. These results suggest that LXMERT (and to a lesser extent, ViLBERT 12-in-1) may be exhibiting catastrophic forgetting, a well-studied problem in neural networks (Robins, 1995) which has received attention in NLP (Kirkpatrick et al., 2017;Yogatama et al., 2019) as well as in V&L tasks in particular (Greco et al., 2019): LXMERT is finetuned on visual question answering in the last 10 epochs of pretraining, and ViLBERT 12-in-1 is finetuned on 12 different tasks. This finetuning may be responsible for the worse results observed, resulting in a downgrading of performance on the task the models were originally pretrained on. \n\nIn summary, all models solve the image-sentence alignment probe well (as expected) but the models show notable differences in performance; we conjecture catastrophic forgetting may be impacting the finetuning procedure of each model differently.",
            "score": 0.5317286688323383,
            "section_title": "Experiments",
            "char_start_offset": 8197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1048
                },
                {
                    "start": 1051,
                    "end": 1296
                }
            ],
            "ref_mentions": [
                {
                    "start": 583,
                    "end": 597,
                    "matchedPaperCorpusId": "22882861"
                },
                {
                    "start": 721,
                    "end": 741,
                    "matchedPaperCorpusId": "184488333"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.265380859375
        },
        {
            "corpus_id": "277628261",
            "title": "Knowledge-Instruct: Effective Continual Pre-training from Limited Data using Instructions",
            "text": "One major fine-tuning risk is the phenomenon of catastrophic forgetting (Kirkpatrick et al., 2017;Goodfellow et al., 2013;Chen et al., 2020;Luo et al., 2023), where models lose some of the capabilities they had prior to the fine-tuning process. \n\nTo address this concern, we perform a casestudy on Llama in the MultiHop-RAG scenario, and evaluate all of the fine-tuned models on a range of common general LLM benchmarks. This includes knowledge-intensive tasks (MMLU, TriviaQA (Hendrycks et al., 2020;Joshi et al., 2017)), reasoning tasks (ARC, GSM8K, Winogrande (Clark et al., 2018;Cobbe et al., 2021;Sakaguchi et al., 2021)), reading comprehension (OpenBookQA (Mihaylov et al., 2018)), and a combination of knowledge with reasoning (MMLU-Pro (Wang et al., 2024)). \n\nGeneral Results: The full results are presented in Table 2. Overall, Knowledge-Instruct preserves general capabilities and mitigates catastrophic forgetting more effectively than other methods. In fact, we observe an improvement in reasoning tasks across all methods, likely due to the inclusion of high-quality SFT data. This aligns with the \"Base + Orca SFT\" column, where OpenOrca alone improves Llama's reasoning ability. While all methods show minor deterioration in general knowledge tasks (MMLU and TriviaQA), they largely maintain or even slightly improve performance on the other benchmarks, likely due to the added supervised fine-tuning (SFT) data. \n\nHowever, there are two notable exceptions. In GSM8K, Knowledge-Instruct shows only minimal deterioration, whereas other methods exhibit significant decline. Similarly, in MMLU-Pro Knowledge-Instruct retains capabilities, even slightly surpassing the base model, while the CPT methods show worse results. \n\nCPT vs. SFT: Figure 3 show the performance of Knowledge-Instruct compared to Synthetic CPT, before and after adding the SFT stage. All SFT models, including the base model that has no prior knowledge at all, achieve over 95% accuracy in the Companies Oracle case.",
            "score": 0.5316022579345121,
            "section_title": "Catastrophic Forgetting Analysis",
            "char_start_offset": 17929,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1427
                },
                {
                    "start": 1430,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1999
                }
            ],
            "ref_mentions": [
                {
                    "start": 602,
                    "end": 625,
                    "matchedPaperCorpusId": "199370376"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.58984375
        },
        {
            "corpus_id": "278481117",
            "title": "Full-Parameter Continual Pretraining of Gemma2: Insights into Fluency and Domain Knowledge",
            "text": "Large language models (LLMs) have shown remarkable effectiveness in multiple domains, including natural language processing (NLP). Despite this, due to the imbalance of training data, the performance of these models in low-resource languages is often significantly worse, leading to the demand for regional LLMs [1,2]. These models are often achieved by integrating a specific language via autoregressive pre-training. However, this often reduces the performance of the model on the previously learned tasks. This undesirable effect is known as catastrophic forgetting. \n\nThe field of mitigating catastrophic forgetting is called Continual Learning (CL). It is a study of algorithms and models that can cope with a learning scenario, in which new tasks should be learned, without losing performance on previously learned tasks. CL model f \u03b8 is usually represented by some parametrised function (e.g., neural network), with parameters \u03b8. Given a sequence of tasks T 1 , ..., .T n represented with data sets D 1 , ..., D n , which arrive over time, this model should be able to learn a new task T i from D i , without access to previous D j (j < i), simultaneously maintaining performance on all previously learned tasks T j (where j < i). \n\nHowever, the field of CL in LLMs is still nascent [3,4]. Unlike classical models, LLMs can possess a diverse domain of knowledge, which can be grouped into factual knowledge, domain knowledge, language knowledge, and preference knowledge [4]. Furthermore, LLM training is performed over multiple steps which impart different knowledge, requiring separation of the CL methods. One such separation would be Continual Pre-training, Domain-Adaptive Pre-Training, and Continual Fine-Tuning [3]. This raises many questions on the relation between different types of knowledge in the context of CL. The regional LLM case can be conceptualised as a model losing domain knowledge while attaining language knowledge. To the best of our knowledge, there has been little investigation into the interplay between different types of knowledge in CL. In this paper, we investigate the applicability of CL to mitigating catastrophic forgetting of domain knowledge after updating language knowledge via a standard autoregressive approach. The contributions of this paper are empirical evidence that",
            "score": 0.5311833582437506,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 318
                },
                {
                    "start": 319,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 654
                },
                {
                    "start": 655,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 936
                },
                {
                    "start": 937,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.438720703125
        },
        {
            "corpus_id": "219558836",
            "title": "On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines",
            "text": "We show the results in Fig. 2 (a) and (b). We can observe that although catastrophic forgetting occurs for the failed models (Fig. 2a) -perplexity on WikiText-2 is indeed degraded for k = 0 -the phenomenon is much more nuanced. Namely, catastrophic forgetting affects only the top layers of the network -in our experiments often around 10 out of 24 layers, and the same is however also true for the successfully fine-tuned models, except for a much smaller increase in perplexity. \n\nAnother important aspect of our experiment is that catastrophic forgetting typically requires that the model at least successfully learns how to perform the new task. However, this is not the case for the failed fine-tuning runs. Not only is the development accuracy equal to that of the majority classifier, but also the training loss on the fine-tuning task (here RTE) is trivial, i.e. close to \u2212 ln( 1 /2) (see Fig. 2 (c)). This suggests that the observed fine-tuning failure is rather an optimization problem causing catastrophic forgetting in the top layers of the pre-trained model. We will show later that the optimization aspect is actually sufficient to explain most of the variance in the fine-tuning performance.",
            "score": 0.5297956727063526,
            "section_title": "Does catastrophic forgetting cause fine-tuning instability?",
            "char_start_offset": 9975,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 43,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 480
                },
                {
                    "start": 483,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1206
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.279052734375
        },
        {
            "corpus_id": "266844262",
            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
            "text": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks (Brown et al., 2020). There is an increasing trend of releasing pre-trained LLMs and fine-tuned variants (Touvron et al., 2023a,b). Many of these fine-tuned variants aim to augment the knowledge or linguistic capabilities of the existing LLM (Roziere et al., 2023;Cui et al., 2023). \n\nWe have noticed that many advancements in finetuned variants adhere to a conventional procedure consisting of two key steps: 1. Conduct further continual pre-training on an existing LLM. 2. Carry out subsequent alignment operations, such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), on the model obtained in Step 1. Among these fine-tuned variants, many developments perform further continual pre-training on existing fine-tuned LLMs (Cui et al., 2023;Lin and Chen, 2023). \n\nPrevious studies have demonstrated that continual pre-training can significantly improve the model's ability to understand and generate specific content (Gupta et al., 2023). However, continual pre-training could lead to catastrophic forgetting (French, 1999), and limited research has explored the abilities forgotten during pre-training on an existing fine-tuned LLM. \n\nSome works have studied continual learning for language models. (Qin et al., 2022) focused on efficient lifelong pre-training on pre-trained language models for emerging data. (Ke et al., 2022) proposed a continual domain-adaptive pre-training method on a masked language model. (Song et al., 2023) introduced continual parameter-efficient tuning for the ongoing adaptation of LLMs to continual tasks. (Xie et al., 2023) investigate an alternative approach to continual pre-training for developing domain-specific LLMs. (Qi et al., 2023) suggests that fine-tuning compromises the safety alignment of LLMs. (Zhai et al., 2023) evaluates the forgetting in fine-tuned multimodal LLMs. \n\nOur work examines the forgetting occurrence during continual pre-training on an existing finetuned LLM.",
            "score": 0.5294953580038415,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 373
                },
                {
                    "start": 376,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1262
                },
                {
                    "start": 1265,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1440
                },
                {
                    "start": 1441,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 1946
                },
                {
                    "start": 1949,
                    "end": 2052
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 111,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1046,
                    "end": 1066,
                    "matchedPaperCorpusId": "260704601"
                },
                {
                    "start": 1138,
                    "end": 1152,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 1329,
                    "end": 1347,
                    "matchedPaperCorpusId": "247447415"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5380859375
        },
        {
            "corpus_id": "258715133",
            "title": "Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models",
            "text": "Catastrophic forgetting is regarded as the most critical reason to prevent neural network architectures being successful models of memory (Goodfellow et al., 2013). To overcome such forgetful nature, many techniques have been derived and investigated, such as continual learning (Zenke et al., 2017;Li and Hoiem, 2017;De Lange et al., 2021), life-long learning (Richardson and Thomas, 2008;Chen and Liu, 2018;Parisi et al., 2019) and multi-task learning (Ruder, 2017;Zhang and Yang, 2021). Unfortunately, catastrophic forgetting has still been observed in many neural network learning processes across different kinds of tasks and architectures (Pf\u00fclb and Gepperth, 2019;Ramasesh et al., 2021). \n\nSuch a retentive-forgetful observation raises an interesting contradiction about the memorizing mechanism of neural network-based language models. On the one hand, pre-trained language models do memorize the knowledge in a large amount of data, which brings to the success of large-scale PLMs. On the other hand, when facing many different downstream tasks, over-parameterized neural networks still show their nature of catastrophic forgetting, and are unable to persistently memorize knowledge from different sources. In this paper, we want to dive into such contradiction, and discover the underlying memorizing mechanisms that make the forgetful neural network architectures become retentive pre-trained language models. Specifically, this paper will explore the following three critical issues of the memorizing mechanism of language models: \n\n\u2022 Does the memory mechanism of language models follow specific patterns? \n\n\u2022 How does pre-training affect the memorizing abilities of language models? \n\n\u2022 What are the underlying factors that influence the memorization of language models? \n\nTo answer the above three questions, this paper uses the acquisition of factual knowledge as an empirical testbed to investigate the memorizing dynamics of current representative language model architectures. Specifically, we group the factual knowledge data according to different knowledge types, gradually and periodically train large-scale neural network-based language models to learn different types of factual knowledge, and then observe the models' memorizing dynamics of the previously learned knowledge at different time scales.",
            "score": 0.5293293313561116,
            "section_title": "Introduction",
            "char_start_offset": 1871,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 990
                },
                {
                    "start": 991,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1542
                },
                {
                    "start": 1545,
                    "end": 1617
                },
                {
                    "start": 1620,
                    "end": 1695
                },
                {
                    "start": 1698,
                    "end": 1783
                },
                {
                    "start": 1786,
                    "end": 1994
                },
                {
                    "start": 1995,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 299,
                    "matchedPaperCorpusId": "10409742"
                },
                {
                    "start": 299,
                    "end": 318,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 318,
                    "end": 340,
                    "matchedPaperCorpusId": "218889912"
                },
                {
                    "start": 361,
                    "end": 390,
                    "matchedPaperCorpusId": "360673"
                },
                {
                    "start": 390,
                    "end": 409,
                    "matchedPaperCorpusId": "264145480"
                },
                {
                    "start": 409,
                    "end": 429,
                    "matchedPaperCorpusId": "73497737"
                },
                {
                    "start": 454,
                    "end": 467,
                    "matchedPaperCorpusId": "90063862"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.432373046875
        },
        {
            "corpus_id": "273507804",
            "title": "Exploring Forgetting in Large Language Model Pre-Training",
            "text": "Our research into the occurrence of catastrophic forgetting during the pre-training of Large Language Models, though innovative, faces significant computational constraints. The necessity for a substantial computational resource, such as 10,000 GPU hours on 8 NVIDIA A100 GPUs equipped with 40 GiB of VRAM, presents a considerable barrier. The core contribution of our work is to emphasize and delve deeply into an often-overlooked potential danger, with the hope of drawing the industry's attention to and optimizing the issue of pre-training forgetting in models. \n\nInformed by the scaling law (Kaplan et al., 2020), we recognize that our findings from a smaller model may provide valuable insights for largerscale experiments. This framework indicates that our study could contribute to the design of future research, acknowledging the limitations in scaling our results. \n\nOur approach to memory replay has shown potential in alleviating catastrophic forgetting, but there is still room for improvement in terms of its effectiveness. Our investigation did not delve deeply into the granular effects of each variable on the experimental outcomes. The complexity of memory replay mechanisms requires a more nuanced analysis to fully understand how different factors interplay and influence the results. \n\nAdditionally, the concentrated learning of memory replay, while beneficial, may engender tradeoffs that affect the model's generalizability. We hypothesize that the focused emphasis of certain data subsets could lead to a diminished capacity for the model to adapt to tasks beyond the focused areas, such as numerical data processing or other cognitively distinct downstream tasks. \n\nWe recognize that forgetting in pre-training differs from that in SFT, each requiring distinct metrics and methods for mitigation. Yet, there are connections between them. In future work, we also aim to explore the impact of our methods on forgetting in downstream tasks. \n\nDespite these limitations, our study exemplifies the scientific endeavor to confront complex problems with rigor and without reservation. Our work is a courageous step towards understanding the intricate processes of memory retention and forgetting in LLMs, reflecting a sincere commitment to advancing our collective knowledge, even in the face of substantial challenges.",
            "score": 0.5269074351858761,
            "section_title": "Limitations",
            "char_start_offset": 28607,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 565
                },
                {
                    "start": 568,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1304
                },
                {
                    "start": 1307,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1688
                },
                {
                    "start": 1691,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1862
                },
                {
                    "start": 1863,
                    "end": 1962
                },
                {
                    "start": 1965,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2337
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12054443359375
        },
        {
            "corpus_id": "270123761",
            "title": "TAIA: Large Language Models are Out-of-Distribution Data Learners",
            "text": "Consequently, the data distribution that LLMs adapt to during fine-tuning often differs significantly from that required for specific tasks. This discrepancy leads to inferior performance on specialized tasks and knowledge forgetting due to disruptions in the parametric knowledge stored in LLMs [14]. Figure 1 also shows that with more out-ofdistribution (OOD) tuning data, the vanilla fine-tune method brings catastrophic forgetting problems, degrading models' performance on downstream tasks. The scarcity of natural data and the suboptimal quality of synthetic data present substantial challenges to effectively adapting LLMs for specialized tasks. In essence, the dependency on in-domain distribution fine-tuning corpora hampers the broader deployment of LLMs. \n\nTo address this, we propose avoiding such data dependency by leveraging the intrinsic properties of fine-tuning and developing an inference-time method that does not rely on high-quality in-distribution data. We first conduct an in-depth investigation of the internal Transformer architecture. We find that during fine-tuning, LLMs enhance their instruction-following ability, primarily controlled by the self-attention module [75]. Conversely, parameterized knowledge is encoded by the key-value intrinsic of the feed-forward network (FFN) module [18,40] during pretraining [56]. Fine-tuning primarily elicits this pretrained knowledge [46,59,71], which remains relatively fixed [86]. This insight prompts us to discard the FFN updates during fine-tuning, as only a small portion positively contributes to downstream performance, while most disrupt the knowledge when fine-tuned on task-mismatched data. \n\nA naive approach is to fine-tune only the attention parameters, but this fails to generalize to OOD data due to insufficient exploration of non-linearity. To ensure sufficient learning of non-linearity, we introduce additional FFN parameters during fine-tuning but retain only the beneficial self-attention updates. This strategy, named Training-All-Inferring-only-Attention (TAIA), achieves both OOD generalization and sufficient optimization space. The comparisons between the proposed method and the vanilla fine-tuning method are shown in Figure 2 Feed Forward Network",
            "score": 0.5267892623447974,
            "section_title": "Introduction",
            "char_start_offset": 1883,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 765
                },
                {
                    "start": 768,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1672
                },
                {
                    "start": 1675,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1990
                },
                {
                    "start": 1991,
                    "end": 2125
                },
                {
                    "start": 2126,
                    "end": 2247
                }
            ],
            "ref_mentions": [
                {
                    "start": 1320,
                    "end": 1323,
                    "matchedPaperCorpusId": "255825985"
                },
                {
                    "start": 1343,
                    "end": 1347,
                    "matchedPaperCorpusId": "211205183"
                },
                {
                    "start": 1405,
                    "end": 1409,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5322265625
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "Despite its potential advantages, depth-wise structured pruning presents inherent challenges. It often leads to accuracy degradation, especially on tasks requiring multi-step reasoning, such as ARC-C (Clark et al., 2018) or GSM8k (Cobbe Instruct models relative to the unpruned baseline, across varying prune block sizes on the HuggingFace OpenLLM Leaderboard v1. The plot compares no fine-tuning, supervised finetuning, and self-data distilled fine-tuning using the OpenMathInstruct dataset. While model quality declines with prune block sizes, self-data distillation consistently achieves superior recovery. et al., 2021), where the structured order of layer outputs plays a crucial role. In these cases, pruning disrupts the flow of information between layers, resulting in poor model quality even after supervised fine-tuning (SFT) (Sun et al., 2024). \n\nWhile SFT can help recover some of the lost quality, it is generally insufficient for tasks with high reasoning complexity, where the structured sequence of layer outputs is essential. In addition, fine-tuning can amplify catastrophic forgetting (McCloskey & Cohen, 1989;Kotha et al., 2024), where the model loses previously learned information, particularly on tasks not represented in the fine-tuning data. Standard mitigation strategies, such as data replay (Ostapenko et al., 2022) or parameter importance-based methods (Kirkpatrick et al., 2017), often become impractical for LLMs due to their scale. Moreover, fine-tuning often leads to distribution shifts, further degrading model quality (Yang et al., 2024). \n\nAs LLMs continue to grow in size and complexity, developing more effective strategies to mitigate these challenges during pruning is critical to unlocking its full potential. \n\nIn our work, we propose a novel approach to mitigate the adverse effects of structured pruning by employing self-data distilled fine-tuning. Our method leverages the original, unpruned model as a seed language model to generate a distilled dataset that upholds semantic equivalence with the original task dataset.",
            "score": 0.5266128624982105,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1989,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 855
                },
                {
                    "start": 858,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1266
                },
                {
                    "start": 1267,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1574
                },
                {
                    "start": 1577,
                    "end": 1751
                },
                {
                    "start": 1754,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2067
                }
            ],
            "ref_mentions": [
                {
                    "start": 1319,
                    "end": 1343,
                    "matchedPaperCorpusId": "250265129"
                },
                {
                    "start": 1382,
                    "end": 1408,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1554,
                    "end": 1573,
                    "matchedPaperCorpusId": "267769989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.40087890625
        },
        {
            "corpus_id": "269773206",
            "title": "A safety realignment framework via subspace-oriented model fusion for large language models",
            "text": "However, catastrophic forgetting may happen during safety fine-tuning, which can cause LLMs to forget downstream task-specific knowledge in addition to restoring safety.EWC [17] restricts the model update during fine-tuning to counteract catastrophic forgetting for safeguard.Nevertheless, it still leads to a significantly lower fine-tuning accuracy.RESTA [18] employs a straightforward addition of a safety vector to the weights of a compromised model.Despite this, the effectiveness of this realignment depends heavily on the agreement between safety degradation and actual recovery during fine-tuning.DARE [19] random sets specific delta parameters to zeros and subsequently rescales the remaining ones.Nonetheless, its effect is evident only when the discarded parameters happen to be highly coincident with parameters that exhibit unsafe behavior.These solutions ignore the exploration of whether the initially securely aligned model can be reused.Despite preliminary exploration conducted by ActSVD [20] through the freezing of safety-critical regions, it remains vulnerable to fine-tuning attacks.In addition, achieving outstanding performance across various downstream tasks without retraining can be addressed through model fusion [21,22].This process involves integrating several independently fine-tuned models into a unified model while preserving the performance capabilities of each constituent model.It is imperative to evaluate whether the safety \u2022 We conduct comprehensive safety recovery experi- ments on both single task-specific models and the fusion of multiple task-specific models.The results demonstrate that our SOMF method can effectively realign models to improve safety while preserving their proficiency on downstream tasks, without incurring significant performance degradation.",
            "score": 0.5261001445278106,
            "section_title": "Introduction",
            "char_start_offset": 2099,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 169,
                    "end": 276
                },
                {
                    "start": 276,
                    "end": 351
                },
                {
                    "start": 351,
                    "end": 454
                },
                {
                    "start": 454,
                    "end": 605
                },
                {
                    "start": 605,
                    "end": 707
                },
                {
                    "start": 707,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1416
                },
                {
                    "start": 1416,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 1241,
                    "end": 1245,
                    "matchedPaperCorpusId": "259262373"
                },
                {
                    "start": 1245,
                    "end": 1248,
                    "matchedPaperCorpusId": "254877510"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "269362510",
            "title": "Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall",
            "text": "Large language models (LLMs) have shown remarkable performance on a variety of NLP tasks, and are being rapidly adopted in a wide range of use cases. It is therefore of vital importance to holistically evaluate the factuality of their generated outputs, as hallucinations remain a challenging issue. In this work, we focus on assessing LLMs' ability to recall factual knowledge learned from pretraining, and the factors that affect this ability. To that end, we construct FACT-BENCH, a representative benchmark covering 20 domains, 134 property types, 3 answer types, and different knowledge popularity levels. We benchmark 31 models from 10 model families and provide a holistic assessment of their strengths and weaknesses. We observe that instruction-tuning hurts knowledge recall, as pretraining-only models consistently outperform their instruction-tuned counterparts, and positive effects of model scaling, as larger models outperform smaller ones for all model families. However, the best performance from GPT-4 still represents a large gap with the upper-bound. We additionally study the role of in-context exemplars using counterfactual demonstrations, which lead to significant degradation of factual knowledge recall for large models. By further decoupling model known and unknown knowledge, we find the degradation is attributed to exemplars that contradict a model's known knowledge, as well as the number of such exemplars. Lastly, we fine-tune LLaMA-7B in different settings of known and unknown knowledge. In particular, fine-tuning on a model's known knowledge is beneficial, and consistently outperforms fine-tuning on unknown and mixed knowledge. We will make our benchmark publicly available.",
            "score": 0.5260361302705973,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2218017578125
        },
        {
            "corpus_id": "268063771",
            "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
            "text": "We first conducted a comparative analysis of our approach against representative CL baselines from two dimensions. First, we assessed the adaptation and memorization capabilities during continual learning on domain-specific CL benchmarks. Second, we evaluated the memorization ability of general knowledge when continually fine-tuning on domain-specific CL benchmarks. \n\nGeneralization Ability Assessment on Domain-specific CL benchmarks. We start with a fine-tuned LLaMA-7B language model on each domain-specific CL benchmark, then test the Acc t performance to evaluate the adaptation performance. From Table 1, we observe that: 1) starting from a fine-tuned LLaMA-7B language model, CL minima on different tasks can be connected by a low-loss valley, and ensembling over the valley shows improved performance and generalization ability. It is obvious that our approach, I-LoRA, consistently outperforms previous methods and shows a remarkable improvement (i.e., ranging from 3% to 10% accuracy gains) over the previous state-of-the-art CL methods. \n\n2) I-LoRA consistently demonstrates superiority with an increasing number of historical tasks. This observation suggests that leveraging mode connectivity in LLMs could enhance long-term memorization ability and validate the effectiveness of long-term memory in I-LoRA. \n\nMemorization Ability Assessment on Domainspecific CL benchmarks. In this part, we explore the memorization capability of continual learning (CL) methods, specifically examining the extent to which these methods can mitigate the issue of catastrophic forgetting. From Table 2, we can make the following observations: 1) I-LoRA exhibits superiority in mitigating forgetting issues and demon-strates remarkable memorization ability. This observation validates our motivation and methodology design. I-LoRA adjusts parameters relying on the interpolation of mode connectivity, and its performance remains relatively stable throughout continual learning processes. 2) Existing CL-based methods exhibit weak performances when facing complex memorization tasks, such as those with high domain diversity and multilingualism. For example, one popular CL algorithm, EWC, shows a forgetting performance of 20.6% and 19.1% after fine-tuning on the mathematical NumGLUEcm and German-based 20Minute dataset respectively.",
            "score": 0.5259102589512122,
            "section_title": "Overall Comparison in CL",
            "char_start_offset": 19954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 368
                },
                {
                    "start": 371,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1322
                },
                {
                    "start": 1325,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1820
                },
                {
                    "start": 1821,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2141
                },
                {
                    "start": 2142,
                    "end": 2331
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.193603515625
        },
        {
            "corpus_id": "270440348",
            "title": "Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference",
            "text": "We use Mistral-7B-instruct [30] as the target LLM.Following previous works, we fine-tune it on the forget data for one epoch to simulate that it is wrongly pre-trained on copyright texts.More details are in Appendix B.3.\n\nImplementation For baseline methods, we set the batch size and learning rate to be 32 and 1e\u22125, and fine-tune for 5 epochs using AdamW optimizer following previous work [9,17].Same as TOFU dataset, the retain weight \u03b2 is set to 1.For our method, we use the same training hyper-parameters as baseline but set the learning rate to be 5e \u2212 4.\n\nWe adopt the same LoRA configuration and the number of assistant LLM layers as in Section 3.2.In this experiment, the augmented forget data D \u2032 f contains paraphrased HarryPotter paragraphs, and the augmented retain data D \u2032 r is the same as the original Dr. Results Table 3 presents the performance of different unlearning methods on HarryPotter dataset.Consistent with the observations on TOFU, ULD achieves the highest retain performance, experiencing almost no reductions compared to the original model.Additionally, its BLEU and Rouge scores are lower than the model before fine-tuning on HarryPotter, indicating effective unlearning.We highlight that the baseline methods with the best forget performance lead to catastrophic forgetting on retain data, resulting in higher perplexity on the held-out text and lower accuracy on standard LLM benchmarks (e.g., NPO+GD has over 5% accuracy decline compared to the finetuned LLM).",
            "score": 0.5244034957441307,
            "section_title": "Experiments on HarryPotter",
            "char_start_offset": 19888,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 50,
                    "end": 187
                },
                {
                    "start": 187,
                    "end": 220
                },
                {
                    "start": 222,
                    "end": 398
                },
                {
                    "start": 398,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 561
                },
                {
                    "start": 563,
                    "end": 657
                },
                {
                    "start": 657,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1070
                },
                {
                    "start": 1070,
                    "end": 1202
                },
                {
                    "start": 1202,
                    "end": 1494
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04986572265625
        },
        {
            "corpus_id": "268379670",
            "title": "CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model",
            "text": "ScienceQA Figure 1: Different behavior of MLLMs when sequentially tuned on CoIN. Blue represents the accuracy for each task evaluated when just tuned on the corresponding task, and Red represents the accuracy evaluated after the models have been sequentially tuned on all tasks. LLaVA [35] and Qwen-VL [3] suffer from catastrophic forgetting while MiniGPT-v2 [9] does not. The sequential training starts clockwise from ScienceQA and ends with OCR-VQA. \n\nPreprint. Under review. \n\nFigure 2: An overview of CoIN benchmark. A selected MLLM is sequentially fine-tuned on 8 instruction datasets spanning diverse tasks. Then, it is evaluated from two perspectives: Truth Alignment and Reasoning Capability, which assess the alignment with ground truth and knowledge preserved for reasoning, respectively. The evaluation example at the bottom presents the results of the model tested on classification after fine-tuning on each task.",
            "score": 0.5237268730291005,
            "section_title": "MiniGPT-v2",
            "char_start_offset": 44,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 520
                },
                {
                    "start": 521,
                    "end": 613
                },
                {
                    "start": 614,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 926
                }
            ],
            "ref_mentions": [
                {
                    "start": 285,
                    "end": 289,
                    "matchedPaperCorpusId": "258179774"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10821533203125
        },
        {
            "corpus_id": "270067747",
            "title": "More Than Catastrophic Forgetting: Integrating General Capabilities For Domain-Specific LLMs",
            "text": "Large Language Models (LLMs) are sequentially trained on general pre-training corpus, pairs of instruction-response and preference-alignment datasets, thus covering tasks involving writing (Touvron et al., 2023a;Jiang et al., 2023;Blum and Blum, 2023;Pan, 2021), math (Imani et al., 2023; Figure 1: SFT on domain data injects domain knowledge into general LLMs. CF aims to keep the LLM performance on the general tasks after training on domain tasks. While GCI aims to enhance the performance on domain tasks by the integration of general capabilities with domain knowledge. Then the LLM is applied to domain-specific scenarios. Liu et al., 2023;Azerbayev et al., 2023), code (Bui et al., 2023;Chen et al., 2021;Rozi\u00e8re et al., 2023), etc. Many popular domain-specific LLMs are finetuned from general chat LLMs (Xiong et al., 2023;Wang et al., 2023a;Yu, 2023). The straightforward procedure is illustrated in Figure 1. \n\nResearchers have identified a challenge known as Catastrophic Forgetting (CF) (Kaushik et al., 2021), where the model's recent learning overshadows and diminishes its previously acquired capabilities and knowledge, leading to a significant performance drop on previous tasks. Current studies to mitigate CF focus on preserving the general capabilities. However, this paper investigates how to effectively harmonize and utilize both general capabilities and domain-specific knowledge, rather than mitigate CF. Our rationale stems from the observation that, even with CF resolved, general capabilities often encounter difficulties integrating with domain-specific knowledge. \n\nSpecifically, we illustrate the enhancement of GCI in legal domain through Figure 2. A general chat LLM focuses on computing solutions for math queries, delivering numerical results. However, with SFT on legal knowledge, the LLM shifts its approach to presenting relevant law article content, rather than providing the calculation result and conclusion, despite users potentially preferring the latter. An optimal GCI-equipped LLM maintains its general capabilities while integrating legal knowledge contextually at the appropriate time steps.",
            "score": 0.5235903761610606,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 918
                },
                {
                    "start": 921,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1593
                },
                {
                    "start": 1596,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1998
                },
                {
                    "start": 1999,
                    "end": 2139
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.33984375
        },
        {
            "corpus_id": "271543955",
            "title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning",
            "text": "2.1. Motivation \n\nCorrelation between Distance and Forgetting. \n\nIn LLM fine-tuning, different training methods typically converge to different minima of the loss function. These minima may yield similarly low fine-tuning loss, all achieving reasonably good fine-tuning performance. However, their distances from the pre-trained model can vary significantly. A key observation in traditional continual learning (CL) is that the extent of forgetting increases as the model deviates further from its original state. This insight has influenced the design of many forgetting-mitigation methods (Kirkpatrick et al., 2017;Li et al., 2018). \n\nIn this work, we conduct exploratory experiments to investigate this correlation among minima produced by different LLM fine-tuning methods. We conduct two sets of experiments. First, we fine-tune Pythia-160M on a subset of the FLAN dataset2 using Adam (Kingma & Ba, 2014) and Lion (Chen et al., 2024). As illustrated in Figure 1 Lion. Further, Adam leads to better preservation in general capability than Lion (see Appendix C.1). \n\nSecond, we conduct a larger-scale exploratory experiment to test the generality of this observation. We fine-tune a larger model, LLaMA2-7B, on the MetaMathQA dataset (Yu et al., 2024a) using three optimizers: Adam, Lion, and our proposed MoFO optimizer (to be introduced in Section 2.2). To achieve varying distances from the pre-trained model, we fine-tune each model for 0.5, 1, 1.5, and 2 epochs (309 steps per epoch). Details are provided in Appendix D.3. (Hendrycks et al., 2021), which measures the preservation of factual knowledge. These findings suggest that in LLM finetuning, staying near the pre-trained state can better mitigate forgetting.",
            "score": 0.522833198476282,
            "section_title": "Momentum Filtered Optimizer (MoFO)",
            "char_start_offset": 6227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 15
                },
                {
                    "start": 18,
                    "end": 62
                },
                {
                    "start": 65,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1067
                },
                {
                    "start": 1070,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1724
                }
            ],
            "ref_mentions": [
                {
                    "start": 617,
                    "end": 633,
                    "matchedPaperCorpusId": "3603048"
                },
                {
                    "start": 919,
                    "end": 938,
                    "matchedPaperCorpusId": "256846990"
                },
                {
                    "start": 1531,
                    "end": 1555,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2186279296875
        },
        {
            "corpus_id": "259137624",
            "title": "The Effect of Masking Strategies on Knowledge Retention by Language Models",
            "text": "). This large parametric memory is often used to better transfer performance on many knowledgeintensive tasks such as question answering, factchecking, and knowledge-base completion. \n\nConsequently, there have been recent studies that aim to measure the amount of factual knowledge in the parametric memory of such large language models by probing (Belinkov, 2021). Probing entails training small classifiers on the LM's representations to predict linguistic properties such as part-of-speech tags (Tenney et al., 2019) or factual knowledge (Petroni et al., 2019). However, a key demerit of probing is that, although it can establish the presence of knowledge, it does not guarantee that the knowledge can be or is indeed used successfully in downstream tasks such as question answering (Belinkov and Glass, 2019;Tamkin et al., 2020). In this paper, we are interested in measuring the presence of such actionable factual knowledge. \n\nOur approach to investigating knowledge acquisition and containment is to use closed-book question answering (CBQA) as a representative task. The aim of CBQA is to generate answers to input questions just by using the parametric memory of the language model under consideration. The distinct advantage of using CBQA as a task, instead of probing, is that we are operating purely on the LM parameters and not relying on external output layer parameters. Second, CBQA is a well-defined downstream task and hence a reasonably good proxy for measuring actionable factual knowledge contained in the parametric memory of LMs. \n\nKnowledge is acquired by LMs by carrying out large-scale pre-training tasks or pre-fine-tuning tasks before the actual fine-tuning over the downstream task. Recent work by Roberts et al. (2020) and Lewis et al. (2021) used an additional round of pre-training to infuse more knowledge into CBQA models. However, it has been well known that fine-tuning over downstream tasks results in catastrophic forgetting. The second objective of this paper is to systematically study how factual knowledge is forgotten under different training regimes.",
            "score": 0.5227208497846031,
            "section_title": "2021",
            "char_start_offset": 7,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 2
                },
                {
                    "start": 3,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 931
                },
                {
                    "start": 934,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2095
                }
            ],
            "ref_mentions": [
                {
                    "start": 498,
                    "end": 519,
                    "matchedPaperCorpusId": "155092004"
                },
                {
                    "start": 541,
                    "end": 563,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 787,
                    "end": 813,
                    "matchedPaperCorpusId": "56657817"
                },
                {
                    "start": 813,
                    "end": 833,
                    "matchedPaperCorpusId": "216869396"
                },
                {
                    "start": 1728,
                    "end": 1749,
                    "matchedPaperCorpusId": "211205183"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4013671875
        },
        {
            "corpus_id": "267500209",
            "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
            "text": "Reasoning ability, as a basic ability of LLMs, has received great attention recently due to its great importance. Despite the notable improvements made by CoT (Wei et al., 2022), LLMs are still struggling with the tasks that require complex or high-order multi-step reasoning, such as logical reasoning and mathematic proof. Therefore, intensive research efforts have been dedicated to addressing the aforementioned issues. Generally, they can be categorized as follows. \n\nFine-tuning-based methods. These methods aim to improve the reasoning ability of LLMs through supervised fine-tuning. Usually, LLMs are fine-tuned by the samples which require manual labeling of reasoning processes, such as (Ouyang et al., 2022;Wang et al., 2022b). However, it can be labor-intensive due to the costly labeling of complex reasoning processes. The works of (Shridhar et al., 2022;Zelikman et al., 2022) first used LLMs to generate reasoning processes, but only the samples with correct results are selected for fine-tuning LLMs to reduce the labeling cost. Additionally, fine-tuned LLMs on specific tasks can suffer from the problem of \"catastrophic forgetting\", which means that the original knowledge inherited by the pre-trained LLMs will be lost and thus the ability to generalize to downstream tasks will be weakened. To this end, Cheng et al. (2023) trained a prompt retriever using the output scores of LLMs. When fine-tuning, LLMs are frozen just as a data labeler which effectively reduces the impact on LLMs. \n\nTool-based methods. Tool-based methods propose to utilize external tools to augment the capabilities of LLMs in accomplishing complex tasks (Qin et al., 2023;Schick et al., 2024). More-over, Jin et al. (2024); Yang et al. (2023) augment LLMs with external real-time knowledge or domain-specific information through specific tools. Additionally, Retrieval-Augmented Generation (RAG) related methods (Gao et al., 2023;Ma et al., 2023;Peng et al., 2024) have received a lot of attention recently, and these methods improve the reasoning ability of LLMs by incorporating external knowledge.",
            "score": 0.5226824316966008,
            "section_title": "Related Work",
            "char_start_offset": 25306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 113
                },
                {
                    "start": 114,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 470
                },
                {
                    "start": 473,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1507
                },
                {
                    "start": 1510,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1689
                },
                {
                    "start": 1690,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2096
                }
            ],
            "ref_mentions": [
                {
                    "start": 159,
                    "end": 177,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 697,
                    "end": 718,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 718,
                    "end": 737,
                    "matchedPaperCorpusId": "253098274"
                },
                {
                    "start": 869,
                    "end": 891,
                    "matchedPaperCorpusId": "247762790"
                },
                {
                    "start": 1325,
                    "end": 1344,
                    "matchedPaperCorpusId": "257532394"
                },
                {
                    "start": 1668,
                    "end": 1688,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 1701,
                    "end": 1718,
                    "matchedPaperCorpusId": "258298113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408447265625
        },
        {
            "corpus_id": "271328900",
            "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
            "text": "Immersed in a dynamic world, individuals mature through an iterative process of memorization, forgetting, error correction, and deepening understanding of the world around them. Similarly, LLMs dynamically encapsulate knowledge into parameters through the process of conflict and integration. \n\nIn the pre-training phase, LLMs start as blank slates, facilitating easier acquisition for new knowledge (Allen-Zhu and Li, 2024). Consequently, numerous experiments demonstrate that LLMs accumulate vast amounts of knowledge during this stage (Cao et al., 2024b;Zhou et al., 2023a;Kaddour et al., 2023;Naveed et al., 2023;Singhal et al., 2022). Aky\u00fcrek et al. (2022) delve further into identifying which training examples are instrumental in endowing LLMs with specific knowledge. However, contradictions during the pre-training stage may induce conflicts among internal parametric knowledge. On the one hand, the false and contradictory information in training corpus propagate and contaminate related memories in LLMs via semantic diffusion, introducing broader detrimental effects beyond direct impacts (Bian et al., 2023). On the other hand, LLMs tend to prioritize memorizing more frequent and challenging facts, which can result in subsequent facts overwriting prior memorization, significantly hindering the memorization of low-frequency facts (Lu et al., 2024). In other words, LLMs struggle with balancing and integrating both low and high-frequency knowledge. \n\nAfter pre-training, LLMs are anticipated to refresh their internal knowledge to keep pace with the evolving world during post-training stage. Although LLMs seem to absorb new knowledge through continued learning, follow user instructions via instruct tuning (Zhang et al., 2023c), and align with human values through alignment tuning (Ziegler et al., 2019), Ji et al. (2024) have noted that LLMs intrinsically resist alignment during the posttraining phase. In other words, LLMs tend to learn factual knowledge through pre-training, whereas fine-tuning4 teaches them to utilize it more efficiently (Gekhman et al., 2024;Zhou et al., 2023a;Ovadia et al., 2024).",
            "score": 0.5225626447096186,
            "section_title": "Individual Evolution",
            "char_start_offset": 25430,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 292
                },
                {
                    "start": 295,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 775
                },
                {
                    "start": 776,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1924
                },
                {
                    "start": 1925,
                    "end": 2127
                }
            ],
            "ref_mentions": [
                {
                    "start": 538,
                    "end": 557,
                    "matchedPaperCorpusId": "258715133"
                },
                {
                    "start": 557,
                    "end": 576,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 617,
                    "end": 638,
                    "matchedPaperCorpusId": "255124952"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07904052734375
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "However, if maintaining generalized capabilities is essential, multitask fine-tuning offers an alternative.This approach involves fine-tuning on multiple tasks simultaneously, requiring more data (50,000-100,000 examples) and computational resources [4].Table 1 presents an overview of datasets and benchmarks commonly used in natural language processing tasks, including their type, citation, and a brief description.Another strategy is parameter-efficient fine-tuning (PEFT), which preserves the original LLM weights by training only a small number of task-specific adapter layers.PEFT is more robust to catastrophic forgetting as it retains most of the pre-trained weights unchanged.This technique is an active research area and shows promising results in maintaining multitask performance while fine-tuning [17].\n\nIn summary, while fine-tuning can enhance performance for specific tasks, it is crucial to consider the trade-offs and potential strategies to avoid catastrophic forgetting based on the application's requirements.",
            "score": 0.5221474082113209,
            "section_title": "Improving Performance of Large Language Models through Fine-Tuning",
            "char_start_offset": 34020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 583
                },
                {
                    "start": 583,
                    "end": 686
                },
                {
                    "start": 686,
                    "end": 816
                },
                {
                    "start": 818,
                    "end": 1031
                }
            ],
            "ref_mentions": [
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "45998148"
                },
                {
                    "start": 811,
                    "end": 815,
                    "matchedPaperCorpusId": "59599816"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273193359375
        },
        {
            "corpus_id": "278208082",
            "title": "Memorization and Knowledge Injection in Gated LLMs",
            "text": "To address these challenges, we introduce MEGa (Memory Embedded in Gated LLMs), a long-term memory framework designed to enable LLMs to sequentially store new memories in a manner that reflects key aspects of human memory. \n\nTo ensure biological plausibility, MEGa encodes new memories by fine-tuning the network's weights. To mitigate catastrophic forgetting, it employs a gating mechanism that, at inference time, routes input queries to a collection of gated memory modules and activates those most relevant to the query. \n\nWe show that MEGa is capable not only of retrieving the learned memories but also of performing questionanswering (QA) tasks based on them, demonstrating the successful integration of the memories into the knowledge base of the LLM. \n\nAcross two datasets-Fictional Character and Wikipedia 2024 Events-and two tasks-memory recall and question answering-MEGa outperforms baseline continual learning (CL) techniques in mitigating both the forgetting of newly acquired memories and the degradation of general language capabilities. These results suggest that MEGa is a promising model for capturing certain aspects of human long-term memory and the structure of underlying brain circuits.",
            "score": 0.5217228977884707,
            "section_title": "Introduction",
            "char_start_offset": 7855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 225,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1211
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1729736328125
        },
        {
            "corpus_id": "268032887",
            "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
            "text": "In contrast, performance on General Knowledge tasks improved with Wiki domain training but went below the base performance led by training on S2ORC. Overall, we conclude that the performance gain/degradation measured by perplexity in the context of CL transfers to downstream performance. \n\nContinual Llama2-7B checkpoints consistently achieve chance-level performance Next, we repeat the same analysis with Llama2-7B checkpoints. As shown in Figure 12, performance drops drastically, often falling below GPT-1, regardless of the task or training order. Notably, the decline in downstream performance after the first task is catastrophic, starkly contrasting with our findings for GPT2-M. Further analysis reveals that continual pretraining severely degrades the model's ability to perform in-context learning and generate coherent dialogues. This aligns with observations from (bench authors, 2023), which highlight the brittleness of language models-their sensitivity to natural language phrasing. Taken together, our results on continual Llama2-7B checkpoints do not provide immediate insight into the extent of forgetting. We address this in the next section by introducing a novel technique to quantify (the accumulation of) model knowledge.",
            "score": 0.5210275603837964,
            "section_title": "Does continual learning influence downstream performance?",
            "char_start_offset": 22461,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06658935546875
        },
        {
            "corpus_id": "271218270",
            "title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models",
            "text": "The results presented in Table 2 are derived from experiments conducted with the base LLM, BLOOMZ-1.1B, retaining 1% training data. As shown in Table 2, the performance of the model continually tuned with Parameter-Efficient Fine-Tuning (PEFT) degrades significantly on previously learned tasks compared to the initial model, as expected. However, our proposed method, SwitchCIT, demonstrates substantial improvements over the initial model across all tasks. This indicates that SwitchCIT effectively mitigates forgetting and reaps the benefits of continual instruction tuning. Notably, SwitchCIT outperforms the PEFT model on the HGen task, which is the last task tuned, suggesting that our approach may effectively reduce negative interference from prior tasks. Furthermore, SwitchCIT exceeds the performance of the rehearsal method that uses the same retention data rate (1%) as studied by Scialom et al. (2022). This comparison highlights that our method more effectively utilizes the retained data than rehearsing previous data. We present studies on using base LLMs of varying scales and different data retention rates in Table 3. We compare SwitchCIT (BLOOMZ-1.1B and 0.01% data retention) relying on PEFT to a much larger fully supervised fine-tuned approach based on the BLOOMZ-7.1b model (Luo et al., 2023;Muennighoff et al., 2023), which is approximately seven times larger than our base LLM. Surprisingly, our method still achieves better performance on most tasks, underscoring its efficiency and effectiveness. We also include results using the larger base LLM, BLOOMZ-7.1B, which achieves the best performance with 1% data retention, demonstrating the scalability of our method. In contrast to the rehearsal method, our approach experiences notably less performance degradation when retaining 100 times less data from 1% to 0.01%. This performance gap across most tasks also remains minimal for our SwitchCIT, when using the larger base LLM, BLOOMZ-7.1B. We hypothesize that due to our high-performing switch network, our method Table 3: The final performance of various methods in different settings on different tasks in the continual instruction tuning. Tasks are presented in the learning order.",
            "score": 0.5209095014454175,
            "section_title": "Continual Instruction Tuning",
            "char_start_offset": 19191,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 458
                },
                {
                    "start": 459,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 915
                },
                {
                    "start": 916,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1524
                },
                {
                    "start": 1525,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1969
                },
                {
                    "start": 1970,
                    "end": 2171
                },
                {
                    "start": 2172,
                    "end": 2214
                }
            ],
            "ref_mentions": [
                {
                    "start": 893,
                    "end": 914,
                    "matchedPaperCorpusId": "252815378"
                },
                {
                    "start": 1316,
                    "end": 1341,
                    "matchedPaperCorpusId": "253264914"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1783447265625
        },
        {
            "corpus_id": "267681754",
            "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models",
            "text": "MUSE divides data into two sets: the Forget set, containing the information to be unlearned, and the Retain set, which measures the impact of unlearning on unrelated knowledge. Ideally, unlearning should be precise, affecting only the Forget set without disturbing the Retain set. MUSE provides fine-tuned models for both sets as optimal reference points. To capture the training dynamics, we compute the average KL divergence between the unlearned model and the MUSE reference models over the Forget and Retain sets. An effective unlearning model should closely match both references, with near-zero divergence indicating successful unlearning and model performance preservation. \n\npress the generation of unwanted memorization at decode time. Task Arithmetic (TA) approaches (Ilharco et al., 2023) also fine-tune a model to memorize the forget set and leverage linear parameter merging (Matena and Raffel, 2022) to remove the memorization in model weights. Majmudar et al. (2022) apply linear interpolation with uniform distribution at the decoding time and show that this satisfies certain differential privacy (DP) criteria. Chen and Yang (2023) tune multiple unlearning layers to handle sequential unlearning requests and then fuse and plug them back into the base LLM. We set aside post-processing methods such as directly prompting LLMs to add a guardrail (Thaker et al., 2024); our focus is on removing knowledge directly from the base LLM via fine-tuning.",
            "score": 0.520526123269305,
            "section_title": "Unlearning in Large Language Models",
            "char_start_offset": 6942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1464
                }
            ],
            "ref_mentions": [
                {
                    "start": 777,
                    "end": 799,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 888,
                    "end": 913,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 959,
                    "end": 981,
                    "matchedPaperCorpusId": "249151985"
                },
                {
                    "start": 1129,
                    "end": 1149,
                    "matchedPaperCorpusId": "264828972"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0170440673828125
        },
        {
            "corpus_id": "271218270",
            "title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models",
            "text": "Continual Learning and Catastrophic Forgetting. The study of continual learning focuses on developing algorithms that learn from a continuous stream of data, enabling a model to acquire new knowledge while retaining previously learned information without catastrophic forgetting (Wang et al., 2024b). Catastrophic forgetting happens when LLMs forget previously learned information as new tasks are learned (Luo et al., 2023). Anonymous (2024a) provides an insightful study empirically showing that pre-trained LLMs may forget domain knowledge and tasks that were not included in the fine-tuning process, while supervised fine-tuning offers substantial benefits to the models. To counter this effect, here we use a switch network to classify tasks and route computations from their instructions. By doing so, we can fine-tune task performance by including extra parameters created by PEFT methods such as LoRA for each task. Continual learning methods are often classified into the following categories: replay-based methods (Chaudhry et al., 2019(Chaudhry et al., , 2018)), regularization-based methods (Hu et al., 2019;Buzzega et al., 2020), and architecture-based methods (Veniat et al., 2021;Yan et al., 2021). Catastrophic forgetting arises when model parameters are updated to account for new data, causing degradation or a complete loss of performance on previously learned tasks. Catastrophic forgetting is not specific to LLMs. Other neural networks also experience this phenomenon, leading to methods such as Elastic Weight Consolidation (Kirkpatrick et al., 2017). Previous research to resolve this problem has scaled the number of parameters in the model (Wang et al., 2024a;Yoon et al., 2018). It has been demonstrated that these solutions work in theory but suffer from over-reliance on scaling LLM parameters. Works such as Hu et al. (2019) avoid this by splitting parameters into two sets: one for tasks learned and one to dynamically be generated. Understanding Transformers. Prior studies have offered insightful understandings of Transformer models with focuses on the internal representations (Wu andVarshney, 2024, 2023;Nanda et al., 2023) and attention mechanisms (Sun and Marasovi\u0107, 2021;Olsson et al., 2022).",
            "score": 0.5196496949844949,
            "section_title": "Related Work",
            "char_start_offset": 5128,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 48,
                    "end": 300
                },
                {
                    "start": 301,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1435
                },
                {
                    "start": 1436,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 1991
                },
                {
                    "start": 1992,
                    "end": 2231
                }
            ],
            "ref_mentions": [
                {
                    "start": 279,
                    "end": 299,
                    "matchedPaperCorpusId": "256459333"
                },
                {
                    "start": 1046,
                    "end": 1072,
                    "matchedPaperCorpusId": "54443381"
                },
                {
                    "start": 1103,
                    "end": 1120,
                    "matchedPaperCorpusId": "108297547"
                },
                {
                    "start": 1120,
                    "end": 1141,
                    "matchedPaperCorpusId": "215768806"
                },
                {
                    "start": 1174,
                    "end": 1195,
                    "matchedPaperCorpusId": "229363613"
                },
                {
                    "start": 1195,
                    "end": 1212,
                    "matchedPaperCorpusId": "232427923"
                },
                {
                    "start": 1547,
                    "end": 1573,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1686,
                    "end": 1704,
                    "matchedPaperCorpusId": "3693512"
                },
                {
                    "start": 1838,
                    "end": 1854,
                    "matchedPaperCorpusId": "108297547"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5859375
        },
        {
            "corpus_id": "259263918",
            "title": "Visual Analytics on Network Forgetting for Task\u2010Incremental Learning",
            "text": "To pinpoint the key challenges in analyzing catastrophic forgetting and meanwhile distill the corresponding design goals, we conducted interviews with two domain experts (E1 and E2) who have been working on Task-IL and general CL research for more than three years. In general, both experts commented that most existing studies focused on proposing new algorithms for mitigating the forgetting issues. In terms of performance evaluation, they only report values of mean accuracy or mean forgetting as defined in Sec. 3.3, but few of them analyzed the exact forgotten contents or the internal dynamics during model updates. E2 also mentioned that there are a few studies trying to provide theoretical explanations behind the forgetting phenomenon, while the work is neither suitable for nonexperts model analysts nor efficient in demonstrating the details in terms of the forgotten data or feature representations. Besides discussing with two experts, we also reviewed survey [DLAM * 21] on CL and other related literature on catastrophic forgetting. In summary, we identified the following three requirements:\n\n\u2022 R1: Revealing when and where catastrophic forgetting happens. Prior to any in-depth analyses, it is crucial to have an overall understanding of how models perform in the current taskincremental scenario. Based on the overview, it is easier for analysts to identify when the updated model fails to achieve an acceptable accuracy on the past datasets. To this end, the system needs to track whether each task can still be recalled at each of the following training stages, and support analysts to select one particular task for further investigations. \u2022 R2: Uncovering drifted information for the selected task.\n\n-R2.1: Capturing the model digression from the original task solution to the final training stage. During the incremental learning process, the neural network updates its parameters upon fitting the new data distribution. The digression in the parameter space provides the most intuitive understanding of what information has been forgotten. However, only visualizing the change of model parameters is insufficient to reflect the severity of forgetting. Thus, we should also provide information that allows analysts to visually connect the model drift with performance degradation. -R2.2: Exploring the forgotten data instances and their underlying patterns. Given the selected task and its subsequent models, analysts want to know whether there exist any trends or patterns hidden in the forgotten data instances. For example, when the updated model misclassifies a",
            "score": 0.5190846988082881,
            "section_title": "Requirements Analysis",
            "char_start_offset": 14601,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.257080078125
        },
        {
            "corpus_id": "269502198",
            "title": "SHED: Shapley-Based Automated Dataset Refinement for Instruction Fine-Tuning",
            "text": "The development of LLMs marks a major leap in machine learning, transforming how we approach natural language processing (NLP) and artificial intelligence (AI) research [1,2,3,4,5]. LLMs such as GPT-3 [2], Mistral [6], and LLaMA/LLaMA2 [3,4] highlight the benefits of pre-training on large and diverse mixtures of data corpora, empowering these LLMs with a wealth of knowledge [7,8]. Moreover, one of the pivotal strengths of LLMs lies in their adaptability to specific tasks through fine-tuning. Fine-tuning, a process that involves adapting LLMs to one or multiple task-specific datasets, enables the pre-trained LLM to acquire task-specific information. Furthermore, it facilitates the alignment of LLMs to more accurately follow human instructions through fine-tuning on a dataset comprised of instructions paired with appropriate responses [9], which is known as instruction tuning. \n\nHowever, fine-tuning LLMs also raises challenges. A primary concern is that noisy data or harmful instances in the fine-tuning dataset can significantly degrade the performance of pre-trained LLMs [10]. While many works have developed large and diverse datasets for fine-tuning purposes, recent research suggests that meticulously curated datasets of high quality, even if smaller in size, can be more effective in harnessing the full potential of LLMs [11,12,13]. Indiscriminately increasing the volume of data can lead to ineffective performance improvements and might even deteriorate LLM performance due to the introduction of noisy and harmful instances. Additionally, for instruction tuning, the LLM has already learned the necessary knowledge in the pre-training stage. The dataset used in the fine-tuning stage merely aims to better align the LLM to follow human instructions, indicating that this process does not necessitate extensive data [14]. Furthermore, fine-tuning LLMs on extensive datasets incurs significant computational costs. The necessity for considerable GPU resources presents a critical challenge [15]. Only researchers and institutions equipped with sufficient computing resources can perform such tasks, limiting broader applications and progress within the LLM community.",
            "score": 0.5189094693283964,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 887
                },
                {
                    "start": 890,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1666
                },
                {
                    "start": 1667,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1937
                },
                {
                    "start": 1938,
                    "end": 2018
                },
                {
                    "start": 2019,
                    "end": 2190
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 172,
                    "end": 174,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 201,
                    "end": 204,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 380,
                    "end": 382,
                    "matchedPaperCorpusId": "267311491"
                },
                {
                    "start": 1087,
                    "end": 1091,
                    "matchedPaperCorpusId": "224801132"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1083984375
        },
        {
            "corpus_id": "267782481",
            "title": "Unveiling Linguistic Regions in Large Language Models",
            "text": "In this section, we demonstrate that stabilizing the core linguistic regions (identified in Section 3.2) during further pre-training mitigates the catastrophic forgetting (CF) issue (McCloskey and Cohen, 1989;Kemker et al., 2018)  maintaining learning proficiency comparable to full-scale fine-tuning in target language. Our experimental setup involves further pre-training LLaMA-2-7B on 100,000 Arabic sentences, with a batch size of 256, a maximum token length of 512, and learning rates (lr) of 5e\u22125 or 5e\u22126, employing perplexity (PPL) as evaluation criterion. \n\nFull-scale Model Fine-tuning Traditional fullscale fine-tuning, when increasing the learning rate or the amount of corpus data, enhances learning in the target language but aggravates forgetting in non-target languages. As shown in Table 2, since LLaMA-2 is primarily trained on English corpora, conducting a second stage of pre-training solely on large-scale Chinese corpora can lead to the forgetting of English competence. Additionally, as depicted on the left side of Figure 6 in blue line, increasing lr from 5e\u22126 (dotted line) to 5e\u22125 (solid line) under full-scale fine-tuning boosts the acquisition of the target language (Arabic), while simultaneously accelerates the forgetting rate of the nontarget languages (English and Chinese), as shown in the middle and right side. \n\nFreeze Core Regions Fine-tuning We hypothesize that CF problem occurs due to the amplification of parameter adjustments when increasing the learning rate, which leads to significant shifts in the core linguistic region, adversely affecting language alignment. To mitigate this, we protect the core linguistic region and key dimensions by freezing the 'Top 5%' core language area for fine-tuning, as shown by the red line in Figure 6. At a lr of 5e\u22126 (dotted line), the difference between freezing finetuning and full-scale fine-tuning is minimal.",
            "score": 0.5186271227904616,
            "section_title": "Further Pre-training Optimization",
            "char_start_offset": 19102,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 563
                },
                {
                    "start": 566,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1346
                },
                {
                    "start": 1349,
                    "end": 1608
                },
                {
                    "start": 1609,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1895
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 229,
                    "matchedPaperCorpusId": "22910766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38623046875
        },
        {
            "corpus_id": "53860287",
            "title": "Experience Replay for Continual Learning",
            "text": "We here demonstrate the efficacy of CLEAR for diminishing catastrophic forgetting (Figure 2). We apply CLEAR to the cyclically repeating sequence of DMLab tasks used in the preceding experiment. \n\nOur method effectively eliminates forgetting on all three tasks, while preserving overall training performance (see \"Sequential\" training in Figure 1 for reference). When the task switches, there is little, if any, dropoff in performance when using CLEAR, and the network picks up immediately where it left off once a task returns later in training. Without behavioral cloning, the mixture of new experience and replay still reduces catastrophic forgetting, though the effect is reduced. \n\nIn Figure 3, we perform a quantitative comparison of the performance for CLEAR against the performance of standard training on sequential tasks, as well as training on tasks separately and simultaneously. In order to perform a comparison that effectively captures the overall performance during continual learning (including the effect of catastrophic forgetting), the reward shown for time t is the average (1/t) P s<t r s , thus effectively measuring the area under the curve for plots such as Figure 2. (We replot the results of our main experiments according to the cumulative performance in Appendix B.) We find that CLEAR attains similar cumulative performance to networks trained on tasks separately and simultaneously -effectively eliminating catastrophic forgetting.",
            "score": 0.5183274531084483,
            "section_title": "Stability",
            "char_start_offset": 12849,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 194
                },
                {
                    "start": 197,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1462
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2015380859375
        },
        {
            "corpus_id": "268513020",
            "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
            "text": "In this paper, we mainly discuss the efficient adaptation of LLMs to continual downstream tasks with instructions.Replay-based CL methods do not require additional modifications to LLMs and fully utilize previous data, mitigating catastrophic forgetting effectively.We proposed InsCL, an effective data-efficient method to mitigate catastrophic forgetting for LLMs instruction tuning.InsCL is a model-agnostic and training-free method, indicating strong transferability.Different from existing replay-based methods, we fully utilize instructions as representative task descriptions to design the replay strategy.InsCL leverages instruction embeddings and distributions to calculate Wasserstein distance for task similarity, adjusting the replay ratio dynamically.Then, with our InsInfo-guided sampling, InsCL selects more high-quality data with complex and diverse instructions.We conduct extensive experiments over 16 tasks with different training orders, observing consistent performance improvements of InsCL.Additionally, we further analyze the forgetting rate and forgetting category, aiming to provide a guideline for future work.\n\nThe promising performance demonstrated by In-sCL is dependent on high-quality instructions.Instead, fuzzy instructions can affect the calculation of task similarity and the InsInfo-guided sampling, which may mislead our InsCL.However, if the instruction-based dataset is unsatisfied, the performance of tuned LLMs will also be greatly affected.Therefore, we tend to use our method after collecting high-quality instruction-based data to further mitigate catastrophic forgetting.",
            "score": 0.5177277013568348,
            "section_title": "Conclusions",
            "char_start_offset": 22373,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 384
                },
                {
                    "start": 384,
                    "end": 470
                },
                {
                    "start": 470,
                    "end": 612
                },
                {
                    "start": 612,
                    "end": 763
                },
                {
                    "start": 763,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 1012
                },
                {
                    "start": 1012,
                    "end": 1136
                },
                {
                    "start": 1138,
                    "end": 1229
                },
                {
                    "start": 1229,
                    "end": 1364
                },
                {
                    "start": 1364,
                    "end": 1482
                },
                {
                    "start": 1482,
                    "end": 1616
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1630859375
        },
        {
            "corpus_id": "273026048",
            "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
            "text": "Factuality and hallucination have been longstanding issues for natural language generation (Lee et al., 2022;Ji et al., 2023). Lee et al. (2022), Li et al. (2023), andChuang et al. (2024) proposed decoding techniques that improved factuality of LMs. Shuster et al. (2021) reduced hallucination by retrieval-augmented generation. (Dhuliawala et al., 2023) proposed chain-of-verification to reduce LLM hallucination. SelfCheckGPT (Manakul et al., 2023) proposed a method to selfcheck factuality by sampling multiple generations. FactScore (Min et al., 2023;Chiang and Lee, 2024) and LongFact (Wei et al., 2024) both introduced frameworks for evaluating factuality of long-form generations. FAVA (Mishra et al., 2024) introduced fine-grained hallucination categories to evaluate the models and provided a detailed view of the hallucination issues of LLMs. Our proposed method also utilize a long-form factuality evaluator, while focusing on leveraging the provided factuality assessments for better factuality alignment. \n\nPrior work has also worked on training LMs to be more factual. FactTune (Tian et al., 2024) leveraged FactScore to construct preference pairs and demonstrated improvement on the bio generation task. FLAME (Lin et al., 2024) introduced factuality-aware alignment which combines FactTune with open-domain prompts. KnowTuning (Lyu et al., 2024) proposed knowledge augmentation which constructs synthetic pairs for DPO training. On the other hand, recent work has shown that fine-tuning LMs on new knowledge might encourage hallucinations (Gekhman et al., 2024;Kang et al., 2024). Our work additionally proposes fKTO for fine-grained factuality alignment, which achieves superior performance.",
            "score": 0.5171663150245398,
            "section_title": "Factuality of Langage Models",
            "char_start_offset": 4456,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 1017
                },
                {
                    "start": 1020,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1708
                }
            ],
            "ref_mentions": [
                {
                    "start": 91,
                    "end": 109,
                    "matchedPaperCorpusId": "249538460"
                },
                {
                    "start": 109,
                    "end": 125,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 127,
                    "end": 144,
                    "matchedPaperCorpusId": "249538460"
                },
                {
                    "start": 167,
                    "end": 187,
                    "matchedPaperCorpusId": "261582463"
                },
                {
                    "start": 250,
                    "end": 271,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 428,
                    "end": 450,
                    "matchedPaperCorpusId": "257557820"
                },
                {
                    "start": 537,
                    "end": 555,
                    "matchedPaperCorpusId": "258841470"
                },
                {
                    "start": 590,
                    "end": 608,
                    "matchedPaperCorpusId": "266551413"
                },
                {
                    "start": 1092,
                    "end": 1111,
                    "matchedPaperCorpusId": "265158181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08154296875
        },
        {
            "corpus_id": "268379635",
            "title": "SOTOPIA-\u03c0: Interactive Learning of Socially Intelligent Language Agents",
            "text": "LLM Alignment and Evaluation Advances in fine-tuning methods like parameter-efficient finetuning (Li and Liang, 2021;Lester et al., 2021;Hu et al., 2021) have improved LLMs' capabilities to better understand the restriction and rules given by human, enhancing their capability for social learning and interaction.Other governance objectives align LLM behaviors via robustness, interpretability, controllability, and ethicality (Ji et al., 2024).We focus on evaluating our trained LLMs' alignment with human social norms via safety and toxicity.\n\nIt has been pointed out that continual fine-tuning can lead to catastrophic forgetting of LLMs, in terms of domain knowledge, reasoning, and reading comprehension (Luo et al., 2023).To test the general question answering and reasoning capabilities of our trained LLMs, we measure their performance on the Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2020), a holistic benchmark designed to test the knowledge of a model across 57 subjects.",
            "score": 0.5169861979969879,
            "section_title": "Reinforcement Learning for LLMs",
            "char_start_offset": 27503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 544
                },
                {
                    "start": 546,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 1017
                }
            ],
            "ref_mentions": [
                {
                    "start": 909,
                    "end": 933,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.316650390625
        },
        {
            "corpus_id": "273345395",
            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
            "text": "In conclusion, we introduce self-data distilled fine-tuning as an effective method to mitigate quality degradation in pruned LLMs, addressing catastrophic forgetting while preserving alignment with the model's original data distribution. \n\nOur approach consistently outperforms standard supervised fine-tuning, demonstrating superior accuracy recovery postpruning across various downstream tasks on the OpenLLM Leaderboard v1. Additionally, model merging via SLERP further enhances recovery, achieving significant quality retention. We also show that our method scales with dataset size, where larger self-distilled datasets lead to improved quality recovery. Moreover, integrating self-data with speculative decoding not only enhances token acceptance rates but also reduces inference latency, offering an effective strategy for deploying pruned LLMs efficiently. These findings highlight self-data distilled fine-tuning as a critical tool for maintaining high model quality post-pruning, offering an efficient solution for model compression. Future work may involve integrating self-data distilled fine-tuning with complementary model compression techniques such as sparsity, quantization or teacher distillation, potentially yielding greater efficiency without sacrificing model quality. Extending these methodologies to next-generation LLM architectures presents a promising avenue for unlocking additional computational efficiency and model robustness. (Ji et al., 2024). While our analysis focuses on the practical selection of calibration datasets, a deeper investigation into calibration dataset characteristics and their influence on pruning decisions remains an open question for future work.",
            "score": 0.5167936427159988,
            "section_title": "CONCLUSION",
            "char_start_offset": 34835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 659
                },
                {
                    "start": 660,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1702
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.427001953125
        },
        {
            "corpus_id": "263830425",
            "title": "TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models",
            "text": "Our experiments with TRACE show that direct end-to-end training might compel LLMs to focus myopically on specific patterns of the target task, such as shortcuts, thereby undermining their capacities in more universal scenarios. Intrinsically, LLMs are trained on large and varied datasets. So, they already have the skills to handle many tasks, and can even learn with very few examples. \n\nBased on the ideas from LIMA's Superficial Alignment Hypothesis (Zhou et al., 2023), our focus should perhaps pivot more towards adeptly aligning LLMs' existing capabilities to novel tasks rather than embarking on learning from scratch. Consequently, strategies like our RCL approach, which capitalize on the LLMs' inherent abilities for quick transfer to new tasks, might also serve as potent tools in mitigating catastrophic forgetting.",
            "score": 0.5163130485427747,
            "section_title": "How should LLMs approach continual learning?",
            "char_start_offset": 28494,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 828
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3916015625
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "Firstly, Figure 3 shows the performance of the continually trained BLOOMZ-7.1b model on the instruction tasks compared to the initial model's performance. For example, after being trained on the Simp training data, the BLOOMZ model achieves a SARI score of 47.24 on the Simp test set, which is 1.98 higher than the initial model's score. Subsequently, when we continually train the model on the Emdg dataset, the obtained model achieves a BLEU score of 53.3, surpassing the initial model's performance by 3.62. These improvements demonstrate that the model can benefit from the instruction tuning process and achieve significantly better performance on the instruction tasks. Additional results for other LLMs are presented in Appendix B. \n\nNext, Figure 2 displays the FG values of BLOOMZ-1.1b and BLOOMZ-7.1B. As we can observe, the performance gradually decreases as we continually tune the model with instruction tasks. For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting. We can also notice that the performance of the BLOOMZ-7.1b model drops more drastically in these evaluation tasks, The main results of forgetting are reported in Table 3. We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively. Interestingly, we observe that the FG values for bias (Table 4) are mostly above zero in the experiments, which suggests that model biases, such as those related to race, color, gender, and so on, are mitigated during continual instruction tuning. For instance, in sentences describing physical appearance, BLOOMZ-7.1b",
            "score": 0.5161712409673523,
            "section_title": "Main Results",
            "char_start_offset": 15167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2175
                },
                {
                    "start": 2176,
                    "end": 2246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "271097791",
            "title": "Mitigating Catastrophic Forgetting in Language Transfer via Model Merging",
            "text": "Catastrophic Forgetting Neural networks trained on a specific task are known to catastrophically forget the previous task when adapted to a new one (French, 1999;Goodfellow et al., 2014;Kemker et al., 2018).While this becomes less pronounced as model and pertaining data size grow (Ramasesh et al., 2022), it remains a severe issue even for modern LLMs (Zhai et al., 2023;Shi et al., 2024;Li and Lee, 2024;Gogoulou et al., 2023).\n\nMitigating Catastrophic Forgetting As LLMs are frequently finetuned or continually pretrained on new tasks, mitigating catastrophic forgetting has become essential and a wide range of methods has been proposed.Lee et al. (2020) suggest to randomly reset weights to their pretrained state.Biderman et al. (2024) show that LORA reduces forgetting at the cost of reduced learning.Model Merging Model merging was originally proposed in federated learning (McMahan et al., 2017) to lower communication costs, and was successfully deployed (Stoica et al., 2023;Matena and Raffel, 2022).As a way to combine multiple models without training, it has recently gained popularity in the LLM community (Goddard et al., 2024).Popular methods include LINEAR or TASK ARITH-METIC (Ilharco et al., 2023) which perform linear interpolation of task vectors, its extension MODEL BREADCRUMBS (Davari and Belilovsky, 2023) which discards large weight changes, TIES (Yadav et al., 2023) which uses heuristics favoring large weight changes, DARES (Yu et al., 2023) which randomly drops weight changes before merging, MODEL STOCK (Jang et al., 2024) which merge weights layer-wise, to in expectation, minimize distance to the center of the task vector distribution, and SLERP (Shoemake, 1985) which averages weights in polar coordinates.\n\nMultiple works have shown that merging during continued pretraining or finetuning, especially on non-IID data, can match or improve the performance of compound training.",
            "score": 0.5159813565653334,
            "section_title": "Related work",
            "char_start_offset": 24631,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 207,
                    "end": 429
                },
                {
                    "start": 431,
                    "end": 641
                },
                {
                    "start": 641,
                    "end": 719
                },
                {
                    "start": 719,
                    "end": 808
                },
                {
                    "start": 808,
                    "end": 1011
                },
                {
                    "start": 1011,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1742
                },
                {
                    "start": 1744,
                    "end": 1913
                }
            ],
            "ref_mentions": [
                {
                    "start": 148,
                    "end": 162,
                    "matchedPaperCorpusId": "2691726"
                },
                {
                    "start": 162,
                    "end": 186,
                    "matchedPaperCorpusId": "12730344"
                },
                {
                    "start": 186,
                    "end": 206,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 281,
                    "end": 304,
                    "matchedPaperCorpusId": "251648120"
                },
                {
                    "start": 641,
                    "end": 658,
                    "matchedPaperCorpusId": "202750126"
                },
                {
                    "start": 882,
                    "end": 904,
                    "matchedPaperCorpusId": "14955348"
                },
                {
                    "start": 986,
                    "end": 1010,
                    "matchedPaperCorpusId": "244345933"
                },
                {
                    "start": 1194,
                    "end": 1216,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1681,
                    "end": 1697,
                    "matchedPaperCorpusId": "11290566"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371337890625
        },
        {
            "corpus_id": "271865581",
            "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
            "text": "To overcome catastrophic forgetting and maintain new learning ability, many works have proposed to merge models for old tasks and update models for new tasks [138,161,209]. Tangent Model Composition [117] proposes fine-tuning each task independently in the tangent space of the pre-trained model and then linearly fine-tuning these models to perform CL. This approach does not depend on the specific settings of CL and can be easily applied to task, class, and domain-incremental learning scenarios. In addition, ITA [143] emphasizes the necessity for the fine-tuned model to be in the same basin as the pre-trained model to ensure the composability of nonlinear models. It introduces a regularization term similar to EWC [94] in traditional CL to constrain the distance between the fine-tuned weights and the pre-trained weights when training the independent model. WARP [152] suggests linearly interpolating the pre-trained LLM's weights with its aligned weights via RLHF on a preference dataset, thus mitigating the forgetting of knowledge from the pre-trained LLM. BAM [7] continuously adapts LLMs to new languages by merging models while preserving general capabilities. Model Tailor [243] explores the problem of catastrophic forgetting during fine-tuning of MLLMs, and proposes to merge only the most important subset of parameters in the fine-tuned MLLM model into the pre-trained MLLM model, so as to retain the generalization ability of the pre-trained model as much as possible, while compensating the selected weights to reduce the performance of the finetuning task. MagMax [125] merges pruned task vectors to further alleviate parameter sign conflicts and old knowledge forgetting. Equifinality, PAINT [70] and LM-Cocktail [213] interpolate the weights of the finetuned model and the zero-shot model to improve accuracy on downstream tasks without degrading accuracy on supported/general tasks. \n\nIn contrast to merging full models, some research focuses on merging parameter-efficient modules. Chitale et al. [22] propose a CL method based on task arithmetic [71].",
            "score": 0.5159149708952482,
            "section_title": "Model Merging to Mitigate Catastrophic Forgetting",
            "char_start_offset": 54317,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 670
                },
                {
                    "start": 671,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1908
                },
                {
                    "start": 1911,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 163,
                    "end": 167,
                    "matchedPaperCorpusId": "256231189"
                },
                {
                    "start": 167,
                    "end": 171,
                    "matchedPaperCorpusId": "237420687"
                },
                {
                    "start": 199,
                    "end": 204,
                    "matchedPaperCorpusId": "259937354"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1587,
                    "end": 1592,
                    "matchedPaperCorpusId": "271064651"
                },
                {
                    "start": 1716,
                    "end": 1720,
                    "matchedPaperCorpusId": "251493208"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.172607421875
        },
        {
            "corpus_id": "264720747",
            "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models",
            "text": "Despite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;Chen et al., 2020). Furthermore, Kumar et al. (2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks. In this work, for the first time, we systematically investigate the behavior of the fine-tuned source code models by carefully designing various OOD scenarios.",
            "score": 0.5158101808535552,
            "section_title": "INTRODUCTION",
            "char_start_offset": 7721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 564
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 203,
                    "matchedPaperCorpusId": "216553067"
                },
                {
                    "start": 218,
                    "end": 237,
                    "matchedPaperCorpusId": "247011290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "269502676",
            "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
            "text": "Second, we find that the standard reward used in the RL stage often prefers longer and more detailed responses (Singhal et al., 2023;Yuan et al., 2024), which tends to stimulate the LLM to yield more false claims, as shown in the black dots in Figure 1.One possible reason is that most existing RLHF or RLAIF approaches rely on a single scalar reward to represent preference, which struggles to cover multiple alignment skill sets (Ye et al., 2024) and is likely to under-present the aspect of factuality (Hosking et al., 2024).\n\nTo address the aforementioned issues, we study the key factors which impact factuality during alignment.In particular, we first conduct a pilot study on the biography generation task (Min et al., 2023) in a more controlled setting where the alignment process focuses solely on factuality (Section 3).Our pilot study reveals that a LLM hallucinates more if it is fine-tuned on new knowledge in either the SFT or the RL stage.For example, a LLM becomes significantly less factual when fine-tuned on responses produced by a model with access to external knowledge (e.g. a retrievalaugmented LLM), even though those responses are more factual themselves.Similarly, hallucination is greatly increased if RLAIF is performed on preference pairs that consist of retrieval-augmented LLM output as positive examples and the LLM's own output as negative examples.As a result, we discover that fine-tuning a pre-trained LLM on (a selected subset of) its own generations yields more factual responses and reduces hallucinations.\n\nOur ultimate goal is to improve the factuality of the standard alignment process, which is challenging since LLMs may be given diverse and complex instructions.As shown in Figure 2, we observe that some instructions require factual responses while the others do not.Motivated by the observation, we propose factuality-aware alignment.We first identify fact-based instructions that require factual responses.For fact-based instructions, we leverage the findings in our pilot study to create additional training data at both SFT and RL stages to explicitly guide LLMs to output factual responses.",
            "score": 0.5157076220043493,
            "section_title": "Introduction",
            "char_start_offset": 1341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 253,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 634
                },
                {
                    "start": 634,
                    "end": 830
                },
                {
                    "start": 830,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1180
                },
                {
                    "start": 1180,
                    "end": 1382
                },
                {
                    "start": 1382,
                    "end": 1545
                },
                {
                    "start": 1547,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1813
                },
                {
                    "start": 1813,
                    "end": 1881
                },
                {
                    "start": 1881,
                    "end": 1954
                },
                {
                    "start": 1954,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 448,
                    "matchedPaperCorpusId": "259991144"
                },
                {
                    "start": 505,
                    "end": 527,
                    "matchedPaperCorpusId": "263134280"
                },
                {
                    "start": 713,
                    "end": 731,
                    "matchedPaperCorpusId": "258841470"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.168701171875
        },
        {
            "corpus_id": "273549315",
            "title": "ToolFlow: Boosting LLM Tool-Calling Through Natural and Coherent Dialogue Synthesis",
            "text": "Compromised by Fine-tuning. \n\nThe fine-tuned model risks catastrophic forgetting, where the capability for tool call is enhanced, but other abilities decline. As an AI assistant, LLM's reasoning and conversational abilities are equally important. Therefore, we tested the tuned model on the MMLU, BBH, and MTBench datasets to examine whether catastrophic forgetting issues have occurred. The results are shown in",
            "score": 0.5152441020817581,
            "section_title": "TOOLFLOW's General Ability Is NOT",
            "char_start_offset": 24159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 30,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60107421875
        },
        {
            "corpus_id": "270371124",
            "title": "Towards Lifelong Learning of Large Language Models: A Survey",
            "text": "Continual Vertical Domain Pretraining [31,47,49,78,102,127,157,158,227,228,235,241] involves continuously training a language model on a series of domain-specific datasets.This method ensures the model performs efficiently across multiple vertical domains or tasks while retaining previously acquired knowledge.For instance, continual pretraining on financial domain data enables LLMs to provide a better analysis of financial texts and data [231].\n\nExperimental investigations in continual vertical domain pretraining primarily focus on addressing catastrophic forgetting [24,58,235].As a pioneering work, Jin et al. [78] revealed that distillation-based approaches are most effective in retaining downstream performance in earlier domains.Building on this, Mehta et al. [131] found that models pretrained on a diverse set of tasks tend to experience less forgetting compared to those trained from scratch, highlighting the benefits of task diversity.Similarly, Cossu et al. [31] demonstrated that continual pretraining can help mitigate forgetting, supporting the notion that sustained exposure to various tasks can enhance model robustness.However, Li et al. [102] emphasized that catastrophic forgetting remains a significant challenge and cannot be fully resolved through straightforward methods such as freezing layers, modules, LoRA, and (IA) 3 [113].These findings collectively underscore the complexity of addressing catastrophic forgetting and the need for innovative approaches in continual vertical domain pretraining.Research on continual vertical domain pretraining has been evolving with various techniques, including but not limited to experience replay [47,78,117,158], parameter-efficient finetuning [47,78,112,178], mixture of experts [50,73], knowledge distillation [78,157], model expansion [17,89,158,219], re-warming [49], and data selection [2,112,127].\n\n3.1.1Parameter-Efficient Fine-Tuning.Parameter Efficient Fine-Tuning is a technique designed to optimize models for specific tasks without requiring extensive computational resources.CorpusBrain++ [47] addresses the dynamic nature of real-world knowledge-intensive language tasks by employing a backbone-adapter architecture and an experience replay strategy.",
            "score": 0.5150767502661772,
            "section_title": "Continual Vertical Domain Pretraining",
            "char_start_offset": 17520,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 448
                },
                {
                    "start": 450,
                    "end": 585
                },
                {
                    "start": 585,
                    "end": 741
                },
                {
                    "start": 741,
                    "end": 952
                },
                {
                    "start": 952,
                    "end": 1143
                },
                {
                    "start": 1143,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1877
                },
                {
                    "start": 1879,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 1916
                },
                {
                    "start": 1916,
                    "end": 2062
                },
                {
                    "start": 2062,
                    "end": 2238
                }
            ],
            "ref_mentions": [
                {
                    "start": 45,
                    "end": 48,
                    "matchedPaperCorpusId": "260704601"
                },
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 59,
                    "end": 63,
                    "matchedPaperCorpusId": "258686422"
                },
                {
                    "start": 63,
                    "end": 67,
                    "matchedPaperCorpusId": "247447415"
                },
                {
                    "start": 67,
                    "end": 71,
                    "matchedPaperCorpusId": "259137655"
                },
                {
                    "start": 79,
                    "end": 83,
                    "matchedPaperCorpusId": "249642442"
                },
                {
                    "start": 442,
                    "end": 447,
                    "matchedPaperCorpusId": "259129734"
                },
                {
                    "start": 577,
                    "end": 580,
                    "matchedPaperCorpusId": "233189563"
                },
                {
                    "start": 618,
                    "end": 622,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 772,
                    "end": 777,
                    "matchedPaperCorpusId": "245329773"
                },
                {
                    "start": 1352,
                    "end": 1357,
                    "matchedPaperCorpusId": "248693283"
                },
                {
                    "start": 1674,
                    "end": 1677,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 1677,
                    "end": 1681,
                    "matchedPaperCorpusId": "243865211"
                },
                {
                    "start": 1681,
                    "end": 1685,
                    "matchedPaperCorpusId": "247447415"
                },
                {
                    "start": 1722,
                    "end": 1725,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 1729,
                    "end": 1733,
                    "matchedPaperCorpusId": "255124952"
                },
                {
                    "start": 1754,
                    "end": 1758,
                    "matchedPaperCorpusId": "236976189"
                },
                {
                    "start": 1758,
                    "end": 1761,
                    "matchedPaperCorpusId": "256627673"
                },
                {
                    "start": 1786,
                    "end": 1790,
                    "matchedPaperCorpusId": "239016173"
                },
                {
                    "start": 1790,
                    "end": 1794,
                    "matchedPaperCorpusId": "258686422"
                },
                {
                    "start": 1812,
                    "end": 1816,
                    "matchedPaperCorpusId": "238856697"
                },
                {
                    "start": 1819,
                    "end": 1823,
                    "matchedPaperCorpusId": "247447415"
                },
                {
                    "start": 1840,
                    "end": 1844,
                    "matchedPaperCorpusId": "260704601"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22021484375
        },
        {
            "corpus_id": "269148645",
            "title": "Towards Practical Tool Usage for Continually Learning LLMs",
            "text": "But editing factual knowledge can warp the innate knowledge structure of LLMs (Gupta et al., 2023) and approaches that do not directly intervene on the parameters require the use of additional memory (Mitchell et al., 2022b;Dong et al., 2022).Another is the usage of lowrank adapters (Hu et al., 2022), which freezes a base model and introduces smaller adapters which can be used to fine-tune the model for down-stream tasks without needing to train it explicitly.However, adapters are task specific, meaning this can be costly once the number of tasks has grown, and it is the adapter that is tasked with handling changes in the data rather than the model itself.\n\nTangential to the knowledge forgetting problem, LLMs are trained to use tools (Schick et al., 2023) through APIs and retrieve information from outside sources rather than parameters directly (Lewis et al., 2020).Furthermore, with tool API the information being stored outside of LLMs allow for independent updates and a model using it only requires maintaining updates to the tools usage to remain up-to-date.Though this provides a reasonable simplification to the differential expiry rates in knowledge, tool-use itself does not make LLMs everlasting, as both the tools themselves and the set of existing tools can change, which tools LLMs must adapt to.As such, tool-use itself is insufficient for the non-stationary setups as discussed in the continual learning (CL) literature (Ring, 1998;Thrun, 1998), where it is the model that must learn to autonomously adapt to change in either the state of the world as well as down-stream tasks.Within this setting, this points at the non-stationarity in the tool definition which can inherently lead to difficulties adjusting to distribution shifts, as learned features for specific tasks often cannot adapt to new ones (Kumar et al., 2022).\n\nSuch simplification of complex tasks also runs the risk of overfitting to present tasks, leading to forgetting the past (McCloskey and Cohen, 1989a;French, 1993;Xie et al., 2021) by large parameteric models.",
            "score": 0.5147238108363982,
            "section_title": "Introduction",
            "char_start_offset": 1711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 664
                },
                {
                    "start": 666,
                    "end": 878
                },
                {
                    "start": 878,
                    "end": 1075
                },
                {
                    "start": 1075,
                    "end": 1321
                },
                {
                    "start": 1321,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1852
                },
                {
                    "start": 1854,
                    "end": 2061
                }
            ],
            "ref_mentions": [
                {
                    "start": 78,
                    "end": 98,
                    "matchedPaperCorpusId": "258865393"
                },
                {
                    "start": 200,
                    "end": 224,
                    "matchedPaperCorpusId": "249642147"
                },
                {
                    "start": 224,
                    "end": 242,
                    "matchedPaperCorpusId": "252762125"
                },
                {
                    "start": 284,
                    "end": 301,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 744,
                    "end": 765,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 857,
                    "end": 877,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1447,
                    "end": 1459,
                    "matchedPaperCorpusId": "2879680"
                },
                {
                    "start": 1459,
                    "end": 1471,
                    "matchedPaperCorpusId": "61636432"
                },
                {
                    "start": 1831,
                    "end": 1851,
                    "matchedPaperCorpusId": "247011290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06280517578125
        },
        {
            "corpus_id": "276557795",
            "title": "LUME: LLM Unlearning with Multitask Evaluations",
            "text": "We propose LUME, a new benchmark covering three distinct tasks to evaluate unlearning in LLMs. Detailed experiments reveal the challenge presented by our benchmark since most algorithms fail to sufficiently unlearn the forget set without substantial degradations on the retain set and model utility. We hope our benchmark spurs further developments in LLM unlearning research. \n\nLimitations and Future Work (Carlini et al., 2022) show that the risk of memorization increases with large model size. However, due to computational limitations and easy availability of large public LLMs, we only provide finetuned checkpoints for 1B and 7B OLMo, and defer release of larger models to future work. Moreover, licensing restrictions prevent us from releasing fine-tuned models based on few publicly available LLMs such as LLaMa (lla, 2023). We acknowledge that LLM-generated data can exhibit specific biases found in their training data set. We partially mitigate this by seeding the generation prompt with pre-sampled character and location names to ensure diversity in generated content. We also conducted manual evaluations of the generated creative content to ensure its quality.",
            "score": 0.5146303362639788,
            "section_title": "Conclusion",
            "char_start_offset": 9601,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1176
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 429,
                    "matchedPaperCorpusId": "246863735"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.038238525390625
        },
        {
            "corpus_id": "275906908",
            "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning",
            "text": "So,  +  = 1  + 1  = +  . Now, we know that + = \u22123 5 and  = 4 5 by the relationship between sum/products of roots and the coefficients of a polynomial. Hence  +  =  +   = \u2212 3 4 \n\nSelf-Output (Llama 3 8B Instruct): Let  1 and  2 be the roots of 5 . Then, we have + = 1 The \"non-forgetting\" impact of low-perplexity training on the performance of tasks not being fine-tuned, which maintains the practicality of the model, is still underexplored. \n\n(3) Since model size plays a role in distillation benefits, it is important to disentangle the effects of model scale and evaluate low-perplexity training using models of the same size. \n\nOur work offers an empirical explanation, grounded in loss and perplexity reduction, for why LLMgenerated data tends to cause less degradation in fine-tuning outcomes. Through systematic analysis, we first show that fine-tuning with different types of LLM-generated data (e.g., self-generated and rephrased responses) significantly mitigates degradation on non-target tasks compared to fine-tuning with ground truth data. Second, we trace this improvement to a key property illustrated in Figure 1: LLM-generated responses exhibit lower sentence-level perplexity and a smaller proportion of high-perplexity tokens. This leads to a practical insight: by simply masking high-perplexity tokens in ground truth training data, we can achieve a comparable reduction in non-target task degradation as observed with LLM-generated data. Importantly, this can be done using the same model prior to training, allowing us to pre-process the data and harness the benefits of low-perplexity training. \n\nWe propose a novel empirical explanation for non-target task performance degradation, introducing Selective Token Masking (STM), a simple yet effective strategy that filters out high-perplexity tokens to enable low-perplexity training. We have applied STM on (1) different perplexity criteria, (2) different model families and scales, and (3) various fine-tuning techniques from full-weight fine-tuning to parameter-efficient fine-tuning.",
            "score": 0.5144133376408798,
            "section_title": "Introduction",
            "char_start_offset": 2167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 175
                },
                {
                    "start": 178,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 442
                },
                {
                    "start": 445,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 800
                },
                {
                    "start": 801,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1619
                },
                {
                    "start": 1622,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2060
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1607666015625
        },
        {
            "corpus_id": "273654435",
            "title": "Improving Multimodal Large Language Models Using Continual Learning",
            "text": "Advances in integrating visual information with large language models (LLMs) have led to the development of multimodal large language models (MLLMs), excelling at many vision-language (VL) tasks [1][2][3][4][5][6][7][8][9][10][11][12]. Recent studies converge on a general recipe for developing MLLMs: Alignment of LLM token embeddings with visual embeddings followed by instruction-tuning on VL tasks like visual question answering (VQA) [8]. However, creating an MLLM often degrades the LLM's natural language understanding (NLU) and generation (NLG) performance, a phenomenon known as catastrophic forgetting [3,6]. For instance, PaLM-E experienced an 87% drop in NLG performance over the base LLM [6]. Similar forgetting has been noted for LLaVA [11], but little work has addressed understanding and mitigating this issue. Multimodal LLMs are designed in part to serve as general multimodal understanding models [13,14]. They must therefore perform well not only on vision-language data, but also retain their linguistic or text-only performance. \n\nHere, we study mitigating the loss of linguistic abilities in the popular LLaVA MLLM using continual learning (CL) techniques designed to mitigate catastrophic forgetting [15]. In CL, a sequence of non-stationary tasks is learned, where we treat the first task as already learned by the base LLM followed by new VL tasks. We study these methods in two paradigms. In the first, we seek to recreate LLaVA 1.5 while mitigating linguistic forgetting through our methods, and in the second, we sequentially learn each VL dataset in the LLaVA recipe. \n\nThis paper makes the following contributions: \n\n1. Using the original LLaVA 1.5 training recipe, we study linguistic forgetting in 9 MLLMs, including 5 built on the Pythia family of LLMs to study the role of model scales and instruction tuning on such linguistic forgetting.",
            "score": 0.5140618719401595,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1874
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "248476411"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 916,
                    "end": 920,
                    "matchedPaperCorpusId": "266174746"
                },
                {
                    "start": 1224,
                    "end": 1228,
                    "matchedPaperCorpusId": "73497737"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51171875
        },
        {
            "corpus_id": "269214372",
            "title": "Exploring the landscape of large language models: Foundations, techniques, and challenges",
            "text": "Fine-tuning LLMs is a crucial process that involves adapting a pre-trained model to specific tasks or domains to enhance its performance and applicability (Radiya-Dixit & Wang, 2020).\n\nInitially trained on extensive and diverse datasets in an unsupervised manner, an LLM can be fine-tuned using supervised training on narrower datasets that align with one or more particular tasks or the user's specific needs.This process typically involves exposing the model to taskspecific examples and retraining it on this targeted data.Fine-tuning allows the model to learn task-specific complexities, vocabulary, and context, tailoring its capabilities to better suit specialized applications such as sentiment analysis, text summarization, or domain-specific conversational interactions (Howard & Ruder, 2018;Wei et al., 2022).The effectiveness of finetuning lies in striking a balance between leveraging the general knowledge gained during pretraining and adapting the model to perform optimally in specific, user-defined contexts, thereby maximizing its utility across a spectrum of real-world tasks.\n\nThe most common approach to fine-tuning LLMs is instruction fine-tuning, also known as fine-tuning with instruction prompts, which emphasizes on the refinement of model behavior based on explicit instructions (Zhang et al., 2023).In instruction fine-tuning, the LLM is trained using examples that demonstrate how the model should respond to a specific instruction such that every prompt-completion pair comes along with a specific instruction to the model.Templates offered by programming libraries can be utilized to convert data samples to instruction samples that are suitable for fine-tuning on a wide range of language processing tasks (Wei et al., 2022).\n\nFine-tuning an LLM on a single task may lead to catastrophic forgetting (Kemker et al., 2018;Kirkpatrick et al., 2017), which refers to overwriting or overshadowing the knowledge previously acquired during pre-training caused by updating the model's weights during finetuning.This may cause a decline in performance across other tasks or domains.",
            "score": 0.5136090857111812,
            "section_title": "Fine-tuning",
            "char_start_offset": 12144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 185,
                    "end": 410
                },
                {
                    "start": 410,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 1094
                },
                {
                    "start": 1096,
                    "end": 1326
                },
                {
                    "start": 1326,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1756
                },
                {
                    "start": 1758,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 779,
                    "end": 801,
                    "matchedPaperCorpusId": "40100965"
                },
                {
                    "start": 801,
                    "end": 818,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1737,
                    "end": 1755,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1830,
                    "end": 1851,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 1851,
                    "end": 1876,
                    "matchedPaperCorpusId": "4704285"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54345703125
        },
        {
            "corpus_id": "277244641",
            "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
            "text": "Multimodal Large Language Models. With the advancement of multimodal learning, the generalization of smallscale multimodal models has been widely explored [19,21,23], and recently extended to Multimodal Large Language Models (MLLMs) [1,2,24,37,38,70]. MLLMs have significantly improved the multimodal understanding, crossmodal reasoning, and problem-solving abilities by integrating visual encoders [51,76] with LLMs [8,10,60,61], demonstrating strong generalization ability. The connector module such as MLPs and the Q-Former [32] enhances alignment between visual and language features, facilitating smooth and efficient interaction between the visual encoder and language model. Visual instruction tuning [38] further adapts MLLMs for specific downstream tasks, improving task-specific performance [22,84]. Catastrophic Forgetting. Deep learning models often suffer from catastrophic forgetting, where previously learned knowledge is lost when learning new tasks [20,21,34,46,47,62,67]. Various continual learning algorithms have been proposed to address this issue, generally categorized into rehearsal-based [5,41,53], regularization-based [29,40,54], and architecture-based [52,68,71,82] approaches. Some traditional methods have explored using pruning to mitigate forgetting [43,44,57,65,66]; however, these are mostly designed for small models and rely heavily on full-model fine-tuning, making them less applicable for fine-tuning large foundation models. In the era of large foundation models, the traditional forgetting problem shifts to maintaining the model's strong generalization ability after downstream task fine-tuning [9,14,84]. Additionally, these models are often closed-source, with only model weight accessible. Recently, SPU [80] demonstrates the feasibility of sparse updates for fine-tuning CLIP to mitigate general knowledge forgetting. Other studies have explored the forgetting issue when training LLMs with LoRA. Lo-RAMoE [9] and SLIM [14] apply MoE architectures within LoRA to address generalization loss in LLMs. CorDA [69] builds task-aware adapters through context-guided weight decomposition to preserve world knowledge.",
            "score": 0.5131908632684883,
            "section_title": "Related Works",
            "char_start_offset": 6087,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 33
                },
                {
                    "start": 34,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 159,
                    "matchedPaperCorpusId": "252917723"
                },
                {
                    "start": 159,
                    "end": 162,
                    "matchedPaperCorpusId": "263135325"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "265150280"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "261101015"
                },
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "269293766"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "263672058"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 399,
                    "end": 403,
                    "matchedPaperCorpusId": "231591445"
                },
                {
                    "start": 403,
                    "end": 406,
                    "matchedPaperCorpusId": "257767223"
                },
                {
                    "start": 527,
                    "end": 531,
                    "matchedPaperCorpusId": "256390509"
                },
                {
                    "start": 708,
                    "end": 712,
                    "matchedPaperCorpusId": "258179774"
                },
                {
                    "start": 805,
                    "end": 808,
                    "matchedPaperCorpusId": "267751282"
                },
                {
                    "start": 966,
                    "end": 970,
                    "matchedPaperCorpusId": "250210682"
                },
                {
                    "start": 970,
                    "end": 973,
                    "matchedPaperCorpusId": "263135325"
                },
                {
                    "start": 976,
                    "end": 979,
                    "matchedPaperCorpusId": "2832081"
                },
                {
                    "start": 979,
                    "end": 982,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 985,
                    "end": 988,
                    "matchedPaperCorpusId": "245218925"
                },
                {
                    "start": 1116,
                    "end": 1119,
                    "matchedPaperCorpusId": "37308416"
                },
                {
                    "start": 1145,
                    "end": 1149,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "44868747"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "21718339"
                },
                {
                    "start": 1184,
                    "end": 1187,
                    "matchedPaperCorpusId": "259370817"
                },
                {
                    "start": 1282,
                    "end": 1286,
                    "matchedPaperCorpusId": "35249701"
                },
                {
                    "start": 1286,
                    "end": 1289,
                    "matchedPaperCorpusId": "3977226"
                },
                {
                    "start": 1289,
                    "end": 1292,
                    "matchedPaperCorpusId": "220525747"
                },
                {
                    "start": 1295,
                    "end": 1298,
                    "matchedPaperCorpusId": "252383174"
                },
                {
                    "start": 1637,
                    "end": 1640,
                    "matchedPaperCorpusId": "271915471"
                },
                {
                    "start": 1643,
                    "end": 1646,
                    "matchedPaperCorpusId": "267751282"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "263828803"
                },
                {
                    "start": 1952,
                    "end": 1955,
                    "matchedPaperCorpusId": "271915471"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3515625
        },
        {
            "corpus_id": "263142667",
            "title": "Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving",
            "text": "Failure of fine-tuning to improve LLM performance has been reported in literature [BIK22]. Further, fine-tuning [WSL + 22] adversely impacts the model's fluency, conversational capability, and in-context learning ability, which is critical to its response to prompts. \n\nAnother approach is to connect LLMs to knowledge graphs [WMWG17], which represent knowledge as a graph of interconnected entities and relationships. Knowledge graphs can encode a wide range of structured and unstructured knowledge, including facts, concepts, and relationships. Methods have been developed to infuse structured knowledge into LLMs by training models on factual triples of knowledge graphs (KGs), and such models pre-trained on knowledge graphs have been shown to outperform baselines [MDAJ22]. But this requires a well-curated and complete knowledge base, building which is a time-consuming and expensive endeavor. Maintaining these knowledge bases with consistent and up-to-date information over time is also challenging. \n\nYet another possibility is to encode external relevant knowledge into a key-value memory that exploits the fast maximum inner product search for memory querying. These memory slots can then be integrated with language models [WZH + 22] for relatively smaller models, such as T5. Such a memory augmentation has been shown to improve the performance of the deep learning model on knowledge-intensive tasks. More recently, recurrent memory transformer (RMT) has been shown to be computationally efficient for large prompts with a million or more tokens [BKB23]. \n\nBut for tasks such as programming or plan generation, the space of possible queries is very large and can have many syntactic variations. Some of these simple syntactic variations could be superfluous with respect to the actual task, and not be relevant to the planning problem. It has been recently shown that LLMs are very sensitive to such irrelevant variations [JEVJ22]. Hence, improving accuracy via expensive fine-tuning or explicit curation of knowledge graphs in such a context would be unrealistic as it would require not just creating variations relevant to the domain but also considering changes irrelevant to the core search problem.",
            "score": 0.5126124525326543,
            "section_title": "Related Work",
            "char_start_offset": 5197,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 267
                },
                {
                    "start": 270,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2218
                }
            ],
            "ref_mentions": [
                {
                    "start": 326,
                    "end": 334,
                    "matchedPaperCorpusId": "19135805"
                },
                {
                    "start": 1937,
                    "end": 1945,
                    "matchedPaperCorpusId": "254238014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1611328125
        },
        {
            "corpus_id": "273502615",
            "title": "Exploring Continual Fine-Tuning for Enhancing Language Ability in Large Language Model",
            "text": "Parameter efficient techniques like LoRA (Hu et al., 2022) are also widely used to efficiently fine-tune LLMs on multilingual data. However, such techniques show both: catastrophic forgetting on English and incapability to exploit the task ability that the model receives from the English fine-tuning data (Aggarwal et al., 2024). \n\nIn such a setting, we want to enhance the model's language ability (other than English) while preserving the task ability achieved via (firstly) English fine-tuning. This setting results in the challenge of catastrophic forgetting, i.e., the model's task ability on English may decline while fine-tuning on multilingual data (Mukhoti et al., 2023). Furthermore, a trivial solution that fine-tunes on the mixture of multilingual and English-only data may be sub-par (e.g., due to language relatedness (Dhamecha et al., 2021)). Hence, it is challenging to improve an LLM's language ability while preserving its performance on English. \n\nOur Approach. We use a two-phase continual fine-tuning (CFT) technique for language adaption. We study the effects of various English and multilingual instruction tuning datasets when the models are fine-tuned in two phases: where Phase 1 is finetuning the model in English to improve its task ability and then fine-tuning it on a proportionally-sized multilingual dataset in Phase 2. In Phase 1, we use ALPACA (Taori et al., 2023) and OPENORCA (Lian et al., 2023), and in Phase 2 we use MULTIAL-PACA (Wei et al., 2023) and MOPENORCA ( \u00a74.1). \n\nWe perform this study on two open-source models, namely LLAMA-3-8B and MISTRAL-7B. We also use fine-tuned versions of them, namely LLAMA-3-8B-INSTRUCT and MISTRAL-7B-INSTRUCT, as off-the-shelf Phase 1 fine-tuned models.",
            "score": 0.5124551279440535,
            "section_title": "Introduction",
            "char_start_offset": 1875,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 330
                },
                {
                    "start": 333,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1732
                }
            ],
            "ref_mentions": [
                {
                    "start": 41,
                    "end": 58,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.289794921875
        },
        {
            "corpus_id": "270620654",
            "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
            "text": "Existing theoretical and empirical study on the associations between learned and forgotten tasks focus on shallower models (Lee et al., 2021;Goldfarb et al., 2024;Ramasesh et al., 2021); the problem is under-explored for LLM forgetting or in an example level. Swayamdipta et al. (2020); Maini et al. (2022) characterize training examples that are prone to forgetting, but they do not touch on how example forgetting depends on the learned tasks. \n\nSpecifically, we start by analyzing the associations between the learned tasks and forgotten upstream examples in LLM fine-tuning. We measure forgetting (in continuous log perplexity increase or binary exact match drop) over N upstream examples, after fine-tuning the model on one of M unseen instruction-tuning tasks, while summarizing the results in a M \u00d7 N matrix. We evaluate the complexity of the associations by measuring the goodness-of-fit of low-rank approximations of the example associations. We then examine how the complexity of the example associations varies across model types (OLMo, OLMo2, MPT, Pythia) and sizes (1B to 13B parameters). \n\nOur findings suggest that the associations between learned tasks and forgotten examples are often well-approximated with low-rank matrices. On OLMo-1B and OLMo-7B, rank-3 approximation fits the associations between 85 learned tasks and 140,000 upstream examples with R 2 > 0.69. We notice that the forgetting of more capable and recent LLM families are more complicated, requiring higher-rank approximations; within the same model family, the complexity of the associations remains stable or increases with the model size. The matrix decomposition further interprets the associations by distinguishing forgetting that are independent of or dependent on what the model learns. \n\nFollowing the low-rank approximations of the associations, we predict example forgetting on unseen tasks by solving a matrix completion problem over the association matrices, analogous to collaborative filtering (Sarwar et al., 2001) in recommender systems, achieving both efficiency and interpretability. \n\nOur matrix factorization (MF) or k-nearest neighbor (KNN) models outperform previous approaches that learn semantic relations of two examples with LMs (Jin & Ren, 2024).",
            "score": 0.5124415868203794,
            "section_title": "Introduction",
            "char_start_offset": 1762,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 445
                },
                {
                    "start": 448,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 951
                },
                {
                    "start": 952,
                    "end": 1101
                },
                {
                    "start": 1104,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1779
                },
                {
                    "start": 1782,
                    "end": 2087
                },
                {
                    "start": 2090,
                    "end": 2259
                }
            ],
            "ref_mentions": [
                {
                    "start": 123,
                    "end": 141,
                    "matchedPaperCorpusId": "235790418"
                },
                {
                    "start": 141,
                    "end": 163,
                    "matchedPaperCorpusId": "267094831"
                },
                {
                    "start": 163,
                    "end": 185,
                    "matchedPaperCorpusId": "220525308"
                },
                {
                    "start": 287,
                    "end": 306,
                    "matchedPaperCorpusId": "253157424"
                },
                {
                    "start": 1994,
                    "end": 2015,
                    "matchedPaperCorpusId": "8047550"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06488037109375
        },
        {
            "corpus_id": "273502289",
            "title": "RAC: Efficient LLM Factuality Correction with Retrieval Augmentation",
            "text": "Hallucination has been a known issue for generation tasks, especially when using LLMs (Maynez et al., 2020;Rawte et al., 2023;Ji et al., 2023;Zhang et al., 2023;Ye et al., 2023a;Huang et al., 2023). Our work focuses on one of the hallucination types for LLMs, factual incorrectness. There are four lines of work regarding reducing factual incorrectness: 1) from the LLM decoding perspective, 2) from the factual enhancement perspective using retrieval augmentation or fine-tuning, 3) from a self-correction or self-alignment perspective, and 4) from a post-correction using retrieved content perspective. \n\nFrom the LLM decoding perspective, Li et al. (2023) proposes Inference-Time Intervention (ITI) to enhance the truthfulness of large language models (LLMs). ITI shifts model activations during inference, following a learned set of directions across a limited number of attention heads. Chuang et al. (2024) introduces a new decoding method that contrasts predictions made by different model layers to improve factuality performance. Das et al. (2024) proposed extrapolating critical token probabilities beyond the last layer for more accurate contrasting during LLMs decoding. \n\nFrom the factual enhancement perspective, there are two sub-types. One sub-type is factuality enhancement training or fine-tuning. Lee et al. (2022) proposed a modified top-k sampling strategy and a factuality-enhanced training method to improve the factuality of text generation. Yang et al. (2023) focus on honesty-based fine-tuning, empowering LLMs to admit limitations by acknowledging \"I don't know.\" Tian et al. (2024) constructed a direct preference optimization (DPO) dataset to fine-tune the LLM to improve factuality using referencebased and reference-free truthfulness annotation techniques. \n\nFor the factual enhancement perspective, the second sub-type is Retrieval Augmented Generation (RAG) (Chen et al., 2017;Lewis et al., 2020;Izacard et al., 2022).",
            "score": 0.5115169390922898,
            "section_title": "Related Work",
            "char_start_offset": 4324,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 282
                },
                {
                    "start": 283,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1787
                },
                {
                    "start": 1790,
                    "end": 1951
                }
            ],
            "ref_mentions": [
                {
                    "start": 86,
                    "end": 107,
                    "matchedPaperCorpusId": "218487034"
                },
                {
                    "start": 126,
                    "end": 142,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 642,
                    "end": 658,
                    "matchedPaperCorpusId": "259088877"
                },
                {
                    "start": 892,
                    "end": 912,
                    "matchedPaperCorpusId": "261582463"
                },
                {
                    "start": 1316,
                    "end": 1333,
                    "matchedPaperCorpusId": "249538460"
                },
                {
                    "start": 1591,
                    "end": 1609,
                    "matchedPaperCorpusId": "265158181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06915283203125
        },
        {
            "corpus_id": "277451521",
            "title": "Reasoning Beyond Limits: Advances and Open Problems for LLMs",
            "text": "Kumar et al. [33] delves into post-training techniques for LLMs, emphasizing that while pre-training establishes a robust linguistic foundation, it is subsequent refinementthrough methods such as fine-tuning, reinforcement learning, and testing-time scaling -that truly enhances the models' reasoning abilities, factual accuracy, and overall adaptability. The work systematically examines these post-training strategies, focusing on their potential to address critical challenges such as catastrophic forgetting, reward hacking, and the tradeoffs encountered during inference. Furthermore, it highlights emerging avenues in model alignment and scalable adaptation, underscoring the importance of these techniques in ensuring that LLMs understand language and perform effectively across a diverse range of real-world tasks. \n\nThe survey by Wang et al. [32] offers a systematic overview of various alignment techniques developed to enhance the reliability of LLMs. It emphasizes that despite LLMs achieving remarkable capabilities through massive pretraining, the variability in data quality can still lead to suboptimal or undesired outputs. To mitigate this, the paper categorizes and examines a range of methods-including reinforcement learning from human feedback (RLHF), reinforcement learning from AI feedback (RLAIF), proximal policy optimization (PPO), and direct preference optimization (DPO)-detailing how each approach contributes to aligning model outputs with human expectations. By organizing these techniques into distinct topics, the survey fills a critical gap in the literature and provides a comprehensive framework that not only aids in understanding current alignment strategies but also guides future research toward improving the performance and safety of LLMs. \n\nThe survey by Tie et al. [34] presents a thorough synthesis of post-training methodologies to overcome the inherent limitations of pre-trained large language models. It critically addresses challenges such as constrained reasoning abilities, ethical ambiguities, and performance issues in specialized domains, underscoring the need for advanced post-training strategies. The authors systematically categorize the evolution of post-training language models into five core paradigms: fine-tuning to enhance task-specific accuracy, alignment to better meet human expectations, reasoning to facilitate multistep inference despite reward design challenges, efficiency to optimize resource use amid growing model complexity, and integration and adaptation to extend capabilities across diverse modalities.",
            "score": 0.5106784465971503,
            "section_title": "C. Post-training and Alignment Strategies",
            "char_start_offset": 11867,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2155
                },
                {
                    "start": 2156,
                    "end": 2584
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2347412109375
        },
        {
            "corpus_id": "270923962",
            "title": "FSM: A Finite State Machine Based Zero-Shot Prompting Paradigm for Multi-Hop Question Answering",
            "text": "LLMs are weak in following instructions.Through manual observation of error examples 5, we infer from the results of baseline methods that the model struggles with completing complex instructions in a single step.LLMs tend to forget previous instructions during reasoning.\n\nTo address these issues, we propose the following strategies to reduce burden for LLMs: a) Iterative Decomposition: Unlike few-shot reasoning approaches, FSM adopts a multi-turn process.Each iteration focuses on addressing a single sub-task, enabling LLMs to understand instructions clearly and execute them accurately.\n\nb) Error Checking and Backtracking: For each reasoning step, FSM conducts a verification check to ensure the correctness of response.If an irregular or incorrect output is identified, the model is allowed to self-revise the answer or backtrack.c) Final Review Step: To minimize distractions from lengthy contexts, we utilize sub-questions, corresponding supporting factual paragraphs, evidence, and answers to further verify the consistency of answers and reasoning, named FSM2.",
            "score": 0.5103568475660767,
            "section_title": "Strategies",
            "char_start_offset": 3448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 40
                },
                {
                    "start": 40,
                    "end": 213
                },
                {
                    "start": 213,
                    "end": 272
                },
                {
                    "start": 274,
                    "end": 460
                },
                {
                    "start": 460,
                    "end": 593
                },
                {
                    "start": 595,
                    "end": 728
                },
                {
                    "start": 728,
                    "end": 839
                },
                {
                    "start": 839,
                    "end": 1073
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01898193359375
        },
        {
            "corpus_id": "261031244",
            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
            "text": "For instance, in sentences describing physical appearance, BLOOMZ-7.1b initially prefers stereotype-conforming sentences with a probability of 75.0%, but this preference decreases to 63.88% after continual instruction tuning . We visualize the FG values of domain knowledge, reasoning, and reading comprehension with respect to model scales in Figure 4. We can observe that the forgetting phenomenon becomes increasingly severe as the model scale increases. For example, the FG values in domain knowledge are 9.54%, 10.72%, 14.63%, and 18.37% in BLOOMZ-1.1b, 1.7b, 3b, and 7.1b, respectively. BLOOMZ-7.1b suffers the most drastic forgetting. As shown in Table 1, the initial performance R s o is boosted by the increasing model scale, but the final performance is relatively similar across different scales, which may explain the varying extent of forgetting. The same pattern can also be observed in the mT0-1.2b and mT0-3.7b models, as shown in Table 3. Regarding the bias in LLMs, FG values do not correlate with the model scales, which is also reflected in the initial model performance. In other words, there is no evident correlation between the initial performance R s o in bias and the model scales. This finding suggests that the degree of bias in LLMs is not directly related to their size, and increasing the model scale does not necessarily lead to a corresponding increase or decrease in bias.",
            "score": 0.5101122198941723,
            "section_title": "Main Results",
            "char_start_offset": 17343,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 457
                },
                {
                    "start": 458,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59228515625
        },
        {
            "corpus_id": "268063771",
            "title": "Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning",
            "text": "Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \\url{https://github.com/which47/LLMCL}.",
            "score": 0.5101006555860417,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80615234375
        },
        {
            "corpus_id": "259203671",
            "title": "Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling",
            "text": "Ensuring specific text structures, such as the logical order of concepts throughout the entire text, can be difficult. This difficulty is amplified for tasks requiring formal rules or grammar. This is because LLMs mainly focus on the local context information of words and sentences during pre-training, while ignoring global syntactic and structural knowledge. A proposal for addressing this problem is to adopt an iterative prompting approach in generating text [59], mimicking the process of human writing. In contrast, KGs offer a structured summary and emphasize the correlation of relevant concepts when complex events involving the same entity extend across multiple sentences [60], thus enhancing the process of structured text generation. \n\nHallucination. When generating factual or knowledgegrounded texts, LLMs may produce content that contradicts existing sources or lack supporting evidence. This challenge widely occurs in existing LLMs and is known as the problem of hallucination, which results in a drop in their performance and poses risks when deploying them for real-world applications. The cause of this issue is related to LLMs' limited ability to utilize correct internal and external knowledge during task-solving. To alleviate this problem, existing studies have resorted to alignment tuning strategies, which incorporate human feedback to fine-tune LLMs. KGs provide structured and explicit representations of knowledge, which can be dynamically incorporated to augment LLMs, resulting in more factual rationales and reduced hallucination in generation [61]. \n\nInconsistency. With the help of the chain-of-thought strat-egy, LLMs are capable of solving some complex reasoning tasks based on step-by-step reasoning. Despite their superior performance, LLMs may at times arrive at the desired answer based on an invalid reasoning path or produce an incorrect answer despite following a correct reasoning process. As a result, inconsistency arises between the derived answer and the underlying reasoning process. Additionally, research [62] has revealed that LLMs' abilities to forecast facts and answer queries are highly influenced by specific prompt templates and related entities. This is because that LLMs rely largely on simple heuristics to make predictions, their generations are correlated with co-occurrence frequencies between the target word and words in the prompt.",
            "score": 0.5091853556833712,
            "section_title": "D. Pros and Cons of LLMs",
            "char_start_offset": 21392,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 747
                },
                {
                    "start": 750,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1106
                },
                {
                    "start": 1107,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1380
                },
                {
                    "start": 1381,
                    "end": 1584
                },
                {
                    "start": 1587,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2401
                }
            ],
            "ref_mentions": [
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "252873593"
                },
                {
                    "start": 684,
                    "end": 688,
                    "matchedPaperCorpusId": "222272210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08197021484375
        },
        {
            "corpus_id": "277451974",
            "title": "If an LLM Were a Character, Would It Know Its Own Story? Evaluating Lifelong Learning in LLMs",
            "text": "Compared to current benchmarks, our dataset features more interactive characters, closed dialogue turns, and richer content (Table 1). Evaluation combines LLM-asjudge with human assistance, using predetermined factual answers as criteria. \n\nWe tested several popular models, including the open-source Llama3.1-8B (AI, 2024), the closed-source GPT-4-turbo (OpenAI, 2023), and the large language reasoning model DeepSeek R1 (DeepSeek-AI et al., 2025). Benchmark-backed experiments show that current models still have much room for improvement in lifelong learning. \n\nOur findings indicate that: (i) Non-parametric methods are more effective for stateful learning, as they leverage more context for richer information. (ii) Regardless of the method, performance tends to decline over time, with parametric models particularly struggling with catastrophic forgetting. All models have significant room for improvement, especially in enhancing relationship shifts across multiple episodes. In summary, our work contributes in three key areas: \n\n\u2022 Two Datasets: We introduce the Hamlet and synthetic datasets, featuring multi-agent episodic timelines and scene details to simulate cumulative experiences. \n\n\u2022 A Benchmark: LIFESTATE-BENCH evaluates LLMs' lifelong learning abilities via fact-checking mechanism, using both non-parametric and parametric memory-testing methods. \n\n\u2022 Findings and Implications: Non-parametric methods outperform parametric ones in lifelong learning, but all models still face challenges with catastrophic forgetting as episodes progress, suggesting that our benchmark could provide valuable insights for further improvements.",
            "score": 0.5091741887391458,
            "section_title": "Introduction",
            "char_start_offset": 4062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 562
                },
                {
                    "start": 565,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1036
                },
                {
                    "start": 1039,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1368
                },
                {
                    "start": 1371,
                    "end": 1647
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.319091796875
        },
        {
            "corpus_id": "261660311",
            "title": "Create Your World: Lifelong Text-to-Image Diffusion",
            "text": "We in this section present the ablation studies of our proposed method to show the contribution of each component. Each ablation study comparisons are justified with the same setup as Sec. 4.3.1 and Sec. 4.3.2. \n\nWithout Catastrophic Forgetting Regularization: As detailed in Sec. 3.3, we propose two modules to tackle catastrophic forgetting on encountered concepts (i.e., Task-Aware Memory Enhancement and Elastic Concept Distillation modules). Here, we eliminate them one by one to evaluate the effect of each module, and compare Ours, Ours w/o TAME, Ours w/o ECD with baselines based on fine-tuning. As the results shown in Fig. 8, Ours w/o TAME&ECD has a lower image-alignment for the earlier concepts, and tends to forget the details of encountered concepts in comparison to Ours model. However, Ours w/o ECD performs better than Ours w/o TAME&ECD in the color details for target concept, e.g., the first concept [V * \n\n1 dog]. These results above lends that both TAME and ECD play a key role in mitigating catastrophic forgetting, which verifies the rationality of Ours. \n\nWithout Catastrophic Neglecting Regularization: To comprehensively evaluate the performance of our method with this setup, i.e., using multi-concept to generate images, we compare Ours, Ours w/o CAA, Ours w/o OAA with DreamBooth+LWF, Custom+LWF, SVDiff+LWF. From the present results in Fig. 9, we can notice that Our model can well address both catastrophic forgetting and catastrophic neglecting issues than baselines. The Ours w/o CAA model lead to a lower text-alignment with the text prompt, and further cause one concept of the prompt is not generated,e.g., \"a [V * \n\n1 dog] and a [V * 5 teddybear] sitting outside\". Although the Ours w/o OAA model can generate correct concept number on the text prompt, it binds attributes or visual details on the wrong concepts. Different from the methods above, our model can generate a higher text-and imagealignment in the qualitative view.",
            "score": 0.5088771786747365,
            "section_title": "Ablation Study",
            "char_start_offset": 38790,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 210
                },
                {
                    "start": 213,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1077
                },
                {
                    "start": 1080,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1650
                },
                {
                    "start": 1653,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1850
                },
                {
                    "start": 1851,
                    "end": 1965
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060302734375
        },
        {
            "corpus_id": "221083147",
            "title": "Better Fine-Tuning by Reducing Representational Collapse",
            "text": "Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989). \n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set. \n\nMeasuring such degradations is non-trivial. Simple metrics such as the distance between pre-trained representations and fine-tuned representations is not sufficient (e.g. adding a constant to the pretrained representations will not change representation power, but will change distances). One approach would be to estimate mutual information of representations across tasks before and after fine-tuning, but estimation of mutual information is notoriously hard, especially in high-dimensions (Tschannen et al., 2019). We instead propose a series of probing experiments meant to provide us with empirical evidence of the existence of representation collapse on the GLUE benchmark (Wang et al., 2018).",
            "score": 0.5088318824053075,
            "section_title": "REPRESENTATIONAL COLLAPSE",
            "char_start_offset": 14719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 848
                },
                {
                    "start": 851,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1550
                }
            ],
            "ref_mentions": [
                {
                    "start": 271,
                    "end": 296,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1530,
                    "end": 1549,
                    "matchedPaperCorpusId": "5034059"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75439453125
        },
        {
            "corpus_id": "268032887",
            "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
            "text": "Prior studies in CL for LLMs have mainly focused on parameter-efficient fine-tuning or adaptation for a limited selection of target domains or tasks. While beneficial, these methods often do not fully address the broader challenge of lifelong learning for LLMs. Our research diverges by exploring continual domain-adaptive pretaining of LLMs across an extensive set of domains to better understand the dynamics of knowledge preservation, new information retention and knowledge transfer. Below, we highlight three key insights and discuss notable observations from our research: Continual pretraining benefits GPT models but degrades Llama2-7B While continual pretraining improves GPT models of all sizes, it negatively impacts Llama2-7B. This trend is consistent across all our metrics -perplexity, downstream task performance, and output rank. Regardless of model size, continual pretraining outperforms domainadaptive pretraining, underscoring the advantages of exhaustive training. Finally, we empirically show that Llama2-7B requires domains to be larger than 100 MB for improved adaptation. \n\nScaling laws for continual pretraining Our experiments reveal that larger models consistently achieve better perplexity and exhibit less forgetting. However, smaller models are more sensitive to continual pretraining, showing both the highest degree of learning and the most pronounced forgetting. \n\nContinual pretraining boosts downstream task performance of GPT family For both GPT2-M and Llama2-7B, performance on BIG-Bench question-answering tasks follows the same trend as perplexity. Further, downstream task success is heavily influenced by the most recent training domains and correlates well with our complementary prediction rank analysis. A key takeaway is that additional generative pretraining before supervised fine-tuning can further enhance performance. \n\nRandomizing training domain order improves knowledge retention With the randomized training order, we observe two benefits: that (i) the final checkpoint achieves superior average performance compared to similar-order training, and (ii) checkpoints exhibit positive backward transfer and reduced forgetting, suggesting that previously learned knowledge remains more intact. \n\nSimilar-order training strengthens domain specialization When consecutive training domains are semantically similar, continual learning enhances specialization.",
            "score": 0.5083510924350012,
            "section_title": "Discussion",
            "char_start_offset": 34532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 261
                },
                {
                    "start": 262,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1096
                },
                {
                    "start": 1099,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1396
                },
                {
                    "start": 1399,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1868
                },
                {
                    "start": 1871,
                    "end": 2244
                },
                {
                    "start": 2247,
                    "end": 2407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1370849609375
        },
        {
            "corpus_id": "274130835",
            "title": "Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey",
            "text": "HuggingGPT Shen et al. [2024] probably exhibits both context and logical inconsistencies, where the outputs generated by the LLM fail to align with the task context or maintain internal logical coherence, leading to unreliable and unpredictable results. \n\nTo mitigate these hallucinations, future research should focus on integrating knowledge graphs or validated data sources to strengthen the factual grounding of LLM-generated content Pan et al. [2024], thereby reducing the likelihood of factuality hallucinations. Additionally, employing ensemble models that cross-validate LLM outputs before final recommendations can help mitigate the risks of factual fabrications. To address faithfulness hallucinations, incorporating contextual validation mechanisms can ensure that LLM-generated outputs remain consistent with user instructions and input contexts. Finally, RLHF technique could be employed to iteratively fine-tune LLM responses, improving alignment with user expectations and reducing inconsistencies. By implementing these solutions, the reliability and trustworthiness of LLM-driven ML workflows could be significantly enhanced, especially in critical applications where hallucinations could have serious consequences. Overall, it is crucial to anticipate the possibility of erroneous outputs when integrating LLMs into ML workflow construction and optimization, and to develop probabilistic methods to effectively detect and mitigate these risks.",
            "score": 0.5080870478678364,
            "section_title": "Hallucination",
            "char_start_offset": 54783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 256,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1461
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 29,
                    "matchedPaperCorpusId": "257833781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.045440673828125
        },
        {
            "corpus_id": "266162497",
            "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
            "text": "\u2022 Reasoning Failure: In certain instances, a language model might possess relevant knowledge about a fact but fail to utilize it properly. This is particularly evident in complex multi-step reasoning tasks (Tan et al., 2023) or when posed with different questions about the same fact, resulting in disparate outcomes (Berglund et al., 2023). \n\nWe observe that most of these issues arise during the pretraining phase, with catastrophic forgetting being the notable exception. Hence, many LLMs will suffer from factual errors of this kind regardless of any post-training process.",
            "score": 0.5076227073489988,
            "section_title": "Knowledge and Reasoning",
            "char_start_offset": 7170,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 577
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 224,
                    "matchedPaperCorpusId": "257505407"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1656494140625
        },
        {
            "corpus_id": "264426280",
            "title": "From Static to Dynamic: A Continual Learning Framework for Large Language Models",
            "text": "The advent of large language models (LLMs) marks a revolutionary shift in the realm of artificial intelligence, showcasing extraordinary competence across a broad array of tasks (Brown et al., 2020;OpenAI, 2023;Touvron et al., 2023;Penedo et al., 2023). With their capacity to generate humanlike texts and address intricate inquiries, LLMs have fundamentally redefined our understanding of machine learning capabilities. However, like any technological breakthrough, LLMs come with their own set of limitations. \n\nOne of the primary drawbacks lies in their static nature (Kemker et al., 2018). The knowledge embedded within LLMs is confined to the fixed parameters established during the training phase. As a result, the process of integrating new knowledge to LLMs through fine-tuning is not only computationally demanding but also prone to catastrophic forgetting (Dong et al., 2021;Scao et al., 2022). More-over, as the knowledge is implicitly encoded within the parameter space, LLMs lack the ability to articulate their understanding explicitly, which may result in the model generating unfounded or hallucinated information, thereby compromising the reliability of their outputs (Azamfirei et al., 2023). These inherent challenges significantly impede the capacity of LLMs to assimilate new knowledge and adapt to evolving environments. \n\nTo tackle the above issues, we propose Dyna-Mind, a novel continual learning framework for LLMs. Continual learning encapsulates the capacity to dynamically adapt cognition by assimilating new knowledge from the environment over time, acting as a fundamental manifestation of human intelligence (Hadsell et al., 2020). Motivated by human continual learning (Eichenbaum, 2004;Hadsell et al., 2020), we carefully design an independent memory module in DynaMind, which empowers LLMs to heuristically search knowledge, store knowledge in the memory and recall relevant knowledge from the memory in subsequent inference. This mechanism equips LLMs with continual learning ability without tuning any model parameters. \n\nTo evaluate DynaMind's continual learning capabilities, we conducted extensive experiments centered on three aspects: Knowledge-driven Complex Reasoning, Knowledge Credibility Perception, and Knowledge Manipulation.",
            "score": 0.5074883677952574,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1960
                },
                {
                    "start": 1961,
                    "end": 2056
                },
                {
                    "start": 2059,
                    "end": 2274
                }
            ],
            "ref_mentions": [
                {
                    "start": 571,
                    "end": 592,
                    "matchedPaperCorpusId": "22910766"
                },
                {
                    "start": 866,
                    "end": 885,
                    "matchedPaperCorpusId": "245116938"
                },
                {
                    "start": 1185,
                    "end": 1209,
                    "matchedPaperCorpusId": "257641567"
                },
                {
                    "start": 1640,
                    "end": 1662,
                    "matchedPaperCorpusId": "226240885"
                },
                {
                    "start": 1702,
                    "end": 1720,
                    "matchedPaperCorpusId": "16043910"
                },
                {
                    "start": 1720,
                    "end": 1741,
                    "matchedPaperCorpusId": "226240885"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3173828125
        },
        {
            "corpus_id": "276622315",
            "title": "Measuring Catastrophic Forgetting in Cross-Lingual Classification: Transfer Paradigms and Tuning Strategies",
            "text": "In reusing LLMs for different tasks, adapters [13], [36] were introduced to avoid updating all of the pretrained model's weights when fine-tuning to a new problem. The idea is to fine-tune only a specific section of the model in a parameter-efficient manner. \n\nIn this work, we investigated the following research questions: \n\n\u2022 How do two different cross-lingual training paradigms, intermediate training and cross-lingual validation, influence cross-lingual transfer results? \n\n\u2022 Is full-model fine-tuning better compared to adapters when it comes to cross-lingual learning and catastrophic forgetting? \n\n\u2022 How does catastrophic forgetting affect previously acquired knowledge in multiple transfer episodes? We describe the related work in Section II, followed by the description of the cross-lingual methodology in Section III, and the experimental setup in Section IV. We summarize the results in Section V and present conclusions in Section VI.",
            "score": 0.5071215794936399,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 3974,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 258
                },
                {
                    "start": 261,
                    "end": 324
                },
                {
                    "start": 327,
                    "end": 477
                },
                {
                    "start": 480,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 709
                },
                {
                    "start": 710,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 949
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 50,
                    "matchedPaperCorpusId": "59599816"
                },
                {
                    "start": 52,
                    "end": 56,
                    "matchedPaperCorpusId": "215826266"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.309814453125
        },
        {
            "corpus_id": "269449294",
            "title": "HFT: Half Fine-Tuning for Large Language Models",
            "text": "Large language models (LLMs) bring immense revolutions to various natural language processing applications with powerful language understanding and generation capabilities.Unsupervised largescale pre-training for learning basic world knowledge (hereinafter referred to as basic knowledge), followed by one or more fine-tuning phases with supervised data or human feedback, is becoming a new training paradigm in the era of LLMs [Ouyang et al., 2022, Achiam et al., 2023, Touvron et al., 2023].As the fine-tuning phase proceeds, the enormous potential of LLMs is gradually unleashed to handle various downstream tasks, while the parametric knowledge previously learned and stored in the pre-trained model might face a considerable risk of catastrophic forgetting [Lin et al., 2024, Neeman et al., 2023, Dong et al., 2024].To maintain intrinsic basic knowledge, the most straightforward idea is to keep the pre-trained parameters unchanged and include extra modules to learn task-specific abilities [Dou et al., 2023, Wu et al., 2024a].However, such modifications to the architecture pose significant obstacles to model deployment and continual fine-tuning.\n\nWithout changing model architecture, vanilla full fine-tuning (FFT) methods update all parameters to improve the performance of downstream tasks [Zhang et al., 2023a], in which the element-wise parameter difference between fine-tuned and pre-trained models (i.e., task vector) represents the knowledge shift during fine-tuning [Ilharco et al., 2023].Herein, a desirable task vector is expected to keep basic knowledge of the pre-trained model and learn new specialized knowledge.Interestingly, recent work shows that partial dropping or trimming of the task vector has only milder impacts on target task [Yadav et al., 2023, Yu et al., 2023].In other words, partial new parameters are sufficient for the learning of new abilities, so the upcoming question is, is it possible that a portion of old parameters could maintain the capabilities of the pre-trained model?",
            "score": 0.5071038326910764,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 172,
                    "end": 493
                },
                {
                    "start": 493,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1034
                },
                {
                    "start": 1034,
                    "end": 1155
                },
                {
                    "start": 1157,
                    "end": 1507
                },
                {
                    "start": 1507,
                    "end": 1636
                },
                {
                    "start": 1636,
                    "end": 1799
                },
                {
                    "start": 1799,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 428,
                    "end": 448,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 762,
                    "end": 779,
                    "matchedPaperCorpusId": "265608902"
                },
                {
                    "start": 779,
                    "end": 800,
                    "matchedPaperCorpusId": "253447228"
                },
                {
                    "start": 1484,
                    "end": 1506,
                    "matchedPaperCorpusId": "254408495"
                },
                {
                    "start": 1761,
                    "end": 1780,
                    "matchedPaperCorpusId": "259064039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1617431640625
        },
        {
            "corpus_id": "273350763",
            "title": "Superficial Safety Alignment Hypothesis",
            "text": "In Section 4.3, we explore the possibility of repurposing redundant units (RU) as part of an alignment budget to minimize the alignment tax. The core idea is that pre-trained LLMs contain a large percentage of parameters that do not contribute significantly to task performance, as noted by Sun et al. (2023) and Ma et al. (2023). These redundant units can be re-purposed to improve safety alignment without sacrificing utility performance. \n\nWe identify redundant units using the same variance-based pruning method described in Section 4.1. Specifically, we compute an importance score for each neuron and channel based on the variance of activations across the Alpaca dataset (We remove safety-related samples from the original version). \n\nOnce the redundant units are identified, we freeze the remaining parts of the model and fine-tune only these redundant units during the alignment process. By carefully adjusting the proportion of redundant units re-purposed, we aim to achieve alignment without incurring the alignment tax-i.e., without sacrificing utility performance. This selective fine-tuning approach significantly reduces the computational burden compared to full model fine-tuning while maintaining high task performance. \n\nEvaluation Benchmarks. To evaluate the effectiveness of repurposing redundant units, we assess the model's performance on both helpfulness (MT-bench) and accuracy (downstream tasks). Our evaluations consist of two main benchmarks: Downstream Tasks. As shown in Table 4, we evaluate the model's performance across a variety of tasks, including: \n\n\u2022 ARC-Challenge (ARC-C) and ARC-Easy (ARC-E) (Clark et al., 2018): These tasks test the model's ability to answer science questions, which require a combination of factual knowledge and reasoning. \n\n\u2022 HellaSwag (Zellers et al., 2019): A commonsense reasoning task requiring the model to predict the next logical action in a situation. \n\n\u2022 WinoGrande (Sakaguchi et al., 2019): A commonsense reasoning task based on resolving pronoun ambiguity. \n\n\u2022 BoolQ (Clark et al., 2019): A binary question-answering task that assesses the model's factual understanding.",
            "score": 0.5070819132194557,
            "section_title": "B.6 DETAILS ON REDUNDANT UNITS AND ALIGNMENT BUDGET",
            "char_start_offset": 54180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 440
                },
                {
                    "start": 443,
                    "end": 541
                },
                {
                    "start": 542,
                    "end": 739
                },
                {
                    "start": 742,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1236
                },
                {
                    "start": 1239,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1421
                },
                {
                    "start": 1422,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1582
                },
                {
                    "start": 1585,
                    "end": 1781
                },
                {
                    "start": 1784,
                    "end": 1919
                },
                {
                    "start": 1922,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2141
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 329,
                    "matchedPaperCorpusId": "258823276"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0455322265625
        },
        {
            "corpus_id": "271218270",
            "title": "SwitchCIT: Switching for Continual Instruction Tuning of Large Language Models",
            "text": "Large language models (LLMs) and multimodal models (MMs) have exhibited impressive capabilities in various domains, particularly in general language understanding and visual reasoning. However, these models, trained on massive data, may not be finely optimized for specific tasks triggered by instructions. Continual instruction tuning is crucial to adapt a large model to evolving tasks and domains, ensuring their effectiveness and relevance across a wide range of applications. In the context of continual instruction tuning, where models are sequentially trained on different tasks, catastrophic forgetting can occur, leading to performance degradation on previously learned tasks. This work addresses the catastrophic forgetting in continual instruction learning through a switching mechanism for routing computations to parameter-efficient tuned models. We demonstrate the effectiveness of our method through experiments on continual instruction tuning of different natural language generation tasks and vision-language tasks. We also showcase the advantages of our proposed method in terms of efficiency, scalability, portability, and privacy preservation.",
            "score": 0.5070529641651143,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.443603515625
        },
        {
            "corpus_id": "258968166",
            "title": "Information Association for Language Model Updating by Mitigating LM-Logical Discrepancy",
            "text": "We also include a replay corpus on past information to mitigate forgetting. For evaluation of generalizability, we propose to use QA pairs querying either the updating information or the past information, created by human or GPT-4 (OpenAI, 2023). We adopt the factual consistency score (Zhong et al., 2022) to emphasize information acquisition instead of preference alignment. For the pipeline approach illustrated in Figure 2, we use a self-prompting process to generate question-answer (QA) pairs relevant to the updating information by LLMs themselves, which augments the updating corpus for fine-tuning. An example of such pair is provided in Table 2. To further improve the generalizability of updating, we analyze the factual errors, exemplified in Table 1, where fine-tuned LLMs mistakenly associating queries with pre-training information. Our analysis suggests that this exposure bias against new information originates from the LM-logical discrepancy and can be mitigated by modeling an information association term. Therefore, we propose a straightforward yet effective associative distillation method, which explicitly incorporates the association term into the fine-tuning objective. \n\nFor experiments, we utilize an instructionfinetuned model from LLaMA-7B as the base model. We curate a corpus of news articles published after March 2023 as the updating corpus. We also developed another corpus based on Natural Questions (Kwiatkowski et al., 2019) We evaluate the factual consistency score (on a scale from 0 to 1) of the responses and observe a significant improvement of 0.16 over baselines that are prone to the exposure bias. Additionally, we study the forgetting problem under a continual learning setting and discover that our approach maintains good performance on past information using a replay corpus containing only 2.3% of the past training data. \n\nTo summarize, our major contributions include: \n\n\u2022 We identify the LM-logical discrepancy as the underlying cause of limitations on generalizability and structurality of existing model updating methods. \n\n\u2022 We introduce Self Information Updating, which is a novel task formulation emphasizing unstructured updating corpus and QA-based generalizability evaluation. Our task formulation addresses the limitations of existing research on model updating.",
            "score": 0.50698506976782,
            "section_title": "Introduction",
            "char_start_offset": 4277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1196
                },
                {
                    "start": 1199,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1874
                },
                {
                    "start": 1877,
                    "end": 1923
                },
                {
                    "start": 1926,
                    "end": 2079
                },
                {
                    "start": 2082,
                    "end": 2240
                },
                {
                    "start": 2241,
                    "end": 2327
                }
            ],
            "ref_mentions": [
                {
                    "start": 286,
                    "end": 306,
                    "matchedPaperCorpusId": "252873117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "272770433",
            "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair",
            "text": "Our detailed analysis of the sampled code completions for such problems when pass rate degrades suggests that the model is generally on the right track but makes \"minor\" errors that are small, detailed, and seemingly trivial. While it is possible that LLMs experience catastrophic forgetting during fine-tuning (Luo et al., 2024a), we do not anticipate this being a major factor due to the low learning rate and the small number of gradient updates (64 steps with 16k data samples). Instead, we believe the primary issue is our inability to ensure the quality of our data, particularly in verifying whether the sampled code solutions correctly solve the code problems.",
            "score": 0.5069250695241505,
            "section_title": "CHALLENGES WITH NON-TEXTUAL REPRESENTATIONS",
            "char_start_offset": 8932,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 668
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054412841796875
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "In this series of experiments, we aim to explore how the Instruction Vector (IV) evolves during continual instruction tuning to better understand the mechanisms underlying forgetting.\n\nFinding 1.Alignment between the fine-tuned computation graph and the IV-associated computation graph correlates with task performance.Fig. 5 shows the relationship between zero-shot performance and the similarity of hidden states to their respective instruction vector, measuring alignment through the cosine similarity Cosine(h l , \u03b8 c ).This similarity is utilized to reflects the alignment between the computation graphs, with h l denotes the hidden state of the l-th layer.The maximum value across all layers is reported.\n\nPost fine-tuning, the model appears to incorporate \u03b8 c into the hidden states, evidenced by a similarity score of 0.249 for Last-Spanish in stage 2, correlating with improved task accuracy (65%).Conversely, a performance decline is linked to a decrease in similarity.For instance, in the Last- ), where \u03b8 i c is the IV after fine-tuning the i-th task.\"Rand sim.\" tracks changes from 10 randomly chosen head outputs, averaged over 100 seeds.Despite IV maintaining stability at 0.95/0.79even into the 6-th phase, compared to random similarity scores of 0.8/0.48,significant model forgetting still occurs by the 6-th phase, with accuracy for Last-Spanish falling to 26% and CommonsenseQA to 17.25%.\n\nFurthermore, experiments with IV-related interventions, where hidden states contribute to IV in the fine-tuned model were replaced with their initial values (stage 0), are shown by the red line in the Fig. 5 (b).The purpose of this experiment was to re-activate the model's capacity to handle the specific task by fully recovering the representation of IV.However, results suggested minimal effectiveness.The findings indicate that after training, the model cannot implicitly utilize \u03b8 c ; hence, the output y becomes detached from \u03b8 c , disrupting the computation graph.Thus, changes in IV before and after fine-tuning do not contribute to the observed forgetting.\n\nFinding 3. Model forgetting stems from suppression by new specialized patterns.We conducted a causal mediate analysis (Sec.",
            "score": 0.5065618506438287,
            "section_title": "Fine-tuning Dynamics",
            "char_start_offset": 17073,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 185,
                    "end": 195
                },
                {
                    "start": 195,
                    "end": 319
                },
                {
                    "start": 319,
                    "end": 524
                },
                {
                    "start": 524,
                    "end": 662
                },
                {
                    "start": 662,
                    "end": 710
                },
                {
                    "start": 712,
                    "end": 907
                },
                {
                    "start": 907,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1152
                },
                {
                    "start": 1152,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1407
                },
                {
                    "start": 1409,
                    "end": 1621
                },
                {
                    "start": 1621,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1814
                },
                {
                    "start": 1814,
                    "end": 1980
                },
                {
                    "start": 1980,
                    "end": 2074
                },
                {
                    "start": 2076,
                    "end": 2155
                },
                {
                    "start": 2155,
                    "end": 2199
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07318115234375
        },
        {
            "corpus_id": "271860164",
            "title": "Amuro & Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models",
            "text": "The alignment stage usually involves either supervised fine-tuning for specific tasks or instruction finetuning for general-purpose usage. Regardless, finetuning (almost always) comes at the end of pretraining and yields remarkable improvements on downstream tasks (Touvron et al., 2023;Groeneveld et al., 2024). Consequently, the benefits of each stage are largely explored independently, with improvements to pretraining being orthogonal to benefits from model alignment. \n\nRather than exploring these two training regimes independently, we ask: What does the model learn and forget during pre-training and fine-tuning? Specifically, how do pretraining and fine-tuning interact to produce the resulting model? Does more pre-training hinder better fine-tuning results? Answering these questions requires us to examine how models learn during pre-training and how this affects fine-tuning. Therefore, we begin by fine-tuning two language models under a variety of conditions to determine how fine-tuning affects model behavior. We explore both supervised and instruction fine-tuning, testing the models' memorization and forgetting when learning specific tasks and serving as general-purpose language-AI tools. We then explore the affect of pre-training on these behaviors by fine-tuning multiple pre-training checkpoints of a large language model (Figure 1), evaluating each checkpoint and its fine-tuned variant on downstream evaluation sets. We track model abilities during pre-training and compare them to improvements achieved after fine-tuning at the corresponding pre-training step. 2  Our experiments yield the following insights into LLM training: (1) although supervised fine-tuning can improve performance on in-distribution tasks, it can also cause the model to forget domain knowledge or tasks that it was previously capable of solving ( \u00a74); (2) fine-tuned models show high sensitivity to evaluation prompts, but this sensitivity can be alleviated by more pre-training ( \u00a74); (3) continued pre-training can improve a model in ways that are only revealed after fine-tuning ( \u00a76); (4) tasks for which the model already performs well during pre-training benefit much less from fine-tuning than those where the model does not demonstrate capabilities ( \u00a75, \u00a76);",
            "score": 0.5064072036146858,
            "section_title": "Introduction",
            "char_start_offset": 1678,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 2271
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2197265625
        },
        {
            "corpus_id": "266999144",
            "title": "LightHouse: A Survey of AGI Hallucination",
            "text": "Recent research underscores the critical importance of timeliness of information in preventing hallucinatory outputs from large models. Like humans, large models sometimes forget previous information. However, updating information in large models is quite challenging, leading to hallucinatory output due to outdated information. In the research, Lin et al. (2023b) has highlighted specific instances where foundational models, during the fine-tuning process, tend to sacrifice their general applicability in favor of becoming more specialized for particular tasks. Furthermore, Zhai et al. (2023b)'s work on evaluating catastrophic forgetting in Multimodel Large Language Models (MLLMs) is no-table. By treating each MLLM as an image classifier, Zhai discovered that initial fine-tuning stages on an image dataset not only enhance performance across various other image datasets but also improve the alignment between textual and visual features. However, as fine-tuning continues, a significant drawback emerges: the MLLMs start to hallucinate. This leads to a considerable decline in their generalizability, a trend that persists even when the image encoder component remains unchanged.",
            "score": 0.5061391253239427,
            "section_title": "Timeliness of Information",
            "char_start_offset": 13438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1189
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51513671875
        },
        {
            "corpus_id": "276647867",
            "title": "Continual Memorization of Factoids in Language Models",
            "text": "n j 0 J t s + R s l 5 z z r c L B y U M / j m m 0 i t b Q O n L Q D j p A p 6 i C q o i i R / S K 3 t G H 8 W S 8 G Z / G V 3 8 0 Z w w i X E F / K p f 7 B m q c q 3 Q = < / l a t e x i t > Acc on DA = 90% / 40% pretraining (Petroni et al., 2019;AlKhamissi et al., 2022;Cohen et al., 2023). However, knowledge is not static-new information accumulates quickly while old knowledge becomes obsolete. This dynamic nature necessitates frequent model updates, making costly pretraining impractical. A common approach is to fine-tune the model directly on new knowledge. However, recent studies have shown that finetuning is brittle: training on long-tail knowledge can lead to unintended disruptions, such as decreased factuality and exacerbated hallucinations (Kang et al., 2024;Gekhman et al., 2024;Zhang et al., 2024). Furthermore, a fine-tuned model might not properly memorize knowledge, failing to recall or manipulate it effectively (Allen- Zhu & Li, 2024a;b). \n\nIn this work, we investigate the dynamics of memorization in a continual learning setting (McCloskey & Cohen, 1989;Ratcliff, 1990), in which the model acquires new informa-tion incrementally, one set at a time. While prior research on continual learning in LMs has focused on general capabilities such as reasoning (Luo et al., 2023a) or broad proxies like language modeling loss over a general corpus (Yildiz et al., 2024), we use this framework to study the memorization dynamics of LMs through fine-tuning. We formalize this as continual memorization, in which a model is first trained on a small collection of factoids (factual associations) and must retain this knowledge after training on additional datasets in a subsequent stage.",
            "score": 0.5050389877887302,
            "section_title": "Introduction",
            "char_start_offset": 2335,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1702
                }
            ],
            "ref_mentions": [
                {
                    "start": 224,
                    "end": 246,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1055,
                    "end": 1080,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1080,
                    "end": 1095,
                    "matchedPaperCorpusId": "18556305"
                },
                {
                    "start": 1367,
                    "end": 1388,
                    "matchedPaperCorpusId": "268032887"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08599853515625
        },
        {
            "corpus_id": "273323302",
            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
            "text": "Previous studies have shown that instruction tuning and RLHF can lead to forgetting, also known as the alignment tax (Ouyang et al., 2022). The changes in world knowledge and commonsense reasoning abilities are examined throughout the iterative process by evaluating its performance on multilingual NLP benchmarks.",
            "score": 0.5050389877887302,
            "section_title": "X-ALPACAEVAL LEADERBOARD",
            "char_start_offset": 19052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 314
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 138,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "263908902",
            "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
            "text": "There have been recent attempts to tune input prompts to improve the performance of LLMs further (Liu et al., 2023b;Lester et al., 2021;Li and Liang, 2021;Qin and Eisner, 2021;Liu et al., 2022Liu et al., , 2023a)). However, directly optimizing prompts is not trivial since changes in the input space may cause non-monotonic performance changes (Hu et al., 2022). Especially, Fichtel et al. (2021) demonstrate that finetuned LMs outperform prompt-tuned LMs on factual knowledge probing tasks. Although LLMs, such as GPT-3 and T0, were primitively designed to perform well on various tasks without finetuning (Brown et al., 2020;Sanh et al., 2022), recent work shows that finetuning improves the linguistic capabilities of LLMs substantially (Ouyang et al., 2022;Wei et al., 2022). Therefore, we consider finetuned LMs for analysis.",
            "score": 0.5050162443234377,
            "section_title": "Prompt Tuning and Finetuning",
            "char_start_offset": 5107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 830
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 136,
                    "matchedPaperCorpusId": "233296808"
                },
                {
                    "start": 136,
                    "end": 155,
                    "matchedPaperCorpusId": "230433941"
                },
                {
                    "start": 155,
                    "end": 176,
                    "matchedPaperCorpusId": "233231453"
                },
                {
                    "start": 176,
                    "end": 192,
                    "matchedPaperCorpusId": "248780177"
                },
                {
                    "start": 344,
                    "end": 361,
                    "matchedPaperCorpusId": "235458009"
                },
                {
                    "start": 607,
                    "end": 627,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 627,
                    "end": 645,
                    "matchedPaperCorpusId": "239009562"
                },
                {
                    "start": 740,
                    "end": 761,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 761,
                    "end": 778,
                    "matchedPaperCorpusId": "237416585"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049591064453125
        },
        {
            "corpus_id": "269635770",
            "title": "Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?",
            "text": "Practical Implications. This work highlights the risk in using supervised fine-tuning to update LLMs' knowledge, as we present empirical evidence that acquiring new knowledge through fine-tuning is correlated with hallucinations w.r.t pre-existing knowledge. Additionally, this work raises important questions for future exploration regarding fine-tuning practices. We saw that Unknown examples are fitted slower than the Known ones, thus their negative effect manifests as a form of overfitting, which emphasizes the importance of using early-stopping instead of a fixed number of fine-tuning steps. However, earlystopping may be less effective when fine-tuning on numerous tasks with distinct optimal stopping points. An alternative solution can be to avoid adding new knowledge, by aligning the fine-tuning data with the model's knowledge through filtering-  2024) showed that models fine-tuned on well-known facts exhibit enhanced factuality compared to those fine-tuned on unpopular facts, which can be attributed to the model's lesser familiarity with unpopular facts. Another example is Lin et al. (2024), who fine-tuned a model using data generated by either a pre-trained model or a retrieval-augmented variant. They found that the latter resulted in reduced factuality, which can be attributed to the introduction of new factual knowledge in the retrieved texts. Ren et al. (2024) have also investigated the effects of introducing new factual knowledge through fine-tuning in a considerably different methodological setup, focusing on multiple-choice questions, conducting relatively short fine-tuning runs, and testing only 100% known and 100% unknown mixtures. Their results align with ours, which further reinforces our conclusions. Lastly, these insights were also integrated into the instruction-tuning phase of Llama 3 models (Dubey et al., 2024), ensuring that the examples are aligned with pre-training knowledge. \n\nAnother line of work explores the model's behavior on new knowledge in test time. Kang et al. (2024) showed that when a fine-tuned LLM encounters unknown queries at test time, its responses mimic the responses associated with the unknown examples in the fine-tuning data.",
            "score": 0.5049354485551613,
            "section_title": "Discussion",
            "char_start_offset": 18431,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1931
                },
                {
                    "start": 1934,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2205
                }
            ],
            "ref_mentions": [
                {
                    "start": 1373,
                    "end": 1390,
                    "matchedPaperCorpusId": "268041894"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.392333984375
        },
        {
            "corpus_id": "265158118",
            "title": "Vision-Language Instruction Tuning: A Review and Analysis",
            "text": "Existing MLLMs have demonstrated strong instruction execution and generalization abilities in some simple tasks. However, they generally have shortcomings when dealing with some special difficult samples, such as fine-grained content understanding, complex reasoning, and multimodal content editing. The chain-of-thought (Wei et al., 2022) and in-context learning (Dong et al., 2022) can alleviate the occurrence of the above problems to some extent, but the cost of obtaining high-quality annotated data is too high, and the model computation cost it brings is also unacceptable. Therefore, how to efficiently solve such difficult samples is a problem worthy of attention. \n\nSelective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes. \n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter. \n\nLimited Emergence. As a significant purpose of instruction tuning, instruction generalization is an advantage that LLM (and MLLM) should possess, stemming from the emergence phenomenon. In the multimodal scenario, emergence shows more characters, such as supporting visual objects that have not appeared. However, recently, a large number of MLLMs have not shown good performance in either of the above two scenarios. Therefore, deeper consideration is needed on how to design MLLMs and corresponding instruction tuning data.",
            "score": 0.5049103603053067,
            "section_title": "Difficult Sample.",
            "char_start_offset": 50998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 902
                },
                {
                    "start": 903,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1330
                },
                {
                    "start": 1333,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2185
                }
            ],
            "ref_mentions": [
                {
                    "start": 321,
                    "end": 339,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "266374841",
            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
            "text": "After Release S 0 S 0 + S 1 S 1 + Figure 1: An LLM will usually evolve through different sessions of training in its life time. Before release, the LLM is first pre-trained (session S 0 ) and then undergoes safety finetuning for alignment (session S 0 +). The released LLM will then be finetuned on some custom downstream data (session S 1 ), which potentially contain unsafe examples. A sequential safety finetuning session (i.e., S 1 +) may be needed again. This work studies the safety concerns of released LLMs by examining the learning process in downstream finetuning and the forgetting patterns during subsequent safety finetuning. Our goal is to design methods that ensure the safety of customized finetuning without compromising learning important downstream knowledge. \n\nthe efficacy of these precautions in resisting potential vulnerabilities during customized finetuning remains uncertain. If aligned LLMs can be jailbroken during customized finetuning, it is crucial to study whether safety finetuning following downstream finetuning is still suitable for recovering the safety in this case. See Figure 1 for a work flow diagram of downstream finetuning and safety finetuning before and after the release of LLMs. Furthermore, catastrophic forgetting (CF) (McCloskey & Cohen, 1989) may happen during safety finetuning, which can cause LLMs to forget previously learned knowledge apart from unsafe knowledge. Therefore, it is imperative to explore strategies in addition to safety finetuning to retain as much downstream knowledge as possible while keeping LLMs safe. \n\nTo this end, in this work we study how LLMs of different scales learn unsafe examples during customized downstream finetuning and more importantly, how they forget those unsafe examples and other data in the sequential safety finetuning stages. We begin by constructing noisy downstream datasets (e.g., question answering) for finetuning, containing a variety of data sources (including unsafe examples). Our investigation confirms the vulnerability of aligned LLMs to downstream finetuning on such noisy datasets containing unsafe examples and shows that larger LMs exhibit a faster acquisition of unsafe knowledge. Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten.",
            "score": 0.5047634547655313,
            "section_title": "Finetuning Safety Finetuning",
            "char_start_offset": 2260,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 778
                },
                {
                    "start": 781,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1420
                },
                {
                    "start": 1421,
                    "end": 1579
                },
                {
                    "start": 1582,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2381
                }
            ],
            "ref_mentions": [
                {
                    "start": 1269,
                    "end": 1293,
                    "matchedPaperCorpusId": "61019113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "277856836",
            "title": "SHA256 at SemEval-2025 Task 4: Selective Amnesia - Constrained Unlearning for Large Language Models via Knowledge Isolation",
            "text": "Further, these methods may be vulnerable to membership inference attacks (MIA) (Chen et al., 2021;Sula et al., 2024), and exhibit difficulty in preserving knowledge within the retain set while effectively unlearning the forget set. \n\nTo address these limitations and foster research into more effective and robust unlearning strategies, SemEval 2025 Task 4, Unlearning Sensitive Content from Large Language Models (Ramakrishna et al., 2025a,b), challenges participants to develop methods that can selectively remove sensitive information from LLMs while preserving their core capabilities. \n\nIn this work, we address the challenge of tar-geted unlearning by first performing knowledge isolation using causal mediation analysis (Vig et al., 2004;Geva et al., 2023). Causal mediation analysis helps identify the specific layers within the LLM responsible for storing the factual knowledge to be unlearned. Through experiments with the provided fine-tuned OLMo models (Groeneveld et al., 2024) (both 1B and 7B parameter versions, fine-tuned by the task organizers to memorize the forget and retain sets), we empirically determine that the initial layers (specifically layers 0-5) have a disproportionately high impact on factual recall. \n\nOur approach combines targeted knowledge removal with a novel joint loss function. By focusing on causally identified lower layers (layers 0-5) and using cross-entropy loss on output tokens, we aim to disrupt specific subject-attribute associations while preserving overall model performance. This method seeks to achieve effective and efficient unlearning of sensitive content in LLMs by isolating knowledge, applying carefully designed loss functions, and implementing targeted parameter updates. \n\nOur method achieves 2nd place in the 1B model track with a with a final score of 0.652, demonstrating a strong task aggregate performance (0.973) while maintaining 88% of baseline MMLU accuracy. The 7B variant shows comparable forget set eradication (0.964 task score) but highlights scalability challenges through a 46% MMLU decrease, underscoring the need for layer-specific capacity analysis in larger models.",
            "score": 0.5045340183281934,
            "section_title": "Introduction",
            "char_start_offset": 1726,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 234,
                    "end": 589
                },
                {
                    "start": 592,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1233
                },
                {
                    "start": 1236,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1734
                },
                {
                    "start": 1737,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2149
                }
            ],
            "ref_mentions": [
                {
                    "start": 79,
                    "end": 98,
                    "matchedPaperCorpusId": "218502126"
                },
                {
                    "start": 745,
                    "end": 763,
                    "matchedPaperCorpusId": "258417932"
                },
                {
                    "start": 965,
                    "end": 990,
                    "matchedPaperCorpusId": "267365485"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10504150390625
        },
        {
            "corpus_id": "267750997",
            "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
            "text": "A potential dilemma is that fine-tuning an LLM on specific tasks might face catastrophic forgetting of its original capabilities. In this section, we explore the forgetting of CultureLLM in two general datasets: BIG-Bench-Hard (BBH) [Suzgun et al., 2022] and GSM8K [Cobbe et al., 2021]. BBH contains 21 tasks covering both semantic understanding and logical reasoning tasks. GSK8K is a widely used data set to evaluate mathematical ability. For BBH, we sample 100 samples for each task to test, due to cost savings. We compare each CultureLLM with the GPT-3.5 baseline model in Figure 5(a). The results show that CultureLLM does not decrease performance in most benchmarks and can even improve their results, such as on BBH. This suggests that there might be some latent relations between the cultural data and the general benchmarks, thus fine-tuning on cultural data can benefit general reasoning abilities.",
            "score": 0.5042686182071665,
            "section_title": "Fine-tuning vs. Forgetting",
            "char_start_offset": 23834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 909
                }
            ],
            "ref_mentions": [
                {
                    "start": 265,
                    "end": 285,
                    "matchedPaperCorpusId": "239998651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2470703125
        },
        {
            "corpus_id": "259137624",
            "title": "The Effect of Masking Strategies on Knowledge Retention by Language Models",
            "text": "In this paper, we study the effect of pre-training tasks on the amount of knowledge retained in the model. To do so, we use the PAQ dataset, which contains a large set of QA pairs and passages that contain ground-truth facts. In our experiments, we make several observations: First, in the current typical approach of additional pre-training followed by CBQA fine-tuning salient span masking outperforms random token masking. On average, the principled approach of PMI masking performs similarly to entity masking, with less variance between runs. Next, we hypothesize that sequentially training on language modeling followed by CBQA leads to factual knowledge no longer being accessible. Therefore, we apply elastic weight consolidation and multi-task learning, two common strategies to counter catastrophic forgetting. We find catastrophic forgetting of knowledge indeed to be a prob-lem, as both applying EWC and MTL leads to substantial improvements in actionable knowledge retained. We hope this study will lead to more research on knowledge acquisition and forgetting in language models.",
            "score": 0.503796621300248,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 19059,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 547
                },
                {
                    "start": 548,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1093
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50830078125
        },
        {
            "corpus_id": "271516497",
            "title": "Synergizing Knowledge Graphs with Large Language Models: A Comprehensive Review and Future Prospects",
            "text": "Outdated Knowledge: Pre-trained LLMs struggle to incorporate up-to-date information, as their knowledge remains static and fails to capture the latest insights [8]. \n\n2) Parameter Rigidity: Fine-tuning these models necessitates high-quality data, entailing significant costs [9], and iterative fine-tuning can result in catastrophic forgetting. \n\n3) Illusory Accuracy: LLMs occasionally produce responses that appear plausible but deviate from factual reality, manifesting as hallucinations [10,11].",
            "score": 0.5021476806372068,
            "section_title": "1)",
            "char_start_offset": 569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 499
                }
            ],
            "ref_mentions": [
                {
                    "start": 491,
                    "end": 495,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49560546875
        },
        {
            "corpus_id": "173990683",
            "title": "Domain Adaptive Inference for Neural Machine Translation",
            "text": "In NMT fine-tuning, a model is first trained on a task A, typically translating a large generaldomain corpus (Luong and Manning, 2015). The optimized parameters \u03b8 * \n\nA are fine-tuned on task B, a new domain. Without regularization, catastrophic forgetting can occur: performance on task A degrades as parameters adjust to the new objective. A regularized objective is: \n\nwhere L A (\u03b8) and L B (\u03b8) are the likelihood of tasks A and B. We compare three cases: \n\n\u2022 No-reg, where \u039b = 0 \n\n\u2022 L2, where F j = 1 for each parameter index j \n\n\u2022 EWC, where F j = E \u2207 2 L A (\u03b8 j ) , a sample estimate of task A Fisher information. This effectively measures the importance of \u03b8 j to task A. \n\nFor L2 and EWC we tune \u039b on the validation sets for new and old tasks to balance forgetting against new-domain performance.",
            "score": 0.5020239181817521,
            "section_title": "Adaptive training",
            "char_start_offset": 1609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 164
                },
                {
                    "start": 167,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 458
                },
                {
                    "start": 461,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 678
                },
                {
                    "start": 681,
                    "end": 804
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 134,
                    "matchedPaperCorpusId": "14646537"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092529296875
        },
        {
            "corpus_id": "271244944",
            "title": "CroMo-Mixup: Augmenting Cross-Model Representations for Continual Self-Supervised Learning",
            "text": "Catastrophic forgetting is the most addressed issue in continual self-supervised learning literature.It represents the significant loss of performance on previous tasks upon learning new ones.Due to forgetting of the previous tasks, the model's within-task prediction performance on the previous tasks decreases substantially,   whereas it remains at a desirable level on the current task.Forgetting also degrades the task-id prediction performance as shown in Figure 2. The earlier CSSL works exploited memory replay to address catastrophic forgetting [19].The stateof-the-art CSSL works have proposed self-supervised learning loss adaptation for knowledge distillation [6,13] and fine-tuning [44] to mitigate this problem.However, the current literature on CSSL does not pay due attention to task confusion challenge that can hinder learning distinctive representations in the absence of labels as explained below.",
            "score": 0.5018714295710561,
            "section_title": "Catastrophic Forgetting",
            "char_start_offset": 12750,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 101,
                    "end": 192
                },
                {
                    "start": 192,
                    "end": 389
                },
                {
                    "start": 389,
                    "end": 558
                },
                {
                    "start": 558,
                    "end": 724
                },
                {
                    "start": 724,
                    "end": 916
                }
            ],
            "ref_mentions": [
                {
                    "start": 674,
                    "end": 677,
                    "matchedPaperCorpusId": "244954199"
                },
                {
                    "start": 694,
                    "end": 698,
                    "matchedPaperCorpusId": "257834122"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.355712890625
        },
        {
            "corpus_id": "275820535",
            "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
            "text": "Thus, we propose layer-wise coefficients to dynamically adjust the balance between task learning and the retention of general knowledge in each layer, allowing some layers to prioritize task learning, while others preserve general knowledge. We use the L2 norm of the computed element-wise importance of each layer weight to capture their contribution to both objectives. \n\nThrough extensive experiments on scientific, physical, and medical tasks using LLMs (GPT-J and LLaMA-3), we demonstrate that our framework achieves state-of-the-art performance, mitigating catastrophic forgetting while enhancing LLM adaptability. To maintain general capabili-ties, it is essential to identify and quantify the importance of various parameters that contribute to these capabilities. The computation of parameter importance is typically timeconsuming, and storing the associated weights requires substantial memory resources. Our experimental results demonstrate that our method is nearly 20 times faster and requires only 10%\u223c15% of the storage memory compared to the previous method, demonstrating the practicality of our approach. Our contributions are as follows: \n\n\u2022 We introduce a framework that first records parameter importance on general data, and then applies regularization constraints during fine-tuning on domain-specific data to effectively address catastrophic forgetting in large language models (LLMs). \n\n\u2022 We propose the element-wise and layer-wise importance metrics to dynamically adjust parameter updates, preserving critical general knowledge while allowing domain-specific expertise to be learned effectively. \n\n\u2022 Our method achieves state-of-the-art performance across multiple datasets using mainstream backbone LLMs. It significantly reduces computational time (20x faster) and storage (10%\u223c15%) for parameter importance estimation compared to prior methods.",
            "score": 0.5016335180503878,
            "section_title": "Introduction",
            "char_start_offset": 4408,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 371
                },
                {
                    "start": 374,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 772
                },
                {
                    "start": 773,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1122
                },
                {
                    "start": 1123,
                    "end": 1156
                },
                {
                    "start": 1159,
                    "end": 1409
                },
                {
                    "start": 1412,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1732
                },
                {
                    "start": 1733,
                    "end": 1874
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.442138671875
        },
        {
            "corpus_id": "266162497",
            "title": "Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs",
            "text": "We emphasize that this knowledge evaluation framework for LLMs is imperfect. Importantly, it doesn't address other quality metrics influencing a model's response. Creating a purely knowledgeintensive dataset without involving some level of reasoning is challenging. Consequently, a model with robust reasoning abilities might excel on unfamiliar knowledge-intensive tasks by making \"educated guesses\" in a multiple-choice exam. Therefore, any evaluation of knowledge in LLMs should consider this, with results seen as part of a broader range of benchmarks for reasoning (Sakaguchi et al., 2021), reading comprehension (Dua et al., 2019), and general language abilities (Srivastava et al., 2022). However, this evaluation framework still strongly emphasizes factual information above all else. \n\nCauses for Factual Errors There are many possible reasons for the failure of models to answer factual questions accurately. In (Wang et al., 2023), Wang et al. introduce a taxonomy of five main model-level causes: \n\n\u2022 Domain knowledge deficit: A language model may lack comprehensive expertise in a specific domain to which it has not been exposed. For example, a model trained exclusively on texts written by William Shakespeare would perform poorly when asked about the works of Mark Twain. \n\n\u2022 Outdated Information: LLMs invariably have a cutoff date determined by their training dataset. Consequently, any events, discoveries, or changes occurring after the last training update will not be within the model's knowledge without access to external sources. \n\n\u2022 Immemorization: Sometimes, a model is exposed to knowledge during its training process but does not retain it. This is especially true for rare facts that appear in the training dataset only scarcely (Kandpal et al., 2023). \n\n\u2022 Forgetting: Language models often undergo additional training after the pre-training phase (fine-tuning). In some cases, this might lead to a phenomenon called catastrophic forgetting (Kirkpatrick et al., 2017;Goodfellow et al., 2013;Chen et al., 2020;Luo et al., 2023), where models lose some of the knowledge they had prior to the finetuning process. \n\n\u2022 Reasoning Failure: In certain instances, a language model might possess relevant knowledge about a fact but fail to utilize it properly.",
            "score": 0.5001434897890058,
            "section_title": "Knowledge and Reasoning",
            "char_start_offset": 5028,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 792
                },
                {
                    "start": 795,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1008
                },
                {
                    "start": 1011,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1287
                },
                {
                    "start": 1290,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1554
                },
                {
                    "start": 1557,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2139
                },
                {
                    "start": 2142,
                    "end": 2280
                }
            ],
            "ref_mentions": [
                {
                    "start": 570,
                    "end": 594,
                    "matchedPaperCorpusId": "199370376"
                },
                {
                    "start": 1759,
                    "end": 1781,
                    "matchedPaperCorpusId": "253522998"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.388427734375
        },
        {
            "corpus_id": "259308985",
            "title": "Modeling Parallel Programs using Large Language Models",
            "text": "We first show the results of fine-tuning the three models selected in Table II. Table IV shows the validation perplexity at the end of fine-tuning. Here perplexity is calculated as the exponential of the loss as described in Section II. Each model converges to a low perplexity score over the separate testing set (between 2 and 4). GPT-Neo and PolyCoder achieve comparable perplexity scores (within 0.01) while GPT2 achieves a higher perplexity. All three have different pre-training datasets and the former two are of a larger size than GPT2 (see Table II). From this we can conclude that for this problem the pre-training dataset had less of an impact on validation perplexity than the model size. The lower perplexity of the larger models means that they model the language better. For the rest of the results presented in this section we will use PolyCoder+HPC, GPT-Neo+HPC, and GPT2+HPC to refer to the respective models fine-tuned on the HPC dataset. \n\nAfter fine-tuning each of the models and evaluating them on the downstream tasks we noticed that the perplexity would keep improving with more fine-tuning, but the downstream evaluation performance would start to decrease. This is likely because LLMs are subject to catastrophic forgetting during fine-tuning. Catastrophic forgetting is the phenomenon where previously learned information is lost or forgotten as the model continues training and updating its weights. It is typically prevented by minimizing the amount of fine-tuning and using a sufficiently low learning rate. \n\nTo explore this phenomenon we ran the code generation tasks every 1000 samples during fine-tuning of the PolyCoder model. Figure 4 presents the results from our evaluation tests during fine-tuning on the PolyCoder model. After seeing about 45,000 samples during fine-tuning the model starts to decrease in evaluation performance. This is in contrast to the perplexity which keeps improving past 45,000 samples. Based on this result we stop fine-tuning at 45,000 samples and use these weights for the rest of the evaluations. Additionally, due to the computation time needed to run this test we use the 45,000 samples stopping point for fine-tuning all the models.",
            "score": 0.4999100312510911,
            "section_title": "A. Fine-tuning on HPC Source Code Data",
            "char_start_offset": 28661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 957
                },
                {
                    "start": 960,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1537
                },
                {
                    "start": 1540,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1950
                },
                {
                    "start": 1951,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2203
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "273098143",
            "title": "Undesirable Memorization in Large Language Models: A Survey",
            "text": "In machine learning, forgetting mechanisms are the processes through which models lose or discard previously learned information [95]. These mechanisms can occur unintentionally as part of the natural training dynamics or be purposefully induced to meet specific objectives, such as improving model generalization or addressing privacy concerns [96,97,98]. Blanco-Justicia et al. [99] provide a recent, detailed overview of forgetting in LLMs. \n\nKirkpatrick et al. [100] initially introduced \"catastrophic forgetting\" in the context of neural networks and continual learning. They propose a method to protect important model weights to retain knowledge. This approach has been effective in maintaining performance on older tasks, even after long periods of non-use. Tirumala et al. [25] observe the forgetting mechanisms of a special batch through the learning process and show that it follows an exponential degradation, reaching a constant value baseline. They show that the mentioned baseline scales with the model size. Jagielski et al. [93]",
            "score": 0.49985522785750247,
            "section_title": "H. Forgetting mechanisms",
            "char_start_offset": 39189,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 356
                },
                {
                    "start": 357,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1045
                }
            ],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 133,
                    "matchedPaperCorpusId": "218889912"
                },
                {
                    "start": 345,
                    "end": 349,
                    "matchedPaperCorpusId": "51801958"
                },
                {
                    "start": 465,
                    "end": 470,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 782,
                    "end": 786,
                    "matchedPaperCorpusId": "248986465"
                },
                {
                    "start": 1041,
                    "end": 1045,
                    "matchedPaperCorpusId": "250243645"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.351318359375
        },
        {
            "corpus_id": "270620350",
            "title": "Preserving Knowledge in Large Language Model with Model-Agnostic Self-Decompression",
            "text": "The problem of catastrophic forgetting in LLM is widely discussed.To verify the problem of catastrophic forgetting in MLLM, we trained (SFT) LLaVA (Liu et al., 2023) for 5 epochs, and evaluated the model every 3000 steps.As shown in Figure 1, we observed that the performance of MLLM benchmarks grew even after the third epoch, but LMM benchmarks started to deteriorate since the third epoch.These different behaviors of the model performance between vision-language benchmarks and pure language benchmarks indicate that MLLM has begun to forget its general language ability rather than simply overfit the data.\n\nThere is a point of view that describes the LLMs as lossless compressors (Rae, 2023;Sutskever, 2023;Gu et al., 2024;Yang et al., 2023;Chiang, 2023).(Del\u00e9tang et al., 2024) from DeepMind explores the connection between predictive models and lossless compression and state that the advancement in self-supervised LLMs can be effectively leveraged for compression tasks.also show competitive performance across different data modalities, such as images and audio.\n\nEnlightened by these works, we considered the process of synthesizing data with LLMs (a.k.a.generating text data) from LLMs as a decompression process.We aim to preserve knowledge from LLMs by taking a snapshot of the LLMs, i.e. using LLMs as offline data generators and dumping the generated corpus.This self-decompression method should be able to apply to any LMM (modelagnostic).By adding the decompressed data during post-pretraining or SFT, the old knowledge could be reminded and kept.For this purpose, we design a novel approach, named Tree-Generation (TG), along with its variants TG-SFT for supervised finetuning the MLLMs.With this model-agnostic approach, we observed the catastrophic forgetting problem can be reduced significantly.\n\nFrom extensive experiments, we show that TG algorithm is useful in reducing catastrophic forgetting.Our contribution on TG algorithm can be summarized as threefolds.",
            "score": 0.499796998502494,
            "section_title": "Introduction",
            "char_start_offset": 1826,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 66,
                    "end": 221
                },
                {
                    "start": 221,
                    "end": 392
                },
                {
                    "start": 392,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1073
                },
                {
                    "start": 1075,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1226
                },
                {
                    "start": 1226,
                    "end": 1375
                },
                {
                    "start": 1375,
                    "end": 1457
                },
                {
                    "start": 1457,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1707
                },
                {
                    "start": 1707,
                    "end": 1819
                },
                {
                    "start": 1821,
                    "end": 1921
                },
                {
                    "start": 1921,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 686,
                    "end": 697,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 761,
                    "end": 784,
                    "matchedPaperCorpusId": "262054258"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314697265625
        },
        {
            "corpus_id": "268513020",
            "title": "InsCL: A Data-efficient Continual Learning Paradigm for Fine-tuning Large Language Models with Instructions",
            "text": "Due to the scaling laws for neural language models, LLMs emerge with capabilities when the scale increases.They can be better adapted to various downstream tasks through instruction tuning, offering immense practical value in real-world applications.The exploration of CL for LLMs is still in its early stages.Continual-T0 (Scialom et al., 2022) first fine-tuned LLMs with instructions in an incremental manner, claiming that well-pretrained models can be continual learners by randomly replaying several previous examples.Several works (Song et al., 2023;Wang et al., 2023) focus on CL methods with parameter-efficient tuning (Hu et al., 2021), largely alleviating the forgetting issue under limited training resources.For full fine-tuning, replay-based methods were preliminarily investigated (Yin et al., 2023), proving that replaying data based on diverse instructions can alleviate catastrophic forgetting and help better generalize to unseen tasks.However, there is still a lack of detailed analysis of replay strategies.\n\nIn this work, we focus on the appropriate replaybased method for LLMs' full fine-tuning with instructions.Considering that instructions naturally provide high-quality task-related descriptions, it is necessary to fully utilize instruction information to customize a replay strategy for instruction tuning.",
            "score": 0.49889659764425887,
            "section_title": "CL for LLMs instruction tuning",
            "char_start_offset": 7236,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 107,
                    "end": 250
                },
                {
                    "start": 250,
                    "end": 310
                },
                {
                    "start": 310,
                    "end": 523
                },
                {
                    "start": 523,
                    "end": 720
                },
                {
                    "start": 720,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1027
                },
                {
                    "start": 1029,
                    "end": 1135
                },
                {
                    "start": 1135,
                    "end": 1334
                }
            ],
            "ref_mentions": [
                {
                    "start": 323,
                    "end": 345,
                    "matchedPaperCorpusId": "252815378"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07421875
        },
        {
            "corpus_id": "267636596",
            "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
            "text": "As shown in Figure 1, concepts are discrete, interpretable units of knowledge that allow for fine-grained analysis of learning and forgetting processes. By focusing on concepts, we can precisely identify what knowledge is acquired, retained, or forgotten, providing clearer insights into the incremental learning abilities of LLMs. Our investigation also delves into how in-context learning, parameter-efficient methods like LoRA (Hu et al., 2021), and factors such as model scale, buffer size, and pretraining influence IL performance. \n\nThrough extensive experiments, we find that (1) LLMs still suffer from catastrophic forgetting when incrementally learning new concepts; (2) Incontext learning, while avoiding the need for parameter updates, does not effectively facilitate the learning of new concepts compared to finetuning; \n\n(3) Despite its efficiency, LoRA restricts the ability to memorize and generalize new knowledge and may lead to more forgeting on training data, contradicting the common belief that LoRA mitigates forgetting by finetuning fewer parameters; (4) Data replay proves to be the most effective IL method, consistently outperforming others and mitigating forgetting; (5) Additionally, larger models, bigger buffers, and extensive pretraining steps contribute significantly to better IL performance; (6) Concepts that are well-defined and concrete are easier for LLMs to learn and retain, whereas abstract and emerging concepts pose greater challenges. \n\nIn summary, this paper presents Concept-1K, a novel dataset designed to rigorously evaluate the incremental learning capabilities of LLMs. Our findings provide valuable insights into the strengths and limitations of current LLMs in IL scenarios and offer a robust benchmark for future research in this area. \n\n2 Concept-1K",
            "score": 0.498455237981115,
            "section_title": "Introduction",
            "char_start_offset": 3898,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 536
                },
                {
                    "start": 539,
                    "end": 831
                },
                {
                    "start": 834,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1788
                },
                {
                    "start": 1791,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 430,
                    "end": 447,
                    "matchedPaperCorpusId": "235458009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.250244140625
        },
        {
            "corpus_id": "274432429",
            "title": "Leveraging Large Language Models and Agent-Based Systems for Scientific Data Analysis: Validation Study",
            "text": "LLMs are AI models trained on massive amounts of text data. They have demonstrated remarkable abilities in various natural language processing tasks, including text generation, translation, summarization, and question-answering. However, their application in scientific domains presents challenges due to their potential for inaccuracies and the need for precise, domain-specific information. Even the most advanced LLMs are prone to recall information from their training data inaccurately. While LLMs demonstrate impressive capabilities in general language processing, their very training methodology presents a challenge for ensuring factual accuracy in specific domain scenarios. The vast amount of data used for training can lead to a phenomenon known as \"catastrophic forgetting,\" [4,5] where the model prioritizes broader language patterns over the nuances of individual data points. This inherent limitation means we need a different approach other than simply training an LLM on a data set and expecting it to provide accurate responses to specific queries within that domain. \n\nTo combat this challenge, a recall system can be used to provide information to the LLM to help anchor its output to a correct context. However, for complex tasks requiring a multistep information retrieval process, where the answer to each query informs the next, a more sophisticated architecture is necessary. These systems are referred to as agents [6]. The architecture behind these agents typically shares 2 characteristics. First, they operate in a loop in which they \"act\" multiple times before returning an answer to a user. Second, they can use existing tools. These tools are subsystems that perform defined operations like a calculator or a search function. Tool use can be facilitated by many different implementations but almost all of them involve the agent generating a string in a specific format that the system can then parse as a command. Once the tool has finished running, the result produced is returned to the LLM and then the loop repeats. Figure 1 shows an example of a tool-using agent's core loop. When the user asks a question the LLM looks at the question and determines the first action that needs to be taken. The LLM generates a string that defines the next tool to be used and the arguments to be passed to it. The parser then reads the string and then calls the indicated tools with corresponding arguments. The selected tool runs with the specified arguments and returns some answers.",
            "score": 0.4978616665131552,
            "section_title": "Background",
            "char_start_offset": 2657,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1445
                },
                {
                    "start": 1446,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1658
                },
                {
                    "start": 1659,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2332
                },
                {
                    "start": 2333,
                    "end": 2430
                },
                {
                    "start": 2431,
                    "end": 2508
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.400390625
        },
        {
            "corpus_id": "268510177",
            "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
            "text": "Large language models (LLMs) make remarkable breakthroughs in recent years (Zhao et al., 2023). LLMs such as PaLM (Chowdhery et al., 2023) and LLaMA (Touvron et al., 2023a) show powerful capabilities in multiple tasks such as information extraction, question answering, commonsense reasoning, and mathematical operations. One of the major issues is how to leverage the knowledge of LLMs pretrained with unsupervised or general objectives to produce results consistent with human intent during task-specific interactions (Zhang et al., 2023b). To endow LLMs with such \"instruction-following\" ability, instruction tuning is proposed as an effective technique that can bridge the gap between the  (MC). Note that IC is a held-out task for evaluation, and LLM at t generates more illegal categories defined in MC (36.4% \u2192 49.6%) as their instructions are similar. \n\ngeneration process of LLMs and the objective of users (Ouyang et al., 2022;Zhang et al., 2023b,a). \n\nAlthough tuning a pretrained LLM with instruction data before deployment gains wide application, it still faces challenges when dealing with incremental data and tasks (Zhang et al., 2023c). Continual learning (CL) (Biesialska et al., 2020) is introduced to avoid costly retraining on all collected instances (Biesialska et al., 2020), and continual instruction tuning (CIT) (Zhang et al., 2023c) is a sub-task of it about instruction data. However, catastrophic forgetting (CF) is still an unavoidable problem during CIT, which refers to the forgetting of previously learned tasks and the deterioration of original generalization ability (Zhao et al., 2022;Zeng et al., 2023b;Zhang et al., 2023c). \n\nRecently, replay, architecture, and regularization are three main strategies to mitigate the CF problem. Replay is the most prevalent strategy that leverages task-specific features to replay a small arXiv:2403.10056v1",
            "score": 0.4977723613858072,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 699
                },
                {
                    "start": 700,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 960
                },
                {
                    "start": 963,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1661
                },
                {
                    "start": 1664,
                    "end": 1768
                },
                {
                    "start": 1769,
                    "end": 1881
                }
            ],
            "ref_mentions": [
                {
                    "start": 114,
                    "end": 138,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1131,
                    "end": 1152,
                    "matchedPaperCorpusId": "264426357"
                },
                {
                    "start": 1178,
                    "end": 1203,
                    "matchedPaperCorpusId": "227231454"
                },
                {
                    "start": 1272,
                    "end": 1297,
                    "matchedPaperCorpusId": "227231454"
                },
                {
                    "start": 1338,
                    "end": 1359,
                    "matchedPaperCorpusId": "264426357"
                },
                {
                    "start": 1602,
                    "end": 1621,
                    "matchedPaperCorpusId": "252907346"
                },
                {
                    "start": 1640,
                    "end": 1660,
                    "matchedPaperCorpusId": "264426357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "270226041",
            "title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback",
            "text": "In Section 1, we introduced two types of datasets to enhance the multilingual capabilities of LLMs: multilingual parallel corpus and multilingual instruction dataset.We study how these two types of data impact the multilingual capabilities of LLMs.\n\nTo do so, we conduct comprehensive comparison experiments on these two types of dataset.For the multilingual parallel corpus, we use the NLLB dataset (Costa-juss\u00e0 et al., 2022) collected by allenai10 .We use the same set of 100 languages as xLLMs-100 for the sake of a fair comparison.\n\nThe experimental results presented in Table 5 clearly show that utilizing the multilingual parallel corpus leads to a significant improvement in the machine translation task.However, there is a notable decrease in performance observed in non machine translation tasks.This phenomenon is referred to as catastrophic forgetting (McCloskey and Cohen, 1989), where the model after fine-tuning achieves better performance on the new task at the expense of the model's performance on other tasks.On the other hand, the model fine-tuned with multilingual instruction data demonstrates improvement across all tasks, indicating that attention should be given to the models' performance on a broad range of tasks during fine-tuning, rather than focusing solely on performance gains in a single task.It is obvious that the multilingual instruction dataset is a better choice compare to the multilingual parallel dataset when finetuning LLMs.",
            "score": 0.49745789894491876,
            "section_title": "Different Datasets for Multilingual Tuning",
            "char_start_offset": 28854,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 248
                },
                {
                    "start": 250,
                    "end": 338
                },
                {
                    "start": 338,
                    "end": 451
                },
                {
                    "start": 451,
                    "end": 535
                },
                {
                    "start": 537,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1326
                },
                {
                    "start": 1326,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 863,
                    "end": 890,
                    "matchedPaperCorpusId": "61019113"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2158203125
        },
        {
            "corpus_id": "258841149",
            "title": "The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning",
            "text": "In this section, we conduct experiments to address the following two research questions: \n\n\u2022 For practitioners, is it more effective to augment CoT rationales across diverse tasks or more instances with a fixed number of tasks? \n\n\u2022 During CoT fine-tuning, does the LM maintain its performance on in-domain tasks without any catastrophic forgetting?",
            "score": 0.4972636992363735,
            "section_title": "Analysis of of CoT Fine-tuning",
            "char_start_offset": 24706,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 91,
                    "end": 227
                },
                {
                    "start": 230,
                    "end": 348
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.17041015625
        },
        {
            "corpus_id": "260164719",
            "title": "This is not correct! Negation-aware Evaluation of Language Generation Systems",
            "text": "A common issue with fine-tuning is catastrophic forgetting (Goodfellow et al., 2013), i.e., the models forget their initial knowledge and overfit the new task. Our fine-tuning approach is successful if it improves negation awareness while retaining performance on other tasks, e.g., not corrupting the detection quality of other errors in candidate sentences. We test our models on common embedding and evaluation benchmarks to test our approach and compare their performances against their respective base models.",
            "score": 0.4971178102743259,
            "section_title": "Evaluation",
            "char_start_offset": 14704,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 514
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "277103777",
            "title": "Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model",
            "text": "Fine-tuning the shared module on one task followed by another consecutively inevitably induces catastrophic forgetting due to an extreme imbalanced data dis- The difference between our method and parameterefficient tuning (PET)-based approaches. While traditional finetuning introduces cumulative parameter drift that erodes both taskspecific skills and core generative capabilities, our approach employs disjoint parameter spaces for individual tasks to avoid forgetting. \n\ntribution [47]. It is hence highly motivated to explore new LLM technique that can continually absorb new skills in a replay-free fashion while overcoming the nature of catastrophic forgetting. \n\nIn this paper, we propose an Analytic Subspace Routing (ASR) framework for continual learning of LLMs. Unlike common LLMs CL techniques, the proposed ASR creatively adopts a classic recursive least squares (RLS) closed-form solution as an important mean to battle catastrophic forgetting. This comes in a hybrid structure, i.e., with an analytic router with a closed-form solution branching out in an earlier frozen segment of LLM layers, and the remaining layers tuned by LoRA using normal gradientbased technique. For each task, we isolate the learning within a subspace of deep layers' features via low-rank adaptation, eliminating knowledge interference among different tasks. We trained a specialized LoRA attachment, and use the analytic router to tell which one to load when inference. The key to avoid forgetting is that the analytic router bears the equivalence between continual training one task at a time, and the joint training by putting all-the-task data simultaneously. The main contributions of this work are listed as follow: \n\n\u2022 We introduce the ASR, a novel closed-form subspace routing approach for continual learning in LLMs, facilitating dynamic and adaptable learning of new tasks. \u2022 Our method structure each task as a subspace training using a specific LoRA model attachment, thereby reducing potential knowledge interference among tasks. \u2022 Our method includes an analytic routing mechanism derived through recursive least squares, which seamlessly incorporate new tasks without forgetting. A theoretical guarantee of this non-forgetting property is provided to affirm its efficacy in mitigating catastrophic forgetting. \u2022 We validate the ASR through comprehensive experiments, achieving state-of-the-art performance on the Trace metric [35].",
            "score": 0.49698531553431846,
            "section_title": "Introduction",
            "char_start_offset": 2106,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 472
                },
                {
                    "start": 475,
                    "end": 490
                },
                {
                    "start": 491,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1656
                },
                {
                    "start": 1657,
                    "end": 1714
                },
                {
                    "start": 1717,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 2035
                },
                {
                    "start": 2036,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2317
                },
                {
                    "start": 2318,
                    "end": 2439
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375732421875
        },
        {
            "corpus_id": "268889793",
            "title": "Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models",
            "text": "Previous analysis show that there is no silver bullet that resolves knowledge conflicts with LLMs without fine-tuning.In order to further address the issue of knowledge conflicts that LLMs may encounter, we employ human annotated data to fine-tune LLMs to resolve knowledge conflicts.\n\nFine-Tuning Setup.The general purpose of finetuning LLMs with the annotated data is two-fold.\n\n(1) To enable LLMs to understand instructions to  To prevent catastrophic forgetting, we mix the fine-tuning dataset with Wikitext (Merity, 2016) and other supervised fine-tuning dataset, e.g., Alpaca dataset (Taori et al., 2023).Finally, we fine-tune the 7B version of LLaMA, Alpaca, Vicuna, and Tulu on the mixed data.We train these models for two epochs at a learning rate of 2 \u00d7 10 \u22125 .We use 3% of the total training steps for linear learning rate warmup and adopt a linear learning rate decay.\n\nFine-Tuning Results.The experiment results after fine-tuning LLMs are shown in Table 5.Generally, a promising performance improvement from the extra training process can be observed.LLaMA-7B has more performance gain compared with the assistant LLMs.It even reaches a comparable capability to resolve knowledge conflicts with complicated reasonings.Moreover, after finetuning, Vicuna-7B reaches a comparable result with LLaMA-2-70B-Chat in Table 2, indicating the role of fine-tuning relatively small scale models in resolving knowledge conflicts as the fallback.",
            "score": 0.496236971766588,
            "section_title": "Fine-Tuning Analysis",
            "char_start_offset": 22282,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 118,
                    "end": 284
                },
                {
                    "start": 286,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 379
                },
                {
                    "start": 381,
                    "end": 611
                },
                {
                    "start": 611,
                    "end": 701
                },
                {
                    "start": 701,
                    "end": 771
                },
                {
                    "start": 771,
                    "end": 880
                },
                {
                    "start": 882,
                    "end": 902
                },
                {
                    "start": 902,
                    "end": 969
                },
                {
                    "start": 969,
                    "end": 1064
                },
                {
                    "start": 1064,
                    "end": 1132
                },
                {
                    "start": 1132,
                    "end": 1231
                },
                {
                    "start": 1231,
                    "end": 1445
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15087890625
        },
        {
            "corpus_id": "259370817",
            "title": "Rehearsal-free Continual Language Learning via Efficient Parameter Isolation",
            "text": "Separating parameters is effective in mitigating catastrophic forgetting, however, it also blocks knowledge transfer among tasks. Given a sequence of learned prefixes P = {P 1 , ..., P t\u22121 }, we try several ways to utilize the knowledge acquired from preceding tasks in the hope that they could improve and accelerate the current task learning, \n\n\u2022 Prefix Fusion. A natural way to combine knowledge from previous tasks is through the use of the attention mechanism. This allows the model to automatically extract useful information from previous tasks and integrate it into the current learning process. To achieve this, we prepend the learned prefixes of previous tasks to the prefix of the current task. Knowledge transfer is automatically facilitated through the multi-head attention mechanism of the Transformer. During new prefix learning, we fix the prefixes of previous tasks to avoid parameter drifting and catastrophic forgetting. Metrics Let a i,j be the testing accuracy on the i-th task after training on j-th task, the metrics for evaluating are, \n\n\u2022 Performance of Continual Learning (CL). The average accuracy of all tasks after training on the last task, 1 \n\nThe degree of forgetting of previous tasks after training on the last task, \n\n\u2022 Accuracy of Task Identification (TI). The accuracy of getting the correct prefix for testing samples after training on all tasks. \n\nBaselines We use the following continual learning techniques as baselines: \n\n\u2022 FT, fine-tuning a model for each task sequentially while catastrophic forgetting occurs. This method is the lower bound of continual learning. \n\n\u2022 MTL, training a model on all tasks as multi-task learning. This method is the upper bound of continual learning. \n\n\u2022 Replay, saving part of the previous tasks as memory, train a model one step on the memory after every \u03b2 ( 10 for our experiments) steps of training on the new task. \n\n\u2022 LwF (Li and Hoiem, 2017), a typical regularization-based approach. We also combine LwF with the replay method as an enhancement. \n\n\u2022 IDBR (Huang et al., 2021b), disentangling information by two simple auxiliary tasks (next sentence prediction and task-id prediction) for learning better generic and specific representation spaces. This approach applies both episodic memory replay and regularization techniques.",
            "score": 0.49602662310452633,
            "section_title": "Knowledge Transfer",
            "char_start_offset": 11864,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1172
                },
                {
                    "start": 1175,
                    "end": 1250
                },
                {
                    "start": 1253,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1461
                },
                {
                    "start": 1464,
                    "end": 1554
                },
                {
                    "start": 1555,
                    "end": 1608
                },
                {
                    "start": 1611,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1894
                },
                {
                    "start": 1897,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2027
                },
                {
                    "start": 2030,
                    "end": 2229
                },
                {
                    "start": 2230,
                    "end": 2310
                }
            ],
            "ref_mentions": [
                {
                    "start": 1903,
                    "end": 1923,
                    "matchedPaperCorpusId": "4853851"
                },
                {
                    "start": 2037,
                    "end": 2058,
                    "matchedPaperCorpusId": "233210110"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06256103515625
        },
        {
            "corpus_id": "267751282",
            "title": "Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models",
            "text": "Table 1 and Table 2 illustrate that Model Tailor substantially mitigates catastrophic forgetting in MLLMs, outperforming current fine-tuning and forgetting mitigation methods in both InstructBLIP and LLaVA frameworks under a sparsity level of 10%. We provide further important observations: \n\n\u2022 Generalization capabilities of MLLMs raise significant concerns, particularly in adapting to unfamiliar tasks. This is highlighted by the zero-shot performance of LLaVA on Flickr30k, and OKVQA is virtually null as shown in Table 2. Furthermore, subsequent fine-tuning to improve performance on such tasks often leads to a detrimental impact on the proficiency of pre-trained tasks. \n\nFor example, LLaVA's performance on Vizwiz, as shown in Table 2, falls sharply from a baseline score of 50.0 to 27.24 and 30.35 after fine-tuning for Flickr30k and OKVQA, respectively. Similarly, the performance of In-structBLIP on COCO, after undergoing fine-tuning, also experiences a significant drop, descending from a high of 143.1 to 114.2 and 109.8, as reported in Table 1. \n\n\u2022 Existing approaches to prevent forgetting demonstrate limitations when applied to MLLMs. DARE, except in the case of InstructBLIP on Flickr30k, fails to sustain performance on target tasks, with LLaVA's performance on OKVQA dropping back to baseline after DARE's selection, as presented in Table 2. Grafting, while offering some preservation of performance on target tasks, still leads to considerable forgetting on tasks for which the model was originally trained. \n\n\u2022 Model Tailor adeptly optimizes for specific tasks while preserving performance on pre-trained tasks. It achieves superior H-scores and average metrics, highlighting its effectiveness in balancing focused enhancements with foundational robustness. This dual optimization is consistently reflected across InstructBLIP and LLaVA, confirming the method's comprehensive adaptability and excellence in the field.",
            "score": 0.4954621243579458,
            "section_title": "Main Results on Single-Task setting",
            "char_start_offset": 21397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 290
                },
                {
                    "start": 293,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1529
                },
                {
                    "start": 1532,
                    "end": 1634
                },
                {
                    "start": 1635,
                    "end": 1780
                },
                {
                    "start": 1781,
                    "end": 1940
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.302734375
        },
        {
            "corpus_id": "271213062",
            "title": "Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment",
            "text": "We initially assess the effectiveness of our proposed Mix-CPT framework, especially the logit swap self-distillation constraint, in mitigating catastrophic forgetting and facilitating the knowledge learning of base LLMs during the continual pre-training stage.To better observe the impact of knowledge mixture continual pre-training on the performance in target domains and general capabilities, we select domain-specific tasks and comprehensive examination tasks.Specifically, we evaluate the domain-specific tasks (e.g., factual question answering (Wiki), math reasoning (Math), and code reasoning (Code)) under few-shot settings and the Chinese and English examination tasks (i.e., multichoice question answering) with perplexity-based zero-shot setting.We show the evaluation results in Table 1.\n\nFirst, we can see that traditional continual pre-training (i.e., + CPT) on the raw domain data does not necessarily enhance the performance of base LLMs in the target domain, and may instead impair their performance therein.Additionally, this method inevitably leads to a certain degree of catas- The phenomenon is also discovered in existing work (Lin et al., 2024), which suggests that the data used for continual pre-training should be of superior quality relative to the data utilized during the initial pre-training phase; otherwise, it may detrimentally affect performance.Indeed, the publicly available domain data we utilized (e.g., AutoMathText and StarCoder) has likely already been used to train the base model.Consequently, further employing it for continual pre-training may impair the model's performance.\n\nSecond, when mixing the domain raw data with the additional instructions and alignment data (i.e., Mix-CPT w/o KD), the domain-specific capability can be further improved, which indicates that the mixed instruction data can benefit the learning of the domain knowledge during continual pretraining.At the same time, it can mitigate the effect of other general capabilities and reduce the degradation of the overall average performance.",
            "score": 0.49543701253467465,
            "section_title": "RESULTS OF BASE LLMS",
            "char_start_offset": 22125,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 260,
                    "end": 464
                },
                {
                    "start": 464,
                    "end": 757
                },
                {
                    "start": 757,
                    "end": 799
                },
                {
                    "start": 801,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1380
                },
                {
                    "start": 1380,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1620
                },
                {
                    "start": 1622,
                    "end": 1920
                },
                {
                    "start": 1920,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1026611328125
        },
        {
            "corpus_id": "278208082",
            "title": "Memorization and Knowledge Injection in Gated LLMs",
            "text": "The source of the gap between the performance of MEGa and other CL fine-tuning methods is the characteristic of catastrophic forgetting. We show the forgetting curves computed by the accuracy of retrieving the first sample in the sequence as a function of the length of the sequence in Figure 2. All curves exhibit roughly similar good performance initially. However, the CL baselines show severe catastrophic forgetting, although L2 regularization helps mitigate it to a limited extent (see Appendix C for a comprehensive analysis on regularization). In contrast, MEGa shows only mild forgetting. The curve of full-parameter batch learning in Figure 2 is almost flat. The very small decrease in the ability to retrieve the early stories in response to questions is analogous to shrinking the basins of attraction of memories in attractor networks even below memorization capacity.",
            "score": 0.49530419486871624,
            "section_title": "Memory Recall",
            "char_start_offset": 27870,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 881
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.41357421875
        },
        {
            "corpus_id": "265158175",
            "title": "Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models",
            "text": "Reliability The results are shown in Table 1. The results support that the method of forgetting with LoRA and then learning with full fine-tuning (noted as F-Learning LoRA\u2212FT ) completely surpassing many baselines such as the directly full fine-tuning, as well as almost nearly match or even surpassing the method of forgetting and learning with full fine-tuning in some cases. In particular, it generally maintains the highest results in locality, which may be owing to the parameter-efficiency of LoRAbased old knowledge forgetting. \n\nAfter conducting experiments, we empirically discovered that utilizing the technique of forgetting through the subtraction of LoRA parameters can approximate the effect achieved by subtracting the parameters during full fine-tuning. We hold that although LoRA-based knowledge forgetting does not eliminate the old knowledge stored in the MLP layers of the LLMs, it alters the patterns and relationships associated with the old knowledge stored in the attention structure (i.e., an implicit knowledge representation), which facilitates the new knowledge learning. This finding holds significant value due to the considerable reduction in time and computational costs associated with LoRA compared to full fine-tuning.",
            "score": 0.4947839912007589,
            "section_title": "Editor Metric",
            "char_start_offset": 17813,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 45
                },
                {
                    "start": 46,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 534
                },
                {
                    "start": 537,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1253
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0413818359375
        },
        {
            "corpus_id": "267657805",
            "title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation",
            "text": "In this paper, we introduce a self-alignment framework, Self-Alignment for Factuality, which harnesses an LLM's self-evaluation capability to mitigate hallucinations. Our approach encourages an LLM to generate prediction confidence scores pertaining to the factuality of its own generated responses through self-asking. Subsequently, these scores are utilized as reward signals to fine-tune the model using the Direct Preference Optimization (DPO) algorithm (Rafailov et al., 2023). Specifically, we incorporate a factuality self-evaluation component, SELF-EVAL, which prompts the LLM to directly validate its responses based on its internal knowledge. To bolster the LLM's universal selfevaluation ability, we introduce SK-TUNING to enhance the LLM's internal knowledge awareness, i.e., prediction confidence estimation and calibration3 (Guo et al., 2017;Tian et al., 2023b), through sufficient tuning across heterogeneous knowledge-oriented tasks. \n\nWe assess the effectiveness of the proposed Self-Alignment for Factuality framework on three crucial knowledge-extensive tasks for LLMs, namely Multi-Choice Question-Answering (MCQA), shortform open-ended generation, and long-form openended generation, using two benchmark datasets: TruthfulQA (Lin et al., 2022) and BioGEN (Min et al., 2023a). The results show that, solely relying on the model's internal knowledge, Self-Alignment for Factuality significantly enhances the factual accuracy of LLAMA family models (Touvron et al., 2023a,b) across all three tasks, notably surpassing the representation-editing methods (Chuang et al., 2023;Li et al., 2023c) and the recent work with consistency-based confidence (Tian et al., 2023a). \n\nIn summary, our contributions are three-fold: \n\n\u2022 We propose Self-Alignment for Factuality, a self-alignment strategy that leverages an LLM's self-evaluation capability to mitigate the model's hallucinations. \n\n\u2022 We introduce SK-TUNING to improve an LLM's confidence estimation and calibration, thereby enhancing its self-evaluation ability.",
            "score": 0.4945442696844112,
            "section_title": "Introduction",
            "char_start_offset": 3219,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 949
                },
                {
                    "start": 952,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1733
                },
                {
                    "start": 1736,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 2029
                }
            ],
            "ref_mentions": [
                {
                    "start": 856,
                    "end": 875,
                    "matchedPaperCorpusId": "258865733"
                },
                {
                    "start": 1246,
                    "end": 1264,
                    "matchedPaperCorpusId": "237532606"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056854248046875
        },
        {
            "corpus_id": "263828951",
            "title": "Do Large Language Models Know about Facts?",
            "text": "To investigate whether LLMs can effectively memorize factual knowledge from structured data, we divided the structural task questions into three subsets according to evidence distribution: evidence in unstructured data (Only text), structured data (Only tables), or both (Combine text and tables). Figure 3(b) shows a notable decline (Avg. -5.5%) in GPT-3.5-Turbo's performance when evidence involves structured data, indicating LLMs' limited ability in extracting knowledge from structured tables. The LLMs also perform less effectively when handling questions requiring the combination of both evidence types, reflecting their incapacity to integrate diverse structured evidence effectively. Temporal Analysis As time progresses, the truthfulness of certain questions may undergo changes. The temporal task encompasses such data, and we leverage this task to explore the ability of LLMs to adapt to factual changes over time. Figure 4(a) illustrates that GPT-3.5-Turbo exhibits superior performance when dealing with outdated data as compared to updated data. This discrepancy arises from the fact that LLMs are pretrained on a corpus of text prior to a specific temporal point. Consequently, LLMs lack the capability to acquire real-time, up-to-date knowledge, rendering them unable to validate questions that hinge on the most recent information for accurate assessments.",
            "score": 0.4945122778682359,
            "section_title": "Structural Knowledge Analysis in LLMs",
            "char_start_offset": 21765,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 498
                },
                {
                    "start": 499,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1375
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.048309326171875
        },
        {
            "corpus_id": "270562404",
            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
            "text": "Fine-tuning large language models (LLMs) can cause them to lose their general capabilities. However, the intrinsic mechanisms behind such forgetting remain unexplored. In this paper, we begin by examining this phenomenon by focusing on knowledge understanding and instruction following, with the latter identified as the main contributor to forgetting during fine-tuning. Consequently, we propose the Instruction Vector (IV) framework to capture model representations highly related to specific instruction-following capabilities, thereby making it possible to understand model-intrinsic forgetting. Through the analysis of IV dynamics pre and post-training, we suggest that fine-tuning mostly adds specialized reasoning patterns instead of erasing previous skills, which may appear as forgetting. Building on this insight, we develop IV-guided training, which aims to preserve original computation graph, thereby mitigating catastrophic forgetting. Empirical tests on three benchmarks confirm the efficacy of this new approach, supporting the relationship between IVs and forgetting. Our code will be made available soon.",
            "score": 0.4943780965672294,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7373046875
        },
        {
            "corpus_id": "276622315",
            "title": "Measuring Catastrophic Forgetting in Cross-Lingual Classification: Transfer Paradigms and Tuning Strategies",
            "text": "Catastrophic forgetting (CF) [25] is a general term for forgetting previously acquired knowledge in machine learning when the model is adapted to a novel task. To overcome the problem, researchers need to opt whether to optimize the model's stability -the ability to retain acquired knowledge, or the model's plasticity -the ability to learn new information effectively. \n\nSun et al. [43] propose choosing a lower learning rate to overcome catastrophic forgetting in neural networks. Xu et al. [56] explore regularization strategies by introducing selective (Elastic Weight Consolidation by Kirkpatrick et al. [17]) and non-selective (\u03bb2 regularisation by Hoerl and Kennard [12]) regularisation terms between the initial and the fine-tuned weights. They see a boost in performance in domain adaptation, task transfer, and continuous learning settings with both regimes. Yang et al. [57] introduce concerted training consisting of model distillation to retain the previous knowledge, a dynamic switching gate to avoid catastrophic forgetting of pre-trained knowledge, and a scheduled policy to adjust the learning rate. Their approach shows promising results for the machine translation task. Vu et al. [48] propose overcoming CF by prompt tuning and improve performance over classical fine-tuning when transferring between lessrelated languages. They opt between mixing unlabeled multilingual data in the prompt tuning or explicitly factoring prompts into composable language and task components. \n\nModel-wise catastrophic forgetting affects both small and large language models [20], as well as multimodal models [60]. Kotha et al. [20] showed that steps like instruction tuning and reinforcement learning improve performance on tasks seen during fine-tuning but hurt performance on others. This suggests that models learn to focus on the training tasks at the expense of their broader capabilities. Ren et al. [38] found that low-rank adaptations for downstream tasks can worsen catastrophic forgetting for LLMs, but using interpolation-based updates between source and target weights can help. However, they only studied English-to-English transfers from large decoder-based models and didn't utilise established metrics for CF assessment. It is still unknown how parameter-efficient tuning affects catastrophic forgetting in cross-lingual scenarios, which motivates our current work.",
            "score": 0.4943173209369438,
            "section_title": "C. CATASTROPHIC FORGETTING",
            "char_start_offset": 8473,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1496
                },
                {
                    "start": 1499,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1791
                },
                {
                    "start": 1792,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2096
                },
                {
                    "start": 2097,
                    "end": 2242
                },
                {
                    "start": 2243,
                    "end": 2387
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 33,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 384,
                    "end": 388,
                    "matchedPaperCorpusId": "153312532"
                },
                {
                    "start": 494,
                    "end": 498,
                    "matchedPaperCorpusId": "207870818"
                },
                {
                    "start": 610,
                    "end": 614,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 882,
                    "end": 886,
                    "matchedPaperCorpusId": "201058686"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "249062610"
                },
                {
                    "start": 1579,
                    "end": 1583,
                    "matchedPaperCorpusId": "262054014"
                },
                {
                    "start": 1633,
                    "end": 1637,
                    "matchedPaperCorpusId": "262054014"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5166015625
        },
        {
            "corpus_id": "268510177",
            "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
            "text": "Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.",
            "score": 0.4940442658411062,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62451171875
        },
        {
            "corpus_id": "273638251",
            "title": "Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning",
            "text": "Fine-tuning LLMs with synthetic data. Synthetic data has shown great potential in fine-tuning LLMs, offering scalability by automatically creating instruction-response pairs with minimal human supervision (Wang et al., 2023;Li et al., 2024;Gupta et al., 2024;Yu et al., 2024;Haluptzok et al., 2023). Additionally, the generation process can be designed to control the training data, allowing models to be trained for specific skills or aligned with particular human values (Sudalairaj et al., 2024;Sun et al., 2023a;2024;Kaur et al., 2024). However, most works do not consider factuality in fine-tuning. The work closest to ours is Jones et al. (2024), which fine-tunes LLMs on synthetic tasks to reduce hallucinations. However, they focus on improving consistency with evidence provided in the context, whereas we improve LLMs' inherent factuality without provided evidence.",
            "score": 0.4939044907147465,
            "section_title": "RELATED WORKS",
            "char_start_offset": 6434,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 875
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 224,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 240,
                    "end": 259,
                    "matchedPaperCorpusId": "264555527"
                },
                {
                    "start": 259,
                    "end": 275,
                    "matchedPaperCorpusId": "262084051"
                },
                {
                    "start": 275,
                    "end": 298,
                    "matchedPaperCorpusId": "270380355"
                },
                {
                    "start": 498,
                    "end": 516,
                    "matchedPaperCorpusId": "258479665"
                },
                {
                    "start": 632,
                    "end": 651,
                    "matchedPaperCorpusId": "263830994"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.032470703125
        },
        {
            "corpus_id": "277955499",
            "title": "DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via Model-Intrinsic Dataset Pruning",
            "text": "DONOD begins with a critical observation regarding ad-hoc instruction fine-tuning (SFT) for large language models (LLMs). Traditional SFT methods are highly data-dependent, and using a narrowly focused dataset can lead to catastrophic forgetting, which leads to performance degradation in non-target domains during specialization. For instance, fine-tuning on mathematical tasks may significantly impair the model's ability in reading comprehension. While mixing diverse data can mitigate this issue, it introduces additional training costs and issues of domain proportion balance. Furthermore, the quality of individual samples significantly impacts fine-tuning efficacy, yet identifying high-quality data remains an open problem. \n\nTo address these issues, DONOD introduces a data-efficient alternative: instead of augmenting datasets, we prune harmful or low-quality samples that impede generalization. Our approach is grounded in a key insight: the Frobenius norm of weight changes (before vs. after fine-tuning) can estimate each sample's impact. Specifically, DONOD consists of three core components: 1). DON and NOD. These metrics quantify a sample's tendency to exacerbate overfitting or catastrophic forgetting. 2). TOPSIS Pruning: Samples are ranked and selected based on DON and NOD, retaining only high-quality data that balances task-specific improvement and cross-domain generalization. \n\n3). Proxy-Based Scalability: We focus on output-layer weight changes as a proxy for full-model behavior, significantly reducing overhead without sacrificing efficacy.",
            "score": 0.49376754677963874,
            "section_title": "Overview",
            "char_start_offset": 1701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 330
                },
                {
                    "start": 331,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1224
                },
                {
                    "start": 1225,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3408203125
        },
        {
            "corpus_id": "274436578",
            "title": "VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs",
            "text": "The Supervised Fine-Tuning (SFT) stage involves further training on the pretrained model to refine it task-solving capabilities and ensure greater alignment with human instructions across multiple domains [41,72]. During the SFT phase, it is a typical practice to employ datasets specific to a particular domain for the fine-tuning of LLMs, which may lead to a decline in performance on non-target domains, a phenomenon commonly referred to as catastrophic forgetting. We conducted experiments on open-sourced models including LLaMA [18,52,53] and Qwen [8,64] series to assess how the model's proficiency in other domains changes when finetuned with data from a single domain, as illustrated in Table 1 and Figure 2. We have regulated the number of training instances per epoch to a fixed count of 10,000. More details about training and evaluation settings can be found in Section 5.1. Our findings indicate that when a model is trained exclusively with data from a single domain, its performance on tasks from other domains tends to degrade progressively over the course of training. \n\nWhile recent research has delved into exploring fine-tuning methods for multi-task enhancement [17,45], they are still in their early stages. However, as shown by proprietary models such as GPT-4 [1] and Gemini [50], which exhibit outstanding multi-task performance, improving a model's versatile capabilities across various domains during the SFT phase is crucial. Therefore, our work systematically investigates methods to enhance multi-domain performance during the SFT stage to bridge this gap.",
            "score": 0.49356545140033903,
            "section_title": "Supervised Fine-Tuning.",
            "char_start_offset": 9185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1586
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 209,
                    "matchedPaperCorpusId": "257365347"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.330322265625
        },
        {
            "corpus_id": "261697277",
            "title": "Mitigating the Alignment Tax of RLHF",
            "text": "LLMs acquire a wide range of abilities during pre-training, but aligning LLMs under Reinforcement Learning with Human Feedback (RLHF) can lead to forgetting pretrained abilities, which is also known as the alignment tax. To investigate alignment tax, we conducted experiments with existing RLHF algorithms using OpenLLaMA-3B, which revealed a pronounced alignment tax in NLP tasks. Whereas, despite various techniques to mitigate forgetting, they are often at odds with the RLHF performance, leading to a trade-off between alignment performance and forgetting mitigation, leading to an alignment-forgetting trade-off. In this paper we show that model averaging, which simply interpolates between pre and post RLHF model weights, surprisingly achieves the most strongest alignment-forgetting Pareto front among a wide range of competing methods. To understand its effectiveness, we offer theoretical insights into model averaging, revealing that it enhances performance Pareto front by increasing feature diversity on the layers where tasks share overlapped feature spaces. Empirical evidence corroborates our analysis by showing the benefits of averaging low-level transformer layers. Building on the analysis and the observation that averaging different layers of the transformer leads to significantly different alignment-forgetting trade-offs, we propose Heterogeneous Model Averaging (HMA) to Heterogeneously find various combination ratios of model layers. HMA seeks to maximize the alignment performance while incurring minimal alignment tax. Moreover, we validate HMA\u2019s performance across a range of RLHF algorithms over OpenLLaMA-3B and further extend our findings to Mistral-7B which is evaluated by open-sourced preference model and GPT4. Code available here.",
            "score": 0.4932649828928788,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.16162109375
        },
        {
            "corpus_id": "272704025",
            "title": "MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts",
            "text": "Nearly all the LLM unlearning algorithms are trying to address the problem of balancing model utility and forget quality, i.e., how to effectively unlearn without causing catastrophic collapse on D r and D g . Therefore, this paper utilizes 2 metrics: \u2776 Model utility: typically measures the performance of M t on D r and D g . \u2777 Forget quality: can be assessed in two ways, measuring the difference between M t and M o on D f , or the similarity between M t and M r . For the former way, both hard and soft unlearning can achieve high forget What does the kitty like to do? \n\nThe kitty likes to memo.",
            "score": 0.49324794860807175,
            "section_title": "Evaluation",
            "char_start_offset": 7138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 574
                },
                {
                    "start": 577,
                    "end": 601
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0285491943359375
        },
        {
            "corpus_id": "277435240",
            "title": "Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization",
            "text": "In the context of LLMs, the problem of maintaining performance across tasks while integrating new information is known as lifelong learning. This field focuses on a system's ability to acquire, integrate, and retain knowledge without catastrophically forgetting previous information. Visual Instruction Tuning is a case of lifelong learning, and it faces the same challenges. One known mitigation strategy is episodic or experience replay (Zheng et al., 2025), which helps prevent catastrophic forgetting by reintroducing previously learned information. \n\nSeveral studies (Liu et al., 2022;Ibrahim et al., 2024) have explored ways to incorporate pretraining data during fine-tuning. Bethune et al. (2025) further analyze the impact of this approach and suggest that even a small amount of pretraining data can help retain previously learned knowledge, reducing the risk of performance degradation. \n\nIn the case of VLMs, NVLM (Dai et al., 2024) and InternVL 2.5 (Chen et al., 2024e) demonstrate that incorporating high-quality text-only data during Visual Instruction Tuning, not only improves the overall text-generation capabilities, but also multimodal performance. Our approach builds upon these findings by integrating multilingual text-only data throughout VLM training to mitigate IFL, without requiring extensive multimodal multilingual data collection.",
            "score": 0.4931015556617264,
            "section_title": "Catastrophic Forgetting Prevention",
            "char_start_offset": 4858,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 553
                },
                {
                    "start": 556,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1361
                }
            ],
            "ref_mentions": [
                {
                    "start": 439,
                    "end": 459,
                    "matchedPaperCorpusId": "270371124"
                },
                {
                    "start": 572,
                    "end": 590,
                    "matchedPaperCorpusId": "249097789"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.21484375
        },
        {
            "corpus_id": "273821122",
            "title": "A Comparative Analysis of Instruction Fine-Tuning LLMs for Financial Text Classification",
            "text": "To address these limitations and enhance the capabilities of LLMs for specific domains, existing methods can be broadly classified into three main approaches: in-context learning, training models from scratch on domain-specific and general data, and fine-tuning existing models using supervised datasets. In-context learning, where LLMs generate results based on a few demonstration examples [24], can be costly, slow in inference, and limited by the model's context window [2]. Moreover, these models can be sensitive to the quality and variability of the provided examples [21]. \n\nTraining models from scratch, while effective, demands significant computational resources and vast domain-specific and general dataset. Fine-tuning existing models is promising alternative but faces challenges such as the need for high-quality datasets and the risk of catastrophic forgetting, where the model's ability to perform general tasks degrades after domain-specific fine-tuning. Techniques like model merging can help mitigate these effects. \n\nPrevious studies have demonstrated that instruction fine-tuning smaller open-source language models can significantly enhance their performance on domain-specific tasks across various fields, such as law and medicine. In many cases, these models have outperformed the zero-shot performance of proprietary LLMs and other state-of-the-art models [19,23,33,61,65]. Within the finance domain, one study fine-tuned the Llama2 model for sentiment analysis, outperforming the FinBERT [62]. Another study fine-tuned Llama2-7B and Llama2-13B models on a variety of financial tasks, including sentiment analysis, relation extraction, question answering, and stock market prediction [54]. However, much of the existing research has focused primarily on fine-tuning models from the Llama family. To address this gap, we explore fine-tuning other powerful, smaller LLMs, specifically Mistral-7B, Phi-3, and Llama2-8B, across four representative financial text classification tasks: sentiment analysis, news headline classification, relation extraction, and hawkish-dovish classification. \n\nOne of the main challenges with fine-tuning LLMs for domain-specific tasks is the degradation of their zero-shot performance on unseen tasks [63].",
            "score": 0.4928042597050655,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1035
                },
                {
                    "start": 1038,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1520
                },
                {
                    "start": 1521,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 2112
                },
                {
                    "start": 2115,
                    "end": 2261
                }
            ],
            "ref_mentions": [
                {
                    "start": 392,
                    "end": 396,
                    "matchedPaperCorpusId": "263835169"
                },
                {
                    "start": 1389,
                    "end": 1392,
                    "matchedPaperCorpusId": "248367393"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "263829356"
                },
                {
                    "start": 1710,
                    "end": 1714,
                    "matchedPaperCorpusId": "268042106"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.44580078125
        },
        {
            "corpus_id": "276409016",
            "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
            "text": "This approach involves limiting changes to the function vectors associated with training tasks through a regularization term, coupled with the adoption of a function vector-guided Kullback-Leibler (KL) divergence loss. This loss function aims to diminish the discrepancies between logits derived from zero-shot input and those adjusted by function vector intervention, ensuring the fine-tuned model remains consistent with the inner task function. Validated across multiple datasets and models, this method significantly alleviates forgetting in both general and in-context learning abilities, confirming the correlation between FV dynamics and forgetting. \n\nMain findings and contributions. (1) We investigate catastrophic forgetting in LLMs covering multiple language models, task sequences, and evaluation metrics, discovering that forgetting in LLMs is highly model-dependent, asserting a new analytical tool for characterizing forgetting behavior. \n\n(2) Using empirical and theoretical analysis based on the function vector framework, we reveal that forgetting generally results from the activation of biased model functions rather than overwriting previous functions. (3) We have developed a function vector guided training approach that preserves and aligns function vectors during fine-tuning, significantly improving both general and in-context learning across multiple continual learning datasets.",
            "score": 0.49215990743704585,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4126,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 656
                },
                {
                    "start": 659,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1407
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.513671875
        },
        {
            "corpus_id": "258967945",
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "text": "Uncertainty estimation of fine-tuned models is challenging due to the small amount of available data for fine-tuning, especially under out-of-domain settings (Desai & Durrett, 2020;Guo et al., 2021). While prior works illustrate that simple calibration techniques such as temperature scaling (Guo et al., 2017) and label smoothing (Szegedy et al., 2016) are not sufficient to calibrate the fine-tuned LMs under both in-domain (ID) and out-of-domain (OD) settings (Desai & Durrett, 2020;Park & Caragea, 2022), several approaches with strong regularization have been developed to calibrate the fine-tuned model on NLU tasks, including knowledge distillation from deep ensembles (Guo et al., 2021), stochastic network architectures (Fan et al., 2020;Zhang et al., 2021), and Mixup (Park & Caragea, 2022). However, these existing works mostly utilize general calibration methods for supervised learning, while specific properties of the pre-training & fine-tuning paradigm are still largely neglected. \n\nIn this work, we tackle the calibration of fine-tuned models from the perspective of better leveraging the powerful PLMs. Through a carefully designed empirical study on both pre-trained and fine-tuned models, we first observe that PLMs themselves are actually well-calibrated on the masked language modeling (MLM) task and robust to higher levels of perturbation to the inputs, which suggests the PLMs can model the predictive uncertainty well across different domains. However, the pre-trained features are only used as initialization and are distorted by the fully discriminative fine-tuning. The phenomenon is known as catastrophic forgetting (McCloskey & Cohen, 1989;Kirkpatrick et al., 2017;Howard & Ruder, 2018). We show that such forgetting can make the fine-tuned language models fail to hold proper predictive confidence toward the OD and outlier samples, which leads to miscalibration on the downstream tasks. Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs.",
            "score": 0.4916026766542686,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 801
                },
                {
                    "start": 802,
                    "end": 997
                },
                {
                    "start": 1000,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2039
                }
            ],
            "ref_mentions": [
                {
                    "start": 158,
                    "end": 181,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 181,
                    "end": 198,
                    "matchedPaperCorpusId": "224803318"
                },
                {
                    "start": 292,
                    "end": 310,
                    "matchedPaperCorpusId": "28671436"
                },
                {
                    "start": 331,
                    "end": 353,
                    "matchedPaperCorpusId": "206593880"
                },
                {
                    "start": 463,
                    "end": 486,
                    "matchedPaperCorpusId": "212747810"
                },
                {
                    "start": 676,
                    "end": 694,
                    "matchedPaperCorpusId": "224803318"
                },
                {
                    "start": 729,
                    "end": 747,
                    "matchedPaperCorpusId": "224814357"
                },
                {
                    "start": 747,
                    "end": 766,
                    "matchedPaperCorpusId": "235377150"
                },
                {
                    "start": 1647,
                    "end": 1672,
                    "matchedPaperCorpusId": "61019113"
                },
                {
                    "start": 1672,
                    "end": 1697,
                    "matchedPaperCorpusId": "4704285"
                },
                {
                    "start": 1697,
                    "end": 1718,
                    "matchedPaperCorpusId": "40100965"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.42919921875
        },
        {
            "corpus_id": "270620350",
            "title": "Preserving Knowledge in Large Language Model with Model-Agnostic Self-Decompression",
            "text": "The Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have been rapidly developed and iterated in recent years.Many of them show a significant leap in the capability of understanding, generation, and interaction following the natural language (OpenAI et al., 2024;Team et al., 2024;Anthropic, 2024).There are lots of LLMs and MLLMs that have been developed in practice (Kaddour et al., 2023;Yin et al., 2024).However, the model trained for the general purpose may have a decline in performance in specific domains such as math, coding, law, healthcare, finance, etc. (Wu et al., 2024), therefore the need for obtaining sufficient training data to develop domain-specific LLMs or MLLMs is crucial.\n\nCollecting extensive domain-specific data and training LMMs from scratch is challenging.As a result, post-pretraining (Gururangan et al., 2020) or supervised fine-tuning (Brown et al., 2020) (SFT) of general LLMs/MLLMs with domain-specific data become the popular strategy for those seeking domain-specific models (Roziere et al., 2023;Huang et al., 2023;Azerbayev et al., 2024;Yunxiang et al., 2023;Li et al., 2023b;Kuckreja et al., 2023).However, this process can impair the models' performance due to catastrophic forgetting (Aleixo et al., 2023;Luo et al., 2024).We need the expert model to be generalizable as the general model on the specific domain.Although Parameter Efficient Finetuning (PEFT) methods (Mangrulkar et al., 2022) can adapt the models to the new domain by adding only a few parameters and maintaining their original capabilities, they often result in less satisfactory performance and are hard to accumulate from different domains.This calls for an approach that integrates domain-specific expertise into LLMs/MLLMs without compromising their general capabilities.\n\nThe problem of catastrophic forgetting in LLM is widely discussed.",
            "score": 0.4914140480341799,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 433
                },
                {
                    "start": 433,
                    "end": 720
                },
                {
                    "start": 722,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 1162
                },
                {
                    "start": 1162,
                    "end": 1289
                },
                {
                    "start": 1289,
                    "end": 1378
                },
                {
                    "start": 1378,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1809
                },
                {
                    "start": 1811,
                    "end": 1877
                }
            ],
            "ref_mentions": [
                {
                    "start": 892,
                    "end": 912,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.153564453125
        },
        {
            "corpus_id": "276580882",
            "title": "SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention and Low-Rank Adaptation in Large Language Models",
            "text": "We propose SECURA, a novel PEFT fine-tune method that integrates our newly proposed S-MagNorm normalization and CABR decomposition to address catastrophic forgetting in LLM fine-tuning. SECURA dynamically balances parameter updates using a Sigmoid-based pruning mechanism, preserving critical knowledge while adapting to new tasks. Experiments on 18 datasets and 5 LLMs show SECURA outperforms standard LoRA, with a 3.63% improvement on MCQ tasks and 2.56% on QA tasks, while retaining over 70% of base knowledge in continual learning scenarios. \n\nPaving the way for sustainable and ethical deployment of large AI models. \n\n7 Limitations",
            "score": 0.4911530001312591,
            "section_title": "Conclusion",
            "char_start_offset": 18613,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 545
                },
                {
                    "start": 548,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 637
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2384033203125
        },
        {
            "corpus_id": "233444261",
            "title": "LIQA: Lifelong Blind Image Quality Assessment",
            "text": "CQ, JPEG, CSA1, WNCC, Q, IN, JP2K, CSA2, WN, CB, DA, MS, MN, DN) are better than that of joint training and the average SRCC of all seen distortions is also better than that of joint training. The performance of the final task illustrates that LIQA has better ability to preserve the previously learned knowledge (the performances of the 7 base distortions of LIQA are comparable with that of joint training). Besides, consolidating the learned knowledge does not interfere with the learning of new knowledge (the performances of the novel distortions are satisfactory). \n\nWhen the incremental step is set to 2, we add two distortions per task following the order of (JP2K, CSA2)\u2192(PIX, WN)\u2192(CB, GB)\u2192(DA, CC)\u2192(BR, NEP)\u2192(MS, MB)\u2192 (MN,LB)\u2192(DN, HS)\u2192(CD, CS), leading to 9 novel tasks. The correlation index and the forgetting index with respect to tasks are shown in Fig. 5(a) and Fig. 5(b) respectively. By comparing Fig. 5(a) and Fig. 4(a), we can find that when each task contains 2 distortions, the catastrophic forgetting of fine-tuning becomes less serious. The worst correlation index of fine-tuning in Fig. 5(a) is 0.47 while the worst correlation index of fine-tuning in Fig. 4(a) is 0.30. The biggest forgetting index in Fig. 5(b) of fine-tuning decreases from 0.48 to 0.30 compared with Fig. 4(b). It is because that the negative effect of certain distortion will be weaken by another distortion. Taking the task#3 in Fig. 5(a) for example, combination of CB and GB will not seriously impair the performance of previous distortions. By comparing the forgetting index of all methods shown in Fig. 5(b), we can find that the forgetting of LIQA is low and stable, and can well preserve the learned knowledge while learning new tasks.",
            "score": 0.4910152533799631,
            "section_title": "Performance with respect to distortion shift",
            "char_start_offset": 36224,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 409
                },
                {
                    "start": 410,
                    "end": 570
                },
                {
                    "start": 573,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1737
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02044677734375
        },
        {
            "paperId": "c892957b47227247e4e52a157a71bf88efe56c57",
            "corpusId": 279243311,
            "title": "Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning",
            "venue": "",
            "year": 2025,
            "referenceCount": 41,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05671, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2363688671",
                    "name": "Yangui Fang"
                },
                {
                    "authorId": "2327961941",
                    "name": "Jing Peng"
                },
                {
                    "authorId": "2328023158",
                    "name": "Xu Li"
                },
                {
                    "authorId": "2364633578",
                    "name": "Yu Xi"
                },
                {
                    "authorId": "2278413613",
                    "name": "Chengwei Zhang"
                },
                {
                    "authorId": "2364653316",
                    "name": "Guohui Zhong"
                },
                {
                    "authorId": "2279432446",
                    "name": "Kai Yu"
                }
            ],
            "abstract": "Recent advances in automatic speech recognition (ASR) have combined speech encoders with large language models (LLMs) through projection, forming Speech LLMs with strong performance. However, adapting them to new domains remains challenging, especially in low-resource settings where paired speech-text data is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using unpaired target-domain text without requiring additional audio. To preserve speech-text alignment, we introduce a real-time evaluation mechanism during fine-tuning. This enables effective domain adaptation while maintaining source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical datasets show that our method achieves competitive recognition performance, with minimal degradation compared to full audio-text fine-tuning. It also improves generalization to new domains without catastrophic forgetting, highlighting the potential of text-only fine-tuning for low-resource domain adaptation of ASR.",
            "corpus_id": "279243311",
            "text": "Recent advances in automatic speech recognition (ASR) have combined speech encoders with large language models (LLMs) through projection, forming Speech LLMs with strong performance. However, adapting them to new domains remains challenging, especially in low-resource settings where paired speech-text data is scarce. We propose a text-only fine-tuning strategy for Speech LLMs using unpaired target-domain text without requiring additional audio. To preserve speech-text alignment, we introduce a real-time evaluation mechanism during fine-tuning. This enables effective domain adaptation while maintaining source-domain performance. Experiments on LibriSpeech, SlideSpeech, and Medical datasets show that our method achieves competitive recognition performance, with minimal degradation compared to full audio-text fine-tuning. It also improves generalization to new domains without catastrophic forgetting, highlighting the potential of text-only fine-tuning for low-resource domain adaptation of ASR.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.150146484375
        },
        {
            "paperId": "2c35f51ee8a78437b68e7b557f64f5adb59a329f",
            "corpusId": 273532304,
            "title": "VoiceTextBlender: Augmenting Large Language Models with Speech Capabilities via Single-Stage Joint Speech-Text Supervised Fine-Tuning",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "referenceCount": 49,
            "citationCount": 5,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17485, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2111014429",
                    "name": "Yifan Peng"
                },
                {
                    "authorId": "2121516",
                    "name": "Krishna C. Puvvada"
                },
                {
                    "authorId": "2258939548",
                    "name": "Zhehuai Chen"
                },
                {
                    "authorId": "2309003968",
                    "name": "Piotr Zelasko"
                },
                {
                    "authorId": "2258757455",
                    "name": "He Huang"
                },
                {
                    "authorId": "79303130",
                    "name": "Kunal Dhawan"
                },
                {
                    "authorId": "2321949461",
                    "name": "Ke Hu"
                },
                {
                    "authorId": "2316525723",
                    "name": "Shinji Watanabe"
                },
                {
                    "authorId": "2494510",
                    "name": "Jagadeesh Balam"
                },
                {
                    "authorId": "2241631811",
                    "name": "Boris Ginsburg"
                }
            ],
            "abstract": "Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs is catastrophic forgetting, where models optimized for speech tasks suffer significant degradation in text-only performance. To mitigate these issues, we propose a novel single-stage joint speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone. Our joint SFT combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based QA, and mixed-modal SFT. Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks. Furthermore, our model shows emergent abilities of effectively handling previously unseen prompts and tasks, including multi-turn, mixed-modal inputs.",
            "corpus_id": "273532304",
            "text": "Recent studies have augmented large language models (LLMs) with speech capabilities, leading to the development of speech language models (SpeechLMs). Earlier SpeechLMs focused on single-turn speech-based question answering (QA), where user input comprised a speech context and a text question. More recent studies have extended this to multi-turn conversations, though they often require complex, multi-stage supervised fine-tuning (SFT) with diverse data. Another critical challenge with SpeechLMs is catastrophic forgetting, where models optimized for speech tasks suffer significant degradation in text-only performance. To mitigate these issues, we propose a novel single-stage joint speech-text SFT approach on the low-rank adaptation (LoRA) of the LLM backbone. Our joint SFT combines text-only SFT data with three types of speech-related data: speech recognition and translation, speech-based QA, and mixed-modal SFT. Compared to previous SpeechLMs with 7B or 13B parameters, our 3B model demonstrates superior performance across various speech benchmarks while preserving the original capabilities on text-only tasks. Furthermore, our model shows emergent abilities of effectively handling previously unseen prompts and tasks, including multi-turn, mixed-modal inputs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.313720703125
        },
        {
            "paperId": "a0ff42bd9ce8ca7989513ca4938d85e90a618621",
            "corpusId": 277634330,
            "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 27,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2353385023",
                    "name": "Nikhil Shivakumar Nayak"
                },
                {
                    "authorId": "2273669923",
                    "name": "Krishnateja Killamsetty"
                },
                {
                    "authorId": "2249472971",
                    "name": "Ligong Han"
                },
                {
                    "authorId": "150894502",
                    "name": "Abhishek Bhandwaldar"
                },
                {
                    "authorId": "2290484410",
                    "name": "Prateek Chanda"
                },
                {
                    "authorId": "2261102120",
                    "name": "Kai Xu"
                },
                {
                    "authorId": "2336343895",
                    "name": "Hao Wang"
                },
                {
                    "authorId": "2288530668",
                    "name": "Aldo Pareja"
                },
                {
                    "authorId": "2354335460",
                    "name": "Oleg Silkin"
                },
                {
                    "authorId": "2166051120",
                    "name": "Mustafa Eyceoz"
                },
                {
                    "authorId": "2265502573",
                    "name": "Akash Srivastava"
                }
            ],
            "abstract": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.",
            "corpus_id": "277634330",
            "text": "Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.56591796875
        },
        {
            "paperId": "db23f850c3eb91c5c53bab5d83bc3f78e4dab70b",
            "corpusId": 271114369,
            "title": "Enhancing Task Performance in Continual Instruction Fine-tuning Through Format Uniformity",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "year": 2024,
            "referenceCount": 42,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3626772.3657920?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3626772.3657920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2310866202",
                    "name": "Xiaoyu Tan"
                },
                {
                    "authorId": "2310856599",
                    "name": "Leijun Cheng"
                },
                {
                    "authorId": "1500386397",
                    "name": "Xihe Qiu"
                },
                {
                    "authorId": "2263773739",
                    "name": "Shaojie Shi"
                },
                {
                    "authorId": "2267330658",
                    "name": "Yuan Cheng"
                },
                {
                    "authorId": "2266389707",
                    "name": "Wei Chu"
                },
                {
                    "authorId": "2266466742",
                    "name": "Yinghui Xu"
                },
                {
                    "authorId": "2192603365",
                    "name": "Yuan Qi"
                }
            ],
            "abstract": "In recent advancements, large language models (LLMs) have demonstrated remarkable capabilities in diverse tasks, primarily through interactive question-answering with humans. This development marks significant progress towards artificial general intelligence (AGI). Despite their superior performance, LLMs often exhibit limitations when adapted to domain-specific tasks through instruction fine-tuning (IF). The primary challenge lies in the discrepancy between the data distribution in general and domain-specific contexts, leading to suboptimal accuracy in specialized tasks. To address this, continual instruction fine-tuning (CIF), particularly supervised fine-tuning (SFT), on targeted domain-specific instruction datasets is necessary. Our ablation study reveals that the structure of these instruction datasets critically influences CIF performance, with substantial data distributional shifts resulting in notable performance degradation. In this paper, we introduce a novel framework that enhances CIF by promoting format uniformity. We assess our approach using the Llama2 chat model across various domain-specific instruction datasets. The results demonstrate not only an improvement in task-specific performance under CIF but also a reduction in catastrophic forgetting (CF). This study contributes to the optimization of LLMs for domain-specific applications, highlighting the significance of data structure and distribution in CIF.",
            "corpus_id": "271114369",
            "text": "In recent advancements, large language models (LLMs) have demonstrated remarkable capabilities in diverse tasks, primarily through interactive question-answering with humans. This development marks significant progress towards artificial general intelligence (AGI). Despite their superior performance, LLMs often exhibit limitations when adapted to domain-specific tasks through instruction fine-tuning (IF). The primary challenge lies in the discrepancy between the data distribution in general and domain-specific contexts, leading to suboptimal accuracy in specialized tasks. To address this, continual instruction fine-tuning (CIF), particularly supervised fine-tuning (SFT), on targeted domain-specific instruction datasets is necessary. Our ablation study reveals that the structure of these instruction datasets critically influences CIF performance, with substantial data distributional shifts resulting in notable performance degradation. In this paper, we introduce a novel framework that enhances CIF by promoting format uniformity. We assess our approach using the Llama2 chat model across various domain-specific instruction datasets. The results demonstrate not only an improvement in task-specific performance under CIF but also a reduction in catastrophic forgetting (CF). This study contributes to the optimization of LLMs for domain-specific applications, highlighting the significance of data structure and distribution in CIF.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.46044921875
        },
        {
            "paperId": "27da7126c6392fce19ea15571dc70649d9840d31",
            "corpusId": 276785440,
            "title": "Evaluating Alignment Techniques for Enhancing LLM Performance in a Closed-Domain Application: A RAG Bench-Marking Study",
            "venue": "International Conference on Machine Learning and Applications",
            "year": 2024,
            "referenceCount": 29,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICMLA61862.2024.00180?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICMLA61862.2024.00180, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1419460292",
                    "name": "Ahmad Al Zuraiqi"
                },
                {
                    "authorId": "2348564459",
                    "name": "Des Greer"
                }
            ],
            "abstract": "In the field of natural language processing (NLP), Large Language Models (LLMs) such as MISTRAL have been pre-trained on diverse tasks and datasets to achieve comprehensive language understanding. Nevertheless, fine-tuning small closed-domain datasets presents limitations, including overfitting, restricted generalization, data representation bias, and catastrophic forgetting issues. To tackle these challenges and align LLMs with ethical standards and specific requirements within closed-domain datasets, we apply a methodology that involves training four prominent LLMs - GEMMA, LLaMA, MISTRAL, and SOLAR - on a small closed-domain dataset using three distinct optimization techniques: Contrastive Preference Optimization (CPO), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO). We evaluate their proficiency by employing the Retrieval Augmented Generation (RAG) benchmark system to assess accuracy. Our research provides researchers with comprehensive insights into the effectiveness of these training methods in improving language model performance within closed-domain datasets while addressing the aforementioned challenges faced by LLMs in specific domains with limited datasets.",
            "corpus_id": "276785440",
            "text": "In the field of natural language processing (NLP), Large Language Models (LLMs) such as MISTRAL have been pre-trained on diverse tasks and datasets to achieve comprehensive language understanding. Nevertheless, fine-tuning small closed-domain datasets presents limitations, including overfitting, restricted generalization, data representation bias, and catastrophic forgetting issues. To tackle these challenges and align LLMs with ethical standards and specific requirements within closed-domain datasets, we apply a methodology that involves training four prominent LLMs - GEMMA, LLaMA, MISTRAL, and SOLAR - on a small closed-domain dataset using three distinct optimization techniques: Contrastive Preference Optimization (CPO), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO). We evaluate their proficiency by employing the Retrieval Augmented Generation (RAG) benchmark system to assess accuracy. Our research provides researchers with comprehensive insights into the effectiveness of these training methods in improving language model performance within closed-domain datasets while addressing the aforementioned challenges faced by LLMs in specific domains with limited datasets.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1845703125
        },
        {
            "paperId": "5f799063f4a041d9f5c3f9f2e423c8a4441b0a01",
            "corpusId": 279410046,
            "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
            "venue": "",
            "year": 2025,
            "referenceCount": 32,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.14387, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": null,
                    "name": "William F. Shen"
                },
                {
                    "authorId": "1702997626",
                    "name": "Xinchi Qiu"
                },
                {
                    "authorId": "2313189467",
                    "name": "Nicola Cancedda"
                },
                {
                    "authorId": "2298756346",
                    "name": "N. Lane"
                }
            ],
            "abstract": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.",
            "corpus_id": "279410046",
            "text": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.61474609375
        },
        {
            "paperId": "47e33a6e0cfd1bf21fc193c2bcb50f04ee79a415",
            "corpusId": 272987467,
            "title": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic Thresholds for Fine-Tuning LLM-based ASR Models",
            "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
            "year": 2024,
            "referenceCount": 36,
            "citationCount": 2,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19878, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2211429378",
                    "name": "Bingshen Mu"
                },
                {
                    "authorId": "2299944267",
                    "name": "Kun Wei"
                },
                {
                    "authorId": "2061559378",
                    "name": "Qijie Shao"
                },
                {
                    "authorId": "2323714781",
                    "name": "Yong Xu"
                },
                {
                    "authorId": "2249732546",
                    "name": "Lei Xie"
                }
            ],
            "abstract": "Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named \\textit{HDMoLE}, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.",
            "corpus_id": "272987467",
            "text": "Recent advancements in integrating Large Language Models (LLM) with automatic speech recognition (ASR) have performed remarkably in general domains. While supervised fine-tuning (SFT) of all model parameters is often employed to adapt pre-trained LLM-based ASR models to specific domains, it imposes high computational costs and notably reduces their performance in general domains. In this paper, we propose a novel parameter-efficient multi-domain fine-tuning method for adapting pre-trained LLM-based ASR models to multi-accent domains without catastrophic forgetting named \\textit{HDMoLE}, which leverages hierarchical routing and dynamic thresholds based on combining low-rank adaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any linear layer. Hierarchical routing establishes a clear correspondence between LoRA experts and accent domains, improving cross-domain collaboration among the LoRA experts. Unlike the static Top-K strategy for activating LoRA experts, dynamic thresholds can adaptively activate varying numbers of LoRA experts at each MoE layer. Experiments on the multi-accent and standard Mandarin datasets demonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model projector module achieves similar performance to full fine-tuning in the target multi-accent domains while using only 9.6% of the trainable parameters required for full fine-tuning and minimal degradation in the source general domain.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1365966796875
        },
        {
            "paperId": "dbe9ad142cf1d8b13ea27d69571f2a23d8f2f1df",
            "corpusId": 277647019,
            "title": "Continuous Model Calibration: Leveraging Feedback-Driven Fine-Tuning for Self- Correcting Large Language Models",
            "venue": "International Journal of Research Publication and Reviews",
            "year": 2025,
            "referenceCount": 42,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.55248/gengpi.6.0325.1208?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.55248/gengpi.6.0325.1208, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354430351",
                    "name": "Opeyemi Joseph Awotunde"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized natural language processing by enabling advanced text generation, comprehension, and interactive capabilities. However, their performance often degrades when confronted with real-world variability, requiring continuous refinement to maintain accuracy, reliability, and ethical integrity. Traditional model calibration relies on periodic updates and static fine-tuning, which fail to address evolving language patterns, contextual nuances, and emergent biases. To overcome these limitations, continuous model calibration introduces a feedback-driven fine-tuning mechanism that enables self-correcting capabilities in LLMs. This approach integrates progressive tuning techniques, real-time human-AI collaboration, and anomaly detection frameworks to dynamically adjust model behavior. Progressive tuning leverages reinforcement learning with human feedback (RLHF) and adaptive loss functions to iteratively refine LLM responses, ensuring alignment with contextual accuracy and user expectations. Human-AI collaboration further enhances model calibration by incorporating domain experts' insights and structured feedback loops to mitigate ethical risks, bias propagation, and factual inconsistencies. Additionally, anomaly detection mechanisms identify distributional shifts and inconsistencies in generated responses, allowing automated interventions to preempt erroneous or misleading outputs. This study explores the interplay between self-correction methodologies and real-world applications, emphasizing the need for transparent governance and robust evaluation metrics. We examine case studies across conversational AI, legal reasoning, and healthcare applications to demonstrate the efficacy of feedback-driven fine-tuning in maintaining model adaptability. By establishing a continuous improvement framework, this research aims to optimize AI reliability, enhance interpretability, and promote ethically aligned decision-making in dynamic environments.",
            "corpus_id": "277647019",
            "text": "Large Language Models (LLMs) have revolutionized natural language processing by enabling advanced text generation, comprehension, and interactive capabilities. However, their performance often degrades when confronted with real-world variability, requiring continuous refinement to maintain accuracy, reliability, and ethical integrity. Traditional model calibration relies on periodic updates and static fine-tuning, which fail to address evolving language patterns, contextual nuances, and emergent biases. To overcome these limitations, continuous model calibration introduces a feedback-driven fine-tuning mechanism that enables self-correcting capabilities in LLMs. This approach integrates progressive tuning techniques, real-time human-AI collaboration, and anomaly detection frameworks to dynamically adjust model behavior. Progressive tuning leverages reinforcement learning with human feedback (RLHF) and adaptive loss functions to iteratively refine LLM responses, ensuring alignment with contextual accuracy and user expectations. Human-AI collaboration further enhances model calibration by incorporating domain experts' insights and structured feedback loops to mitigate ethical risks, bias propagation, and factual inconsistencies. Additionally, anomaly detection mechanisms identify distributional shifts and inconsistencies in generated responses, allowing automated interventions to preempt erroneous or misleading outputs. This study explores the interplay between self-correction methodologies and real-world applications, emphasizing the need for transparent governance and robust evaluation metrics. We examine case studies across conversational AI, legal reasoning, and healthcare applications to demonstrate the efficacy of feedback-driven fine-tuning in maintaining model adaptability. By establishing a continuous improvement framework, this research aims to optimize AI reliability, enhance interpretability, and promote ethically aligned decision-making in dynamic environments.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0863037109375
        },
        {
            "paperId": "490fd85bc40ac129bc8d0152e0c48ebc098edbfe",
            "corpusId": 272987801,
            "title": "AstroMLab 2: AstroLLaMA-2-70B Model and Benchmarking Specialised LLMs for Astronomy",
            "venue": "SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis",
            "year": 2024,
            "referenceCount": 51,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2409.19750, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2277742348",
                    "name": "Rui Pan"
                },
                {
                    "authorId": "2323501341",
                    "name": "Tuan Dung Nguyen"
                },
                {
                    "authorId": "2121219459",
                    "name": "Hardik Arora"
                },
                {
                    "authorId": "2311506761",
                    "name": "Alberto Accomazzi"
                },
                {
                    "authorId": "2301583035",
                    "name": "Tirthankar Ghosal"
                },
                {
                    "authorId": "2260337474",
                    "name": "Y.-S. Ting"
                }
            ],
            "abstract": "Continual pretraining of large language models on domain-specific data has been proposed to enhance performance on downstream tasks. In astronomy, the previous absence of astronomy-focused benchmarks has hindered objective evaluation of these specialized LLM models. Leveraging a recent initiative to curate high-quality astronomical MCQs, this study aims to quantitatively assess specialized LLMs in astronomy. We find that the previously released AstroLLaMA series, based on LLaMA-2-7B, underperforms compared to the base model. We demonstrate that this performance degradation can be partially mitigated by utilizing high-quality data for continual pretraining, such as summarized text from arXiv. Despite the observed catastrophic forgetting in smaller models, our results indicate that continual pretraining on the 70B model can yield improvements. However, the current supervised fine-tuning dataset still constrains the performance of instruct models. In conjunction with this study, we introduce a new set of models, AstroLLaMA-3-8B and AstroLLaMA-2-70B, building upon the previous AstroLLaMA series.",
            "corpus_id": "272987801",
            "text": "Continual pretraining of large language models on domain-specific data has been proposed to enhance performance on downstream tasks. In astronomy, the previous absence of astronomy-focused benchmarks has hindered objective evaluation of these specialized LLM models. Leveraging a recent initiative to curate high-quality astronomical MCQs, this study aims to quantitatively assess specialized LLMs in astronomy. We find that the previously released AstroLLaMA series, based on LLaMA-2-7B, underperforms compared to the base model. We demonstrate that this performance degradation can be partially mitigated by utilizing high-quality data for continual pretraining, such as summarized text from arXiv. Despite the observed catastrophic forgetting in smaller models, our results indicate that continual pretraining on the 70B model can yield improvements. However, the current supervised fine-tuning dataset still constrains the performance of instruct models. In conjunction with this study, we introduce a new set of models, AstroLLaMA-3-8B and AstroLLaMA-2-70B, building upon the previous AstroLLaMA series.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0880126953125
        },
        {
            "paperId": "05c865477043b1c2e001f408216bcc2a13014988",
            "corpusId": 202579262,
            "title": "Adversarial Feature Alignment: Avoid Catastrophic Forgetting in Incremental Task Lifelong Learning",
            "venue": "Neural Computation",
            "year": 2019,
            "referenceCount": 39,
            "citationCount": 22,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1910.10986",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/1910.10986, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2115584973",
                    "name": "Xin Yao"
                },
                {
                    "authorId": "41021998",
                    "name": "Tianchi Huang"
                },
                {
                    "authorId": "33846296",
                    "name": "Chenglei Wu"
                },
                {
                    "authorId": "48263901",
                    "name": "Ruixiao Zhang"
                },
                {
                    "authorId": "1720782061",
                    "name": "Lifeng Sun"
                }
            ],
            "abstract": "Humans are able to master a variety of knowledge and skills with ongoing learning. By contrast, dramatic performance degradation is observed when new tasks are added to an existing neural network model. This phenomenon, termed catastrophic forgetting, is one of the major roadblocks that prevent deep neural networks from achieving human-level artificial intelligence. Several research efforts (e.g., lifelong or continual learning algorithms) have proposed to tackle this problem. However, they either suffer from an accumulating drop in performance as the task sequence grows longer, or require storing an excessive number of model parameters for historical memory, or cannot obtain competitive performance on the new tasks. In this letter, we focus on the incremental multitask image classification scenario. Inspired by the learning process of students, who usually decompose complex tasks into easier goals, we propose an adversarial feature alignment method to avoid catastrophic forgetting. In our design, both the low-level visual features and high-level semantic features serve as soft targets and guide the training process in multiple stages, which provide sufficient supervised information of the old tasks and help to reduce forgetting. Due to the knowledge distillation and regularization phenomena, the proposed method gains even better performance than fine-tuning on the new tasks, which makes it stand out from other methods. Extensive experiments in several typical lifelong learning scenarios demonstrate that our method outperforms the state-of-the-art methods in both accuracy on new tasks and performance preservation on old tasks.",
            "corpus_id": "202579262",
            "text": "Humans are able to master a variety of knowledge and skills with ongoing learning. By contrast, dramatic performance degradation is observed when new tasks are added to an existing neural network model. This phenomenon, termed catastrophic forgetting, is one of the major roadblocks that prevent deep neural networks from achieving human-level artificial intelligence. Several research efforts (e.g., lifelong or continual learning algorithms) have proposed to tackle this problem. However, they either suffer from an accumulating drop in performance as the task sequence grows longer, or require storing an excessive number of model parameters for historical memory, or cannot obtain competitive performance on the new tasks. In this letter, we focus on the incremental multitask image classification scenario. Inspired by the learning process of students, who usually decompose complex tasks into easier goals, we propose an adversarial feature alignment method to avoid catastrophic forgetting. In our design, both the low-level visual features and high-level semantic features serve as soft targets and guide the training process in multiple stages, which provide sufficient supervised information of the old tasks and help to reduce forgetting. Due to the knowledge distillation and regularization phenomena, the proposed method gains even better performance than fine-tuning on the new tasks, which makes it stand out from other methods. Extensive experiments in several typical lifelong learning scenarios demonstrate that our method outperforms the state-of-the-art methods in both accuracy on new tasks and performance preservation on old tasks.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.296875
        },
        {
            "paperId": "8d28d2ef602e8b518b7daecc39a0f2f8d2caaa09",
            "corpusId": 266818099,
            "title": "MLLM-Protector: Ensuring MLLM\u2019s Safety without Hurting Performance",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "referenceCount": 63,
            "citationCount": 69,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.02906, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2066420772",
                    "name": "Renjie Pi"
                },
                {
                    "authorId": "2278220435",
                    "name": "Tianyang Han"
                },
                {
                    "authorId": "2305732949",
                    "name": "Jianshu Zhang"
                },
                {
                    "authorId": "2154871075",
                    "name": "Yueqi Xie"
                },
                {
                    "authorId": null,
                    "name": "Rui Pan"
                },
                {
                    "authorId": "2237987004",
                    "name": "Qing Lian"
                },
                {
                    "authorId": "35279146",
                    "name": "Hanze Dong"
                },
                {
                    "authorId": "2275280334",
                    "name": "Jipeng Zhang"
                },
                {
                    "authorId": "50561049",
                    "name": "Jipeng Zhang"
                }
            ],
            "abstract": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a \u201cforeign language\u201d that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.",
            "corpus_id": "266818099",
            "text": "The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. This paper investigates the novel challenge of defending MLLMs against such attacks. Compared to large language models (LLMs), MLLMs include an additional image modality. We discover that images act as a \u201cforeign language\u201d that is not considered during safety alignment, making MLLMs more prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover all possible scenarios. This vulnerability is exacerbated by the fact that most state-of-the-art MLLMs are fine-tuned on limited image-text pairs that are much fewer than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during safety fine-tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play strategy that solves two subtasks: 1) identifying harmful responses via a lightweight harm detector, and 2) transforming harmful responses into harmless ones via a detoxifier. This approach effectively mitigates the risks posed by malicious visual inputs without compromising the original performance of MLLMs. Our results demonstrate that MLLM-Protector offers a robust solution to a previously unaddressed aspect of MLLM security.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1302490234375
        },
        {
            "paperId": "59b4b5b1e2198f264536d83e33d96b0a45ed3bac",
            "corpusId": 267751418,
            "title": "MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 88,
            "citationCount": 45,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.11260, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284694841",
                    "name": "Shu Yang"
                },
                {
                    "authorId": "2260606436",
                    "name": "Muhammad Asif Ali"
                },
                {
                    "authorId": "2257398718",
                    "name": "Cheng-Long Wang"
                },
                {
                    "authorId": "2153121378",
                    "name": "Lijie Hu"
                },
                {
                    "authorId": "2260640875",
                    "name": "Di Wang"
                }
            ],
            "abstract": "Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 30.15% improvement in\"RA\"for Phi-2-2.7B compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.",
            "corpus_id": "267751418",
            "text": "Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 30.15% improvement in\"RA\"for Phi-2-2.7B compared to closed-book (for models fine-tuned with MoRAL); (ii) MoRAL shows higher performance improvement for models with a greater number of parameters; (iii) MoRAL is robust to catastrophic forgetting offering better knowledge retention compared to baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1175537109375
        },
        {
            "paperId": "c0ebf1852b82d4ca52ab8429003be4bff884bc5b",
            "corpusId": 273507837,
            "title": "LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "referenceCount": 114,
            "citationCount": 13,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.17146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257324343",
                    "name": "Ke Wang"
                },
                {
                    "authorId": "153670517",
                    "name": "Nikolaos Dimitriadis"
                },
                {
                    "authorId": "2088881127",
                    "name": "Alessandro Favero"
                },
                {
                    "authorId": "2257000703",
                    "name": "Guillermo Ortiz-Jim\u00e9nez"
                },
                {
                    "authorId": "2721983",
                    "name": "F. Fleuret"
                },
                {
                    "authorId": "2256985187",
                    "name": "Pascal Frossard"
                }
            ],
            "abstract": "Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, \\textit{(i)} fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and \\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to significant performance loss. To address these challenges, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Our method is simple to implement, computationally efficient and complementary to many existing techniques. Our source code is available at https://github.com/wang-kee/LiNeS",
            "corpus_id": "273507837",
            "text": "Fine-tuning pre-trained models has become the standard approach to endow them with specialized knowledge, but it poses fundamental challenges. In particular, \\textit{(i)} fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks, and \\textit{(ii)} merging fine-tuned checkpoints from disparate tasks can lead to significant performance loss. To address these challenges, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. In multi-task model merging scenarios, layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Our method is simple to implement, computationally efficient and complementary to many existing techniques. Our source code is available at https://github.com/wang-kee/LiNeS",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.345458984375
        },
        {
            "paperId": "8f9c84c4c80f972e2be5e92590827914ffb68958",
            "corpusId": 275342393,
            "title": "SLAM: Towards Efficient Multilingual Reasoning via Selective Language Alignment",
            "venue": "International Conference on Computational Linguistics",
            "year": 2025,
            "referenceCount": 29,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.03681, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2218552428",
                    "name": "Yuchun Fan"
                },
                {
                    "authorId": "2040853265",
                    "name": "Yongyu Mu"
                },
                {
                    "authorId": "2338967562",
                    "name": "Yilin Wang"
                },
                {
                    "authorId": "2265930173",
                    "name": "Lei Huang"
                },
                {
                    "authorId": "2316399834",
                    "name": "Junhao Ruan"
                },
                {
                    "authorId": "2291200534",
                    "name": "Bei Li"
                },
                {
                    "authorId": "2261739712",
                    "name": "Tong Xiao"
                },
                {
                    "authorId": "2339084258",
                    "name": "Shujian Huang"
                },
                {
                    "authorId": "2320840740",
                    "name": "Xiaocheng Feng"
                },
                {
                    "authorId": "2240940961",
                    "name": "Jingbo Zhu"
                }
            ],
            "abstract": "Despite the significant improvements achieved by large language models (LLMs) in English reasoning tasks, these models continue to struggle with multilingual reasoning. Recent studies leverage a full-parameter and two-stage training paradigm to teach models to first understand non-English questions and then reason. However, this method suffers from both substantial computational resource computing and catastrophic forgetting. The fundamental cause is that, with the primary goal of enhancing multilingual comprehension, an excessive number of irrelevant layers and parameters are tuned during the first stage. Given our findings that the representation learning of languages is merely conducted in lower-level layers, we propose an efficient multilingual reasoning alignment approach that precisely identifies and fine-tunes the layers responsible for handling multilingualism. Experimental results show that our method, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of all parameters within 7B and 13B LLMs, achieving superior average performance than all strong baselines across 10 languages. Meanwhile, SLAM only involves one training stage, reducing training time by 4.1-11.9 compared to the two-stage method.",
            "corpus_id": "275342393",
            "text": "Despite the significant improvements achieved by large language models (LLMs) in English reasoning tasks, these models continue to struggle with multilingual reasoning. Recent studies leverage a full-parameter and two-stage training paradigm to teach models to first understand non-English questions and then reason. However, this method suffers from both substantial computational resource computing and catastrophic forgetting. The fundamental cause is that, with the primary goal of enhancing multilingual comprehension, an excessive number of irrelevant layers and parameters are tuned during the first stage. Given our findings that the representation learning of languages is merely conducted in lower-level layers, we propose an efficient multilingual reasoning alignment approach that precisely identifies and fine-tunes the layers responsible for handling multilingualism. Experimental results show that our method, SLAM, only tunes 6 layers' feed-forward sub-layers including 6.5-8% of all parameters within 7B and 13B LLMs, achieving superior average performance than all strong baselines across 10 languages. Meanwhile, SLAM only involves one training stage, reducing training time by 4.1-11.9 compared to the two-stage method.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.10577392578125
        },
        {
            "paperId": "5cb369fcf312deb35e7a2cccea29b0fe64496e97",
            "corpusId": 277667251,
            "title": "LoRI: Reducing Cross-Task Interference in Multi-Task Low-Rank Adaptation",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 67,
            "citationCount": 4,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.07448, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2354604405",
                    "name": "Juzheng Zhang"
                },
                {
                    "authorId": "2354601383",
                    "name": "Jiacheng You"
                },
                {
                    "authorId": "2334571664",
                    "name": "Ashwinee Panda"
                },
                {
                    "authorId": "2354556140",
                    "name": "Tom Goldstein"
                }
            ],
            "abstract": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI",
            "corpus_id": "277667251",
            "text": "Low-Rank Adaptation (LoRA) has emerged as a popular parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), yet it still incurs notable overhead and suffers from parameter interference in multi-task scenarios. We propose LoRA with Reduced Interference (LoRI), a simple yet effective approach that freezes the projection matrices $A$ as random projections and sparsifies the matrices $B$ using task-specific masks. This design substantially reduces the number of trainable parameters while maintaining strong task performance. Moreover, LoRI minimizes cross-task interference in adapter merging by leveraging the orthogonality between adapter subspaces, and supports continual learning by using sparsity to mitigate catastrophic forgetting. Extensive experiments across natural language understanding, mathematical reasoning, code generation, and safety alignment tasks demonstrate that LoRI outperforms full fine-tuning and existing PEFT methods, while using up to 95% fewer trainable parameters than LoRA. In multi-task experiments, LoRI enables effective adapter merging and continual learning with reduced cross-task interference. Code is available at: https://github.com/juzhengz/LoRI",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2171630859375
        }
    ],
    "quotes": {
        "cost": 0.183915,
        "quotes": [
            {
                "idx": 0,
                "key": "[221083147 | Aghajanyan et al. | 2020 | Citations: 210]",
                "snippets": "Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989).\n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "REPRESENTATIONAL COLLAPSE",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 847,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 297
                            },
                            {
                                "start": 300,
                                "end": 484
                            },
                            {
                                "start": 485,
                                "end": 641
                            },
                            {
                                "start": 642,
                                "end": 848
                            }
                        ],
                        "ref_mentions": [
                            "61019113"
                        ],
                        "quote": "Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989).\n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[224704480 | Wallat et al. | 2020 | Citations: 60]",
                "snippets": "However, one thing that has been studied is the effect that models lose the ability to do their pre-training task when being fine-tuned. This sequential learning problem has also been referred to as catastrophic inference (McCloskey and Cohen 1989) or catastrophic forgetting (Zacarias et al., 2018)Mosbach, Andriushchenko, and Klakow 2020). This is an ongoing problem as general AI models will need to be able to learn and perform multiple tasks without forgetting how to perform the former. (Zacarias et al., 2018) suggest slowing down learning of important weights for the initial task, whereas Mosbach, Andriushchenko, and Klakow (2020) investigated the role of catastrophic forgetting on fine-tuning stability, finding that the last few layers are usually replaced with task-specific knowledge. If and how factual knowledge is affected by catastrophic forgetting is largely unexplored. In this work, we will take a first step into understanding how fine-tuning tasks and data effect both the learning and forgetting of factual knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[3528793 | Zacarias et al. | 2018 | Citations: 12]": "Lifelong learning aims to develop machine learning systems that can learn new tasks while preserving the performance on previous learned tasks. In this paper we present a method to overcome catastrophic forgetting on convolutional neural networks, that learns new tasks and preserves the performance on old tasks without accessing the data of the original model, by selective network augmentation (SeNA-CNN). The experiment results showed that SeNA-CNN, in some scenarios, outperforms the state-of-art Learning without Forgetting algorithm. Results also showed that in some situations it is better to use SeNA-CNN instead of training a neural network using isolated learning."
                },
                "metadata": [
                    {
                        "section_title": "Learning and forgetting",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1054,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 136
                            },
                            {
                                "start": 137,
                                "end": 347
                            },
                            {
                                "start": 348,
                                "end": 498
                            },
                            {
                                "start": 499,
                                "end": 811
                            },
                            {
                                "start": 812,
                                "end": 902
                            },
                            {
                                "start": 903,
                                "end": 1054
                            }
                        ],
                        "ref_mentions": [
                            "3528793",
                            "3528793"
                        ],
                        "quote": "However, one thing that has been studied is the effect that models lose the ability to do their pre-training task when being fine-tuned. This sequential learning problem has also been referred to as catastrophic inference (McCloskey and Cohen 1989) or catastrophic forgetting (Zacarias et al., 2018)Mosbach, Andriushchenko, and Klakow 2020). This is an ongoing problem as general AI models will need to be able to learn and perform multiple tasks without forgetting how to perform the former. (Zacarias et al., 2018) suggest slowing down learning of important weights for the initial task, whereas Mosbach, Andriushchenko, and Klakow (2020) investigated the role of catastrophic forgetting on fine-tuning stability, finding that the last few layers are usually replaced with task-specific knowledge. If and how factual knowledge is affected by catastrophic forgetting is largely unexplored. In this work, we will take a first step into understanding how fine-tuning tasks and data effect both the learning and forgetting of factual knowledge."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[259951356 | Wang et al. | 2023 | Citations: 51]",
                "snippets": "Zhang et al. (Zhang et al., 2024) conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text...recent research (Zhao et al., 2023) highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter (Zhao et al., 2023) introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[266374841 | Zhao et al. | 2023 | Citations: 18]": "As the number of large language models (LLMs) released to the public grows, there is a pressing need to understand the safety implications associated with these models learning from third-party custom finetuning data. We explore the behavior of LLMs finetuned on noisy custom data containing unsafe content, represented by datasets that contain biases, toxicity, and harmfulness, finding that while aligned LLMs can readily learn this unsafe content, they also tend to forget it more significantly than other examples when subsequently finetuned on safer content. Drawing inspiration from the discrepancies in forgetting, we introduce the\"ForgetFilter\"algorithm, which filters unsafe data based on how strong the model's forgetting signal is for that data. We demonstrate that the ForgetFilter algorithm ensures safety in customized finetuning without compromising downstream task performance, unlike sequential safety finetuning. ForgetFilter outperforms alternative strategies like replay and moral self-correction in curbing LLMs' ability to assimilate unsafe content during custom finetuning, e.g. 75% lower than not applying any safety measures and 62% lower than using self-correction in toxicity score."
                },
                "metadata": [
                    {
                        "section_title": "Forgetting in Fine-Tuning Foundation Models",
                        "pdf_hash": "",
                        "start": 1495,
                        "end": 1639,
                        "sentence_offsets": [
                            {
                                "start": 1495,
                                "end": 1640
                            }
                        ],
                        "ref_mentions": [
                            "271745852"
                        ],
                        "quote": "Zhang et al. (Zhang et al., 2024) conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text"
                    },
                    {
                        "section_title": "Forgetting in Fine-Tuning Foundation Models",
                        "pdf_hash": "",
                        "start": 1994,
                        "end": 2325,
                        "sentence_offsets": [
                            {
                                "start": 1984,
                                "end": 2114
                            },
                            {
                                "start": 2115,
                                "end": 2324
                            }
                        ],
                        "ref_mentions": [
                            "266374841",
                            "266374841"
                        ],
                        "quote": "recent research (Zhao et al., 2023) highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter (Zhao et al., 2023) introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[261031244 | Luo et al. | 2023 | Citations: 318]",
                "snippets": "The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM....Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning....For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting....We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 589,
                        "end": 898,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 412,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 77
                            },
                            {
                                "start": 78,
                                "end": 187
                            },
                            {
                                "start": 188,
                                "end": 408
                            },
                            {
                                "start": 409,
                                "end": 521
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning"
                    },
                    {
                        "section_title": "Main Results",
                        "pdf_hash": "",
                        "start": 923,
                        "end": 1320,
                        "sentence_offsets": [
                            {
                                "start": 923,
                                "end": 967
                            },
                            {
                                "start": 968,
                                "end": 1055
                            },
                            {
                                "start": 1056,
                                "end": 1205
                            },
                            {
                                "start": 1206,
                                "end": 1316
                            },
                            {
                                "start": 1317,
                                "end": 1375
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting"
                    },
                    {
                        "section_title": "Main Results",
                        "pdf_hash": "",
                        "start": 1488,
                        "end": 1929,
                        "sentence_offsets": [
                            {
                                "start": 1488,
                                "end": 1680
                            },
                            {
                                "start": 1681,
                                "end": 1781
                            },
                            {
                                "start": 1782,
                                "end": 1823
                            },
                            {
                                "start": 1824,
                                "end": 1835
                            },
                            {
                                "start": 1836,
                                "end": 1927
                            },
                            {
                                "start": 1928,
                                "end": 2175
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[264720747 | Hajipour et al. | 2022 | Citations: 5]",
                "snippets": "Despite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;(Chen et al., 2020). Furthermore, (Kumar et al., 2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[216553067 | Chen et al. | 2020 | Citations: 225]": "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.",
                    "[247011290 | Kumar et al. | 2022 | Citations: 685]": "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer\u2014the \u201chead\u201d). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \u2192 STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head\u2014this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning)."
                },
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 404,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 204
                            },
                            {
                                "start": 205,
                                "end": 404
                            }
                        ],
                        "ref_mentions": [
                            "216553067",
                            "247011290"
                        ],
                        "quote": "Despite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;(Chen et al., 2020). Furthermore, (Kumar et al., 2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[265158118 | Li et al. | 2023 | Citations: 11]",
                "snippets": "Selective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes.\n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Difficult Sample.",
                        "pdf_hash": "",
                        "start": 546,
                        "end": 1526,
                        "sentence_offsets": [
                            {
                                "start": 300,
                                "end": 580
                            },
                            {
                                "start": 581,
                                "end": 673
                            },
                            {
                                "start": 676,
                                "end": 697
                            },
                            {
                                "start": 698,
                                "end": 902
                            },
                            {
                                "start": 903,
                                "end": 1180
                            },
                            {
                                "start": 1181,
                                "end": 1330
                            },
                            {
                                "start": 1333,
                                "end": 1418
                            },
                            {
                                "start": 1419,
                                "end": 1591
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Selective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes.\n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[266374841 | Zhao et al. | 2023 | Citations: 18]",
                "snippets": "Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Finetuning Safety Finetuning",
                        "pdf_hash": "",
                        "start": 2199,
                        "end": 2381,
                        "sentence_offsets": [
                            {
                                "start": 2199,
                                "end": 2381
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[266844262 | Li et al. | 2024 | Citations: 10]",
                "snippets": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 801,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[266933290 | Kalajdzievski | 2024 | Citations: 11]",
                "snippets": "In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts.\n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts.\n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[267636596 | Zheng et al. | 2024 | Citations: 0]",
                "snippets": "Main Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "RQ1: Can LLMs learn new concepts incrementally without forgetting?",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 911,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 105
                            },
                            {
                                "start": 106,
                                "end": 178
                            },
                            {
                                "start": 179,
                                "end": 307
                            },
                            {
                                "start": 308,
                                "end": 432
                            },
                            {
                                "start": 435,
                                "end": 541
                            },
                            {
                                "start": 542,
                                "end": 696
                            },
                            {
                                "start": 697,
                                "end": 779
                            },
                            {
                                "start": 780,
                                "end": 911
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Main Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[267657819 | Wo'zniak et al. | 2024 | Citations: 20]",
                "snippets": "Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting (French, 1999), where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- (Zhai et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[262055661 | Zhai et al. | 2023 | Citations: 88]": "Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement."
                },
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 194,
                        "end": 624,
                        "sentence_offsets": [
                            {
                                "start": 194,
                                "end": 296
                            },
                            {
                                "start": 297,
                                "end": 454
                            },
                            {
                                "start": 455,
                                "end": 624
                            }
                        ],
                        "ref_mentions": [
                            "2691726",
                            "262055661"
                        ],
                        "quote": "Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting (French, 1999), where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- (Zhai et al., 2023)."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[267769989 | Yang et al. | 2024 | Citations: 49]",
                "snippets": "Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[268063771 | Ren et al. | 2024 | Citations: 39]",
                "snippets": "Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 361,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[268247412 | Wang et al. | 2024 | Citations: 7]",
                "snippets": "It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 24,
                        "end": 328,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 234
                            },
                            {
                                "start": 235,
                                "end": 328
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[268510177 | He et al. | 2024 | Citations: 5]",
                "snippets": "However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 144,
                        "end": 318,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[270068369 | Contal et al. | 2024 | Citations: 5]",
                "snippets": "Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1083,
                        "end": 1613,
                        "sentence_offsets": [
                            {
                                "start": 1083,
                                "end": 1245
                            },
                            {
                                "start": 1246,
                                "end": 1295
                            },
                            {
                                "start": 1296,
                                "end": 1462
                            },
                            {
                                "start": 1463,
                                "end": 1613
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data."
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[270357611 | Li et al. | 2024 | Citations: 19]",
                "snippets": "Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 450,
                        "end": 788,
                        "sentence_offsets": [
                            {
                                "start": 450,
                                "end": 788
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[270559716 | Hu et al. | 2024 | Citations: 3]",
                "snippets": "PPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Limitations",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 717,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 135
                            },
                            {
                                "start": 136,
                                "end": 219
                            },
                            {
                                "start": 220,
                                "end": 449
                            },
                            {
                                "start": 450,
                                "end": 545
                            },
                            {
                                "start": 546,
                                "end": 717
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "PPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[270562404 | Jiang et al. | 2024 | Citations: 1]",
                "snippets": "Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2021)Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;(Kotha et al., 2023)Wu et al., 2024b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[237416585 | Wei et al. | 2021 | Citations: 3788]": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
                    "[262054014 | Kotha et al. | 2023 | Citations: 71]": "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT."
                },
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 54,
                        "end": 566,
                        "sentence_offsets": [
                            {
                                "start": 54,
                                "end": 318
                            },
                            {
                                "start": 318,
                                "end": 566
                            }
                        ],
                        "ref_mentions": [
                            "237416585",
                            "262054014"
                        ],
                        "quote": "Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2021)Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;(Kotha et al., 2023)Wu et al., 2024b)."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[271244634 | Roffo | 2024 | Citations: 0]",
                "snippets": "Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Improving Performance of Large Language Models through Fine-Tuning",
                        "pdf_hash": "",
                        "start": 1505,
                        "end": 1967,
                        "sentence_offsets": [
                            {
                                "start": 1505,
                                "end": 1678
                            },
                            {
                                "start": 1678,
                                "end": 1810
                            },
                            {
                                "start": 1810,
                                "end": 1967
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27]."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273345395 | Thangarasa et al. | 2024 | Citations: 2]",
                "snippets": "One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;(Korbak et al., 2021).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[249921347 | Korbak et al. | 2021 | Citations: 34]": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 667,
                        "end": 886,
                        "sentence_offsets": [
                            {
                                "start": 643,
                                "end": 886
                            }
                        ],
                        "ref_mentions": [
                            "249921347"
                        ],
                        "quote": "One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;(Korbak et al., 2021)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[274234789 | Lobo et al. | 2024 | Citations: 10]",
                "snippets": "prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)...Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 9,
                        "end": 181,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 463
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)"
                    },
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 659,
                        "end": 825,
                        "sentence_offsets": [
                            {
                                "start": 645,
                                "end": 824
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[274982256 | Ung et al. | 2024 | Citations: 0]",
                "snippets": "Catastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;(Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2013)Kirkpatrick et al., 2016;(Kemker et al., 2017). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;(Razdaibiedina et al., 2023)Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[12730344 | Goodfellow et al. | 2013 | Citations: 1455]": "Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models \"forget\" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.",
                    "[18556305 | Ratcliff | 1990 | Citations: 1018]": "Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning.",
                    "[22910766 | Kemker et al. | 2017 | Citations: 722]": "\n \n Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.\n \n",
                    "[246634238 | Perez et al. | 2022 | Citations: 668]": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\u201cred teaming\u201d) using another LM. We evaluate the target LM\u2019s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot\u2019s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",
                    "[256390383 | Razdaibiedina et al. | 2023 | Citations: 142]": "We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement>20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1034,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 310
                            },
                            {
                                "start": 311,
                                "end": 505
                            },
                            {
                                "start": 506,
                                "end": 782
                            },
                            {
                                "start": 783,
                                "end": 1034
                            }
                        ],
                        "ref_mentions": [
                            "18556305",
                            "12730344",
                            "22910766",
                            "256390383",
                            "246634238"
                        ],
                        "quote": "Catastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;(Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2013)Kirkpatrick et al., 2016;(Kemker et al., 2017). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;(Razdaibiedina et al., 2023)Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023)."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[275820535 | Song et al. | 2025 | Citations: 1]",
                "snippets": "As shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 682,
                        "end": 1425,
                        "sentence_offsets": [
                            {
                                "start": 682,
                                "end": 859
                            },
                            {
                                "start": 860,
                                "end": 1001
                            },
                            {
                                "start": 1002,
                                "end": 1129
                            },
                            {
                                "start": 1130,
                                "end": 1276
                            },
                            {
                                "start": 1277,
                                "end": 1425
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "As shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[276287820 | Bhushan et al. | 2025 | Citations: 2]",
                "snippets": "Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 316,
                        "end": 832,
                        "sentence_offsets": [
                            {
                                "start": 316,
                                "end": 530
                            },
                            {
                                "start": 531,
                                "end": 673
                            },
                            {
                                "start": 674,
                                "end": 832
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[276421775 | Xu et al. | 2025 | Citations: 22]",
                "snippets": "We empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 177,
                        "end": 579,
                        "sentence_offsets": [
                            {
                                "start": 177,
                                "end": 245
                            },
                            {
                                "start": 246,
                                "end": 445
                            },
                            {
                                "start": 446,
                                "end": 579
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024)."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[276482659 | Pletenev et al. | 2025 | Citations: 1]",
                "snippets": "However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2023)Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[266348792 | Aleixo et al. | 2023 | Citations: 26]": "Deep Learning models have achieved remarkable performance in tasks such as image classification or generation, often surpassing human accuracy. However, they can struggle to learn new tasks and update their knowledge without access to previous data, leading to a significant loss of accuracy known as Catastrophic Forgetting (CF). This phenomenon was first observed by McCloskey and Cohen in 1989 and remains an active research topic. Incremental learning without forgetting is widely recognized as a crucial aspect in building better AI systems, as it allows models to adapt to new tasks without losing the ability to perform previously learned ones. This article surveys recent studies that tackle CF in modern Deep Learning models that use gradient descent as their learning algorithm. Although several solutions have been proposed, a definitive solution or consensus on assessing CF is yet to be established. The article provides a comprehensive review of recent solutions, proposes a taxonomy to organize them, and identifies research gaps in this area."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 481,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 243
                            },
                            {
                                "start": 244,
                                "end": 481
                            }
                        ],
                        "ref_mentions": [
                            "266348792"
                        ],
                        "quote": "However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2023)Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark."
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[276574830 | Zhang et al. | 2024 | Citations: 0]",
                "snippets": "Despite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265294420 | Li et al. | 2023 | Citations: 208]": "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 414,
                        "end": 1179,
                        "sentence_offsets": [
                            {
                                "start": 414,
                                "end": 594
                            },
                            {
                                "start": 595,
                                "end": 715
                            },
                            {
                                "start": 716,
                                "end": 908
                            },
                            {
                                "start": 909,
                                "end": 1179
                            }
                        ],
                        "ref_mentions": [
                            "265294420"
                        ],
                        "quote": "Despite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c)."
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[277501673 | Haque | 2025 | Citations: 0]",
                "snippets": "(Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Works",
                        "pdf_hash": "",
                        "start": 1370,
                        "end": 1665,
                        "sentence_offsets": [
                            {
                                "start": 1370,
                                "end": 1665
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "(Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[278327022 | Chen et al. | 2025 | Citations: 2]",
                "snippets": "(Li et al., 2023) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[260334888 | Li et al. | 2023 | Citations: 571]": "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability."
                },
                "metadata": [
                    {
                        "section_title": "A.5. Understanding Catastrophic Forgetting",
                        "pdf_hash": "",
                        "start": 103,
                        "end": 568,
                        "sentence_offsets": [
                            {
                                "start": 89,
                                "end": 310
                            },
                            {
                                "start": 311,
                                "end": 568
                            }
                        ],
                        "ref_mentions": [
                            "260334888"
                        ],
                        "quote": "(Li et al., 2023) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[279410046 | Shen et al. | 2025 | Citations: 0]",
                "snippets": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.028035,
        "cot": "For this query about empirical evidence on catastrophic forgetting or performance degradation in LLMs after fine-tuning for alignment or factuality tasks, I'll organize the quotes into logical dimensions that provide a comprehensive answer.\n\n1. \"Introduction\" will be in synthesis format to provide background on what catastrophic forgetting is and why it matters for LLMs. This will help users understand the fundamental concept before diving into specific evidence.\n\n2. \"Empirical Evidence of Catastrophic Forgetting in LLMs\" will be in synthesis format because we need to coherently present the empirical studies that demonstrate catastrophic forgetting in LLMs, including the severity, patterns, and conditions under which it occurs.\n\n3. \"Impact on Specific Capabilities and Knowledge\" will be in list format to clearly enumerate the different capabilities and knowledge types that are affected by catastrophic forgetting, making it easy for the user to scan through the various impacts.\n\n4. \"Factors Influencing Catastrophic Forgetting\" will be in synthesis format to present a cohesive analysis of what influences the severity of catastrophic forgetting in LLMs, including model size, training methods, and other factors.\n\n5. \"Safety and Alignment Implications\" will be in synthesis format because this section requires a nuanced discussion of how catastrophic forgetting specifically affects alignment and safety capabilities, which is directly relevant to the user's query.\n\n6. \"Mitigation Approaches\" will be in list format to provide a clear overview of the different strategies that researchers have proposed to address catastrophic forgetting, which will be valuable practical information for the user.",
        "plan": {
            "Introduction to Catastrophic Forgetting in LLMs (synthesis)": [],
            "Empirical Evidence of Catastrophic Forgetting in LLMs (synthesis)": [
                3,
                8,
                9,
                11,
                16,
                25,
                26,
                28,
                29
            ],
            "Impact on Specific Capabilities and Knowledge (list)": [
                0,
                1,
                2,
                4,
                7,
                12,
                13,
                14,
                15,
                17,
                18,
                19,
                23,
                30
            ],
            "Factors Influencing Catastrophic Forgetting (synthesis)": [
                3,
                5,
                11,
                29
            ],
            "Safety and Alignment Implications (synthesis)": [
                2,
                6,
                11,
                22,
                30
            ],
            "Mitigation Approaches (list)": [
                5,
                10,
                11,
                20,
                21,
                24,
                27
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Catastrophic Forgetting in LLMs",
                "tldr": "Catastrophic forgetting refers to the phenomenon where LLMs lose previously learned capabilities or knowledge when fine-tuned on new tasks. This challenge is particularly concerning when models are fine-tuned for alignment or factuality, as they may trade off general capabilities for improvements in targeted areas. (LLM Memory)",
                "text": "\nCatastrophic forgetting is a well-documented phenomenon in neural networks where models lose previously acquired knowledge or skills when trained on new data or tasks. In the context of Large Language Models (LLMs), catastrophic forgetting manifests as the degradation of certain capabilities, factual recall, or reasoning skills after fine-tuning for specific objectives such as alignment, safety, or factuality.\n\nThe phenomenon becomes particularly relevant as organizations increasingly fine-tune foundation models to align with human preferences, reduce harmful outputs, or improve factual accuracy. During these fine-tuning processes, models may inadvertently sacrifice capabilities from their pre-training phase. This trade-off creates tension between improving targeted behaviors and maintaining the broad capabilities that make LLMs valuable.\n\nCatastrophic forgetting in LLMs differs somewhat from traditional neural networks due to the massive scale of parameters and the diverse knowledge encoded during pre-training. When fine-tuned on relatively small, specialized datasets, LLMs can experience disproportionate shifts in their output distribution, overemphasizing recently learned patterns at the expense of pre-existing knowledge. This is especially problematic when alignment fine-tuning introduces constraints that conflict with capabilities acquired during pre-training. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Empirical Evidence of Catastrophic Forgetting in LLMs",
                "tldr": "Numerous studies have demonstrated catastrophic forgetting in LLMs across various model sizes and fine-tuning scenarios, with evidence showing performance degradation in specific capabilities like reasoning, reading comprehension, and domain knowledge after fine-tuning for new tasks. (11 sources)",
                "text": "\nResearch has consistently documented the phenomenon of catastrophic forgetting in LLMs during fine-tuning processes. A key study by Luo et al. revealed that catastrophic forgetting occurs in LLMs ranging from 1B to 7B parameters, with the surprising finding that larger models actually experience more severe forgetting, possibly due to their initially stronger performance having more room to degrade <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Their experiments showed significant performance drops after continual instruction tuning, with BLOOMZ-7.1b's performance on social science tasks declining from 36.18% to 26.06% <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This finding is corroborated by Huang et al., who also found forgetting becomes more severe as model size increases <Paper corpusId=\"277501673\" paperTitle=\"(Haque, 2025)\" isShortName></Paper>.\n\nFine-tuning impacts different capabilities disproportionately. Reading comprehension appears to suffer the most dramatic forgetting (26.75% in BLOOMZ-7.1b), followed by domain knowledge (18.37%) and reasoning (13.62%) <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Similarly, Zheng et al. demonstrated that when LLaMa-2-7B is sequentially fine-tuned on new concepts, it shows clear evidence of forgetting previously learned knowledge despite achieving perfect memorization of each new task <Paper corpusId=\"267636596\" paperTitle=\"(Zheng et al., 2024)\" isShortName></Paper>.\n\nThe impact of forgetting extends to specialized capabilities as well. Yang et al. observed that vanilla fine-tuning on the OpenFunctions dataset led to a 27% decline in coding ability as measured by the HumanEval benchmark, with performance dropping from 13.4 to 9.8 <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Xu et al. found that supervised fine-tuning of LLaMA3.1-8B models on reasoning tasks can lead to performance degradation compared to zero-shot settings <Paper corpusId=\"276421775\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>. Even more concerning, Li et al. highlight research showing that fine-tuned models can perform worse than their foundation counterparts on several tasks <Paper corpusId=\"270357611\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nSafety alignment is particularly vulnerable to catastrophic forgetting. Studies show that fine-tuning with LoRA on adversarial datasets can effectively remove all safety guardrails previously embedded in pre-trained models like Llama 2 70B <Paper corpusId=\"266933290\" paperTitle=\"(Kalajdzievski, 2024)\" isShortName></Paper>. Similarly, Pletenev et al. demonstrated that increasing the amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance on the TruthfulQA benchmark <Paper corpusId=\"276482659\" paperTitle=\"(Pletenev et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266348792\" paperTitle=\"(Aleixo et al., 2023)\" isShortName></Paper>.\n\nResearchers have begun to identify possible mechanisms behind catastrophic forgetting. Li et al. showed that forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong correlation between sharpness and forgetting <Paper corpusId=\"278327022\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"260334888\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Additionally, Wang et al. found that when fine-tuned on tasks requiring specific output formats, LLMs catastrophically forget how to perform other tasks such as question-answering, even with few-shot prompts <Paper corpusId=\"266933290\" paperTitle=\"(Kalajdzievski, 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM",
                            ".Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning",
                            ".For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting",
                            ".We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively."
                        ],
                        "paper": {
                            "corpus_id": 261031244,
                            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                            "authors": [
                                {
                                    "authorId": "2181146702",
                                    "name": "Yun Luo"
                                },
                                {
                                    "authorId": "2149231975",
                                    "name": "Zhen Yang"
                                },
                                {
                                    "authorId": "33427918",
                                    "name": "Fandong Meng"
                                },
                                {
                                    "authorId": "2110450452",
                                    "name": "Yafu Li"
                                },
                                {
                                    "authorId": "48128428",
                                    "name": "Jie Zhou"
                                },
                                {
                                    "authorId": "2167740183",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 318
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Haque, 2025)",
                        "snippets": [
                            "(Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension."
                        ],
                        "paper": {
                            "corpus_id": 277501673,
                            "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks",
                            "authors": [
                                {
                                    "authorId": "2353260836",
                                    "name": "Naimul Haque"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.7470703125
                    },
                    {
                        "id": "(Zheng et al., 2024)",
                        "snippets": [
                            "Main Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data."
                        ],
                        "paper": {
                            "corpus_id": 267636596,
                            "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?",
                            "authors": [
                                {
                                    "authorId": "2809051",
                                    "name": "Junhao Zheng"
                                },
                                {
                                    "authorId": "2273927657",
                                    "name": "Shengjie Qiu"
                                },
                                {
                                    "authorId": "2274027644",
                                    "name": "Qianli Ma"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.69873046875
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."
                        ],
                        "paper": {
                            "corpus_id": 267769989,
                            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2284865968",
                                    "name": "Zhaorui Yang"
                                },
                                {
                                    "authorId": "1409707585",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2285032001",
                                    "name": "Han Wang"
                                },
                                {
                                    "authorId": "46854712",
                                    "name": "H. Feng"
                                },
                                {
                                    "authorId": "145314938",
                                    "name": "Minfeng Zhu"
                                },
                                {
                                    "authorId": "2256716159",
                                    "name": "Wei Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 49
                        },
                        "score": 0.75244140625
                    },
                    {
                        "id": "(Xu et al., 2025)",
                        "snippets": [
                            "We empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024)."
                        ],
                        "paper": {
                            "corpus_id": 276421775,
                            "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs",
                            "authors": [
                                {
                                    "authorId": "26339093",
                                    "name": "Yige Xu"
                                },
                                {
                                    "authorId": "2325014301",
                                    "name": "Xu Guo"
                                },
                                {
                                    "authorId": "49512311",
                                    "name": "Zhiwei Zeng"
                                },
                                {
                                    "authorId": "2238949687",
                                    "name": "Chunyan Miao"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.74072265625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities."
                        ],
                        "paper": {
                            "corpus_id": 270357611,
                            "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning",
                            "authors": [
                                {
                                    "authorId": "2305524549",
                                    "name": "Hongyu Li"
                                },
                                {
                                    "authorId": "46573238",
                                    "name": "Liang Ding"
                                },
                                {
                                    "authorId": "2305485528",
                                    "name": "Meng Fang"
                                },
                                {
                                    "authorId": "2255502438",
                                    "name": "D. Tao"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 19
                        },
                        "score": 0.7431640625
                    },
                    {
                        "id": "(Kalajdzievski, 2024)",
                        "snippets": [
                            "In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts.\n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model."
                        ],
                        "paper": {
                            "corpus_id": 266933290,
                            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
                            "authors": [
                                {
                                    "authorId": "3086897",
                                    "name": "Damjan Kalajdzievski"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 11
                        },
                        "score": 0.79150390625
                    },
                    {
                        "id": "(Pletenev et al., 2025)",
                        "snippets": [
                            "However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2023)Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark."
                        ],
                        "paper": {
                            "corpus_id": 276482659,
                            "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?",
                            "authors": [
                                {
                                    "authorId": "2282523455",
                                    "name": "Sergey Pletenev"
                                },
                                {
                                    "authorId": "2346325484",
                                    "name": "Maria Marina"
                                },
                                {
                                    "authorId": "2268318729",
                                    "name": "Daniil Moskovskiy"
                                },
                                {
                                    "authorId": "2329735486",
                                    "name": "Vasily Konovalov"
                                },
                                {
                                    "authorId": "2346329178",
                                    "name": "Pavel Braslavski"
                                },
                                {
                                    "authorId": "2282523462",
                                    "name": "Alexander Panchenko"
                                },
                                {
                                    "authorId": "2253458749",
                                    "name": "M. Salnikov"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.70068359375
                    },
                    {
                        "id": "(Aleixo et al., 2023)",
                        "snippets": [
                            "Deep Learning models have achieved remarkable performance in tasks such as image classification or generation, often surpassing human accuracy. However, they can struggle to learn new tasks and update their knowledge without access to previous data, leading to a significant loss of accuracy known as Catastrophic Forgetting (CF). This phenomenon was first observed by McCloskey and Cohen in 1989 and remains an active research topic. Incremental learning without forgetting is widely recognized as a crucial aspect in building better AI systems, as it allows models to adapt to new tasks without losing the ability to perform previously learned ones. This article surveys recent studies that tackle CF in modern Deep Learning models that use gradient descent as their learning algorithm. Although several solutions have been proposed, a definitive solution or consensus on assessing CF is yet to be established. The article provides a comprehensive review of recent solutions, proposes a taxonomy to organize them, and identifies research gaps in this area."
                        ],
                        "paper": {
                            "corpus_id": 266348792,
                            "title": "Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy",
                            "authors": [
                                {
                                    "authorId": "2082603039",
                                    "name": "Everton L. Aleixo"
                                },
                                {
                                    "authorId": "1966519",
                                    "name": "J. Colonna"
                                },
                                {
                                    "authorId": "2275055914",
                                    "name": "Marco Cristo"
                                },
                                {
                                    "authorId": "2275056709",
                                    "name": "Everlandio Fernandes"
                                }
                            ],
                            "year": 2023,
                            "venue": "Journal of the Brazilian Computer Society",
                            "n_citations": 26
                        },
                        "score": 0
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "(Li et al., 2023) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen."
                        ],
                        "paper": {
                            "corpus_id": 278327022,
                            "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2217529598",
                                    "name": "Jinpeng Chen"
                                },
                                {
                                    "authorId": "2258230512",
                                    "name": "Runmin Cong"
                                },
                                {
                                    "authorId": "2334518513",
                                    "name": "Yuzhi Zhao"
                                },
                                {
                                    "authorId": "2359170282",
                                    "name": "Hongzheng Yang"
                                },
                                {
                                    "authorId": "2337369321",
                                    "name": "Guangneng Hu"
                                },
                                {
                                    "authorId": "2221824976",
                                    "name": "H. Ip"
                                },
                                {
                                    "authorId": "2287942988",
                                    "name": "Sam Kwong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.791015625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability."
                        ],
                        "paper": {
                            "corpus_id": 260334888,
                            "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
                            "authors": [
                                {
                                    "authorId": "1491798621",
                                    "name": "Bohao Li"
                                },
                                {
                                    "authorId": "2151036422",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "47226701",
                                    "name": "Guangzhi Wang"
                                },
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "1387190008",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 571
                        },
                        "score": 0
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact on Specific Capabilities and Knowledge",
                "tldr": "Fine-tuning LLMs for specific tasks leads to degradation across multiple capability dimensions, with particularly severe impacts on reasoning abilities, knowledge retention, output formatting, and safety guardrails. (17 sources)",
                "text": "\nResearch has identified several specific capabilities and knowledge areas that experience significant degradation during fine-tuning:\n\n* **Reasoning and problem-solving abilities**: LLMs show marked decline in logical reasoning capabilities after being fine-tuned on narrow tasks, undermining their ability to tackle complex problems <Paper corpusId=\"268510177\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275820535\" paperTitle=\"(Song et al., 2025)\" isShortName></Paper>.\n\n* **Knowledge retention**: Fine-tuning can distort the pre-training features that encode factual knowledge, resulting in reduced capacity to access previously learned information <Paper corpusId=\"264720747\" paperTitle=\"(Hajipour et al., 2022)\" isShortName></Paper> <Paper corpusId=\"216553067\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>.\n\n* **Output formatting capabilities**: Models fine-tuned on specific formats can lose their flexibility in generating different types of outputs, leading to rigid response patterns that may not match user expectations <Paper corpusId=\"266844262\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\n* **Domain generalization**: When LLMs are tuned for domain-specific tasks, they often experience severe performance degradation on tasks from other domains <Paper corpusId=\"268247412\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270068369\" paperTitle=\"(Contal et al., 2024)\" isShortName></Paper>.\n\n* **Instruction-following capability**: PPO fine-tuning for specific objectives can impair a model's general ability to follow diverse instructions <Paper corpusId=\"270559716\" paperTitle=\"(Hu et al., 2024)\" isShortName></Paper>.\n\n* **Cross-lingual capabilities**: Fine-tuning predominantly on English data can diminish performance on non-English tasks, suggesting a language-specific aspect to catastrophic forgetting <Paper corpusId=\"270562404\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"262054014\" paperTitle=\"(Kotha et al., 2023)\" isShortName></Paper>.\n\n* **Task versatility**: Models fine-tuned on single tasks experience significant deterioration in previously mastered tasks, creating a direct trade-off between specialization and generalization <Paper corpusId=\"271244634\" paperTitle=\"(Roffo, 2024)\" isShortName></Paper>.\n\n* **Safety alignment**: Fine-tuning can erode safety guardrails implemented during alignment, potentially leading to unsafe outputs when prompted adversarially <Paper corpusId=\"259951356\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266374841\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\n* **Knowledge uncertainty expression**: A critical capability that degrades during fine-tuning is the model's ability to express ignorance when appropriate, which increases the risk of hallucinations <Paper corpusId=\"279410046\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>.\n\n* **Representational collapse**: Fine-tuning can collapse the rich, generalizable representations from pre-training into a narrower set focused only on the immediate task, limiting transfer to new contexts <Paper corpusId=\"221083147\" paperTitle=\"(Aghajanyan et al., 2020)\" isShortName></Paper>.\n\n* **Out-of-distribution performance**: While fine-tuning often improves in-distribution task performance, it can substantially degrade performance on out-of-distribution examples, compromising model robustness <Paper corpusId=\"264720747\" paperTitle=\"(Hajipour et al., 2022)\" isShortName></Paper> <Paper corpusId=\"247011290\" paperTitle=\"(Kumar et al., 2022)\" isShortName></Paper>.\n\n* **In-context learning**: The ability to learn from examples provided in the prompt can diminish after instruction tuning, particularly affecting zero-shot and few-shot learning capabilities <Paper corpusId=\"270562404\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237416585\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(He et al., 2024)",
                        "snippets": [
                            "However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded."
                        ],
                        "paper": {
                            "corpus_id": 268510177,
                            "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2261355905",
                                    "name": "Yongquan He"
                                },
                                {
                                    "authorId": "2292011040",
                                    "name": "Xuancheng Huang"
                                },
                                {
                                    "authorId": "2261366844",
                                    "name": "Minghao Tang"
                                },
                                {
                                    "authorId": "2292412508",
                                    "name": "Lingxun Meng"
                                },
                                {
                                    "authorId": "2292290608",
                                    "name": "Xiang Li"
                                },
                                {
                                    "authorId": "2291964148",
                                    "name": "Wei Lin"
                                },
                                {
                                    "authorId": "2108347848",
                                    "name": "Wenyuan Zhang"
                                },
                                {
                                    "authorId": "2118545089",
                                    "name": "Yifu Gao"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.62451171875
                    },
                    {
                        "id": "(Song et al., 2025)",
                        "snippets": [
                            "As shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering."
                        ],
                        "paper": {
                            "corpus_id": 275820535,
                            "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization",
                            "authors": [
                                {
                                    "authorId": "2152569963",
                                    "name": "Shezheng Song"
                                },
                                {
                                    "authorId": "2341857958",
                                    "name": "Hao Xu"
                                },
                                {
                                    "authorId": "2152610427",
                                    "name": "Jun Ma"
                                },
                                {
                                    "authorId": "1484986130",
                                    "name": "Shasha Li"
                                },
                                {
                                    "authorId": "2154357858",
                                    "name": "Long Peng"
                                },
                                {
                                    "authorId": "2304446070",
                                    "name": "Qian Wan"
                                },
                                {
                                    "authorId": "2110779334",
                                    "name": "Xiaodong Liu"
                                },
                                {
                                    "authorId": "2116457427",
                                    "name": "Jie Yu"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.736328125
                    },
                    {
                        "id": "(Hajipour et al., 2022)",
                        "snippets": [
                            "Despite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;(Chen et al., 2020). Furthermore, (Kumar et al., 2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks."
                        ],
                        "paper": {
                            "corpus_id": 264720747,
                            "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models",
                            "authors": [
                                {
                                    "authorId": "3021793",
                                    "name": "Hossein Hajipour"
                                },
                                {
                                    "authorId": "2263370859",
                                    "name": "Ning Yu"
                                },
                                {
                                    "authorId": "40141524",
                                    "name": "Cristian-Alexandru Staicu"
                                },
                                {
                                    "authorId": "2260336471",
                                    "name": "Mario Fritz"
                                }
                            ],
                            "year": 2022,
                            "venue": "NAACL-HLT",
                            "n_citations": 5
                        },
                        "score": 0.62890625
                    },
                    {
                        "id": "(Chen et al., 2020)",
                        "snippets": [
                            "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community."
                        ],
                        "paper": {
                            "corpus_id": 216553067,
                            "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting",
                            "authors": [
                                {
                                    "authorId": "2107970655",
                                    "name": "Sanyuan Chen"
                                },
                                {
                                    "authorId": "8471176",
                                    "name": "Yutai Hou"
                                },
                                {
                                    "authorId": "3043830",
                                    "name": "Yiming Cui"
                                },
                                {
                                    "authorId": "2256319",
                                    "name": "Wanxiang Che"
                                },
                                {
                                    "authorId": "40282288",
                                    "name": "Ting Liu"
                                },
                                {
                                    "authorId": "2218115449",
                                    "name": "Xiangzhan Yu"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 225
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue."
                        ],
                        "paper": {
                            "corpus_id": 266844262,
                            "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2186278878",
                                    "name": "Chen-An Li"
                                },
                                {
                                    "authorId": "2278588523",
                                    "name": "Hung-yi Lee"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.7939453125
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency."
                        ],
                        "paper": {
                            "corpus_id": 268247412,
                            "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2151036536",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "2248150493",
                                    "name": "Fei Mi"
                                },
                                {
                                    "authorId": "2165302640",
                                    "name": "Yi Chen"
                                },
                                {
                                    "authorId": "2248039956",
                                    "name": "Boyang Xue"
                                },
                                {
                                    "authorId": "22642319",
                                    "name": "Hongru Wang"
                                },
                                {
                                    "authorId": "2269768949",
                                    "name": "Qi Zhu"
                                },
                                {
                                    "authorId": "2274098358",
                                    "name": "Kam-Fai Wong"
                                },
                                {
                                    "authorId": "8233941",
                                    "name": "Rui-Lan Xu"
                                }
                            ],
                            "year": 2024,
                            "venue": "NAACL-HLT",
                            "n_citations": 7
                        },
                        "score": 0.68212890625
                    },
                    {
                        "id": "(Contal et al., 2024)",
                        "snippets": [
                            "Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data."
                        ],
                        "paper": {
                            "corpus_id": 270068369,
                            "title": "RAGSys: Item-Cold-Start Recommender as RAG System",
                            "authors": [
                                {
                                    "authorId": "2303471987",
                                    "name": "Emile Contal"
                                },
                                {
                                    "authorId": "2283934772",
                                    "name": "Garrin McGoldrick"
                                }
                            ],
                            "year": 2024,
                            "venue": "IR-RAG@SIGIR",
                            "n_citations": 5
                        },
                        "score": 0.61376953125
                    },
                    {
                        "id": "(Hu et al., 2024)",
                        "snippets": [
                            "PPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement."
                        ],
                        "paper": {
                            "corpus_id": 270559716,
                            "title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2249778653",
                                    "name": "Minda Hu"
                                },
                                {
                                    "authorId": "2121435800",
                                    "name": "Licheng Zong"
                                },
                                {
                                    "authorId": "22642319",
                                    "name": "Hongru Wang"
                                },
                                {
                                    "authorId": "30887444",
                                    "name": "Jingyan Zhou"
                                },
                                {
                                    "authorId": "2274072714",
                                    "name": "Jingjing Li"
                                },
                                {
                                    "authorId": "2307592921",
                                    "name": "Yichen Gao"
                                },
                                {
                                    "authorId": "2237563835",
                                    "name": "Kam-Fai Wong"
                                },
                                {
                                    "authorId": "2307192846",
                                    "name": "Yu Li"
                                },
                                {
                                    "authorId": "2258549144",
                                    "name": "Irwin King"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 3
                        },
                        "score": 0.67529296875
                    },
                    {
                        "id": "(Jiang et al., 2024)",
                        "snippets": [
                            "Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2021)Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;(Kotha et al., 2023)Wu et al., 2024b)."
                        ],
                        "paper": {
                            "corpus_id": 270562404,
                            "title": "Refine Large Language Model Fine-tuning via Instruction Vector",
                            "authors": [
                                {
                                    "authorId": "66129075",
                                    "name": "Gangwei Jiang"
                                },
                                {
                                    "authorId": "2273744210",
                                    "name": "Zhaoyi Li"
                                },
                                {
                                    "authorId": "2266241113",
                                    "name": "Defu Lian"
                                },
                                {
                                    "authorId": "2261250492",
                                    "name": "Ying Wei"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 1
                        },
                        "score": 0.7373046875
                    },
                    {
                        "id": "(Kotha et al., 2023)",
                        "snippets": [
                            "We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT."
                        ],
                        "paper": {
                            "corpus_id": 262054014,
                            "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference",
                            "authors": [
                                {
                                    "authorId": "2150573121",
                                    "name": "Suhas Kotha"
                                },
                                {
                                    "authorId": "153481934",
                                    "name": "Jacob Mitchell Springer"
                                },
                                {
                                    "authorId": "2655157",
                                    "name": "Aditi Raghunathan"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 71
                        },
                        "score": 0
                    },
                    {
                        "id": "(Roffo, 2024)",
                        "snippets": [
                            "Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27]."
                        ],
                        "paper": {
                            "corpus_id": 271244634,
                            "title": "Exploring Advanced Large Language Models with LLMsuite",
                            "authors": [
                                {
                                    "authorId": "2311696660",
                                    "name": "Giorgio Roffo"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.61279296875
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Zhang et al. (Zhang et al., 2024) conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text",
                            "recent research (Zhao et al., 2023) highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter (Zhao et al., 2023) introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 259951356,
                            "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning",
                            "authors": [
                                {
                                    "authorId": "2920297",
                                    "name": "Zhenyi Wang"
                                },
                                {
                                    "authorId": "151497321",
                                    "name": "Enneng Yang"
                                },
                                {
                                    "authorId": "2172820082",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2151748239",
                                    "name": "Heng Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 51
                        },
                        "score": 0.62060546875
                    },
                    {
                        "id": "(Zhao et al., 2023)",
                        "snippets": [
                            "Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten."
                        ],
                        "paper": {
                            "corpus_id": 266374841,
                            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2275190738",
                                    "name": "Jiachen Zhao"
                                },
                                {
                                    "authorId": "2275284008",
                                    "name": "Zhun Deng"
                                },
                                {
                                    "authorId": "40373515",
                                    "name": "David Madras"
                                },
                                {
                                    "authorId": "2276186660",
                                    "name": "James Zou"
                                },
                                {
                                    "authorId": "2540599",
                                    "name": "Mengye Ren"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 18
                        },
                        "score": 0.70703125
                    },
                    {
                        "id": "(Shen et al., 2025)",
                        "snippets": [
                            "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations."
                        ],
                        "paper": {
                            "corpus_id": 279410046,
                            "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "William F. Shen"
                                },
                                {
                                    "authorId": "1702997626",
                                    "name": "Xinchi Qiu"
                                },
                                {
                                    "authorId": "2313189467",
                                    "name": "Nicola Cancedda"
                                },
                                {
                                    "authorId": "2298756346",
                                    "name": "N. Lane"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Aghajanyan et al., 2020)",
                        "snippets": [
                            "Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989).\n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set."
                        ],
                        "paper": {
                            "corpus_id": 221083147,
                            "title": "Better Fine-Tuning by Reducing Representational Collapse",
                            "authors": [
                                {
                                    "authorId": "2201435",
                                    "name": "Armen Aghajanyan"
                                },
                                {
                                    "authorId": "1519979046",
                                    "name": "Akshat Shrivastava"
                                },
                                {
                                    "authorId": "3377939",
                                    "name": "Anchit Gupta"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "2701297",
                                    "name": "S. Gupta"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 210
                        },
                        "score": 0.75439453125
                    },
                    {
                        "id": "(Kumar et al., 2022)",
                        "snippets": [
                            "When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer\u2014the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \u2192 STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head\u2014this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning)."
                        ],
                        "paper": {
                            "corpus_id": 247011290,
                            "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
                            "authors": [
                                {
                                    "authorId": "32423266",
                                    "name": "Ananya Kumar"
                                },
                                {
                                    "authorId": "2655157",
                                    "name": "Aditi Raghunathan"
                                },
                                {
                                    "authorId": "48368647",
                                    "name": "Robbie Jones"
                                },
                                {
                                    "authorId": "2114186424",
                                    "name": "Tengyu Ma"
                                },
                                {
                                    "authorId": "145419642",
                                    "name": "Percy Liang"
                                }
                            ],
                            "year": 2022,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 685
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wei et al., 2021)",
                        "snippets": [
                            "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."
                        ],
                        "paper": {
                            "corpus_id": 237416585,
                            "title": "Finetuned Language Models Are Zero-Shot Learners",
                            "authors": [
                                {
                                    "authorId": "144026731",
                                    "name": "Jason Wei"
                                },
                                {
                                    "authorId": "40377863",
                                    "name": "Maarten Bosma"
                                },
                                {
                                    "authorId": "2664737",
                                    "name": "Vincent Zhao"
                                },
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "40625240",
                                    "name": "Adams Wei Yu"
                                },
                                {
                                    "authorId": "144104130",
                                    "name": "Brian Lester"
                                },
                                {
                                    "authorId": "2140321952",
                                    "name": "Nan Du"
                                },
                                {
                                    "authorId": "2555924",
                                    "name": "Andrew M. Dai"
                                },
                                {
                                    "authorId": "2827616",
                                    "name": "Quoc V. Le"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 3788
                        },
                        "score": 0
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Factors Influencing Catastrophic Forgetting",
                "tldr": "Multiple factors influence the severity of catastrophic forgetting in LLMs, including model size, fine-tuning dataset characteristics, and optimization dynamics. Research shows that larger models may experience more severe forgetting, while training data composition and loss landscape sharpness also play significant roles. (5 sources)",
                "text": "\nSeveral key factors have been identified as influencing the severity and nature of catastrophic forgetting in Large Language Models during fine-tuning:\n\n**Model size and architecture**: Contrary to what might be expected, evidence suggests that larger models may experience more severe forgetting. Luo et al. found that as model scale increases from 1B to 7B parameters, the severity of forgetting intensifies, possibly because larger models with stronger initial performance have more room for degradation <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This pattern challenges the assumption that scale inherently provides robustness against forgetting.\n\n**Fine-tuning data characteristics**: The composition, diversity, and distribution of fine-tuning datasets significantly impact forgetting patterns. When there's substantial distribution shift between the pre-training data and fine-tuning data, models are more likely to experience severe capability degradation <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This is particularly evident when fine-tuning on narrowly focused datasets that differ significantly from the broad distribution of pre-training data.\n\n**Training dynamics and optimization**: The loss landscape dynamics during fine-tuning play a crucial role in forgetting. Research shows that catastrophic forgetting becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between loss landscape sharpness and forgetting severity <Paper corpusId=\"278327022\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"260334888\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This indicates that optimization strategies that maintain flatter loss landscapes might help mitigate forgetting.\n\n**Task sequence and continual learning settings**: The order and manner in which new tasks are introduced affect forgetting patterns. As more instruction tasks are introduced sequentially, general knowledge suffers increasingly significant forgetting <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This suggests that careful task ordering and interleaving might help preserve capabilities.\n\n**Hyperparameter choices**: Fine-tuning hyperparameters, particularly learning rate, significantly impact forgetting. Existing studies have shown that regulating learning rates and minimizing overall loss can mitigate forgetting to some extent <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>. However, these approaches alone are often insufficient to fully preserve original capabilities.\n\n**Modality interactions**: In multimodal models, catastrophic forgetting can manifest through shifted attention between modalities. During fine-tuning, models may shift from general visual-text alignment to dataset-specific patterns, resulting in performance degradation even when parts of the architecture (like the vision encoder) remain frozen <Paper corpusId=\"278327022\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\n**Balance between old and new knowledge**: The repetition frequency of previously learned examples and the ratio of new to old tasks in fine-tuning datasets can help mitigate forgetting. Appropriate settings for sample repetition and balanced task ratios have shown some success in preserving earlier capabilities <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\nUnderstanding these factors is crucial for developing effective strategies to mitigate catastrophic forgetting while still achieving the desired improvements from fine-tuning.",
                "citations": [
                    {
                        "id": "(Luo et al., 2023)",
                        "snippets": [
                            "The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM",
                            ".Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning",
                            ".For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting",
                            ".We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively."
                        ],
                        "paper": {
                            "corpus_id": 261031244,
                            "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
                            "authors": [
                                {
                                    "authorId": "2181146702",
                                    "name": "Yun Luo"
                                },
                                {
                                    "authorId": "2149231975",
                                    "name": "Zhen Yang"
                                },
                                {
                                    "authorId": "33427918",
                                    "name": "Fandong Meng"
                                },
                                {
                                    "authorId": "2110450452",
                                    "name": "Yafu Li"
                                },
                                {
                                    "authorId": "48128428",
                                    "name": "Jie Zhou"
                                },
                                {
                                    "authorId": "2167740183",
                                    "name": "Yue Zhang"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 318
                        },
                        "score": 0.93212890625
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."
                        ],
                        "paper": {
                            "corpus_id": 267769989,
                            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2284865968",
                                    "name": "Zhaorui Yang"
                                },
                                {
                                    "authorId": "1409707585",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2285032001",
                                    "name": "Han Wang"
                                },
                                {
                                    "authorId": "46854712",
                                    "name": "H. Feng"
                                },
                                {
                                    "authorId": "145314938",
                                    "name": "Minfeng Zhu"
                                },
                                {
                                    "authorId": "2256716159",
                                    "name": "Wei Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 49
                        },
                        "score": 0.75244140625
                    },
                    {
                        "id": "(Chen et al., 2025)",
                        "snippets": [
                            "(Li et al., 2023) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen."
                        ],
                        "paper": {
                            "corpus_id": 278327022,
                            "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning",
                            "authors": [
                                {
                                    "authorId": "2217529598",
                                    "name": "Jinpeng Chen"
                                },
                                {
                                    "authorId": "2258230512",
                                    "name": "Runmin Cong"
                                },
                                {
                                    "authorId": "2334518513",
                                    "name": "Yuzhi Zhao"
                                },
                                {
                                    "authorId": "2359170282",
                                    "name": "Hongzheng Yang"
                                },
                                {
                                    "authorId": "2337369321",
                                    "name": "Guangneng Hu"
                                },
                                {
                                    "authorId": "2221824976",
                                    "name": "H. Ip"
                                },
                                {
                                    "authorId": "2287942988",
                                    "name": "Sam Kwong"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.791015625
                    },
                    {
                        "id": "(Li et al., 2023)",
                        "snippets": [
                            "Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability."
                        ],
                        "paper": {
                            "corpus_id": 260334888,
                            "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension",
                            "authors": [
                                {
                                    "authorId": "1491798621",
                                    "name": "Bohao Li"
                                },
                                {
                                    "authorId": "2151036422",
                                    "name": "Rui Wang"
                                },
                                {
                                    "authorId": "47226701",
                                    "name": "Guangzhi Wang"
                                },
                                {
                                    "authorId": "51123495",
                                    "name": "Yuying Ge"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "1387190008",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 571
                        },
                        "score": 0
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "Selective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes.\n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter."
                        ],
                        "paper": {
                            "corpus_id": 265158118,
                            "title": "Vision-Language Instruction Tuning: A Review and Analysis",
                            "authors": [
                                {
                                    "authorId": "2256784925",
                                    "name": "Chen Li"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2243307404",
                                    "name": "Dian Li"
                                },
                                {
                                    "authorId": "2265579883",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 11
                        },
                        "score": 0.62451171875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Safety and Alignment Implications",
                "tldr": "Catastrophic forgetting poses significant challenges for LLM safety, as fine-tuning can inadvertently compromise carefully implemented safety guardrails and alignment features, leading to models that may produce harmful outputs or fail to express uncertainty appropriately. (6 sources)",
                "text": "\nThe interplay between catastrophic forgetting and safety alignment represents a critical concern in LLM development. Research has revealed that even when models are initially well-aligned for safety, subsequent fine-tuning\u2014including on seemingly benign datasets\u2014can significantly erode these safety guardrails. Yang et al. found that distribution shifts during fine-tuning can lead to degradation not only in general task capabilities but also in models' safety alignment and helpfulness <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\nThis safety degradation manifests in multiple ways. Adversarial testing or \"red teaming\" has shown that safety mechanisms can be bypassed after fine-tuning, and safety metrics can deteriorate even when models are subsequently fine-tuned on benign downstream tasks <Paper corpusId=\"274982256\" paperTitle=\"(Ung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246634238\" paperTitle=\"(Perez et al., 2022)\" isShortName></Paper>. Zhao et al. demonstrated that while sequential safety fine-tuning can efficiently recover some safety aspects, it leads to catastrophic forgetting where both unsafe and important downstream examples are forgotten <Paper corpusId=\"266374841\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nA particularly concerning manifestation of catastrophic forgetting in safety contexts is the degradation of a model's ability to express ignorance when appropriate. Shen et al. found that this capability is significantly impaired during conventional fine-tuning, which can lead to increased hallucinations\u2014a serious safety issue when models generate fabricated information with high confidence <Paper corpusId=\"279410046\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>.\n\nThe bidirectional nature of safety and forgetting presents additional challenges. Wang et al. noted that aligned LLMs can readily learn unsafe content during custom fine-tuning, though they tend to forget this unsafe content more significantly than other examples when subsequently fine-tuned on safer content <Paper corpusId=\"259951356\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266374841\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. This inconsistent retention pattern complicates safety assurance across the model's development lifecycle.\n\nThese findings highlight the delicate balance required when fine-tuning LLMs: improvements in specific capabilities or task performance must be weighed against potential degradation in safety alignment. The phenomenon suggests that safety is not a static property that can be permanently embedded but rather a dynamic characteristic that requires continuous monitoring and maintenance throughout a model's development and deployment lifecycle.",
                "citations": [
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."
                        ],
                        "paper": {
                            "corpus_id": 267769989,
                            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2284865968",
                                    "name": "Zhaorui Yang"
                                },
                                {
                                    "authorId": "1409707585",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2285032001",
                                    "name": "Han Wang"
                                },
                                {
                                    "authorId": "46854712",
                                    "name": "H. Feng"
                                },
                                {
                                    "authorId": "145314938",
                                    "name": "Minfeng Zhu"
                                },
                                {
                                    "authorId": "2256716159",
                                    "name": "Wei Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 49
                        },
                        "score": 0.75244140625
                    },
                    {
                        "id": "(Ung et al., 2024)",
                        "snippets": [
                            "Catastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;(Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2013)Kirkpatrick et al., 2016;(Kemker et al., 2017). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;(Razdaibiedina et al., 2023)Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 274982256,
                            "title": "Chained Tuning Leads to Biased Forgetting",
                            "authors": [
                                {
                                    "authorId": "2268761197",
                                    "name": "Megan Ung"
                                },
                                {
                                    "authorId": "2336870239",
                                    "name": "Alicia Sun"
                                },
                                {
                                    "authorId": "2330399051",
                                    "name": "Samuel J. Bell"
                                },
                                {
                                    "authorId": "2219919981",
                                    "name": "Bhaktipriya Radharapu"
                                },
                                {
                                    "authorId": "2253337895",
                                    "name": "Levent Sagun"
                                },
                                {
                                    "authorId": "2330433992",
                                    "name": "Adina Williams"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.85400390625
                    },
                    {
                        "id": "(Perez et al., 2022)",
                        "snippets": [
                            "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\"red teaming\") using another LM. We evaluate the target LM\u2019s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot\u2019s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users."
                        ],
                        "paper": {
                            "corpus_id": 246634238,
                            "title": "Red Teaming Language Models with Language Models",
                            "authors": [
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "2059836321",
                                    "name": "Francis Song"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "9958912",
                                    "name": "John Aslanides"
                                },
                                {
                                    "authorId": "2105840001",
                                    "name": "Amelia Glaese"
                                },
                                {
                                    "authorId": "147687624",
                                    "name": "Nat McAleese"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                }
                            ],
                            "year": 2022,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 668
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhao et al., 2023)",
                        "snippets": [
                            "Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten."
                        ],
                        "paper": {
                            "corpus_id": 266374841,
                            "title": "Learning and Forgetting Unsafe Examples in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2275190738",
                                    "name": "Jiachen Zhao"
                                },
                                {
                                    "authorId": "2275284008",
                                    "name": "Zhun Deng"
                                },
                                {
                                    "authorId": "40373515",
                                    "name": "David Madras"
                                },
                                {
                                    "authorId": "2276186660",
                                    "name": "James Zou"
                                },
                                {
                                    "authorId": "2540599",
                                    "name": "Mengye Ren"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 18
                        },
                        "score": 0.70703125
                    },
                    {
                        "id": "(Shen et al., 2025)",
                        "snippets": [
                            "Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations."
                        ],
                        "paper": {
                            "corpus_id": 279410046,
                            "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": null,
                                    "name": "William F. Shen"
                                },
                                {
                                    "authorId": "1702997626",
                                    "name": "Xinchi Qiu"
                                },
                                {
                                    "authorId": "2313189467",
                                    "name": "Nicola Cancedda"
                                },
                                {
                                    "authorId": "2298756346",
                                    "name": "N. Lane"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Wang et al., 2023)",
                        "snippets": [
                            "Zhang et al. (Zhang et al., 2024) conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text",
                            "recent research (Zhao et al., 2023) highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter (Zhao et al., 2023) introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs."
                        ],
                        "paper": {
                            "corpus_id": 259951356,
                            "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning",
                            "authors": [
                                {
                                    "authorId": "2920297",
                                    "name": "Zhenyi Wang"
                                },
                                {
                                    "authorId": "151497321",
                                    "name": "Enneng Yang"
                                },
                                {
                                    "authorId": "2172820082",
                                    "name": "Li Shen"
                                },
                                {
                                    "authorId": "2151748239",
                                    "name": "Heng Huang"
                                }
                            ],
                            "year": 2023,
                            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                            "n_citations": 51
                        },
                        "score": 0.62060546875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Mitigation Approaches",
                "tldr": "Researchers have developed several approaches to mitigate catastrophic forgetting in LLMs, including replay-based methods, selective distillation, parameter-efficient fine-tuning, and specialized optimization techniques that maintain model capabilities while adding new knowledge. (10 sources)",
                "text": "\n* **Replay-based methods**: These approaches incorporate examples from previous tasks during fine-tuning to maintain earlier capabilities. Recent advances in replay-based approaches for language models have shown promise in reducing catastrophic forgetting by rehearsing prior task data during training <Paper corpusId=\"276287820\" paperTitle=\"(Bhushan et al., 2025)\" isShortName></Paper>.\n\n* **Selective Distillation Fine-Tuning (SDFT)**: This technique has demonstrated effectiveness in preserving model capabilities while achieving comparable or superior performance on downstream tasks compared to vanilla fine-tuning. For instance, while vanilla fine-tuning on the OpenFunctions dataset led to a 27% decline in coding ability on the HumanEval benchmark, SDFT not only mitigated this degradation but marginally enhanced accuracy from 13.4 to 15.2 <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n* **Learning rate regulation**: Controlling the learning rate during fine-tuning can help reduce catastrophic forgetting by limiting the extent of parameter updates. Existing studies have shown that regulating learning rates and minimizing overall loss can mitigate forgetting to some extent <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\n* **Sample repetition and task balancing**: Appropriately setting the repetition frequency of previously learned examples and balancing the ratio of new to old tasks in fine-tuning datasets can help preserve earlier capabilities. The training data in instructional tuning can be equipped with these settings to mitigate forgetting to a certain extent <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\n* **Parameter-efficient fine-tuning methods**: These approaches modify only a subset of model parameters during fine-tuning, which can help preserve general capabilities while adding new knowledge. However, even these methods face challenges with catastrophic forgetting, requiring additional techniques for mitigation <Paper corpusId=\"273345395\" paperTitle=\"(Thangarasa et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249921347\" paperTitle=\"(Korbak et al., 2021)\" isShortName></Paper>.\n\n* **Domain-specific adaptation frameworks**: Researchers have developed decision frameworks to guide the selection of appropriate LLM adaptation techniques based on use case constraints around data, compute, and performance needs, which can help balance specialization with versatility <Paper corpusId=\"276574830\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265294420\" paperTitle=\"(Li et al._2, 2023)\" isShortName></Paper>.\n\n* **Conditional Distributional Policy Gradients (CDPG)**: This approach extends Distributional Policy Gradients to conditional tasks, allowing fine-tuning that moves pretrained models closer to meeting control objectives without resulting in catastrophic forgetting, unlike baseline approaches <Paper corpusId=\"249921347\" paperTitle=\"(Korbak et al., 2021)\" isShortName></Paper>.\n\n* **Multimodal alignment preservation**: For multimodal LLMs, careful management of the fine-tuning process is needed to prevent the model from shifting its attention between modalities in ways that cause performance degradation. Early-stage fine-tuning on image datasets can improve performance across other image datasets by enhancing the alignment of text and visual features, but extended fine-tuning must be carefully controlled to avoid hallucination <Paper corpusId=\"267657819\" paperTitle=\"(Wo'zniak et al., 2024)\" isShortName></Paper> <Paper corpusId=\"262055661\" paperTitle=\"(Zhai et al., 2023)\" isShortName></Paper>.\n\n* **Safety-preserving fine-tuning**: Specialized techniques that explicitly maintain safety alignment during fine-tuning are emerging as research demonstrates that even fine-tuning with benign datasets can compromise the safety of seed LLMs <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n* **Continual learning approaches**: Methods that enable models to continuously learn new tasks without forgetting previous knowledge are being adapted for LLMs. These approaches are particularly important as LLMs are increasingly deployed in dynamic environments requiring ongoing adaptation <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Bhushan et al., 2025)",
                        "snippets": [
                            "Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting."
                        ],
                        "paper": {
                            "corpus_id": 276287820,
                            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                            "authors": [
                                {
                                    "authorId": "2256382094",
                                    "name": "Kushagra Bhushan"
                                },
                                {
                                    "authorId": "1392630568",
                                    "name": "Yatin Nandwani"
                                },
                                {
                                    "authorId": "2345003462",
                                    "name": "Dinesh Khandelwal"
                                },
                                {
                                    "authorId": "2320314900",
                                    "name": "Sonam Gupta"
                                },
                                {
                                    "authorId": "2345005348",
                                    "name": "Gaurav Pandey"
                                },
                                {
                                    "authorId": "1916865",
                                    "name": "Dinesh Raghu"
                                },
                                {
                                    "authorId": "2243011716",
                                    "name": "Sachindra Joshi"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 2
                        },
                        "score": 0.61474609375
                    },
                    {
                        "id": "(Yang et al., 2024)",
                        "snippets": [
                            "Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."
                        ],
                        "paper": {
                            "corpus_id": 267769989,
                            "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2284865968",
                                    "name": "Zhaorui Yang"
                                },
                                {
                                    "authorId": "1409707585",
                                    "name": "Qian Liu"
                                },
                                {
                                    "authorId": "19201674",
                                    "name": "Tianyu Pang"
                                },
                                {
                                    "authorId": "2285032001",
                                    "name": "Han Wang"
                                },
                                {
                                    "authorId": "46854712",
                                    "name": "H. Feng"
                                },
                                {
                                    "authorId": "145314938",
                                    "name": "Minfeng Zhu"
                                },
                                {
                                    "authorId": "2256716159",
                                    "name": "Wei Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 49
                        },
                        "score": 0.75244140625
                    },
                    {
                        "id": "(Li et al._1, 2023)",
                        "snippets": [
                            "Selective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes.\n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter."
                        ],
                        "paper": {
                            "corpus_id": 265158118,
                            "title": "Vision-Language Instruction Tuning: A Review and Analysis",
                            "authors": [
                                {
                                    "authorId": "2256784925",
                                    "name": "Chen Li"
                                },
                                {
                                    "authorId": "152988335",
                                    "name": "Yixiao Ge"
                                },
                                {
                                    "authorId": "2243307404",
                                    "name": "Dian Li"
                                },
                                {
                                    "authorId": "2265579883",
                                    "name": "Ying Shan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Trans. Mach. Learn. Res.",
                            "n_citations": 11
                        },
                        "score": 0.62451171875
                    },
                    {
                        "id": "(Thangarasa et al., 2024)",
                        "snippets": [
                            "One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;(Korbak et al., 2021)."
                        ],
                        "paper": {
                            "corpus_id": 273345395,
                            "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models",
                            "authors": [
                                {
                                    "authorId": "51153332",
                                    "name": "Vithursan Thangarasa"
                                },
                                {
                                    "authorId": "2325876819",
                                    "name": "Ganesh Venkatesh"
                                },
                                {
                                    "authorId": "2325902410",
                                    "name": "Nish Sinnadurai"
                                },
                                {
                                    "authorId": "2212029838",
                                    "name": "Sean Lie"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.640625
                    },
                    {
                        "id": "(Korbak et al., 2021)",
                        "snippets": [
                            "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting."
                        ],
                        "paper": {
                            "corpus_id": 249921347,
                            "title": "Controlling Conditional Language Models without Catastrophic Forgetting",
                            "authors": [
                                {
                                    "authorId": "30023177",
                                    "name": "Tomasz Korbak"
                                },
                                {
                                    "authorId": "2218938",
                                    "name": "Hady ElSahar"
                                },
                                {
                                    "authorId": "2067996",
                                    "name": "Germ\u00e1n Kruszewski"
                                },
                                {
                                    "authorId": "2954698",
                                    "name": "Marc Dymetman"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 34
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Despite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c)."
                        ],
                        "paper": {
                            "corpus_id": 276574830,
                            "title": "Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2290987204",
                                    "name": "Hengyuan Zhang"
                                },
                                {
                                    "authorId": "2297192655",
                                    "name": "Yanru Wu"
                                },
                                {
                                    "authorId": "2346989732",
                                    "name": "Dawei Li"
                                },
                                {
                                    "authorId": "2346998771",
                                    "name": "Sak Yang"
                                },
                                {
                                    "authorId": "2287784354",
                                    "name": "Rui Zhao"
                                },
                                {
                                    "authorId": "2290854859",
                                    "name": "Yong Jiang"
                                },
                                {
                                    "authorId": "2186741338",
                                    "name": "Fei Tan"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.66357421875
                    },
                    {
                        "id": "(Li et al._2, 2023)",
                        "snippets": [
                            "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI."
                        ],
                        "paper": {
                            "corpus_id": 265294420,
                            "title": "Large Language Models in Finance: A Survey",
                            "authors": [
                                {
                                    "authorId": "1527089960",
                                    "name": "Yinheng Li"
                                },
                                {
                                    "authorId": "2267859332",
                                    "name": "Shaofei Wang"
                                },
                                {
                                    "authorId": "2267679737",
                                    "name": "Han Ding"
                                },
                                {
                                    "authorId": "2267367945",
                                    "name": "Hang Chen"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on AI in Finance",
                            "n_citations": 208
                        },
                        "score": 0
                    },
                    {
                        "id": "(Wo'zniak et al., 2024)",
                        "snippets": [
                            "Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting (French, 1999), where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- (Zhai et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 267657819,
                            "title": "Personalized Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2284064954",
                                    "name": "Stanislaw Wo'zniak"
                                },
                                {
                                    "authorId": "2208962106",
                                    "name": "Bartlomiej Koptyra"
                                },
                                {
                                    "authorId": "32559047",
                                    "name": "Arkadiusz Janz"
                                },
                                {
                                    "authorId": "1724788",
                                    "name": "Przemyslaw Kazienko"
                                },
                                {
                                    "authorId": "2284064003",
                                    "name": "Jan Koco'n"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
                            "n_citations": 20
                        },
                        "score": 0.62548828125
                    },
                    {
                        "id": "(Zhai et al., 2023)",
                        "snippets": [
                            "Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement."
                        ],
                        "paper": {
                            "corpus_id": 262055661,
                            "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models",
                            "authors": [
                                {
                                    "authorId": "119692515",
                                    "name": "Yuexiang Zhai"
                                },
                                {
                                    "authorId": "2143202419",
                                    "name": "Shengbang Tong"
                                },
                                {
                                    "authorId": "2261479326",
                                    "name": "Xiao Li"
                                },
                                {
                                    "authorId": "2053144019",
                                    "name": "Mu Cai"
                                },
                                {
                                    "authorId": "144403436",
                                    "name": "Qing Qu"
                                },
                                {
                                    "authorId": "2244031889",
                                    "name": "Yong Jae Lee"
                                },
                                {
                                    "authorId": "144987142",
                                    "name": "Y. Ma"
                                }
                            ],
                            "year": 2023,
                            "venue": "arXiv.org",
                            "n_citations": 88
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lobo et al., 2024)",
                        "snippets": [
                            "prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)",
                            "Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks."
                        ],
                        "paper": {
                            "corpus_id": 274234789,
                            "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning",
                            "authors": [
                                {
                                    "authorId": "2332098876",
                                    "name": "Elita Lobo"
                                },
                                {
                                    "authorId": "40228633",
                                    "name": "Chirag Agarwal"
                                },
                                {
                                    "authorId": "1892673",
                                    "name": "Himabindu Lakkaraju"
                                }
                            ],
                            "year": 2024,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 10
                        },
                        "score": 0.890625
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.152511
    }
}